IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 12:52:20,869	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 12:52:20,869	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 12:52:23,521	SUCC scripts.py:747 -- --------------------
2024-01-07 12:52:23,521	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 12:52:23,522	SUCC scripts.py:749 -- --------------------
2024-01-07 12:52:23,522	INFO scripts.py:751 -- Next steps
2024-01-07 12:52:23,522	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 12:52:23,522	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 12:52:23,522	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 12:52:23,522	INFO scripts.py:773 -- import ray
2024-01-07 12:52:23,522	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 12:52:23,522	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 12:52:23,522	INFO scripts.py:791 --   ray status
2024-01-07 12:52:23,522	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 12:52:23,522	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 12:52:23,522	INFO scripts.py:810 --   ray stop
2024-01-07 12:52:23,523	INFO scripts.py:891 -- --block
2024-01-07 12:52:23,523	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 12:52:23,523	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              18425690773930388249
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f6d7de0b070>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 2.00
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!

Supervised model resnet18 with random weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 12:53:06,562	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 12:53:06,589	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 12:53:25,905	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 12:53:26 (running for 00:00:18.37)
Memory usage on this node: 13.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |
| train_52b21_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_52b21_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_52b21_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=143647)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=143647)[0m Configuration completed!
[2m[36m(func pid=143647)[0m New optimizer parameters:
[2m[36m(func pid=143647)[0m SGD (
[2m[36m(func pid=143647)[0m Parameter Group 0
[2m[36m(func pid=143647)[0m     dampening: 0
[2m[36m(func pid=143647)[0m     differentiable: False
[2m[36m(func pid=143647)[0m     foreach: None
[2m[36m(func pid=143647)[0m     lr: 0.0001
[2m[36m(func pid=143647)[0m     maximize: False
[2m[36m(func pid=143647)[0m     momentum: 0.99
[2m[36m(func pid=143647)[0m     nesterov: False
[2m[36m(func pid=143647)[0m     weight_decay: 0
[2m[36m(func pid=143647)[0m )
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0733 | Steps: 4 | Val loss: 2.3312 | Batch size: 32 | lr: 0.0001 | Duration: 6.31s
[2m[36m(func pid=143647)[0m top1: 0.14412313432835822
[2m[36m(func pid=143647)[0m top5: 0.5279850746268657
[2m[36m(func pid=143647)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=143647)[0m f1_macro: 0.04164478114478115
[2m[36m(func pid=143647)[0m f1_weighted: 0.08037965161565909
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.0, 0.0, 0.255, 0.0, 0.0, 0.0, 0.162, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 12:53:41 (running for 00:00:33.69)
Memory usage on this node: 15.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |
| train_52b21_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_52b21_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144027)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144027)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=144027)[0m Configuration completed!
[2m[36m(func pid=144027)[0m New optimizer parameters:
[2m[36m(func pid=144027)[0m SGD (
[2m[36m(func pid=144027)[0m Parameter Group 0
[2m[36m(func pid=144027)[0m     dampening: 0
[2m[36m(func pid=144027)[0m     differentiable: False
[2m[36m(func pid=144027)[0m     foreach: None
[2m[36m(func pid=144027)[0m     lr: 0.001
[2m[36m(func pid=144027)[0m     maximize: False
[2m[36m(func pid=144027)[0m     momentum: 0.99
[2m[36m(func pid=144027)[0m     nesterov: False
[2m[36m(func pid=144027)[0m     weight_decay: 0
[2m[36m(func pid=144027)[0m )
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1186 | Steps: 4 | Val loss: 2.3334 | Batch size: 32 | lr: 0.001 | Duration: 5.13s
[2m[36m(func pid=144027)[0m top1: 0.09701492537313433
[2m[36m(func pid=144027)[0m top5: 0.4916044776119403
[2m[36m(func pid=144027)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=144027)[0m f1_macro: 0.04194516609525378
[2m[36m(func pid=144027)[0m f1_weighted: 0.06547428888448745
[2m[36m(func pid=144027)[0m f1_per_class: [0.034, 0.0, 0.041, 0.202, 0.0, 0.0, 0.0, 0.144, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 12:53:50 (running for 00:00:42.89)
Memory usage on this node: 18.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |
| train_52b21_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144451)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=144451)[0m Configuration completed!
[2m[36m(func pid=144451)[0m New optimizer parameters:
[2m[36m(func pid=144451)[0m SGD (
[2m[36m(func pid=144451)[0m Parameter Group 0
[2m[36m(func pid=144451)[0m     dampening: 0
[2m[36m(func pid=144451)[0m     differentiable: False
[2m[36m(func pid=144451)[0m     foreach: None
[2m[36m(func pid=144451)[0m     lr: 0.01
[2m[36m(func pid=144451)[0m     maximize: False
[2m[36m(func pid=144451)[0m     momentum: 0.99
[2m[36m(func pid=144451)[0m     nesterov: False
[2m[36m(func pid=144451)[0m     weight_decay: 0
[2m[36m(func pid=144451)[0m )
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8615 | Steps: 4 | Val loss: 2.9260 | Batch size: 32 | lr: 0.01 | Duration: 5.10s
[2m[36m(func pid=144451)[0m top1: 0.006063432835820896
[2m[36m(func pid=144451)[0m top5: 0.6222014925373134
[2m[36m(func pid=144451)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=144451)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=144451)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 12:53:58 (running for 00:00:51.04)
Memory usage on this node: 20.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 12:54:07 (running for 00:00:59.52)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  3.073 |      0.042 |                    1 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |        |            |                      |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |        |            |                      |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |        |            |                      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144880)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=144880)[0m Configuration completed!
[2m[36m(func pid=144880)[0m New optimizer parameters:
[2m[36m(func pid=144880)[0m SGD (
[2m[36m(func pid=144880)[0m Parameter Group 0
[2m[36m(func pid=144880)[0m     dampening: 0
[2m[36m(func pid=144880)[0m     differentiable: False
[2m[36m(func pid=144880)[0m     foreach: None
[2m[36m(func pid=144880)[0m     lr: 0.1
[2m[36m(func pid=144880)[0m     maximize: False
[2m[36m(func pid=144880)[0m     momentum: 0.99
[2m[36m(func pid=144880)[0m     nesterov: False
[2m[36m(func pid=144880)[0m     weight_decay: 0
[2m[36m(func pid=144880)[0m )
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0045 | Steps: 4 | Val loss: 2.3181 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7058 | Steps: 4 | Val loss: 2.3150 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.7305 | Steps: 4 | Val loss: 11.0809 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.3617 | Steps: 4 | Val loss: 867.0488 | Batch size: 32 | lr: 0.1 | Duration: 4.85s
== Status ==
Current time: 2024-01-07 12:54:12 (running for 00:01:04.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  3.073 |      0.042 |                    1 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  3.119 |      0.042 |                    1 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.861 |      0.001 |                    1 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |        |            |                      |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.19682835820895522
[2m[36m(func pid=143647)[0m top5: 0.5685634328358209
[2m[36m(func pid=143647)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=143647)[0m f1_macro: 0.040315492683701486
[2m[36m(func pid=143647)[0m f1_weighted: 0.10043975248434095
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.0, 0.0, 0.353, 0.0, 0.0, 0.0, 0.032, 0.0, 0.018]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.01912313432835821
[2m[36m(func pid=144027)[0m top5: 0.4925373134328358
[2m[36m(func pid=144027)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=144027)[0m f1_macro: 0.007526103949133757
[2m[36m(func pid=144027)[0m f1_weighted: 0.0013092187229285528
[2m[36m(func pid=144027)[0m f1_per_class: [0.059, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m top1: 0.027052238805970148
[2m[36m(func pid=144451)[0m top5: 0.5928171641791045
[2m[36m(func pid=144451)[0m f1_micro: 0.027052238805970148
[2m[36m(func pid=144451)[0m f1_macro: 0.01135306443987381
[2m[36m(func pid=144451)[0m f1_weighted: 0.013297369514855124
[2m[36m(func pid=144451)[0m f1_per_class: [0.041, 0.072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.020522388059701493
[2m[36m(func pid=144880)[0m top5: 0.49580223880597013
[2m[36m(func pid=144880)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=144880)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=144880)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=144880)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9134 | Steps: 4 | Val loss: 2.3178 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.4106 | Steps: 4 | Val loss: 7.8267 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7958 | Steps: 4 | Val loss: 2.3773 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 13.1176 | Steps: 4 | Val loss: 1428507.5000 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:54:18 (running for 00:01:10.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  3.005 |      0.04  |                    2 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.706 |      0.008 |                    2 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.411 |      0.056 |                    3 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  5.362 |      0.004 |                    1 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.07742537313432836
[2m[36m(func pid=144451)[0m top5: 0.8451492537313433
[2m[36m(func pid=144451)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=144451)[0m f1_macro: 0.05632609685416814
[2m[36m(func pid=144451)[0m f1_weighted: 0.09762574195887602
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.337, 0.022, 0.115, 0.0, 0.04, 0.0, 0.049, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.22014925373134328
[2m[36m(func pid=143647)[0m top5: 0.5634328358208955
[2m[36m(func pid=143647)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=143647)[0m f1_macro: 0.04130081300813008
[2m[36m(func pid=143647)[0m f1_weighted: 0.10782596165513893
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.0, 0.0, 0.38, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.0648320895522388
[2m[36m(func pid=144027)[0m top5: 0.5666977611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=144027)[0m f1_macro: 0.051841819882780193
[2m[36m(func pid=144027)[0m f1_weighted: 0.06935728458158431
[2m[36m(func pid=144027)[0m f1_per_class: [0.036, 0.0, 0.05, 0.0, 0.167, 0.0, 0.22, 0.0, 0.046, 0.0]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.006063432835820896
[2m[36m(func pid=144880)[0m top5: 0.5093283582089553
[2m[36m(func pid=144880)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=144880)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=144880)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.3968 | Steps: 4 | Val loss: 33.1221 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8518 | Steps: 4 | Val loss: 2.3081 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7119 | Steps: 4 | Val loss: 2.4132 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 16.7210 | Steps: 4 | Val loss: 729255.4375 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:54:23 (running for 00:01:15.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.913 |      0.041 |                    3 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.796 |      0.052 |                    3 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.397 |      0.011 |                    4 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      | 13.118 |      0.001 |                    2 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.03451492537313433
[2m[36m(func pid=144451)[0m top5: 0.4944029850746269
[2m[36m(func pid=144451)[0m f1_micro: 0.03451492537313433
[2m[36m(func pid=144451)[0m f1_macro: 0.011494174166985227
[2m[36m(func pid=144451)[0m f1_weighted: 0.007155045979500893
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.071, 0.029]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.23460820895522388
[2m[36m(func pid=143647)[0m top5: 0.5569029850746269
[2m[36m(func pid=143647)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=143647)[0m f1_macro: 0.05610123753062879
[2m[36m(func pid=143647)[0m f1_weighted: 0.11999784085213414
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.049, 0.105, 0.395, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.22061567164179105
[2m[36m(func pid=144027)[0m top5: 0.6702425373134329
[2m[36m(func pid=144027)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=144027)[0m f1_macro: 0.07799484407128143
[2m[36m(func pid=144027)[0m f1_weighted: 0.17335476099240157
[2m[36m(func pid=144027)[0m f1_per_class: [0.071, 0.0, 0.0, 0.0, 0.123, 0.0, 0.572, 0.0, 0.013, 0.0]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.2980410447761194
[2m[36m(func pid=144880)[0m top5: 0.5149253731343284
[2m[36m(func pid=144880)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=144880)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 4.3298 | Steps: 4 | Val loss: 32.3455 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8004 | Steps: 4 | Val loss: 2.2894 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8667 | Steps: 4 | Val loss: 2.2731 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 13.8467 | Steps: 4 | Val loss: 123165.2734 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:54:28 (running for 00:01:21.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.852 |      0.056 |                    4 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.712 |      0.078 |                    4 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  4.33  |      0.036 |                    5 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      | 16.721 |      0.046 |                    3 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.032182835820895525
[2m[36m(func pid=144451)[0m top5: 0.4239738805970149
[2m[36m(func pid=144451)[0m f1_micro: 0.032182835820895525
[2m[36m(func pid=144451)[0m f1_macro: 0.03584274167382554
[2m[36m(func pid=144451)[0m f1_weighted: 0.02918819920105284
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.037, 0.044, 0.0, 0.182, 0.0, 0.07, 0.0, 0.0, 0.025]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.19962686567164178
[2m[36m(func pid=143647)[0m top5: 0.5867537313432836
[2m[36m(func pid=143647)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=143647)[0m f1_macro: 0.05239358929391762
[2m[36m(func pid=143647)[0m f1_weighted: 0.10845204802790463
[2m[36m(func pid=143647)[0m f1_per_class: [0.048, 0.054, 0.072, 0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.26865671641791045
[2m[36m(func pid=144027)[0m top5: 0.7919776119402985
[2m[36m(func pid=144027)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=144027)[0m f1_macro: 0.10758855705720735
[2m[36m(func pid=144027)[0m f1_weighted: 0.2345578314807688
[2m[36m(func pid=144027)[0m f1_per_class: [0.093, 0.319, 0.0, 0.0, 0.069, 0.0, 0.595, 0.0, 0.0, 0.0]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.03311567164179104
[2m[36m(func pid=144880)[0m top5: 0.5149253731343284
[2m[36m(func pid=144880)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=144880)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=144880)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.4587 | Steps: 4 | Val loss: 23.5579 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7677 | Steps: 4 | Val loss: 2.2681 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3090 | Steps: 4 | Val loss: 2.0134 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 4.0486 | Steps: 4 | Val loss: 1812093.6250 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=144451)[0m top1: 0.052705223880597014
[2m[36m(func pid=144451)[0m top5: 0.6791044776119403
[2m[36m(func pid=144451)[0m f1_micro: 0.05270522388059702
[2m[36m(func pid=144451)[0m f1_macro: 0.04455414549462212
[2m[36m(func pid=144451)[0m f1_weighted: 0.05723188768281409
[2m[36m(func pid=144451)[0m f1_per_class: [0.044, 0.197, 0.0, 0.047, 0.087, 0.068, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
== Status ==
Current time: 2024-01-07 12:54:34 (running for 00:01:26.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.8   |      0.052 |                    5 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.867 |      0.108 |                    5 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.459 |      0.045 |                    6 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      | 13.847 |      0.006 |                    4 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.1455223880597015
[2m[36m(func pid=143647)[0m top5: 0.597481343283582
[2m[36m(func pid=143647)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=143647)[0m f1_macro: 0.05107915279375456
[2m[36m(func pid=143647)[0m f1_weighted: 0.10260852734704437
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.136, 0.042, 0.27, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.30597014925373134
[2m[36m(func pid=144027)[0m top5: 0.8754664179104478
[2m[36m(func pid=144027)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=144027)[0m f1_macro: 0.14638613711988485
[2m[36m(func pid=144027)[0m f1_weighted: 0.2380899646844833
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.355, 0.423, 0.0, 0.066, 0.0, 0.579, 0.0, 0.04, 0.0]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.2980410447761194
[2m[36m(func pid=144880)[0m top5: 0.5149253731343284
[2m[36m(func pid=144880)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=144880)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.8768 | Steps: 4 | Val loss: 63.6590 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7036 | Steps: 4 | Val loss: 2.2513 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.2022 | Steps: 4 | Val loss: 2.0238 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9795 | Steps: 4 | Val loss: 197394.9844 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=144451)[0m top1: 0.08162313432835822
[2m[36m(func pid=144451)[0m top5: 0.45988805970149255
[2m[36m(func pid=144451)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=144451)[0m f1_macro: 0.052677510852823704
[2m[36m(func pid=144451)[0m f1_weighted: 0.08086307307505365
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.246, 0.0, 0.127, 0.083, 0.0, 0.0, 0.0, 0.07, 0.0]
== Status ==
Current time: 2024-01-07 12:54:39 (running for 00:01:31.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.768 |      0.051 |                    6 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.309 |      0.146 |                    6 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.877 |      0.053 |                    7 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  4.049 |      0.046 |                    5 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.11194029850746269
[2m[36m(func pid=143647)[0m top5: 0.6455223880597015
[2m[36m(func pid=143647)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=143647)[0m f1_macro: 0.07846881406558363
[2m[36m(func pid=143647)[0m f1_weighted: 0.10484642322282933
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.272, 0.026, 0.125, 0.0, 0.036, 0.0, 0.325, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.26259328358208955
[2m[36m(func pid=144027)[0m top5: 0.847481343283582
[2m[36m(func pid=144027)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=144027)[0m f1_macro: 0.18795524206436925
[2m[36m(func pid=144027)[0m f1_weighted: 0.20364816404657873
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.384, 0.571, 0.0, 0.075, 0.008, 0.373, 0.307, 0.084, 0.077]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.2980410447761194
[2m[36m(func pid=144880)[0m top5: 0.5149253731343284
[2m[36m(func pid=144880)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=144880)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7590 | Steps: 4 | Val loss: 2.2432 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6593 | Steps: 4 | Val loss: 68.3615 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2465 | Steps: 4 | Val loss: 2.0683 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 5.8512 | Steps: 4 | Val loss: 45158.0391 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:54:44 (running for 00:01:37.22)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.704 |      0.078 |                    7 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.202 |      0.188 |                    7 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.659 |      0.047 |                    8 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.979 |      0.046 |                    6 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.12220149253731344
[2m[36m(func pid=143647)[0m top5: 0.7196828358208955
[2m[36m(func pid=143647)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=143647)[0m f1_macro: 0.0836444862237786
[2m[36m(func pid=143647)[0m f1_weighted: 0.11775711491177579
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.376, 0.021, 0.055, 0.0, 0.157, 0.027, 0.201, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.054104477611940295
[2m[36m(func pid=144451)[0m top5: 0.48274253731343286
[2m[36m(func pid=144451)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=144451)[0m f1_macro: 0.047037070666543584
[2m[36m(func pid=144451)[0m f1_weighted: 0.06978593376291033
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.126, 0.043, 0.096, 0.073, 0.046, 0.048, 0.011, 0.0, 0.027]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1865671641791045
[2m[36m(func pid=144027)[0m top5: 0.8348880597014925
[2m[36m(func pid=144027)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=144027)[0m f1_macro: 0.1338921759730471
[2m[36m(func pid=144027)[0m f1_weighted: 0.10431959187885122
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.383, 0.16, 0.003, 0.08, 0.024, 0.034, 0.269, 0.143, 0.242]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.29617537313432835
[2m[36m(func pid=144880)[0m top5: 0.5167910447761194
[2m[36m(func pid=144880)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=144880)[0m f1_macro: 0.04621542940320232
[2m[36m(func pid=144880)[0m f1_weighted: 0.13774094864107408
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.462, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6301 | Steps: 4 | Val loss: 2.2289 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.7975 | Steps: 4 | Val loss: 58.5352 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3943 | Steps: 4 | Val loss: 2.3994 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:54:50 (running for 00:01:42.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.759 |      0.084 |                    8 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.247 |      0.134 |                    8 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.797 |      0.032 |                    9 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  5.851 |      0.046 |                    7 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.17537313432835822
[2m[36m(func pid=143647)[0m top5: 0.8022388059701493
[2m[36m(func pid=143647)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=143647)[0m f1_macro: 0.10102856424142463
[2m[36m(func pid=143647)[0m f1_weighted: 0.1833814748027129
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.411, 0.024, 0.073, 0.0, 0.292, 0.194, 0.016, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.048507462686567165
[2m[36m(func pid=144451)[0m top5: 0.6156716417910447
[2m[36m(func pid=144451)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=144451)[0m f1_macro: 0.03232049069551938
[2m[36m(func pid=144451)[0m f1_weighted: 0.05718792468420938
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.1, 0.0, 0.038, 0.0, 0.0, 0.083, 0.077, 0.0, 0.026]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.1027 | Steps: 4 | Val loss: 29410.8555 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=144027)[0m top1: 0.2224813432835821
[2m[36m(func pid=144027)[0m top5: 0.8390858208955224
[2m[36m(func pid=144027)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=144027)[0m f1_macro: 0.15465361473935693
[2m[36m(func pid=144027)[0m f1_weighted: 0.21311963155046004
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.451, 0.065, 0.344, 0.097, 0.063, 0.037, 0.264, 0.123, 0.104]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.05783582089552239
[2m[36m(func pid=144880)[0m top5: 0.5237873134328358
[2m[36m(func pid=144880)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=144880)[0m f1_macro: 0.010992907801418438
[2m[36m(func pid=144880)[0m f1_weighted: 0.006357838467238276
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5563 | Steps: 4 | Val loss: 2.2021 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6565 | Steps: 4 | Val loss: 16.6128 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9994 | Steps: 4 | Val loss: 2.1859 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:54:55 (running for 00:01:47.99)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.63  |      0.101 |                    9 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.394 |      0.155 |                    9 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.656 |      0.081 |                   10 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  4.103 |      0.011 |                    8 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.2667910447761194
[2m[36m(func pid=143647)[0m top5: 0.8763992537313433
[2m[36m(func pid=143647)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.14799679886566003
[2m[36m(func pid=143647)[0m f1_weighted: 0.289427506736778
[2m[36m(func pid=143647)[0m f1_per_class: [0.024, 0.476, 0.035, 0.099, 0.0, 0.376, 0.454, 0.016, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.11707089552238806
[2m[36m(func pid=144451)[0m top5: 0.784981343283582
[2m[36m(func pid=144451)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=144451)[0m f1_macro: 0.0806370941833288
[2m[36m(func pid=144451)[0m f1_weighted: 0.08778638964740874
[2m[36m(func pid=144451)[0m f1_per_class: [0.064, 0.379, 0.0, 0.013, 0.122, 0.062, 0.0, 0.167, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.2149 | Steps: 4 | Val loss: 9253.6104 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=144027)[0m top1: 0.2490671641791045
[2m[36m(func pid=144027)[0m top5: 0.8740671641791045
[2m[36m(func pid=144027)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=144027)[0m f1_macro: 0.1546481146583679
[2m[36m(func pid=144027)[0m f1_weighted: 0.23052230015571357
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.202, 0.082, 0.489, 0.184, 0.111, 0.092, 0.256, 0.04, 0.09]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.05783582089552239
[2m[36m(func pid=144880)[0m top5: 0.5578358208955224
[2m[36m(func pid=144880)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=144880)[0m f1_macro: 0.01133973479652492
[2m[36m(func pid=144880)[0m f1_weighted: 0.006558428706945383
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8480 | Steps: 4 | Val loss: 13.1859 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7120 | Steps: 4 | Val loss: 2.1935 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3081 | Steps: 4 | Val loss: 2.1503 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:55:01 (running for 00:01:53.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.556 |      0.148 |                   10 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.999 |      0.155 |                   10 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.848 |      0.083 |                   11 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.215 |      0.011 |                    9 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.12033582089552239
[2m[36m(func pid=144451)[0m top5: 0.8059701492537313
[2m[36m(func pid=144451)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=144451)[0m f1_macro: 0.08261889529937248
[2m[36m(func pid=144451)[0m f1_weighted: 0.10051991600936915
[2m[36m(func pid=144451)[0m f1_per_class: [0.086, 0.192, 0.0, 0.154, 0.103, 0.091, 0.0, 0.201, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.283115671641791
[2m[36m(func pid=143647)[0m top5: 0.8978544776119403
[2m[36m(func pid=143647)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=143647)[0m f1_macro: 0.14633835284901736
[2m[36m(func pid=143647)[0m f1_weighted: 0.2884153550396588
[2m[36m(func pid=143647)[0m f1_per_class: [0.038, 0.403, 0.053, 0.054, 0.0, 0.38, 0.535, 0.0, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.2543 | Steps: 4 | Val loss: 4079.5137 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=144027)[0m top1: 0.24253731343283583
[2m[36m(func pid=144027)[0m top5: 0.8810634328358209
[2m[36m(func pid=144027)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=144027)[0m f1_macro: 0.16998672965660122
[2m[36m(func pid=144027)[0m f1_weighted: 0.2360806171328741
[2m[36m(func pid=144027)[0m f1_per_class: [0.185, 0.115, 0.142, 0.463, 0.124, 0.182, 0.155, 0.227, 0.0, 0.106]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.4480 | Steps: 4 | Val loss: 2.1292 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6015 | Steps: 4 | Val loss: 7.6193 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=144880)[0m top1: 0.058768656716417914
[2m[36m(func pid=144880)[0m top5: 0.5970149253731343
[2m[36m(func pid=144880)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=144880)[0m f1_macro: 0.013858232752440885
[2m[36m(func pid=144880)[0m f1_weighted: 0.008573727772154443
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.128, 0.008, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9318 | Steps: 4 | Val loss: 2.2106 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:55:06 (running for 00:01:58.75)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.448 |      0.132 |                   12 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.308 |      0.17  |                   11 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.848 |      0.083 |                   11 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.254 |      0.014 |                   10 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.283115671641791
[2m[36m(func pid=143647)[0m top5: 0.9095149253731343
[2m[36m(func pid=143647)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=143647)[0m f1_macro: 0.13200606618967284
[2m[36m(func pid=143647)[0m f1_weighted: 0.24594187828105518
[2m[36m(func pid=143647)[0m f1_per_class: [0.065, 0.258, 0.142, 0.017, 0.0, 0.3, 0.539, 0.0, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.1707089552238806
[2m[36m(func pid=144451)[0m top5: 0.8157649253731343
[2m[36m(func pid=144451)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=144451)[0m f1_macro: 0.1312588901984314
[2m[36m(func pid=144451)[0m f1_weighted: 0.17236642968993335
[2m[36m(func pid=144451)[0m f1_per_class: [0.093, 0.317, 0.074, 0.273, 0.083, 0.218, 0.0, 0.212, 0.043, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2294776119402985
[2m[36m(func pid=144027)[0m top5: 0.8586753731343284
[2m[36m(func pid=144027)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=144027)[0m f1_macro: 0.18438456378064585
[2m[36m(func pid=144027)[0m f1_weighted: 0.2637522605728336
[2m[36m(func pid=144027)[0m f1_per_class: [0.14, 0.178, 0.343, 0.352, 0.146, 0.123, 0.356, 0.132, 0.0, 0.072]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.3520 | Steps: 4 | Val loss: 1486.0310 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4863 | Steps: 4 | Val loss: 2.0881 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.6301 | Steps: 4 | Val loss: 6.2577 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=144880)[0m top1: 0.12453358208955224
[2m[36m(func pid=144880)[0m top5: 0.6403917910447762
[2m[36m(func pid=144880)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=144880)[0m f1_macro: 0.03905843461506944
[2m[36m(func pid=144880)[0m f1_weighted: 0.09901028330039881
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.324, 0.008, 0.058, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.6882 | Steps: 4 | Val loss: 2.0363 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:55:11 (running for 00:02:04.16)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.486 |      0.144 |                   13 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.932 |      0.184 |                   12 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.601 |      0.131 |                   12 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.352 |      0.039 |                   11 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.29151119402985076
[2m[36m(func pid=143647)[0m top5: 0.8969216417910447
[2m[36m(func pid=143647)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=143647)[0m f1_macro: 0.14375701845769068
[2m[36m(func pid=143647)[0m f1_weighted: 0.21674704143745505
[2m[36m(func pid=143647)[0m f1_per_class: [0.109, 0.101, 0.293, 0.003, 0.1, 0.294, 0.537, 0.0, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.12406716417910447
[2m[36m(func pid=144451)[0m top5: 0.7014925373134329
[2m[36m(func pid=144451)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=144451)[0m f1_macro: 0.09773852737923536
[2m[36m(func pid=144451)[0m f1_weighted: 0.09784323542699062
[2m[36m(func pid=144451)[0m f1_per_class: [0.0, 0.441, 0.0, 0.0, 0.174, 0.051, 0.0, 0.202, 0.086, 0.023]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.18423507462686567
[2m[36m(func pid=144027)[0m top5: 0.8451492537313433
[2m[36m(func pid=144027)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=144027)[0m f1_macro: 0.2274644089362055
[2m[36m(func pid=144027)[0m f1_weighted: 0.20961250576207455
[2m[36m(func pid=144027)[0m f1_per_class: [0.254, 0.364, 0.774, 0.156, 0.102, 0.139, 0.216, 0.188, 0.031, 0.051]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.0849 | Steps: 4 | Val loss: 439.4785 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5732 | Steps: 4 | Val loss: 2.0585 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7439 | Steps: 4 | Val loss: 7.6802 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=144880)[0m top1: 0.27005597014925375
[2m[36m(func pid=144880)[0m top5: 0.6930970149253731
[2m[36m(func pid=144880)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=144880)[0m f1_macro: 0.05203558829364224
[2m[36m(func pid=144880)[0m f1_weighted: 0.15238314701988817
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.5, 0.01, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.6984 | Steps: 4 | Val loss: 2.3389 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:55:17 (running for 00:02:09.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.573 |      0.163 |                   14 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.688 |      0.227 |                   13 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  4.63  |      0.098 |                   13 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.085 |      0.052 |                   12 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3003731343283582
[2m[36m(func pid=143647)[0m top5: 0.8847947761194029
[2m[36m(func pid=143647)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=143647)[0m f1_macro: 0.16328601567619364
[2m[36m(func pid=143647)[0m f1_weighted: 0.21768143157959113
[2m[36m(func pid=143647)[0m f1_per_class: [0.12, 0.082, 0.486, 0.003, 0.083, 0.321, 0.537, 0.0, 0.0, 0.0]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.12686567164179105
[2m[36m(func pid=144451)[0m top5: 0.5732276119402985
[2m[36m(func pid=144451)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=144451)[0m f1_macro: 0.07720120006421825
[2m[36m(func pid=144451)[0m f1_weighted: 0.09317989127761477
[2m[36m(func pid=144451)[0m f1_per_class: [0.039, 0.503, 0.0, 0.0, 0.119, 0.0, 0.0, 0.052, 0.059, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1501865671641791
[2m[36m(func pid=144027)[0m top5: 0.8166977611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=144027)[0m f1_macro: 0.18667813967063784
[2m[36m(func pid=144027)[0m f1_weighted: 0.1252872734605202
[2m[36m(func pid=144027)[0m f1_per_class: [0.043, 0.381, 0.786, 0.026, 0.102, 0.127, 0.05, 0.244, 0.046, 0.061]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.4887 | Steps: 4 | Val loss: 700.1094 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3968 | Steps: 4 | Val loss: 2.0268 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6985 | Steps: 4 | Val loss: 7.4613 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=144880)[0m top1: 0.020522388059701493
[2m[36m(func pid=144880)[0m top5: 0.6972947761194029
[2m[36m(func pid=144880)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=144880)[0m f1_macro: 0.008648531493122712
[2m[36m(func pid=144880)[0m f1_weighted: 0.022264396965318504
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.012, 0.003, 0.0, 0.0, 0.071, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.5997 | Steps: 4 | Val loss: 2.6320 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:55:22 (running for 00:02:15.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.397 |      0.178 |                   15 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.698 |      0.187 |                   14 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.744 |      0.077 |                   14 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.489 |      0.009 |                   13 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3087686567164179
[2m[36m(func pid=143647)[0m top5: 0.8754664179104478
[2m[36m(func pid=143647)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=143647)[0m f1_macro: 0.1781717180249947
[2m[36m(func pid=143647)[0m f1_weighted: 0.21511306596054852
[2m[36m(func pid=143647)[0m f1_per_class: [0.135, 0.047, 0.516, 0.003, 0.154, 0.299, 0.55, 0.0, 0.0, 0.077]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.10494402985074627
[2m[36m(func pid=144451)[0m top5: 0.5158582089552238
[2m[36m(func pid=144451)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=144451)[0m f1_macro: 0.06541815646804147
[2m[36m(func pid=144451)[0m f1_weighted: 0.08162152637003618
[2m[36m(func pid=144451)[0m f1_per_class: [0.076, 0.435, 0.0, 0.0, 0.067, 0.0, 0.003, 0.053, 0.02, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.16884328358208955
[2m[36m(func pid=144027)[0m top5: 0.7957089552238806
[2m[36m(func pid=144027)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=144027)[0m f1_macro: 0.15142873047740538
[2m[36m(func pid=144027)[0m f1_weighted: 0.12736122886175336
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.43, 0.329, 0.02, 0.119, 0.231, 0.006, 0.236, 0.061, 0.082]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8975 | Steps: 4 | Val loss: 737.4868 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4303 | Steps: 4 | Val loss: 1.9964 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 4.9470 | Steps: 4 | Val loss: 5.6448 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8893 | Steps: 4 | Val loss: 2.5128 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=144880)[0m top1: 0.01632462686567164
[2m[36m(func pid=144880)[0m top5: 0.7080223880597015
[2m[36m(func pid=144880)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=144880)[0m f1_macro: 0.03019592647679158
[2m[36m(func pid=144880)[0m f1_weighted: 0.008113642186095274
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.002, 0.01, 0.24, 0.0, 0.003, 0.047, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:55:28 (running for 00:02:20.55)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.43  |      0.179 |                   16 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.6   |      0.151 |                   15 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.699 |      0.065 |                   15 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.898 |      0.03  |                   14 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3087686567164179
[2m[36m(func pid=143647)[0m top5: 0.863339552238806
[2m[36m(func pid=143647)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=143647)[0m f1_macro: 0.17865646445750075
[2m[36m(func pid=143647)[0m f1_weighted: 0.2127551905877837
[2m[36m(func pid=143647)[0m f1_per_class: [0.145, 0.053, 0.516, 0.007, 0.178, 0.263, 0.548, 0.0, 0.0, 0.077]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.17397388059701493
[2m[36m(func pid=144451)[0m top5: 0.6380597014925373
[2m[36m(func pid=144451)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=144451)[0m f1_macro: 0.10331429177198188
[2m[36m(func pid=144451)[0m f1_weighted: 0.1628703850690122
[2m[36m(func pid=144451)[0m f1_per_class: [0.075, 0.524, 0.0, 0.0, 0.062, 0.0, 0.205, 0.168, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.197294776119403
[2m[36m(func pid=144027)[0m top5: 0.8120335820895522
[2m[36m(func pid=144027)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=144027)[0m f1_macro: 0.1790407031039656
[2m[36m(func pid=144027)[0m f1_weighted: 0.16197501917573215
[2m[36m(func pid=144027)[0m f1_per_class: [0.122, 0.447, 0.185, 0.083, 0.161, 0.306, 0.009, 0.24, 0.131, 0.107]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.6383 | Steps: 4 | Val loss: 1158.9906 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4702 | Steps: 4 | Val loss: 1.9765 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5092 | Steps: 4 | Val loss: 3.7269 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9815 | Steps: 4 | Val loss: 2.3014 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=144880)[0m top1: 0.061567164179104475
[2m[36m(func pid=144880)[0m top5: 0.7094216417910447
[2m[36m(func pid=144880)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=144880)[0m f1_macro: 0.025181792375942418
[2m[36m(func pid=144880)[0m f1_weighted: 0.012175406970655304
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.007, 0.007, 0.078, 0.008, 0.0, 0.152, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:55:33 (running for 00:02:26.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.47  |      0.177 |                   17 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.889 |      0.179 |                   16 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  4.947 |      0.103 |                   16 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.638 |      0.025 |                   15 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3101679104477612
[2m[36m(func pid=143647)[0m top5: 0.8502798507462687
[2m[36m(func pid=143647)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=143647)[0m f1_macro: 0.17694863839907596
[2m[36m(func pid=143647)[0m f1_weighted: 0.21496976575701413
[2m[36m(func pid=143647)[0m f1_per_class: [0.155, 0.063, 0.486, 0.003, 0.163, 0.271, 0.55, 0.0, 0.0, 0.077]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2294776119402985
[2m[36m(func pid=144027)[0m top5: 0.8381529850746269
[2m[36m(func pid=144027)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=144027)[0m f1_macro: 0.2833644477097092
[2m[36m(func pid=144027)[0m f1_weighted: 0.19070653380912464
[2m[36m(func pid=144027)[0m f1_per_class: [0.35, 0.487, 0.828, 0.117, 0.219, 0.297, 0.018, 0.253, 0.157, 0.108]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m top1: 0.25279850746268656
[2m[36m(func pid=144451)[0m top5: 0.664179104477612
[2m[36m(func pid=144451)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=144451)[0m f1_macro: 0.1464507379196977
[2m[36m(func pid=144451)[0m f1_weighted: 0.23210152408829512
[2m[36m(func pid=144451)[0m f1_per_class: [0.074, 0.513, 0.0, 0.0, 0.0, 0.12, 0.361, 0.357, 0.0, 0.04]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.4146 | Steps: 4 | Val loss: 1864.7212 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4078 | Steps: 4 | Val loss: 1.9634 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2548 | Steps: 4 | Val loss: 2.0918 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4849 | Steps: 4 | Val loss: 3.5174 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=144880)[0m top1: 0.008861940298507462
[2m[36m(func pid=144880)[0m top5: 0.6986940298507462
[2m[36m(func pid=144880)[0m f1_micro: 0.008861940298507462
[2m[36m(func pid=144880)[0m f1_macro: 0.009203973831672225
[2m[36m(func pid=144880)[0m f1_weighted: 0.0020725335448580615
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.007, 0.0, 0.057, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:55:39 (running for 00:02:31.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.47  |      0.177 |                   17 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.981 |      0.283 |                   17 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.485 |      0.112 |                   18 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.415 |      0.009 |                   16 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.17490671641791045
[2m[36m(func pid=144451)[0m top5: 0.6749067164179104
[2m[36m(func pid=144451)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=144451)[0m f1_macro: 0.11151487224838488
[2m[36m(func pid=144451)[0m f1_weighted: 0.13338050990156775
[2m[36m(func pid=144451)[0m f1_per_class: [0.068, 0.478, 0.0, 0.0, 0.0, 0.237, 0.018, 0.294, 0.0, 0.02]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.32882462686567165
[2m[36m(func pid=143647)[0m top5: 0.8675373134328358
[2m[36m(func pid=143647)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=143647)[0m f1_macro: 0.19202703520226247
[2m[36m(func pid=143647)[0m f1_weighted: 0.2564417033050571
[2m[36m(func pid=143647)[0m f1_per_class: [0.168, 0.24, 0.377, 0.016, 0.167, 0.321, 0.557, 0.0, 0.0, 0.074]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.271455223880597
[2m[36m(func pid=144027)[0m top5: 0.8647388059701493
[2m[36m(func pid=144027)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=144027)[0m f1_macro: 0.284266248689141
[2m[36m(func pid=144027)[0m f1_weighted: 0.22237096881385215
[2m[36m(func pid=144027)[0m f1_per_class: [0.404, 0.5, 0.632, 0.208, 0.235, 0.354, 0.009, 0.292, 0.086, 0.122]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.1077 | Steps: 4 | Val loss: 1885.9772 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.9239 | Steps: 4 | Val loss: 3.1905 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2418 | Steps: 4 | Val loss: 1.9287 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7112 | Steps: 4 | Val loss: 2.2756 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=144880)[0m top1: 0.0065298507462686565
[2m[36m(func pid=144880)[0m top5: 0.722481343283582
[2m[36m(func pid=144880)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=144880)[0m f1_macro: 0.009645926426069101
[2m[36m(func pid=144880)[0m f1_weighted: 0.0018734437988320494
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.008, 0.003, 0.08, 0.0, 0.0, 0.005, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:55:44 (running for 00:02:36.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.408 |      0.192 |                   18 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.255 |      0.284 |                   18 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.924 |      0.103 |                   19 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.108 |      0.01  |                   17 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.16511194029850745
[2m[36m(func pid=144451)[0m top5: 0.7611940298507462
[2m[36m(func pid=144451)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=144451)[0m f1_macro: 0.10293334032538246
[2m[36m(func pid=144451)[0m f1_weighted: 0.1175202303330255
[2m[36m(func pid=144451)[0m f1_per_class: [0.042, 0.401, 0.0, 0.0, 0.0, 0.262, 0.0, 0.303, 0.0, 0.022]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3493470149253731
[2m[36m(func pid=143647)[0m top5: 0.8847947761194029
[2m[36m(func pid=143647)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=143647)[0m f1_macro: 0.21668718972376627
[2m[36m(func pid=143647)[0m f1_weighted: 0.2902620611008755
[2m[36m(func pid=143647)[0m f1_per_class: [0.223, 0.349, 0.4, 0.029, 0.14, 0.355, 0.576, 0.0, 0.03, 0.065]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.25513059701492535
[2m[36m(func pid=144027)[0m top5: 0.8791977611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=144027)[0m f1_macro: 0.19284242546601216
[2m[36m(func pid=144027)[0m f1_weighted: 0.21351112439195546
[2m[36m(func pid=144027)[0m f1_per_class: [0.314, 0.453, 0.0, 0.295, 0.119, 0.236, 0.003, 0.254, 0.025, 0.23]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.0485 | Steps: 4 | Val loss: 1642.1855 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9687 | Steps: 4 | Val loss: 3.0810 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3848 | Steps: 4 | Val loss: 1.9109 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6595 | Steps: 4 | Val loss: 2.6614 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=144880)[0m top1: 0.012126865671641791
[2m[36m(func pid=144880)[0m top5: 0.7444029850746269
[2m[36m(func pid=144880)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=144880)[0m f1_macro: 0.009822773838248514
[2m[36m(func pid=144880)[0m f1_weighted: 0.00811323546460483
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.013, 0.026, 0.051, 0.0, 0.0, 0.008, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:55:49 (running for 00:02:42.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.242 |      0.217 |                   19 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.711 |      0.193 |                   19 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.969 |      0.144 |                   20 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.049 |      0.01  |                   18 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2523320895522388
[2m[36m(func pid=144451)[0m top5: 0.8213619402985075
[2m[36m(func pid=144451)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=144451)[0m f1_macro: 0.14406842269200015
[2m[36m(func pid=144451)[0m f1_weighted: 0.24709020822039138
[2m[36m(func pid=144451)[0m f1_per_class: [0.044, 0.345, 0.0, 0.364, 0.0, 0.32, 0.114, 0.254, 0.0, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3474813432835821
[2m[36m(func pid=143647)[0m top5: 0.8759328358208955
[2m[36m(func pid=143647)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=143647)[0m f1_macro: 0.25597574308784554
[2m[36m(func pid=143647)[0m f1_weighted: 0.3006199885488218
[2m[36m(func pid=143647)[0m f1_per_class: [0.287, 0.358, 0.471, 0.035, 0.144, 0.378, 0.576, 0.0, 0.041, 0.27]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.22574626865671643
[2m[36m(func pid=144027)[0m top5: 0.8675373134328358
[2m[36m(func pid=144027)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=144027)[0m f1_macro: 0.1668921825017619
[2m[36m(func pid=144027)[0m f1_weighted: 0.20776851589776463
[2m[36m(func pid=144027)[0m f1_per_class: [0.229, 0.275, 0.0, 0.396, 0.069, 0.181, 0.021, 0.254, 0.0, 0.242]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.0777 | Steps: 4 | Val loss: 1360.6577 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.1822 | Steps: 4 | Val loss: 2.7696 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2645 | Steps: 4 | Val loss: 3.7504 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=144880)[0m top1: 0.02751865671641791
[2m[36m(func pid=144880)[0m top5: 0.7803171641791045
[2m[36m(func pid=144880)[0m f1_micro: 0.02751865671641791
[2m[36m(func pid=144880)[0m f1_macro: 0.016079708209453668
[2m[36m(func pid=144880)[0m f1_weighted: 0.031168157246709422
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.016, 0.11, 0.034, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2411 | Steps: 4 | Val loss: 1.9049 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 12:55:55 (running for 00:02:47.48)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.385 |      0.256 |                   20 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.66  |      0.167 |                   20 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.182 |      0.191 |                   21 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.078 |      0.016 |                   19 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3125
[2m[36m(func pid=144451)[0m top5: 0.8493470149253731
[2m[36m(func pid=144451)[0m f1_micro: 0.3125
[2m[36m(func pid=144451)[0m f1_macro: 0.19084890637798835
[2m[36m(func pid=144451)[0m f1_weighted: 0.2939284886619874
[2m[36m(func pid=144451)[0m f1_per_class: [0.06, 0.297, 0.19, 0.465, 0.154, 0.0, 0.289, 0.334, 0.118, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.17583955223880596
[2m[36m(func pid=144027)[0m top5: 0.8605410447761194
[2m[36m(func pid=144027)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=144027)[0m f1_macro: 0.12572735465676693
[2m[36m(func pid=144027)[0m f1_weighted: 0.15304492304352266
[2m[36m(func pid=144027)[0m f1_per_class: [0.127, 0.016, 0.0, 0.33, 0.085, 0.228, 0.036, 0.274, 0.026, 0.135]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m top1: 0.35027985074626866
[2m[36m(func pid=143647)[0m top5: 0.875
[2m[36m(func pid=143647)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=143647)[0m f1_macro: 0.26703536875953404
[2m[36m(func pid=143647)[0m f1_weighted: 0.3147805176645309
[2m[36m(func pid=143647)[0m f1_per_class: [0.31, 0.397, 0.48, 0.039, 0.131, 0.41, 0.581, 0.0, 0.078, 0.244]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.5268 | Steps: 4 | Val loss: 966.8936 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.4132 | Steps: 4 | Val loss: 2.4888 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3259 | Steps: 4 | Val loss: 5.6301 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1522 | Steps: 4 | Val loss: 1.9013 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=144880)[0m top1: 0.03684701492537314
[2m[36m(func pid=144880)[0m top5: 0.8330223880597015
[2m[36m(func pid=144880)[0m f1_micro: 0.03684701492537314
[2m[36m(func pid=144880)[0m f1_macro: 0.03361112814962315
[2m[36m(func pid=144880)[0m f1_weighted: 0.048440706957519786
[2m[36m(func pid=144880)[0m f1_per_class: [0.035, 0.109, 0.007, 0.052, 0.056, 0.038, 0.031, 0.008, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:56:00 (running for 00:02:52.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.241 |      0.267 |                   21 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.264 |      0.126 |                   21 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  4.413 |      0.201 |                   22 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.527 |      0.034 |                   20 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.29850746268656714
[2m[36m(func pid=144451)[0m top5: 0.8288246268656716
[2m[36m(func pid=144451)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=144451)[0m f1_macro: 0.20113110690114072
[2m[36m(func pid=144451)[0m f1_weighted: 0.2829335657290181
[2m[36m(func pid=144451)[0m f1_per_class: [0.114, 0.01, 0.312, 0.422, 0.132, 0.0, 0.424, 0.497, 0.1, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.17863805970149255
[2m[36m(func pid=144027)[0m top5: 0.7509328358208955
[2m[36m(func pid=144027)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=144027)[0m f1_macro: 0.14610314392446672
[2m[36m(func pid=144027)[0m f1_weighted: 0.1519201710600049
[2m[36m(func pid=144027)[0m f1_per_class: [0.104, 0.005, 0.0, 0.24, 0.101, 0.377, 0.049, 0.329, 0.061, 0.194]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m top1: 0.32369402985074625
[2m[36m(func pid=143647)[0m top5: 0.8652052238805971
[2m[36m(func pid=143647)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=143647)[0m f1_macro: 0.2815293696546071
[2m[36m(func pid=143647)[0m f1_weighted: 0.2955815457127195
[2m[36m(func pid=143647)[0m f1_per_class: [0.29, 0.367, 0.55, 0.023, 0.131, 0.394, 0.529, 0.102, 0.107, 0.323]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.2594 | Steps: 4 | Val loss: 898.3617 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0486 | Steps: 4 | Val loss: 3.2006 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.7128 | Steps: 4 | Val loss: 5.9403 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1727 | Steps: 4 | Val loss: 1.9115 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m top1: 0.05690298507462686
[2m[36m(func pid=144880)[0m top5: 0.8055037313432836
[2m[36m(func pid=144880)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=144880)[0m f1_macro: 0.04065413423502008
[2m[36m(func pid=144880)[0m f1_weighted: 0.04640976631021962
[2m[36m(func pid=144880)[0m f1_per_class: [0.022, 0.156, 0.02, 0.003, 0.0, 0.081, 0.009, 0.094, 0.021, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:56:05 (running for 00:02:58.20)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.152 |      0.282 |                   22 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.326 |      0.146 |                   22 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.049 |      0.141 |                   23 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.259 |      0.041 |                   21 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2140858208955224
[2m[36m(func pid=144451)[0m top5: 0.7896455223880597
[2m[36m(func pid=144451)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=144451)[0m f1_macro: 0.14130669311088023
[2m[36m(func pid=144451)[0m f1_weighted: 0.2197781985785982
[2m[36m(func pid=144451)[0m f1_per_class: [0.038, 0.0, 0.0, 0.329, 0.105, 0.0, 0.323, 0.453, 0.098, 0.067]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.29617537313432835
[2m[36m(func pid=143647)[0m top5: 0.8586753731343284
[2m[36m(func pid=143647)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=143647)[0m f1_macro: 0.2905525210050495
[2m[36m(func pid=143647)[0m f1_weighted: 0.2672078695471738
[2m[36m(func pid=143647)[0m f1_per_class: [0.316, 0.409, 0.524, 0.003, 0.15, 0.393, 0.355, 0.492, 0.138, 0.126]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.15485074626865672
[2m[36m(func pid=144027)[0m top5: 0.7103544776119403
[2m[36m(func pid=144027)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=144027)[0m f1_macro: 0.14268554835988997
[2m[36m(func pid=144027)[0m f1_weighted: 0.1310803580931311
[2m[36m(func pid=144027)[0m f1_per_class: [0.097, 0.073, 0.0, 0.192, 0.066, 0.283, 0.009, 0.35, 0.135, 0.22]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9783 | Steps: 4 | Val loss: 1116.3419 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7146 | Steps: 4 | Val loss: 3.5089 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2875 | Steps: 4 | Val loss: 1.9463 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=144880)[0m top1: 0.14505597014925373
[2m[36m(func pid=144880)[0m top5: 0.6940298507462687
[2m[36m(func pid=144880)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=144880)[0m f1_macro: 0.07287084556149395
[2m[36m(func pid=144880)[0m f1_weighted: 0.09211440376914296
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.422, 0.023, 0.0, 0.0, 0.062, 0.0, 0.199, 0.023, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0439 | Steps: 4 | Val loss: 4.9566 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 12:56:11 (running for 00:03:03.51)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.173 |      0.291 |                   23 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.713 |      0.143 |                   23 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.715 |      0.093 |                   24 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.978 |      0.073 |                   22 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.15438432835820895
[2m[36m(func pid=144451)[0m top5: 0.7887126865671642
[2m[36m(func pid=144451)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=144451)[0m f1_macro: 0.09274944563352065
[2m[36m(func pid=144451)[0m f1_weighted: 0.12767227812491092
[2m[36m(func pid=144451)[0m f1_per_class: [0.026, 0.0, 0.0, 0.317, 0.096, 0.0, 0.052, 0.333, 0.104, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.22201492537313433
[2m[36m(func pid=143647)[0m top5: 0.8586753731343284
[2m[36m(func pid=143647)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=143647)[0m f1_macro: 0.23260436152282163
[2m[36m(func pid=143647)[0m f1_weighted: 0.16185080055281803
[2m[36m(func pid=143647)[0m f1_per_class: [0.338, 0.432, 0.355, 0.01, 0.148, 0.286, 0.018, 0.537, 0.141, 0.062]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1646455223880597
[2m[36m(func pid=144027)[0m top5: 0.7229477611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=144027)[0m f1_macro: 0.16245137866739132
[2m[36m(func pid=144027)[0m f1_weighted: 0.14803711490451799
[2m[36m(func pid=144027)[0m f1_per_class: [0.104, 0.334, 0.0, 0.104, 0.08, 0.26, 0.012, 0.297, 0.156, 0.278]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.1933 | Steps: 4 | Val loss: 1223.6023 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.3299 | Steps: 4 | Val loss: 4.2910 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1041 | Steps: 4 | Val loss: 1.9693 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.8608 | Steps: 4 | Val loss: 4.2240 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=144880)[0m top1: 0.1417910447761194
[2m[36m(func pid=144880)[0m top5: 0.6394589552238806
[2m[36m(func pid=144880)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.07322138359116044
[2m[36m(func pid=144880)[0m f1_weighted: 0.09111196723929998
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.404, 0.042, 0.0, 0.0, 0.086, 0.0, 0.2, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:56:16 (running for 00:03:08.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.288 |      0.233 |                   24 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.044 |      0.162 |                   24 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.33  |      0.155 |                   25 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.193 |      0.073 |                   23 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.15625
[2m[36m(func pid=144451)[0m top5: 0.8073694029850746
[2m[36m(func pid=144451)[0m f1_micro: 0.15625
[2m[36m(func pid=144451)[0m f1_macro: 0.15535129058201264
[2m[36m(func pid=144451)[0m f1_weighted: 0.12012469863718568
[2m[36m(func pid=144451)[0m f1_per_class: [0.088, 0.0, 0.69, 0.339, 0.067, 0.0, 0.003, 0.241, 0.126, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.20475746268656717
[2m[36m(func pid=143647)[0m top5: 0.8344216417910447
[2m[36m(func pid=143647)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=143647)[0m f1_macro: 0.20914903910511312
[2m[36m(func pid=143647)[0m f1_weighted: 0.13778392011163135
[2m[36m(func pid=143647)[0m f1_per_class: [0.267, 0.424, 0.386, 0.007, 0.144, 0.168, 0.0, 0.52, 0.126, 0.049]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.19169776119402984
[2m[36m(func pid=144027)[0m top5: 0.7583955223880597
[2m[36m(func pid=144027)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=144027)[0m f1_macro: 0.1573958992154861
[2m[36m(func pid=144027)[0m f1_weighted: 0.13876435695501754
[2m[36m(func pid=144027)[0m f1_per_class: [0.155, 0.512, 0.123, 0.01, 0.085, 0.031, 0.062, 0.27, 0.093, 0.233]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 5.9779 | Steps: 4 | Val loss: 1022.6893 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0680 | Steps: 4 | Val loss: 5.0002 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1303 | Steps: 4 | Val loss: 1.9791 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.7156 | Steps: 4 | Val loss: 4.3903 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=144880)[0m top1: 0.17490671641791045
[2m[36m(func pid=144880)[0m top5: 0.6557835820895522
[2m[36m(func pid=144880)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=144880)[0m f1_macro: 0.08553805385616474
[2m[36m(func pid=144880)[0m f1_weighted: 0.10316531852639418
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.449, 0.062, 0.0, 0.0, 0.109, 0.0, 0.209, 0.026, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:56:21 (running for 00:03:14.03)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.104 |      0.209 |                   25 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.861 |      0.157 |                   25 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.068 |      0.182 |                   26 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  5.978 |      0.086 |                   24 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.20522388059701493
[2m[36m(func pid=144451)[0m top5: 0.8246268656716418
[2m[36m(func pid=144451)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=144451)[0m f1_macro: 0.1815563839833954
[2m[36m(func pid=144451)[0m f1_weighted: 0.1916801584007261
[2m[36m(func pid=144451)[0m f1_per_class: [0.096, 0.387, 0.545, 0.353, 0.058, 0.016, 0.006, 0.225, 0.129, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.19869402985074627
[2m[36m(func pid=143647)[0m top5: 0.8055037313432836
[2m[36m(func pid=143647)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=143647)[0m f1_macro: 0.15491902117778686
[2m[36m(func pid=143647)[0m f1_weighted: 0.12148514251759558
[2m[36m(func pid=143647)[0m f1_per_class: [0.042, 0.421, 0.276, 0.01, 0.143, 0.143, 0.0, 0.42, 0.038, 0.055]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2196828358208955
[2m[36m(func pid=144027)[0m top5: 0.7075559701492538
[2m[36m(func pid=144027)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=144027)[0m f1_macro: 0.18264659316086654
[2m[36m(func pid=144027)[0m f1_weighted: 0.175232570341654
[2m[36m(func pid=144027)[0m f1_per_class: [0.145, 0.523, 0.19, 0.0, 0.128, 0.046, 0.171, 0.316, 0.099, 0.208]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0088 | Steps: 4 | Val loss: 679.1226 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7732 | Steps: 4 | Val loss: 4.0978 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.0757 | Steps: 4 | Val loss: 1.9892 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9260 | Steps: 4 | Val loss: 4.7177 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=144880)[0m top1: 0.1912313432835821
[2m[36m(func pid=144880)[0m top5: 0.6259328358208955
[2m[36m(func pid=144880)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=144880)[0m f1_macro: 0.08269793134282664
[2m[36m(func pid=144880)[0m f1_weighted: 0.0966042905306419
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.449, 0.105, 0.0, 0.0, 0.052, 0.0, 0.222, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 12:56:27 (running for 00:03:19.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.13  |      0.155 |                   26 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.716 |      0.183 |                   26 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.773 |      0.207 |                   27 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.009 |      0.083 |                   25 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.228544776119403
[2m[36m(func pid=144451)[0m top5: 0.8297574626865671
[2m[36m(func pid=144451)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=144451)[0m f1_macro: 0.2069590541462368
[2m[36m(func pid=144451)[0m f1_weighted: 0.21571707345734098
[2m[36m(func pid=144451)[0m f1_per_class: [0.111, 0.452, 0.64, 0.346, 0.036, 0.13, 0.009, 0.243, 0.103, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.20708955223880596
[2m[36m(func pid=143647)[0m top5: 0.7873134328358209
[2m[36m(func pid=143647)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=143647)[0m f1_macro: 0.14910718795204417
[2m[36m(func pid=143647)[0m f1_weighted: 0.12315061983038315
[2m[36m(func pid=143647)[0m f1_per_class: [0.043, 0.414, 0.247, 0.01, 0.136, 0.21, 0.0, 0.361, 0.0, 0.07]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.19449626865671643
[2m[36m(func pid=144027)[0m top5: 0.6879664179104478
[2m[36m(func pid=144027)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=144027)[0m f1_macro: 0.19246922314384485
[2m[36m(func pid=144027)[0m f1_weighted: 0.19086231593028344
[2m[36m(func pid=144027)[0m f1_per_class: [0.172, 0.504, 0.264, 0.003, 0.148, 0.152, 0.212, 0.202, 0.077, 0.19]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.0251 | Steps: 4 | Val loss: 475.8110 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.8164 | Steps: 4 | Val loss: 2.7946 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.1030 | Steps: 4 | Val loss: 2.0055 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6827 | Steps: 4 | Val loss: 6.2376 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:56:32 (running for 00:03:24.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.076 |      0.149 |                   27 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.926 |      0.192 |                   27 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.773 |      0.207 |                   27 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.025 |      0.046 |                   26 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.06809701492537314
[2m[36m(func pid=144880)[0m top5: 0.5377798507462687
[2m[36m(func pid=144880)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=144880)[0m f1_macro: 0.04602795496169672
[2m[36m(func pid=144880)[0m f1_weighted: 0.053169409066778486
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.265, 0.051, 0.0, 0.0, 0.016, 0.0, 0.061, 0.049, 0.018]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.25
[2m[36m(func pid=144451)[0m top5: 0.8311567164179104
[2m[36m(func pid=144451)[0m f1_micro: 0.25
[2m[36m(func pid=144451)[0m f1_macro: 0.1692783771445363
[2m[36m(func pid=144451)[0m f1_weighted: 0.23984526625694788
[2m[36m(func pid=144451)[0m f1_per_class: [0.118, 0.459, 0.0, 0.348, 0.05, 0.234, 0.047, 0.302, 0.058, 0.077]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.2126865671641791
[2m[36m(func pid=143647)[0m top5: 0.7915111940298507
[2m[36m(func pid=143647)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=143647)[0m f1_macro: 0.1455557090784619
[2m[36m(func pid=143647)[0m f1_weighted: 0.12616853289759555
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.416, 0.238, 0.007, 0.112, 0.269, 0.0, 0.32, 0.0, 0.094]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.11380597014925373
[2m[36m(func pid=144027)[0m top5: 0.6893656716417911
[2m[36m(func pid=144027)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=144027)[0m f1_macro: 0.09925862847327256
[2m[36m(func pid=144027)[0m f1_weighted: 0.10551424661329992
[2m[36m(func pid=144027)[0m f1_per_class: [0.131, 0.354, 0.0, 0.026, 0.15, 0.111, 0.057, 0.015, 0.059, 0.09]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.9822 | Steps: 4 | Val loss: 416.7131 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.2546 | Steps: 4 | Val loss: 2.3086 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.1646 | Steps: 4 | Val loss: 2.0398 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3245 | Steps: 4 | Val loss: 7.5206 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:56:37 (running for 00:03:29.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.103 |      0.146 |                   28 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.683 |      0.099 |                   28 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.816 |      0.169 |                   28 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.982 |      0.025 |                   27 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.03871268656716418
[2m[36m(func pid=144880)[0m top5: 0.65625
[2m[36m(func pid=144880)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=144880)[0m f1_macro: 0.025242441677887244
[2m[36m(func pid=144880)[0m f1_weighted: 0.019941856698872443
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.049, 0.039, 0.003, 0.0, 0.051, 0.0, 0.036, 0.074, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.29151119402985076
[2m[36m(func pid=144451)[0m top5: 0.824160447761194
[2m[36m(func pid=144451)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=144451)[0m f1_macro: 0.198741325699923
[2m[36m(func pid=144451)[0m f1_weighted: 0.3293227691260428
[2m[36m(func pid=144451)[0m f1_per_class: [0.103, 0.425, 0.0, 0.326, 0.0, 0.281, 0.349, 0.45, 0.02, 0.033]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.21128731343283583
[2m[36m(func pid=143647)[0m top5: 0.7910447761194029
[2m[36m(func pid=143647)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=143647)[0m f1_macro: 0.14550961409928234
[2m[36m(func pid=143647)[0m f1_weighted: 0.1222040161350244
[2m[36m(func pid=143647)[0m f1_per_class: [0.0, 0.407, 0.247, 0.003, 0.103, 0.268, 0.0, 0.288, 0.0, 0.139]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.10027985074626866
[2m[36m(func pid=144027)[0m top5: 0.6842350746268657
[2m[36m(func pid=144027)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=144027)[0m f1_macro: 0.09406466356470641
[2m[36m(func pid=144027)[0m f1_weighted: 0.08732331322145026
[2m[36m(func pid=144027)[0m f1_per_class: [0.117, 0.257, 0.0, 0.052, 0.179, 0.116, 0.027, 0.0, 0.069, 0.125]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.9067 | Steps: 4 | Val loss: 308.9298 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.7981 | Steps: 4 | Val loss: 2.3148 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1199 | Steps: 4 | Val loss: 2.0638 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6927 | Steps: 4 | Val loss: 7.6621 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:56:42 (running for 00:03:35.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.165 |      0.146 |                   29 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.324 |      0.094 |                   29 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.255 |      0.199 |                   29 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.907 |      0.021 |                   28 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.03871268656716418
[2m[36m(func pid=144880)[0m top5: 0.6026119402985075
[2m[36m(func pid=144880)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=144880)[0m f1_macro: 0.021452180573544664
[2m[36m(func pid=144880)[0m f1_weighted: 0.024080495688131705
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.018, 0.037, 0.016, 0.0, 0.0, 0.04, 0.03, 0.072, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.31902985074626866
[2m[36m(func pid=144451)[0m top5: 0.8288246268656716
[2m[36m(func pid=144451)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=144451)[0m f1_macro: 0.19291865061562524
[2m[36m(func pid=144451)[0m f1_weighted: 0.3514658403532754
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.403, 0.0, 0.317, 0.15, 0.076, 0.558, 0.214, 0.086, 0.026]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.208955223880597
[2m[36m(func pid=143647)[0m top5: 0.7840485074626866
[2m[36m(func pid=143647)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=143647)[0m f1_macro: 0.15615231989091566
[2m[36m(func pid=143647)[0m f1_weighted: 0.12089374819920952
[2m[36m(func pid=143647)[0m f1_per_class: [0.12, 0.408, 0.259, 0.003, 0.085, 0.232, 0.0, 0.285, 0.0, 0.168]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.11613805970149253
[2m[36m(func pid=144027)[0m top5: 0.6907649253731343
[2m[36m(func pid=144027)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=144027)[0m f1_macro: 0.10253833483106001
[2m[36m(func pid=144027)[0m f1_weighted: 0.10511824359487212
[2m[36m(func pid=144027)[0m f1_per_class: [0.149, 0.317, 0.0, 0.076, 0.167, 0.109, 0.03, 0.0, 0.072, 0.106]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.1460 | Steps: 4 | Val loss: 219.4319 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4827 | Steps: 4 | Val loss: 2.3693 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9387 | Steps: 4 | Val loss: 2.0839 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4680 | Steps: 4 | Val loss: 7.4272 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:56:48 (running for 00:03:40.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.12  |      0.156 |                   30 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.693 |      0.103 |                   30 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.798 |      0.193 |                   30 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.146 |      0.032 |                   29 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.05503731343283582
[2m[36m(func pid=144880)[0m top5: 0.5909514925373134
[2m[36m(func pid=144880)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=144880)[0m f1_macro: 0.03195511590381324
[2m[36m(func pid=144880)[0m f1_weighted: 0.05062567655215538
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.035, 0.05, 0.0, 0.0, 0.103, 0.059, 0.073, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.333955223880597
[2m[36m(func pid=144451)[0m top5: 0.8190298507462687
[2m[36m(func pid=144451)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=144451)[0m f1_macro: 0.1899147047718031
[2m[36m(func pid=144451)[0m f1_weighted: 0.35571044527653994
[2m[36m(func pid=144451)[0m f1_per_class: [0.096, 0.415, 0.0, 0.318, 0.062, 0.016, 0.579, 0.238, 0.136, 0.038]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.20848880597014927
[2m[36m(func pid=143647)[0m top5: 0.7737873134328358
[2m[36m(func pid=143647)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=143647)[0m f1_macro: 0.17593225693723574
[2m[36m(func pid=143647)[0m f1_weighted: 0.1267572486593582
[2m[36m(func pid=143647)[0m f1_per_class: [0.273, 0.402, 0.273, 0.003, 0.064, 0.253, 0.0, 0.31, 0.0, 0.182]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.13152985074626866
[2m[36m(func pid=144027)[0m top5: 0.7136194029850746
[2m[36m(func pid=144027)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=144027)[0m f1_macro: 0.09066728257357863
[2m[36m(func pid=144027)[0m f1_weighted: 0.1179552521081481
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.374, 0.0, 0.15, 0.211, 0.008, 0.021, 0.0, 0.06, 0.083]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.0365 | Steps: 4 | Val loss: 278.8816 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8375 | Steps: 4 | Val loss: 2.4743 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8593 | Steps: 4 | Val loss: 2.1036 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2649 | Steps: 4 | Val loss: 5.3279 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:56:53 (running for 00:03:46.11)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.939 |      0.176 |                   31 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.468 |      0.091 |                   31 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.483 |      0.19  |                   31 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.036 |      0.057 |                   30 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.14272388059701493
[2m[36m(func pid=144880)[0m top5: 0.6371268656716418
[2m[36m(func pid=144880)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=144880)[0m f1_macro: 0.05677777036645415
[2m[36m(func pid=144880)[0m f1_weighted: 0.12252162859428908
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.035, 0.381, 0.0, 0.0, 0.038, 0.047, 0.068, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.2798507462686567
[2m[36m(func pid=144451)[0m top5: 0.8069029850746269
[2m[36m(func pid=144451)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=144451)[0m f1_macro: 0.18370756165225285
[2m[36m(func pid=144451)[0m f1_weighted: 0.3008329559867653
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.386, 0.0, 0.31, 0.067, 0.0, 0.395, 0.409, 0.093, 0.078]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.20988805970149255
[2m[36m(func pid=143647)[0m top5: 0.7649253731343284
[2m[36m(func pid=143647)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=143647)[0m f1_macro: 0.18376875673630289
[2m[36m(func pid=143647)[0m f1_weighted: 0.1268106077142612
[2m[36m(func pid=143647)[0m f1_per_class: [0.288, 0.419, 0.375, 0.01, 0.057, 0.188, 0.0, 0.349, 0.0, 0.152]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1884328358208955
[2m[36m(func pid=144027)[0m top5: 0.7887126865671642
[2m[36m(func pid=144027)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=144027)[0m f1_macro: 0.10908363852703214
[2m[36m(func pid=144027)[0m f1_weighted: 0.18389233417725298
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.394, 0.0, 0.363, 0.164, 0.0, 0.036, 0.0, 0.061, 0.073]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6255 | Steps: 4 | Val loss: 2.6898 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7178 | Steps: 4 | Val loss: 213.3351 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.0477 | Steps: 4 | Val loss: 2.1754 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.7870 | Steps: 4 | Val loss: 3.2435 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:56:59 (running for 00:03:51.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.859 |      0.184 |                   32 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.265 |      0.109 |                   32 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.626 |      0.156 |                   33 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.036 |      0.057 |                   30 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.22388059701492538
[2m[36m(func pid=144451)[0m top5: 0.7845149253731343
[2m[36m(func pid=144451)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=144451)[0m f1_macro: 0.1561367905186156
[2m[36m(func pid=144451)[0m f1_weighted: 0.23348534766348752
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.328, 0.0, 0.306, 0.082, 0.0, 0.221, 0.295, 0.155, 0.074]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.11240671641791045
[2m[36m(func pid=144880)[0m top5: 0.6888992537313433
[2m[36m(func pid=144880)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=144880)[0m f1_macro: 0.05258731983026025
[2m[36m(func pid=144880)[0m f1_weighted: 0.10804099858357223
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.036, 0.371, 0.05, 0.0, 0.0, 0.069, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.20988805970149255
[2m[36m(func pid=143647)[0m top5: 0.7369402985074627
[2m[36m(func pid=143647)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=143647)[0m f1_macro: 0.18082950984902915
[2m[36m(func pid=143647)[0m f1_weighted: 0.13923053173227615
[2m[36m(func pid=143647)[0m f1_per_class: [0.284, 0.44, 0.353, 0.07, 0.046, 0.095, 0.0, 0.406, 0.0, 0.115]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3069029850746269
[2m[36m(func pid=144027)[0m top5: 0.8544776119402985
[2m[36m(func pid=144027)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=144027)[0m f1_macro: 0.1781048184409414
[2m[36m(func pid=144027)[0m f1_weighted: 0.30185178508168964
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.449, 0.0, 0.493, 0.122, 0.016, 0.215, 0.257, 0.13, 0.099]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.3421 | Steps: 4 | Val loss: 2.7129 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.0303 | Steps: 4 | Val loss: 171.7185 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0634 | Steps: 4 | Val loss: 2.1672 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8514 | Steps: 4 | Val loss: 2.6267 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:57:04 (running for 00:03:57.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.048 |      0.181 |                   33 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.787 |      0.178 |                   33 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.342 |      0.134 |                   34 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.718 |      0.053 |                   31 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.19076492537313433
[2m[36m(func pid=144451)[0m top5: 0.7751865671641791
[2m[36m(func pid=144451)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=144451)[0m f1_macro: 0.13393626695062003
[2m[36m(func pid=144451)[0m f1_weighted: 0.19332046732647346
[2m[36m(func pid=144451)[0m f1_per_class: [0.101, 0.294, 0.0, 0.255, 0.087, 0.008, 0.155, 0.291, 0.148, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.08815298507462686
[2m[36m(func pid=144880)[0m top5: 0.7131529850746269
[2m[36m(func pid=144880)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=144880)[0m f1_macro: 0.046311565551693315
[2m[36m(func pid=144880)[0m f1_weighted: 0.0867638829461116
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.044, 0.291, 0.039, 0.0, 0.0, 0.09, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.2178171641791045
[2m[36m(func pid=143647)[0m top5: 0.738339552238806
[2m[36m(func pid=143647)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=143647)[0m f1_macro: 0.19815938062059468
[2m[36m(func pid=143647)[0m f1_weighted: 0.16716706107672039
[2m[36m(func pid=143647)[0m f1_per_class: [0.271, 0.457, 0.462, 0.173, 0.046, 0.062, 0.0, 0.4, 0.0, 0.111]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3880597014925373
[2m[36m(func pid=144027)[0m top5: 0.8689365671641791
[2m[36m(func pid=144027)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=144027)[0m f1_macro: 0.22260474387045806
[2m[36m(func pid=144027)[0m f1_weighted: 0.4141105894626944
[2m[36m(func pid=144027)[0m f1_per_class: [0.015, 0.41, 0.0, 0.51, 0.103, 0.264, 0.539, 0.016, 0.201, 0.169]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8442 | Steps: 4 | Val loss: 126.1203 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1683 | Steps: 4 | Val loss: 2.9590 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8886 | Steps: 4 | Val loss: 2.1350 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9420 | Steps: 4 | Val loss: 3.8445 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=144451)[0m top1: 0.14039179104477612
[2m[36m(func pid=144451)[0m top5: 0.7877798507462687
[2m[36m(func pid=144451)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=144451)[0m f1_macro: 0.10642579537583793
[2m[36m(func pid=144451)[0m f1_weighted: 0.12462903145873669
[2m[36m(func pid=144451)[0m f1_per_class: [0.112, 0.222, 0.008, 0.165, 0.068, 0.093, 0.027, 0.269, 0.102, 0.0]
== Status ==
Current time: 2024-01-07 12:57:10 (running for 00:04:02.42)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  2.063 |      0.198 |                   34 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.851 |      0.223 |                   34 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.168 |      0.106 |                   35 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.03  |      0.046 |                   32 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.05457089552238806
[2m[36m(func pid=144880)[0m top5: 0.7136194029850746
[2m[36m(func pid=144880)[0m f1_micro: 0.05457089552238806
[2m[36m(func pid=144880)[0m f1_macro: 0.04895906507685085
[2m[36m(func pid=144880)[0m f1_weighted: 0.03554200906275886
[2m[36m(func pid=144880)[0m f1_per_class: [0.121, 0.0, 0.086, 0.059, 0.03, 0.082, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.240205223880597
[2m[36m(func pid=143647)[0m top5: 0.7574626865671642
[2m[36m(func pid=143647)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=143647)[0m f1_macro: 0.23127144533616514
[2m[36m(func pid=143647)[0m f1_weighted: 0.20137419824290376
[2m[36m(func pid=143647)[0m f1_per_class: [0.244, 0.507, 0.571, 0.253, 0.035, 0.073, 0.0, 0.417, 0.0, 0.212]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.20382462686567165
[2m[36m(func pid=144027)[0m top5: 0.7719216417910447
[2m[36m(func pid=144027)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=144027)[0m f1_macro: 0.17254952616227898
[2m[36m(func pid=144027)[0m f1_weighted: 0.22818416170307035
[2m[36m(func pid=144027)[0m f1_per_class: [0.058, 0.357, 0.364, 0.44, 0.12, 0.091, 0.08, 0.0, 0.144, 0.073]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.8503 | Steps: 4 | Val loss: 4.6566 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.8538 | Steps: 4 | Val loss: 76.1245 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.6676 | Steps: 4 | Val loss: 7.6096 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7789 | Steps: 4 | Val loss: 2.0401 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:57:15 (running for 00:04:07.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.889 |      0.231 |                   35 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.942 |      0.173 |                   35 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.85  |      0.134 |                   36 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.844 |      0.049 |                   33 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.1646455223880597
[2m[36m(func pid=144451)[0m top5: 0.769589552238806
[2m[36m(func pid=144451)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=144451)[0m f1_macro: 0.13425973131435132
[2m[36m(func pid=144451)[0m f1_weighted: 0.12521201822741168
[2m[36m(func pid=144451)[0m f1_per_class: [0.145, 0.241, 0.042, 0.063, 0.089, 0.339, 0.003, 0.351, 0.07, 0.0]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.07509328358208955
[2m[36m(func pid=144880)[0m top5: 0.707089552238806
[2m[36m(func pid=144880)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=144880)[0m f1_macro: 0.05806384342785368
[2m[36m(func pid=144880)[0m f1_weighted: 0.06454523543329421
[2m[36m(func pid=144880)[0m f1_per_class: [0.124, 0.0, 0.0, 0.0, 0.027, 0.121, 0.126, 0.183, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.2593283582089552
[2m[36m(func pid=143647)[0m top5: 0.792910447761194
[2m[36m(func pid=143647)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=143647)[0m f1_macro: 0.26439125478497133
[2m[36m(func pid=143647)[0m f1_weighted: 0.2353267019520208
[2m[36m(func pid=143647)[0m f1_per_class: [0.241, 0.515, 0.667, 0.338, 0.041, 0.157, 0.0, 0.375, 0.0, 0.311]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.16417910447761194
[2m[36m(func pid=144027)[0m top5: 0.6301305970149254
[2m[36m(func pid=144027)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=144027)[0m f1_macro: 0.1610334303778561
[2m[36m(func pid=144027)[0m f1_weighted: 0.17378512900479837
[2m[36m(func pid=144027)[0m f1_per_class: [0.129, 0.408, 0.453, 0.277, 0.103, 0.019, 0.044, 0.0, 0.128, 0.049]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2179 | Steps: 4 | Val loss: 7.6397 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8788 | Steps: 4 | Val loss: 52.0498 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8861 | Steps: 4 | Val loss: 1.9662 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.5338 | Steps: 4 | Val loss: 5.6047 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:57:20 (running for 00:04:13.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.779 |      0.264 |                   36 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.668 |      0.161 |                   36 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.218 |      0.1   |                   37 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.854 |      0.058 |                   34 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.14272388059701493
[2m[36m(func pid=144451)[0m top5: 0.715018656716418
[2m[36m(func pid=144451)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=144451)[0m f1_macro: 0.10045637589247158
[2m[36m(func pid=144451)[0m f1_weighted: 0.10091598547303572
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.277, 0.007, 0.039, 0.122, 0.315, 0.0, 0.013, 0.057, 0.077]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.10494402985074627
[2m[36m(func pid=144880)[0m top5: 0.7010261194029851
[2m[36m(func pid=144880)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=144880)[0m f1_macro: 0.07657779896094297
[2m[36m(func pid=144880)[0m f1_weighted: 0.10000791685981127
[2m[36m(func pid=144880)[0m f1_per_class: [0.126, 0.0, 0.0, 0.0, 0.029, 0.17, 0.218, 0.223, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.2733208955223881
[2m[36m(func pid=143647)[0m top5: 0.816231343283582
[2m[36m(func pid=143647)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=143647)[0m f1_macro: 0.2639425257217213
[2m[36m(func pid=143647)[0m f1_weighted: 0.24990129580337123
[2m[36m(func pid=143647)[0m f1_per_class: [0.211, 0.519, 0.643, 0.38, 0.049, 0.194, 0.0, 0.352, 0.0, 0.291]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.17210820895522388
[2m[36m(func pid=144027)[0m top5: 0.7098880597014925
[2m[36m(func pid=144027)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=144027)[0m f1_macro: 0.18070091200801253
[2m[36m(func pid=144027)[0m f1_weighted: 0.1693570280425424
[2m[36m(func pid=144027)[0m f1_per_class: [0.152, 0.449, 0.538, 0.14, 0.068, 0.026, 0.096, 0.193, 0.078, 0.066]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4727 | Steps: 4 | Val loss: 8.5500 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7685 | Steps: 4 | Val loss: 39.0806 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8662 | Steps: 4 | Val loss: 1.8681 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6504 | Steps: 4 | Val loss: 4.2192 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:57:26 (running for 00:04:18.44)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.886 |      0.264 |                   37 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.534 |      0.181 |                   37 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.473 |      0.114 |                   38 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.879 |      0.077 |                   35 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.16417910447761194
[2m[36m(func pid=144451)[0m top5: 0.6389925373134329
[2m[36m(func pid=144451)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=144451)[0m f1_macro: 0.11428676545183122
[2m[36m(func pid=144451)[0m f1_weighted: 0.12392909944533333
[2m[36m(func pid=144451)[0m f1_per_class: [0.109, 0.362, 0.0, 0.064, 0.151, 0.326, 0.0, 0.014, 0.043, 0.074]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.16791044776119404
[2m[36m(func pid=144880)[0m top5: 0.6347947761194029
[2m[36m(func pid=144880)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=144880)[0m f1_macro: 0.08868062426362035
[2m[36m(func pid=144880)[0m f1_weighted: 0.1269799414275431
[2m[36m(func pid=144880)[0m f1_per_class: [0.132, 0.0, 0.0, 0.0, 0.0, 0.206, 0.287, 0.261, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.30130597014925375
[2m[36m(func pid=143647)[0m top5: 0.8418843283582089
[2m[36m(func pid=143647)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=143647)[0m f1_macro: 0.27910926497483723
[2m[36m(func pid=143647)[0m f1_weighted: 0.28351426134333546
[2m[36m(func pid=143647)[0m f1_per_class: [0.234, 0.499, 0.621, 0.478, 0.058, 0.293, 0.0, 0.317, 0.0, 0.291]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.18330223880597016
[2m[36m(func pid=144027)[0m top5: 0.8199626865671642
[2m[36m(func pid=144027)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=144027)[0m f1_macro: 0.13599715264955062
[2m[36m(func pid=144027)[0m f1_weighted: 0.1484247730112337
[2m[36m(func pid=144027)[0m f1_per_class: [0.159, 0.406, 0.0, 0.094, 0.088, 0.043, 0.076, 0.285, 0.092, 0.117]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6348 | Steps: 4 | Val loss: 5.4622 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6445 | Steps: 4 | Val loss: 20.8496 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6911 | Steps: 4 | Val loss: 1.8188 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8631 | Steps: 4 | Val loss: 3.6325 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:57:31 (running for 00:04:23.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.866 |      0.279 |                   38 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.65  |      0.136 |                   38 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.635 |      0.143 |                   39 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.768 |      0.089 |                   36 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.20662313432835822
[2m[36m(func pid=144451)[0m top5: 0.699160447761194
[2m[36m(func pid=144451)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=144451)[0m f1_macro: 0.14336517406393362
[2m[36m(func pid=144451)[0m f1_weighted: 0.1766611128555819
[2m[36m(func pid=144451)[0m f1_per_class: [0.114, 0.471, 0.0, 0.184, 0.103, 0.32, 0.0, 0.028, 0.027, 0.188]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.19869402985074627
[2m[36m(func pid=144880)[0m top5: 0.6119402985074627
[2m[36m(func pid=144880)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=144880)[0m f1_macro: 0.1061968270082793
[2m[36m(func pid=144880)[0m f1_weighted: 0.1651831445272777
[2m[36m(func pid=144880)[0m f1_per_class: [0.137, 0.02, 0.0, 0.0, 0.0, 0.211, 0.394, 0.3, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.31716417910447764
[2m[36m(func pid=143647)[0m top5: 0.8470149253731343
[2m[36m(func pid=143647)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=143647)[0m f1_macro: 0.29241974371965185
[2m[36m(func pid=143647)[0m f1_weighted: 0.297869029578767
[2m[36m(func pid=143647)[0m f1_per_class: [0.231, 0.455, 0.692, 0.523, 0.072, 0.313, 0.018, 0.328, 0.027, 0.264]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1828358208955224
[2m[36m(func pid=144027)[0m top5: 0.8512126865671642
[2m[36m(func pid=144027)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=144027)[0m f1_macro: 0.13776446004313522
[2m[36m(func pid=144027)[0m f1_weighted: 0.15529363674868507
[2m[36m(func pid=144027)[0m f1_per_class: [0.186, 0.339, 0.0, 0.15, 0.096, 0.0, 0.102, 0.271, 0.08, 0.153]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1083 | Steps: 4 | Val loss: 6.0260 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.2677 | Steps: 4 | Val loss: 28.4374 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.8948 | Steps: 4 | Val loss: 1.7956 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0984 | Steps: 4 | Val loss: 4.4470 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:57:36 (running for 00:04:29.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.691 |      0.292 |                   39 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.863 |      0.138 |                   39 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.108 |      0.145 |                   40 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.644 |      0.106 |                   37 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.20942164179104478
[2m[36m(func pid=144451)[0m top5: 0.7094216417910447
[2m[36m(func pid=144451)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=144451)[0m f1_macro: 0.14539104273036446
[2m[36m(func pid=144451)[0m f1_weighted: 0.19809711606876002
[2m[36m(func pid=144451)[0m f1_per_class: [0.109, 0.448, 0.0, 0.252, 0.088, 0.353, 0.003, 0.073, 0.043, 0.085]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.24347014925373134
[2m[36m(func pid=144880)[0m top5: 0.6082089552238806
[2m[36m(func pid=144880)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=144880)[0m f1_macro: 0.09762575600244386
[2m[36m(func pid=144880)[0m f1_weighted: 0.16469857976873353
[2m[36m(func pid=144880)[0m f1_per_class: [0.091, 0.387, 0.0, 0.0, 0.0, 0.0, 0.281, 0.218, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.355410447761194
[2m[36m(func pid=143647)[0m top5: 0.8395522388059702
[2m[36m(func pid=143647)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.2987216782654015
[2m[36m(func pid=143647)[0m f1_weighted: 0.35275178565636045
[2m[36m(func pid=143647)[0m f1_per_class: [0.243, 0.446, 0.571, 0.55, 0.071, 0.31, 0.176, 0.394, 0.024, 0.202]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.16930970149253732
[2m[36m(func pid=144027)[0m top5: 0.8148320895522388
[2m[36m(func pid=144027)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=144027)[0m f1_macro: 0.15862102351666735
[2m[36m(func pid=144027)[0m f1_weighted: 0.14982258987487435
[2m[36m(func pid=144027)[0m f1_per_class: [0.218, 0.258, 0.228, 0.189, 0.094, 0.088, 0.052, 0.285, 0.096, 0.079]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.0575 | Steps: 4 | Val loss: 4.6110 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7807 | Steps: 4 | Val loss: 20.3231 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6359 | Steps: 4 | Val loss: 1.7642 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5833 | Steps: 4 | Val loss: 6.1578 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:57:42 (running for 00:04:34.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.895 |      0.299 |                   40 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.098 |      0.159 |                   40 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.058 |      0.17  |                   41 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.268 |      0.098 |                   38 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2224813432835821
[2m[36m(func pid=144451)[0m top5: 0.7821828358208955
[2m[36m(func pid=144451)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=144451)[0m f1_macro: 0.17036397982809132
[2m[36m(func pid=144451)[0m f1_weighted: 0.24559717895410088
[2m[36m(func pid=144451)[0m f1_per_class: [0.092, 0.454, 0.047, 0.343, 0.099, 0.27, 0.079, 0.221, 0.031, 0.069]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.2080223880597015
[2m[36m(func pid=144880)[0m top5: 0.5928171641791045
[2m[36m(func pid=144880)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=144880)[0m f1_macro: 0.06816486098173713
[2m[36m(func pid=144880)[0m f1_weighted: 0.09375170899717751
[2m[36m(func pid=144880)[0m f1_per_class: [0.035, 0.38, 0.0, 0.0, 0.0, 0.0, 0.051, 0.216, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3987873134328358
[2m[36m(func pid=143647)[0m top5: 0.8460820895522388
[2m[36m(func pid=143647)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=143647)[0m f1_macro: 0.31456453358881586
[2m[36m(func pid=143647)[0m f1_weighted: 0.4200096054281776
[2m[36m(func pid=143647)[0m f1_per_class: [0.263, 0.444, 0.417, 0.554, 0.072, 0.31, 0.384, 0.47, 0.048, 0.185]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.1646455223880597
[2m[36m(func pid=144027)[0m top5: 0.7714552238805971
[2m[36m(func pid=144027)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=144027)[0m f1_macro: 0.16725946806336642
[2m[36m(func pid=144027)[0m f1_weighted: 0.16432573156827066
[2m[36m(func pid=144027)[0m f1_per_class: [0.174, 0.145, 0.227, 0.183, 0.08, 0.095, 0.141, 0.417, 0.157, 0.054]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.0669 | Steps: 4 | Val loss: 3.8622 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7498 | Steps: 4 | Val loss: 19.8993 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5603 | Steps: 4 | Val loss: 1.7121 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9436 | Steps: 4 | Val loss: 6.4966 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=144451)[0m top1: 0.2271455223880597
[2m[36m(func pid=144451)[0m top5: 0.8017723880597015
[2m[36m(func pid=144451)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=144451)[0m f1_macro: 0.15622619202029067
[2m[36m(func pid=144451)[0m f1_weighted: 0.24456923396905028
[2m[36m(func pid=144451)[0m f1_per_class: [0.095, 0.298, 0.125, 0.416, 0.1, 0.0, 0.192, 0.27, 0.0, 0.066]
[2m[36m(func pid=144451)[0m 
== Status ==
Current time: 2024-01-07 12:57:47 (running for 00:04:39.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.636 |      0.315 |                   41 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.583 |      0.167 |                   41 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.067 |      0.156 |                   42 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.781 |      0.068 |                   39 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.19916044776119404
[2m[36m(func pid=144880)[0m top5: 0.6114738805970149
[2m[36m(func pid=144880)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=144880)[0m f1_macro: 0.07193454049839228
[2m[36m(func pid=144880)[0m f1_weighted: 0.0784628461346039
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.384, 0.144, 0.0, 0.0, 0.008, 0.0, 0.184, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.4337686567164179
[2m[36m(func pid=143647)[0m top5: 0.8577425373134329
[2m[36m(func pid=143647)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=143647)[0m f1_macro: 0.33299813909097964
[2m[36m(func pid=143647)[0m f1_weighted: 0.46012326257751396
[2m[36m(func pid=143647)[0m f1_per_class: [0.337, 0.426, 0.381, 0.557, 0.098, 0.323, 0.517, 0.463, 0.044, 0.183]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.17583955223880596
[2m[36m(func pid=144027)[0m top5: 0.7854477611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=144027)[0m f1_macro: 0.15870756109191383
[2m[36m(func pid=144027)[0m f1_weighted: 0.20619120243135244
[2m[36m(func pid=144027)[0m f1_per_class: [0.198, 0.13, 0.275, 0.201, 0.071, 0.145, 0.325, 0.047, 0.137, 0.058]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.8126 | Steps: 4 | Val loss: 2.8796 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5464 | Steps: 4 | Val loss: 18.5525 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6293 | Steps: 4 | Val loss: 1.7463 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9451 | Steps: 4 | Val loss: 5.1920 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=144451)[0m top1: 0.20942164179104478
[2m[36m(func pid=144451)[0m top5: 0.8134328358208955
[2m[36m(func pid=144451)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=144451)[0m f1_macro: 0.15521583906909875
[2m[36m(func pid=144451)[0m f1_weighted: 0.21366786326055615
[2m[36m(func pid=144451)[0m f1_per_class: [0.131, 0.256, 0.203, 0.406, 0.115, 0.0, 0.124, 0.235, 0.0, 0.082]
== Status ==
Current time: 2024-01-07 12:57:52 (running for 00:04:45.24)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.56  |      0.333 |                   42 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.944 |      0.159 |                   42 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.813 |      0.155 |                   43 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.75  |      0.072 |                   40 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.02332089552238806
[2m[36m(func pid=144880)[0m top5: 0.7346082089552238
[2m[36m(func pid=144880)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=144880)[0m f1_macro: 0.02271043472988705
[2m[36m(func pid=144880)[0m f1_weighted: 0.020808351061474367
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.084, 0.015, 0.113, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m top1: 0.42117537313432835
[2m[36m(func pid=143647)[0m top5: 0.8479477611940298
[2m[36m(func pid=143647)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=143647)[0m f1_macro: 0.2944196088148322
[2m[36m(func pid=143647)[0m f1_weighted: 0.4480859766207792
[2m[36m(func pid=143647)[0m f1_per_class: [0.406, 0.454, 0.182, 0.55, 0.093, 0.283, 0.526, 0.238, 0.041, 0.171]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.19263059701492538
[2m[36m(func pid=144027)[0m top5: 0.7807835820895522
[2m[36m(func pid=144027)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=144027)[0m f1_macro: 0.1865473575376469
[2m[36m(func pid=144027)[0m f1_weighted: 0.21269868613990736
[2m[36m(func pid=144027)[0m f1_per_class: [0.2, 0.149, 0.489, 0.299, 0.076, 0.241, 0.212, 0.0, 0.128, 0.071]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3583 | Steps: 4 | Val loss: 2.5470 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.3745 | Steps: 4 | Val loss: 1.7882 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7875 | Steps: 4 | Val loss: 16.1642 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3459 | Steps: 4 | Val loss: 4.8255 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:57:58 (running for 00:04:50.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.629 |      0.294 |                   43 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.945 |      0.187 |                   43 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.358 |      0.166 |                   44 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.546 |      0.023 |                   41 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.22294776119402984
[2m[36m(func pid=144451)[0m top5: 0.8180970149253731
[2m[36m(func pid=144451)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=144451)[0m f1_macro: 0.16567560474993465
[2m[36m(func pid=144451)[0m f1_weighted: 0.23168676715539838
[2m[36m(func pid=144451)[0m f1_per_class: [0.115, 0.277, 0.225, 0.4, 0.12, 0.0, 0.173, 0.266, 0.0, 0.081]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.4006529850746269
[2m[36m(func pid=143647)[0m top5: 0.8512126865671642
[2m[36m(func pid=143647)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=143647)[0m f1_macro: 0.25199738787870307
[2m[36m(func pid=143647)[0m f1_weighted: 0.4276634511283491
[2m[36m(func pid=143647)[0m f1_per_class: [0.278, 0.452, 0.147, 0.541, 0.114, 0.269, 0.528, 0.0, 0.055, 0.136]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.054104477611940295
[2m[36m(func pid=144880)[0m top5: 0.7178171641791045
[2m[36m(func pid=144880)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=144880)[0m f1_macro: 0.041282694076152385
[2m[36m(func pid=144880)[0m f1_weighted: 0.06449555933310623
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.037, 0.163, 0.197, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.23041044776119404
[2m[36m(func pid=144027)[0m top5: 0.7653917910447762
[2m[36m(func pid=144027)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=144027)[0m f1_macro: 0.18128671064944463
[2m[36m(func pid=144027)[0m f1_weighted: 0.24138948886099015
[2m[36m(func pid=144027)[0m f1_per_class: [0.16, 0.37, 0.222, 0.391, 0.079, 0.286, 0.09, 0.0, 0.073, 0.143]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5416 | Steps: 4 | Val loss: 2.3002 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3680 | Steps: 4 | Val loss: 1.8419 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6219 | Steps: 4 | Val loss: 36.8296 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6242 | Steps: 4 | Val loss: 5.2087 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:58:03 (running for 00:04:56.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.374 |      0.252 |                   44 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.346 |      0.181 |                   44 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.542 |      0.21  |                   45 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.787 |      0.041 |                   42 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.26259328358208955
[2m[36m(func pid=144451)[0m top5: 0.8288246268656716
[2m[36m(func pid=144451)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=144451)[0m f1_macro: 0.21018138783345938
[2m[36m(func pid=144451)[0m f1_weighted: 0.27433376302353485
[2m[36m(func pid=144451)[0m f1_per_class: [0.121, 0.364, 0.387, 0.383, 0.138, 0.0, 0.257, 0.358, 0.023, 0.071]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.38899253731343286
[2m[36m(func pid=143647)[0m top5: 0.8507462686567164
[2m[36m(func pid=143647)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=143647)[0m f1_macro: 0.23433822628902234
[2m[36m(func pid=143647)[0m f1_weighted: 0.41947248651147034
[2m[36m(func pid=143647)[0m f1_per_class: [0.184, 0.465, 0.128, 0.543, 0.115, 0.234, 0.513, 0.0, 0.051, 0.111]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.03264925373134328
[2m[36m(func pid=144880)[0m top5: 0.652518656716418
[2m[36m(func pid=144880)[0m f1_micro: 0.03264925373134328
[2m[36m(func pid=144880)[0m f1_macro: 0.019490389226836664
[2m[36m(func pid=144880)[0m f1_weighted: 0.025850307376895445
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.064, 0.115, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.22061567164179105
[2m[36m(func pid=144027)[0m top5: 0.7541977611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=144027)[0m f1_macro: 0.16019883614393668
[2m[36m(func pid=144027)[0m f1_weighted: 0.2251170708143355
[2m[36m(func pid=144027)[0m f1_per_class: [0.123, 0.383, 0.0, 0.408, 0.032, 0.227, 0.038, 0.0, 0.042, 0.35]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7086 | Steps: 4 | Val loss: 2.3079 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6231 | Steps: 4 | Val loss: 1.9453 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.0292 | Steps: 4 | Val loss: 29.9121 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.0492 | Steps: 4 | Val loss: 5.2443 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:58:08 (running for 00:05:01.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.368 |      0.234 |                   45 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.624 |      0.16  |                   45 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.709 |      0.191 |                   46 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.622 |      0.019 |                   43 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.25513059701492535
[2m[36m(func pid=144451)[0m top5: 0.8208955223880597
[2m[36m(func pid=144451)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=144451)[0m f1_macro: 0.19101074530528245
[2m[36m(func pid=144451)[0m f1_weighted: 0.27092829738278307
[2m[36m(func pid=144451)[0m f1_per_class: [0.105, 0.251, 0.222, 0.349, 0.137, 0.0, 0.339, 0.39, 0.045, 0.073]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.355410447761194
[2m[36m(func pid=143647)[0m top5: 0.8507462686567164
[2m[36m(func pid=143647)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.21710250644849735
[2m[36m(func pid=143647)[0m f1_weighted: 0.39306708407426594
[2m[36m(func pid=143647)[0m f1_per_class: [0.127, 0.448, 0.128, 0.507, 0.115, 0.172, 0.493, 0.0, 0.079, 0.103]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.22994402985074627
[2m[36m(func pid=144880)[0m top5: 0.65625
[2m[36m(func pid=144880)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=144880)[0m f1_macro: 0.0860900330612605
[2m[36m(func pid=144880)[0m f1_weighted: 0.1842315413896713
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.055, 0.476, 0.0, 0.0, 0.14, 0.125, 0.064, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2234141791044776
[2m[36m(func pid=144027)[0m top5: 0.7877798507462687
[2m[36m(func pid=144027)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=144027)[0m f1_macro: 0.17399026783204813
[2m[36m(func pid=144027)[0m f1_weighted: 0.23998552922230254
[2m[36m(func pid=144027)[0m f1_per_class: [0.111, 0.396, 0.0, 0.412, 0.062, 0.262, 0.03, 0.187, 0.049, 0.23]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7266 | Steps: 4 | Val loss: 2.3727 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5771 | Steps: 4 | Val loss: 2.0151 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6496 | Steps: 4 | Val loss: 17.1322 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:58:14 (running for 00:05:06.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.623 |      0.217 |                   46 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.049 |      0.174 |                   46 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.727 |      0.168 |                   47 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.029 |      0.086 |                   44 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5830 | Steps: 4 | Val loss: 3.4497 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=144451)[0m top1: 0.25093283582089554
[2m[36m(func pid=144451)[0m top5: 0.8176305970149254
[2m[36m(func pid=144451)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=144451)[0m f1_macro: 0.16799516942879486
[2m[36m(func pid=144451)[0m f1_weighted: 0.2601865133604647
[2m[36m(func pid=144451)[0m f1_per_class: [0.107, 0.184, 0.0, 0.27, 0.153, 0.0, 0.411, 0.433, 0.048, 0.074]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.33722014925373134
[2m[36m(func pid=143647)[0m top5: 0.8577425373134329
[2m[36m(func pid=143647)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=143647)[0m f1_macro: 0.2184174138768201
[2m[36m(func pid=143647)[0m f1_weighted: 0.38049131206046677
[2m[36m(func pid=143647)[0m f1_per_class: [0.143, 0.422, 0.116, 0.503, 0.11, 0.191, 0.454, 0.0, 0.15, 0.095]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.29850746268656714
[2m[36m(func pid=144880)[0m top5: 0.6487873134328358
[2m[36m(func pid=144880)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=144880)[0m f1_macro: 0.11064867751372105
[2m[36m(func pid=144880)[0m f1_weighted: 0.23988147041436916
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.091, 0.509, 0.0, 0.0, 0.286, 0.197, 0.024, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.292910447761194
[2m[36m(func pid=144027)[0m top5: 0.8470149253731343
[2m[36m(func pid=144027)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=144027)[0m f1_macro: 0.2222252083528251
[2m[36m(func pid=144027)[0m f1_weighted: 0.29149181186279083
[2m[36m(func pid=144027)[0m f1_per_class: [0.157, 0.529, 0.0, 0.41, 0.032, 0.24, 0.079, 0.434, 0.117, 0.224]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.1037 | Steps: 4 | Val loss: 2.8457 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5692 | Steps: 4 | Val loss: 2.0042 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4588 | Steps: 4 | Val loss: 10.9378 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:58:19 (running for 00:05:11.62)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.577 |      0.218 |                   47 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.583 |      0.222 |                   47 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.104 |      0.151 |                   48 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.65  |      0.111 |                   45 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1833 | Steps: 4 | Val loss: 3.2214 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=144451)[0m top1: 0.22108208955223882
[2m[36m(func pid=144451)[0m top5: 0.8236940298507462
[2m[36m(func pid=144451)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=144451)[0m f1_macro: 0.15061642118090163
[2m[36m(func pid=144451)[0m f1_weighted: 0.2253506207593065
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.252, 0.0, 0.295, 0.091, 0.0, 0.251, 0.333, 0.025, 0.159]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.35261194029850745
[2m[36m(func pid=143647)[0m top5: 0.8652052238805971
[2m[36m(func pid=143647)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=143647)[0m f1_macro: 0.26037393993310814
[2m[36m(func pid=143647)[0m f1_weighted: 0.40837870451055297
[2m[36m(func pid=143647)[0m f1_per_class: [0.167, 0.451, 0.122, 0.469, 0.111, 0.207, 0.49, 0.319, 0.171, 0.097]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2798507462686567
[2m[36m(func pid=144027)[0m top5: 0.863339552238806
[2m[36m(func pid=144027)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=144027)[0m f1_macro: 0.19451968879854734
[2m[36m(func pid=144027)[0m f1_weighted: 0.2611226690988033
[2m[36m(func pid=144027)[0m f1_per_class: [0.19, 0.516, 0.0, 0.361, 0.045, 0.157, 0.084, 0.334, 0.093, 0.165]
[2m[36m(func pid=144880)[0m top1: 0.30830223880597013
[2m[36m(func pid=144880)[0m top5: 0.7630597014925373
[2m[36m(func pid=144880)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=144880)[0m f1_macro: 0.1283998793451875
[2m[36m(func pid=144880)[0m f1_weighted: 0.22946428260561286
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.141, 0.516, 0.0, 0.118, 0.179, 0.279, 0.05, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.9919 | Steps: 4 | Val loss: 3.7671 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.4616 | Steps: 4 | Val loss: 2.0465 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:58:24 (running for 00:05:17.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.569 |      0.26  |                   48 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.183 |      0.195 |                   48 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.992 |      0.177 |                   49 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.459 |      0.128 |                   46 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.259794776119403
[2m[36m(func pid=144451)[0m top5: 0.8092350746268657
[2m[36m(func pid=144451)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=144451)[0m f1_macro: 0.17689860371611793
[2m[36m(func pid=144451)[0m f1_weighted: 0.2667752786731917
[2m[36m(func pid=144451)[0m f1_per_class: [0.122, 0.34, 0.0, 0.348, 0.083, 0.013, 0.283, 0.331, 0.0, 0.247]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.0443 | Steps: 4 | Val loss: 3.2632 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7059 | Steps: 4 | Val loss: 10.7635 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=143647)[0m top1: 0.32975746268656714
[2m[36m(func pid=143647)[0m top5: 0.8624067164179104
[2m[36m(func pid=143647)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=143647)[0m f1_macro: 0.26815513856496753
[2m[36m(func pid=143647)[0m f1_weighted: 0.3809663124364644
[2m[36m(func pid=143647)[0m f1_per_class: [0.261, 0.471, 0.155, 0.434, 0.103, 0.222, 0.394, 0.382, 0.168, 0.09]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.3185634328358209
[2m[36m(func pid=144880)[0m top5: 0.6431902985074627
[2m[36m(func pid=144880)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=144880)[0m f1_macro: 0.13300120616589634
[2m[36m(func pid=144880)[0m f1_weighted: 0.23660969945407656
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.158, 0.53, 0.0, 0.147, 0.182, 0.263, 0.05, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2453358208955224
[2m[36m(func pid=144027)[0m top5: 0.8647388059701493
[2m[36m(func pid=144027)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=144027)[0m f1_macro: 0.22878862906088515
[2m[36m(func pid=144027)[0m f1_weighted: 0.2295744003922294
[2m[36m(func pid=144027)[0m f1_per_class: [0.278, 0.46, 0.429, 0.255, 0.061, 0.088, 0.129, 0.272, 0.125, 0.192]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1840 | Steps: 4 | Val loss: 3.6368 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6497 | Steps: 4 | Val loss: 2.0196 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=144451)[0m top1: 0.30923507462686567
[2m[36m(func pid=144451)[0m top5: 0.8078358208955224
[2m[36m(func pid=144451)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=144451)[0m f1_macro: 0.21237911667939285
[2m[36m(func pid=144451)[0m f1_weighted: 0.31453984693584114
[2m[36m(func pid=144451)[0m f1_per_class: [0.117, 0.361, 0.0, 0.356, 0.082, 0.012, 0.404, 0.408, 0.021, 0.364]
== Status ==
Current time: 2024-01-07 12:58:30 (running for 00:05:22.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.462 |      0.268 |                   49 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  2.044 |      0.229 |                   49 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.184 |      0.212 |                   50 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.706 |      0.133 |                   47 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.9663 | Steps: 4 | Val loss: 3.0117 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8415 | Steps: 4 | Val loss: 12.8262 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=143647)[0m top1: 0.3260261194029851
[2m[36m(func pid=143647)[0m top5: 0.8591417910447762
[2m[36m(func pid=143647)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=143647)[0m f1_macro: 0.28291436429782857
[2m[36m(func pid=143647)[0m f1_weighted: 0.36457320934635573
[2m[36m(func pid=143647)[0m f1_per_class: [0.345, 0.484, 0.169, 0.392, 0.103, 0.257, 0.332, 0.484, 0.167, 0.095]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.1166044776119403
[2m[36m(func pid=144880)[0m top5: 0.6231343283582089
[2m[36m(func pid=144880)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=144880)[0m f1_macro: 0.0904190022452668
[2m[36m(func pid=144880)[0m f1_weighted: 0.12626704196109972
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.005, 0.15, 0.19, 0.0, 0.166, 0.131, 0.183, 0.078, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2555970149253731
[2m[36m(func pid=144027)[0m top5: 0.8530783582089553
[2m[36m(func pid=144027)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=144027)[0m f1_macro: 0.2320071250164948
[2m[36m(func pid=144027)[0m f1_weighted: 0.2346689162835834
[2m[36m(func pid=144027)[0m f1_per_class: [0.354, 0.44, 0.357, 0.151, 0.086, 0.073, 0.252, 0.292, 0.137, 0.179]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6864 | Steps: 4 | Val loss: 3.3282 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.2036 | Steps: 4 | Val loss: 1.8110 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:58:35 (running for 00:05:27.91)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.65  |      0.283 |                   50 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.966 |      0.232 |                   50 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.686 |      0.239 |                   51 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.841 |      0.09  |                   48 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.353544776119403
[2m[36m(func pid=144451)[0m top5: 0.8083022388059702
[2m[36m(func pid=144451)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=144451)[0m f1_macro: 0.23876633148217802
[2m[36m(func pid=144451)[0m f1_weighted: 0.3402452503386416
[2m[36m(func pid=144451)[0m f1_per_class: [0.112, 0.32, 0.098, 0.304, 0.09, 0.013, 0.535, 0.526, 0.044, 0.346]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.9404 | Steps: 4 | Val loss: 22.8223 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6939 | Steps: 4 | Val loss: 2.9248 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=143647)[0m top1: 0.35027985074626866
[2m[36m(func pid=143647)[0m top5: 0.8763992537313433
[2m[36m(func pid=143647)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=143647)[0m f1_macro: 0.30245218382717315
[2m[36m(func pid=143647)[0m f1_weighted: 0.3607441964387976
[2m[36m(func pid=143647)[0m f1_per_class: [0.343, 0.49, 0.189, 0.366, 0.098, 0.362, 0.293, 0.494, 0.167, 0.222]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.29197761194029853
[2m[36m(func pid=144027)[0m top5: 0.8596082089552238
[2m[36m(func pid=144027)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=144027)[0m f1_macro: 0.2118610679113866
[2m[36m(func pid=144027)[0m f1_weighted: 0.23145657322020322
[2m[36m(func pid=144027)[0m f1_per_class: [0.4, 0.471, 0.0, 0.026, 0.093, 0.032, 0.345, 0.345, 0.149, 0.258]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.08115671641791045
[2m[36m(func pid=144880)[0m top5: 0.628731343283582
[2m[36m(func pid=144880)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=144880)[0m f1_macro: 0.0678360490757133
[2m[36m(func pid=144880)[0m f1_weighted: 0.0792610655886746
[2m[36m(func pid=144880)[0m f1_per_class: [0.05, 0.021, 0.095, 0.026, 0.0, 0.226, 0.109, 0.141, 0.011, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3677 | Steps: 4 | Val loss: 2.7131 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4794 | Steps: 4 | Val loss: 1.8115 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:58:41 (running for 00:05:33.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.204 |      0.302 |                   51 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.694 |      0.212 |                   51 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.368 |      0.261 |                   52 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.94  |      0.068 |                   49 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.38013059701492535
[2m[36m(func pid=144451)[0m top5: 0.8269589552238806
[2m[36m(func pid=144451)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=144451)[0m f1_macro: 0.2605952791484242
[2m[36m(func pid=144451)[0m f1_weighted: 0.3647037778044453
[2m[36m(func pid=144451)[0m f1_per_class: [0.075, 0.317, 0.292, 0.352, 0.081, 0.008, 0.571, 0.524, 0.109, 0.278]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5474 | Steps: 4 | Val loss: 3.1777 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.9964 | Steps: 4 | Val loss: 20.9385 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=143647)[0m top1: 0.3292910447761194
[2m[36m(func pid=143647)[0m top5: 0.8810634328358209
[2m[36m(func pid=143647)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.3030524243826972
[2m[36m(func pid=143647)[0m f1_weighted: 0.3099755246105224
[2m[36m(func pid=143647)[0m f1_per_class: [0.322, 0.488, 0.308, 0.319, 0.109, 0.392, 0.168, 0.392, 0.192, 0.341]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3512126865671642
[2m[36m(func pid=144027)[0m top5: 0.8498134328358209
[2m[36m(func pid=144027)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=144027)[0m f1_macro: 0.21125301087790596
[2m[36m(func pid=144027)[0m f1_weighted: 0.27489715488917965
[2m[36m(func pid=144027)[0m f1_per_class: [0.118, 0.476, 0.0, 0.0, 0.146, 0.008, 0.519, 0.435, 0.181, 0.23]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.07929104477611941
[2m[36m(func pid=144880)[0m top5: 0.5970149253731343
[2m[36m(func pid=144880)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=144880)[0m f1_macro: 0.07376688721289047
[2m[36m(func pid=144880)[0m f1_weighted: 0.0780385862814868
[2m[36m(func pid=144880)[0m f1_per_class: [0.145, 0.026, 0.101, 0.0, 0.026, 0.212, 0.135, 0.092, 0.0, 0.0]
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1255 | Steps: 4 | Val loss: 2.4114 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.2162 | Steps: 4 | Val loss: 1.8985 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:58:46 (running for 00:05:38.65)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.479 |      0.303 |                   52 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.547 |      0.211 |                   52 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.126 |      0.232 |                   53 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.996 |      0.074 |                   50 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.40158582089552236
[2m[36m(func pid=144451)[0m top5: 0.8577425373134329
[2m[36m(func pid=144451)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=144451)[0m f1_macro: 0.23222223871845324
[2m[36m(func pid=144451)[0m f1_weighted: 0.3949471260856083
[2m[36m(func pid=144451)[0m f1_per_class: [0.066, 0.17, 0.0, 0.5, 0.08, 0.043, 0.61, 0.536, 0.13, 0.188]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8683 | Steps: 4 | Val loss: 2.9474 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.0673 | Steps: 4 | Val loss: 17.8035 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=143647)[0m top1: 0.3031716417910448
[2m[36m(func pid=143647)[0m top5: 0.8656716417910447
[2m[36m(func pid=143647)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=143647)[0m f1_macro: 0.28925310722032227
[2m[36m(func pid=143647)[0m f1_weighted: 0.25492538520039887
[2m[36m(func pid=143647)[0m f1_per_class: [0.29, 0.499, 0.444, 0.263, 0.116, 0.379, 0.039, 0.37, 0.178, 0.313]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3628731343283582
[2m[36m(func pid=144027)[0m top5: 0.8568097014925373
[2m[36m(func pid=144027)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=144027)[0m f1_macro: 0.19488097792291453
[2m[36m(func pid=144027)[0m f1_weighted: 0.2910981660534775
[2m[36m(func pid=144027)[0m f1_per_class: [0.122, 0.463, 0.0, 0.0, 0.134, 0.061, 0.611, 0.189, 0.171, 0.197]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0739 | Steps: 4 | Val loss: 2.3281 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=144880)[0m top1: 0.0853544776119403
[2m[36m(func pid=144880)[0m top5: 0.6655783582089553
[2m[36m(func pid=144880)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=144880)[0m f1_macro: 0.07458298387195081
[2m[36m(func pid=144880)[0m f1_weighted: 0.1032603827045852
[2m[36m(func pid=144880)[0m f1_per_class: [0.058, 0.085, 0.091, 0.0, 0.027, 0.17, 0.209, 0.082, 0.0, 0.024]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5031 | Steps: 4 | Val loss: 1.9987 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 12:58:51 (running for 00:05:44.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.216 |      0.289 |                   53 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.868 |      0.195 |                   53 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.074 |      0.225 |                   54 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.067 |      0.075 |                   51 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3689365671641791
[2m[36m(func pid=144451)[0m top5: 0.8568097014925373
[2m[36m(func pid=144451)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=144451)[0m f1_macro: 0.22467957960444068
[2m[36m(func pid=144451)[0m f1_weighted: 0.3621353017208229
[2m[36m(func pid=144451)[0m f1_per_class: [0.033, 0.021, 0.0, 0.486, 0.099, 0.106, 0.573, 0.545, 0.117, 0.267]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5485 | Steps: 4 | Val loss: 2.6906 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6269 | Steps: 4 | Val loss: 11.8067 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=143647)[0m top1: 0.2971082089552239
[2m[36m(func pid=143647)[0m top5: 0.8540111940298507
[2m[36m(func pid=143647)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=143647)[0m f1_macro: 0.2917745760340261
[2m[36m(func pid=143647)[0m f1_weighted: 0.23158047528057618
[2m[36m(func pid=143647)[0m f1_per_class: [0.269, 0.517, 0.606, 0.204, 0.131, 0.39, 0.003, 0.378, 0.15, 0.269]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m top1: 0.32649253731343286
[2m[36m(func pid=144027)[0m top5: 0.8736007462686567
[2m[36m(func pid=144027)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=144027)[0m f1_macro: 0.20647985228019222
[2m[36m(func pid=144027)[0m f1_weighted: 0.29840967072055835
[2m[36m(func pid=144027)[0m f1_per_class: [0.216, 0.453, 0.0, 0.003, 0.095, 0.357, 0.557, 0.0, 0.148, 0.235]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0256 | Steps: 4 | Val loss: 2.3445 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=144880)[0m top1: 0.1142723880597015
[2m[36m(func pid=144880)[0m top5: 0.6721082089552238
[2m[36m(func pid=144880)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=144880)[0m f1_macro: 0.08149576415054896
[2m[36m(func pid=144880)[0m f1_weighted: 0.1413835963377022
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.123, 0.078, 0.0, 0.029, 0.106, 0.337, 0.114, 0.0, 0.027]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3330 | Steps: 4 | Val loss: 2.0190 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:58:57 (running for 00:05:49.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.503 |      0.292 |                   54 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.549 |      0.206 |                   54 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.026 |      0.203 |                   55 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.627 |      0.081 |                   52 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2523320895522388
[2m[36m(func pid=144451)[0m top5: 0.840018656716418
[2m[36m(func pid=144451)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=144451)[0m f1_macro: 0.20286418932334258
[2m[36m(func pid=144451)[0m f1_weighted: 0.24557886382760447
[2m[36m(func pid=144451)[0m f1_per_class: [0.028, 0.0, 0.063, 0.325, 0.118, 0.328, 0.281, 0.411, 0.111, 0.364]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2585 | Steps: 4 | Val loss: 2.5484 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6253 | Steps: 4 | Val loss: 6.6379 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=143647)[0m top1: 0.2943097014925373
[2m[36m(func pid=143647)[0m top5: 0.8582089552238806
[2m[36m(func pid=143647)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=143647)[0m f1_macro: 0.30082018312780345
[2m[36m(func pid=143647)[0m f1_weighted: 0.22857079654697174
[2m[36m(func pid=143647)[0m f1_per_class: [0.269, 0.506, 0.69, 0.203, 0.134, 0.389, 0.0, 0.36, 0.166, 0.292]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1348 | Steps: 4 | Val loss: 2.5720 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=144880)[0m top1: 0.10587686567164178
[2m[36m(func pid=144880)[0m top5: 0.6674440298507462
[2m[36m(func pid=144880)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=144880)[0m f1_macro: 0.078935444921718
[2m[36m(func pid=144880)[0m f1_weighted: 0.10842565081855432
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.101, 0.13, 0.0, 0.028, 0.0, 0.247, 0.283, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.26399253731343286
[2m[36m(func pid=144027)[0m top5: 0.9034514925373134
[2m[36m(func pid=144027)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=144027)[0m f1_macro: 0.22598749374212188
[2m[36m(func pid=144027)[0m f1_weighted: 0.2745059944485338
[2m[36m(func pid=144027)[0m f1_per_class: [0.199, 0.402, 0.112, 0.143, 0.082, 0.311, 0.379, 0.0, 0.194, 0.438]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7934 | Steps: 4 | Val loss: 2.1145 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:59:02 (running for 00:05:54.79)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.333 |      0.301 |                   55 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.258 |      0.226 |                   55 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.135 |      0.172 |                   56 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.625 |      0.079 |                   53 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.1982276119402985
[2m[36m(func pid=144451)[0m top5: 0.835820895522388
[2m[36m(func pid=144451)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=144451)[0m f1_macro: 0.17207710607652432
[2m[36m(func pid=144451)[0m f1_weighted: 0.13851656939853235
[2m[36m(func pid=144451)[0m f1_per_class: [0.067, 0.0, 0.086, 0.198, 0.122, 0.346, 0.03, 0.414, 0.119, 0.338]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.3171 | Steps: 4 | Val loss: 9.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2837 | Steps: 4 | Val loss: 2.7032 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=143647)[0m top1: 0.27845149253731344
[2m[36m(func pid=143647)[0m top5: 0.8619402985074627
[2m[36m(func pid=143647)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=143647)[0m f1_macro: 0.29371054581232675
[2m[36m(func pid=143647)[0m f1_weighted: 0.21086173593183516
[2m[36m(func pid=143647)[0m f1_per_class: [0.278, 0.475, 0.72, 0.161, 0.112, 0.387, 0.0, 0.349, 0.167, 0.288]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.9356 | Steps: 4 | Val loss: 2.5065 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=144027)[0m top1: 0.269589552238806
[2m[36m(func pid=144027)[0m top5: 0.8936567164179104
[2m[36m(func pid=144027)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=144027)[0m f1_macro: 0.234189284684363
[2m[36m(func pid=144027)[0m f1_weighted: 0.3050636994377093
[2m[36m(func pid=144027)[0m f1_per_class: [0.19, 0.214, 0.084, 0.376, 0.083, 0.292, 0.337, 0.269, 0.151, 0.346]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.09235074626865672
[2m[36m(func pid=144880)[0m top5: 0.644589552238806
[2m[36m(func pid=144880)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=144880)[0m f1_macro: 0.09473011580418103
[2m[36m(func pid=144880)[0m f1_weighted: 0.08316864482069031
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.083, 0.364, 0.0, 0.027, 0.0, 0.176, 0.231, 0.0, 0.068]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6340 | Steps: 4 | Val loss: 2.1522 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:59:07 (running for 00:06:00.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.793 |      0.294 |                   56 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.284 |      0.234 |                   56 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.936 |      0.181 |                   57 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.317 |      0.095 |                   54 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.20289179104477612
[2m[36m(func pid=144451)[0m top5: 0.8423507462686567
[2m[36m(func pid=144451)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=144451)[0m f1_macro: 0.18124682606676606
[2m[36m(func pid=144451)[0m f1_weighted: 0.14848249415034975
[2m[36m(func pid=144451)[0m f1_per_class: [0.131, 0.103, 0.096, 0.196, 0.096, 0.315, 0.009, 0.449, 0.114, 0.303]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5042 | Steps: 4 | Val loss: 2.9097 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 4.3903 | Steps: 4 | Val loss: 3.7638 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=143647)[0m top1: 0.27705223880597013
[2m[36m(func pid=143647)[0m top5: 0.8460820895522388
[2m[36m(func pid=143647)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=143647)[0m f1_macro: 0.2678077526296363
[2m[36m(func pid=143647)[0m f1_weighted: 0.20576837136385356
[2m[36m(func pid=143647)[0m f1_per_class: [0.248, 0.503, 0.606, 0.143, 0.125, 0.358, 0.0, 0.386, 0.123, 0.185]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4497 | Steps: 4 | Val loss: 2.5006 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=144027)[0m top1: 0.27658582089552236
[2m[36m(func pid=144027)[0m top5: 0.871268656716418
[2m[36m(func pid=144027)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=144027)[0m f1_macro: 0.23444259378127144
[2m[36m(func pid=144027)[0m f1_weighted: 0.2911449310064218
[2m[36m(func pid=144027)[0m f1_per_class: [0.183, 0.15, 0.1, 0.419, 0.088, 0.301, 0.25, 0.475, 0.128, 0.251]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144880)[0m top1: 0.1394589552238806
[2m[36m(func pid=144880)[0m top5: 0.6184701492537313
[2m[36m(func pid=144880)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=144880)[0m f1_macro: 0.13359102536807604
[2m[36m(func pid=144880)[0m f1_weighted: 0.15700382186347123
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.121, 0.429, 0.0, 0.022, 0.0, 0.375, 0.368, 0.0, 0.021]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3854 | Steps: 4 | Val loss: 2.3053 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:59:13 (running for 00:06:05.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.634 |      0.268 |                   57 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.504 |      0.234 |                   57 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.45  |      0.184 |                   58 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  4.39  |      0.134 |                   55 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.19962686567164178
[2m[36m(func pid=144451)[0m top5: 0.8376865671641791
[2m[36m(func pid=144451)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=144451)[0m f1_macro: 0.1841275947898512
[2m[36m(func pid=144451)[0m f1_weighted: 0.15189256117965183
[2m[36m(func pid=144451)[0m f1_per_class: [0.119, 0.228, 0.118, 0.144, 0.099, 0.31, 0.0, 0.47, 0.088, 0.266]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.1905 | Steps: 4 | Val loss: 2.8461 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5291 | Steps: 4 | Val loss: 2.9193 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=143647)[0m top1: 0.26026119402985076
[2m[36m(func pid=143647)[0m top5: 0.8213619402985075
[2m[36m(func pid=143647)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=143647)[0m f1_macro: 0.23542193019963592
[2m[36m(func pid=143647)[0m f1_weighted: 0.19805958340597607
[2m[36m(func pid=143647)[0m f1_per_class: [0.229, 0.507, 0.417, 0.149, 0.148, 0.288, 0.0, 0.423, 0.065, 0.13]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.5707 | Steps: 4 | Val loss: 2.3523 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=144880)[0m top1: 0.11800373134328358
[2m[36m(func pid=144880)[0m top5: 0.5345149253731343
[2m[36m(func pid=144880)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=144880)[0m f1_macro: 0.1511099940517005
[2m[36m(func pid=144880)[0m f1_weighted: 0.14403768682020512
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.014, 0.846, 0.0, 0.02, 0.0, 0.416, 0.216, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2905783582089552
[2m[36m(func pid=144027)[0m top5: 0.8638059701492538
[2m[36m(func pid=144027)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=144027)[0m f1_macro: 0.24169991154094567
[2m[36m(func pid=144027)[0m f1_weighted: 0.2776211593658371
[2m[36m(func pid=144027)[0m f1_per_class: [0.188, 0.152, 0.171, 0.46, 0.091, 0.317, 0.154, 0.47, 0.17, 0.244]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.7582 | Steps: 4 | Val loss: 2.7393 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:59:18 (running for 00:06:11.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.385 |      0.235 |                   58 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.191 |      0.242 |                   58 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.571 |      0.194 |                   59 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.529 |      0.151 |                   56 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.20382462686567165
[2m[36m(func pid=144451)[0m top5: 0.8418843283582089
[2m[36m(func pid=144451)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=144451)[0m f1_macro: 0.19400327622237853
[2m[36m(func pid=144451)[0m f1_weighted: 0.15980826853270788
[2m[36m(func pid=144451)[0m f1_per_class: [0.127, 0.344, 0.125, 0.111, 0.082, 0.284, 0.006, 0.44, 0.052, 0.369]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.1492 | Steps: 4 | Val loss: 2.6772 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=143647)[0m top1: 0.23833955223880596
[2m[36m(func pid=143647)[0m top5: 0.7793843283582089
[2m[36m(func pid=143647)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=143647)[0m f1_macro: 0.20652264249751875
[2m[36m(func pid=143647)[0m f1_weighted: 0.18771340534940306
[2m[36m(func pid=143647)[0m f1_per_class: [0.226, 0.524, 0.286, 0.148, 0.118, 0.175, 0.0, 0.453, 0.052, 0.084]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.2252 | Steps: 4 | Val loss: 2.8837 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.3033 | Steps: 4 | Val loss: 2.2991 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=144880)[0m top1: 0.10914179104477612
[2m[36m(func pid=144880)[0m top5: 0.507929104477612
[2m[36m(func pid=144880)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=144880)[0m f1_macro: 0.13476711476398853
[2m[36m(func pid=144880)[0m f1_weighted: 0.14011456458974872
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.632, 0.0, 0.019, 0.067, 0.383, 0.247, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.28638059701492535
[2m[36m(func pid=144027)[0m top5: 0.8488805970149254
[2m[36m(func pid=144027)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=144027)[0m f1_macro: 0.24307337366496556
[2m[36m(func pid=144027)[0m f1_weighted: 0.2524306141382878
[2m[36m(func pid=144027)[0m f1_per_class: [0.272, 0.186, 0.286, 0.49, 0.08, 0.278, 0.042, 0.391, 0.191, 0.215]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1355 | Steps: 4 | Val loss: 3.2437 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:59:24 (running for 00:06:16.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.758 |      0.207 |                   59 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.225 |      0.243 |                   59 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.303 |      0.191 |                   60 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.149 |      0.135 |                   57 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.21222014925373134
[2m[36m(func pid=144451)[0m top5: 0.8456156716417911
[2m[36m(func pid=144451)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=144451)[0m f1_macro: 0.19100364632631106
[2m[36m(func pid=144451)[0m f1_weighted: 0.16512107091622052
[2m[36m(func pid=144451)[0m f1_per_class: [0.142, 0.434, 0.159, 0.103, 0.069, 0.252, 0.006, 0.383, 0.023, 0.34]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.21082089552238806
[2m[36m(func pid=143647)[0m top5: 0.7378731343283582
[2m[36m(func pid=143647)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=143647)[0m f1_macro: 0.18748313602515393
[2m[36m(func pid=143647)[0m f1_weighted: 0.18018011301128056
[2m[36m(func pid=143647)[0m f1_per_class: [0.211, 0.527, 0.253, 0.165, 0.139, 0.074, 0.009, 0.41, 0.025, 0.061]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.8802 | Steps: 4 | Val loss: 5.0062 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5422 | Steps: 4 | Val loss: 3.1030 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8030 | Steps: 4 | Val loss: 2.2228 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=144880)[0m top1: 0.16324626865671643
[2m[36m(func pid=144880)[0m top5: 0.5345149253731343
[2m[36m(func pid=144880)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=144880)[0m f1_macro: 0.07440628283697562
[2m[36m(func pid=144880)[0m f1_weighted: 0.11423957409721641
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.213, 0.246, 0.285, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4818 | Steps: 4 | Val loss: 2.5846 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=144027)[0m top1: 0.2681902985074627
[2m[36m(func pid=144027)[0m top5: 0.8232276119402985
[2m[36m(func pid=144027)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=144027)[0m f1_macro: 0.24816216493312687
[2m[36m(func pid=144027)[0m f1_weighted: 0.24108157979006908
[2m[36m(func pid=144027)[0m f1_per_class: [0.309, 0.248, 0.361, 0.474, 0.085, 0.22, 0.006, 0.381, 0.16, 0.239]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:29 (running for 00:06:21.93)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.136 |      0.187 |                   60 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.542 |      0.248 |                   60 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.803 |      0.207 |                   61 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.88  |      0.074 |                   58 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.21735074626865672
[2m[36m(func pid=144451)[0m top5: 0.8493470149253731
[2m[36m(func pid=144451)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=144451)[0m f1_macro: 0.2071438608550749
[2m[36m(func pid=144451)[0m f1_weighted: 0.16594019272821914
[2m[36m(func pid=144451)[0m f1_per_class: [0.156, 0.456, 0.28, 0.106, 0.057, 0.213, 0.009, 0.353, 0.0, 0.442]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.25046641791044777
[2m[36m(func pid=143647)[0m top5: 0.8022388059701493
[2m[36m(func pid=143647)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=143647)[0m f1_macro: 0.21503011778313502
[2m[36m(func pid=143647)[0m f1_weighted: 0.24944047673769007
[2m[36m(func pid=143647)[0m f1_per_class: [0.192, 0.532, 0.198, 0.259, 0.102, 0.187, 0.104, 0.431, 0.038, 0.107]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.8917 | Steps: 4 | Val loss: 4.2953 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4426 | Steps: 4 | Val loss: 3.0738 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5030 | Steps: 4 | Val loss: 2.1909 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=144880)[0m top1: 0.13292910447761194
[2m[36m(func pid=144880)[0m top5: 0.784981343283582
[2m[36m(func pid=144880)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.053547251233133086
[2m[36m(func pid=144880)[0m f1_weighted: 0.05609875632901262
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.214, 0.054, 0.267, 0.0, 0.0]
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.0657 | Steps: 4 | Val loss: 1.9795 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.28451492537313433
[2m[36m(func pid=144027)[0m top5: 0.832089552238806
[2m[36m(func pid=144027)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=144027)[0m f1_macro: 0.2686178185348723
[2m[36m(func pid=144027)[0m f1_weighted: 0.2629230859751506
[2m[36m(func pid=144027)[0m f1_per_class: [0.392, 0.364, 0.306, 0.466, 0.085, 0.251, 0.006, 0.346, 0.175, 0.295]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:34 (running for 00:06:27.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.482 |      0.215 |                   61 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.443 |      0.269 |                   61 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.503 |      0.203 |                   62 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.892 |      0.054 |                   59 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.22994402985074627
[2m[36m(func pid=144451)[0m top5: 0.8432835820895522
[2m[36m(func pid=144451)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=144451)[0m f1_macro: 0.20295997171398508
[2m[36m(func pid=144451)[0m f1_weighted: 0.19934408385707458
[2m[36m(func pid=144451)[0m f1_per_class: [0.188, 0.443, 0.311, 0.11, 0.06, 0.085, 0.166, 0.419, 0.0, 0.248]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.33302238805970147
[2m[36m(func pid=143647)[0m top5: 0.8642723880597015
[2m[36m(func pid=143647)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=143647)[0m f1_macro: 0.2806447800692856
[2m[36m(func pid=143647)[0m f1_weighted: 0.3565042387899774
[2m[36m(func pid=143647)[0m f1_per_class: [0.204, 0.521, 0.167, 0.317, 0.086, 0.329, 0.346, 0.431, 0.099, 0.306]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.8760 | Steps: 4 | Val loss: 3.1023 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.4171 | Steps: 4 | Val loss: 2.8289 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.4729 | Steps: 4 | Val loss: 2.2090 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=144880)[0m top1: 0.06436567164179105
[2m[36m(func pid=144880)[0m top5: 0.808768656716418
[2m[36m(func pid=144880)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=144880)[0m f1_macro: 0.04827929470594559
[2m[36m(func pid=144880)[0m f1_weighted: 0.03251109007946314
[2m[36m(func pid=144880)[0m f1_per_class: [0.053, 0.0, 0.0, 0.0, 0.0, 0.111, 0.003, 0.296, 0.019, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.1201 | Steps: 4 | Val loss: 1.9283 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=144027)[0m top1: 0.31716417910447764
[2m[36m(func pid=144027)[0m top5: 0.8493470149253731
[2m[36m(func pid=144027)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=144027)[0m f1_macro: 0.28656581950237753
[2m[36m(func pid=144027)[0m f1_weighted: 0.29313101995703467
[2m[36m(func pid=144027)[0m f1_per_class: [0.321, 0.501, 0.349, 0.462, 0.093, 0.318, 0.009, 0.352, 0.177, 0.283]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:40 (running for 00:06:32.70)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.066 |      0.281 |                   62 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.417 |      0.287 |                   62 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.473 |      0.217 |                   63 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.876 |      0.048 |                   60 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.25326492537313433
[2m[36m(func pid=144451)[0m top5: 0.8325559701492538
[2m[36m(func pid=144451)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=144451)[0m f1_macro: 0.21712125746158786
[2m[36m(func pid=144451)[0m f1_weighted: 0.25506484167093824
[2m[36m(func pid=144451)[0m f1_per_class: [0.187, 0.407, 0.333, 0.16, 0.056, 0.016, 0.344, 0.453, 0.049, 0.167]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.35634328358208955
[2m[36m(func pid=143647)[0m top5: 0.8666044776119403
[2m[36m(func pid=143647)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=143647)[0m f1_macro: 0.2994143058391789
[2m[36m(func pid=143647)[0m f1_weighted: 0.38803995400639457
[2m[36m(func pid=143647)[0m f1_per_class: [0.225, 0.53, 0.134, 0.372, 0.077, 0.276, 0.405, 0.456, 0.114, 0.405]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.1233 | Steps: 4 | Val loss: 3.1068 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.8791 | Steps: 4 | Val loss: 2.8210 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.6377 | Steps: 4 | Val loss: 2.3495 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2722 | Steps: 4 | Val loss: 1.8664 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=144880)[0m top1: 0.2989738805970149
[2m[36m(func pid=144880)[0m top5: 0.7919776119402985
[2m[36m(func pid=144880)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=144880)[0m f1_macro: 0.08693104455484679
[2m[36m(func pid=144880)[0m f1_weighted: 0.16656425084912974
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.514, 0.0, 0.047, 0.0, 0.308, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.31343283582089554
[2m[36m(func pid=144027)[0m top5: 0.8610074626865671
[2m[36m(func pid=144027)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=144027)[0m f1_macro: 0.2801543433276083
[2m[36m(func pid=144027)[0m f1_weighted: 0.2948287364701193
[2m[36m(func pid=144027)[0m f1_per_class: [0.187, 0.564, 0.327, 0.395, 0.107, 0.339, 0.036, 0.371, 0.204, 0.272]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:45 (running for 00:06:38.11)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.12  |      0.299 |                   63 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.123 |      0.28  |                   63 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.638 |      0.235 |                   64 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.879 |      0.087 |                   61 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2644589552238806
[2m[36m(func pid=144451)[0m top5: 0.8115671641791045
[2m[36m(func pid=144451)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=144451)[0m f1_macro: 0.23473130366260747
[2m[36m(func pid=144451)[0m f1_weighted: 0.2836793985402092
[2m[36m(func pid=144451)[0m f1_per_class: [0.196, 0.377, 0.444, 0.173, 0.05, 0.0, 0.445, 0.48, 0.044, 0.137]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.37546641791044777
[2m[36m(func pid=143647)[0m top5: 0.8819962686567164
[2m[36m(func pid=143647)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=143647)[0m f1_macro: 0.30862503804649366
[2m[36m(func pid=143647)[0m f1_weighted: 0.4090222684047989
[2m[36m(func pid=143647)[0m f1_per_class: [0.29, 0.479, 0.15, 0.426, 0.07, 0.267, 0.45, 0.481, 0.116, 0.357]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.0164 | Steps: 4 | Val loss: 2.3340 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8478 | Steps: 4 | Val loss: 4.1962 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7328 | Steps: 4 | Val loss: 2.2146 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2277 | Steps: 4 | Val loss: 1.8748 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=144880)[0m top1: 0.30223880597014924
[2m[36m(func pid=144880)[0m top5: 0.800839552238806
[2m[36m(func pid=144880)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=144880)[0m f1_macro: 0.08925699815368585
[2m[36m(func pid=144880)[0m f1_weighted: 0.16480976129634262
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.516, 0.0, 0.0, 0.0, 0.339, 0.038, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.32276119402985076
[2m[36m(func pid=144027)[0m top5: 0.8544776119402985
[2m[36m(func pid=144027)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=144027)[0m f1_macro: 0.2940856799191195
[2m[36m(func pid=144027)[0m f1_weighted: 0.3576887395281641
[2m[36m(func pid=144027)[0m f1_per_class: [0.12, 0.525, 0.25, 0.309, 0.103, 0.314, 0.346, 0.461, 0.217, 0.296]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:51 (running for 00:06:43.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.272 |      0.309 |                   64 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.848 |      0.294 |                   64 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.733 |      0.27  |                   65 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.016 |      0.089 |                   62 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3125
[2m[36m(func pid=144451)[0m top5: 0.8460820895522388
[2m[36m(func pid=144451)[0m f1_micro: 0.3125
[2m[36m(func pid=144451)[0m f1_macro: 0.2702022788825529
[2m[36m(func pid=144451)[0m f1_weighted: 0.33221699765044044
[2m[36m(func pid=144451)[0m f1_per_class: [0.242, 0.328, 0.545, 0.229, 0.057, 0.0, 0.569, 0.516, 0.048, 0.167]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3712686567164179
[2m[36m(func pid=143647)[0m top5: 0.8894589552238806
[2m[36m(func pid=143647)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=143647)[0m f1_macro: 0.30114258673356653
[2m[36m(func pid=143647)[0m f1_weighted: 0.40503297193013726
[2m[36m(func pid=143647)[0m f1_per_class: [0.301, 0.402, 0.153, 0.411, 0.069, 0.28, 0.49, 0.494, 0.105, 0.306]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7707 | Steps: 4 | Val loss: 2.3616 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6821 | Steps: 4 | Val loss: 3.4057 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9188 | Steps: 4 | Val loss: 2.2254 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1929 | Steps: 4 | Val loss: 1.8576 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=144880)[0m top1: 0.07602611940298508
[2m[36m(func pid=144880)[0m top5: 0.8083022388059702
[2m[36m(func pid=144880)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=144880)[0m f1_macro: 0.04300504229933148
[2m[36m(func pid=144880)[0m f1_weighted: 0.023820594102216923
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.354, 0.073, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.355410447761194
[2m[36m(func pid=144027)[0m top5: 0.8656716417910447
[2m[36m(func pid=144027)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=144027)[0m f1_macro: 0.2902257310731876
[2m[36m(func pid=144027)[0m f1_weighted: 0.40030607177363775
[2m[36m(func pid=144027)[0m f1_per_class: [0.148, 0.494, 0.194, 0.408, 0.105, 0.319, 0.43, 0.398, 0.189, 0.219]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 12:59:56 (running for 00:06:48.98)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.228 |      0.301 |                   65 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.682 |      0.29  |                   65 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.919 |      0.272 |                   66 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.771 |      0.043 |                   63 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3199626865671642
[2m[36m(func pid=144451)[0m top5: 0.8652052238805971
[2m[36m(func pid=144451)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=144451)[0m f1_macro: 0.27166335206172737
[2m[36m(func pid=144451)[0m f1_weighted: 0.3377752386079817
[2m[36m(func pid=144451)[0m f1_per_class: [0.251, 0.275, 0.545, 0.26, 0.054, 0.0, 0.598, 0.414, 0.144, 0.174]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.38899253731343286
[2m[36m(func pid=143647)[0m top5: 0.8852611940298507
[2m[36m(func pid=143647)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=143647)[0m f1_macro: 0.31109099592088163
[2m[36m(func pid=143647)[0m f1_weighted: 0.42721872688765844
[2m[36m(func pid=143647)[0m f1_per_class: [0.333, 0.413, 0.142, 0.45, 0.07, 0.293, 0.514, 0.506, 0.078, 0.31]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.7225 | Steps: 4 | Val loss: 2.2433 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9477 | Steps: 4 | Val loss: 2.8335 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.4316 | Steps: 4 | Val loss: 2.1388 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.1849 | Steps: 4 | Val loss: 1.8240 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=144880)[0m top1: 0.07882462686567164
[2m[36m(func pid=144880)[0m top5: 0.6338619402985075
[2m[36m(func pid=144880)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=144880)[0m f1_macro: 0.045937617031590224
[2m[36m(func pid=144880)[0m f1_weighted: 0.03124179404085484
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027, 0.357, 0.076, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.33488805970149255
[2m[36m(func pid=144027)[0m top5: 0.8610074626865671
[2m[36m(func pid=144027)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=144027)[0m f1_macro: 0.2695520923914704
[2m[36m(func pid=144027)[0m f1_weighted: 0.3611736299852148
[2m[36m(func pid=144027)[0m f1_per_class: [0.223, 0.482, 0.32, 0.478, 0.093, 0.243, 0.298, 0.24, 0.17, 0.149]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 13:00:01 (running for 00:06:54.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.193 |      0.311 |                   66 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.948 |      0.27  |                   66 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.432 |      0.274 |                   67 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.722 |      0.046 |                   64 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3376865671641791
[2m[36m(func pid=144451)[0m top5: 0.875
[2m[36m(func pid=144451)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=144451)[0m f1_macro: 0.27395132081888984
[2m[36m(func pid=144451)[0m f1_weighted: 0.3573405010120429
[2m[36m(func pid=144451)[0m f1_per_class: [0.268, 0.25, 0.545, 0.364, 0.049, 0.0, 0.599, 0.312, 0.139, 0.213]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.40578358208955223
[2m[36m(func pid=143647)[0m top5: 0.8847947761194029
[2m[36m(func pid=143647)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=143647)[0m f1_macro: 0.3278044415296643
[2m[36m(func pid=143647)[0m f1_weighted: 0.44494050493727794
[2m[36m(func pid=143647)[0m f1_per_class: [0.377, 0.448, 0.131, 0.459, 0.075, 0.313, 0.53, 0.516, 0.109, 0.321]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6989 | Steps: 4 | Val loss: 2.3489 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4769 | Steps: 4 | Val loss: 2.6504 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2589 | Steps: 4 | Val loss: 2.1612 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9463 | Steps: 4 | Val loss: 1.7635 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=144880)[0m top1: 0.060167910447761194
[2m[36m(func pid=144880)[0m top5: 0.6296641791044776
[2m[36m(func pid=144880)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.046812647047067155
[2m[36m(func pid=144880)[0m f1_weighted: 0.04844719356739437
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.094, 0.337, 0.021, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3414179104477612
[2m[36m(func pid=144027)[0m top5: 0.8624067164179104
[2m[36m(func pid=144027)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=144027)[0m f1_macro: 0.29103748130895474
[2m[36m(func pid=144027)[0m f1_weighted: 0.35868858464556286
[2m[36m(func pid=144027)[0m f1_per_class: [0.345, 0.379, 0.408, 0.544, 0.095, 0.224, 0.262, 0.356, 0.167, 0.13]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 13:00:07 (running for 00:06:59.82)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.185 |      0.328 |                   67 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.477 |      0.291 |                   67 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.259 |      0.253 |                   68 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.699 |      0.047 |                   65 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.3353544776119403
[2m[36m(func pid=144451)[0m top5: 0.8628731343283582
[2m[36m(func pid=144451)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=144451)[0m f1_macro: 0.25303448404964585
[2m[36m(func pid=144451)[0m f1_weighted: 0.35736641141902054
[2m[36m(func pid=144451)[0m f1_per_class: [0.23, 0.221, 0.56, 0.48, 0.043, 0.0, 0.547, 0.146, 0.092, 0.212]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.4230410447761194
[2m[36m(func pid=143647)[0m top5: 0.8931902985074627
[2m[36m(func pid=143647)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.3451066067411274
[2m[36m(func pid=143647)[0m f1_weighted: 0.45826257861140324
[2m[36m(func pid=143647)[0m f1_per_class: [0.389, 0.444, 0.15, 0.497, 0.075, 0.308, 0.537, 0.517, 0.114, 0.42]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.7253 | Steps: 4 | Val loss: 2.5428 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8967 | Steps: 4 | Val loss: 2.5240 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7616 | Steps: 4 | Val loss: 2.4770 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1610 | Steps: 4 | Val loss: 1.7361 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=144880)[0m top1: 0.07929104477611941
[2m[36m(func pid=144880)[0m top5: 0.5503731343283582
[2m[36m(func pid=144880)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=144880)[0m f1_macro: 0.05716147655114073
[2m[36m(func pid=144880)[0m f1_weighted: 0.08627014773134713
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.225, 0.332, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.33722014925373134
[2m[36m(func pid=144027)[0m top5: 0.8731343283582089
[2m[36m(func pid=144027)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=144027)[0m f1_macro: 0.28072782920944633
[2m[36m(func pid=144027)[0m f1_weighted: 0.34513279245393863
[2m[36m(func pid=144027)[0m f1_per_class: [0.044, 0.357, 0.564, 0.522, 0.101, 0.252, 0.246, 0.418, 0.15, 0.152]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 13:00:12 (running for 00:07:05.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.946 |      0.345 |                   68 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.897 |      0.281 |                   68 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.762 |      0.234 |                   69 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.725 |      0.057 |                   66 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.29011194029850745
[2m[36m(func pid=144451)[0m top5: 0.8479477611940298
[2m[36m(func pid=144451)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=144451)[0m f1_macro: 0.2342633613614095
[2m[36m(func pid=144451)[0m f1_weighted: 0.30344351925660606
[2m[36m(func pid=144451)[0m f1_per_class: [0.253, 0.108, 0.621, 0.545, 0.052, 0.0, 0.363, 0.192, 0.089, 0.12]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.4361007462686567
[2m[36m(func pid=143647)[0m top5: 0.8927238805970149
[2m[36m(func pid=143647)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=143647)[0m f1_macro: 0.36002828964011746
[2m[36m(func pid=143647)[0m f1_weighted: 0.47029185827775466
[2m[36m(func pid=143647)[0m f1_per_class: [0.441, 0.456, 0.166, 0.524, 0.08, 0.331, 0.533, 0.488, 0.151, 0.43]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 3.4695 | Steps: 4 | Val loss: 2.7268 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8791 | Steps: 4 | Val loss: 2.6290 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.8945 | Steps: 4 | Val loss: 2.5077 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6168 | Steps: 4 | Val loss: 1.7063 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=144880)[0m top1: 0.0830223880597015
[2m[36m(func pid=144880)[0m top5: 0.5494402985074627
[2m[36m(func pid=144880)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=144880)[0m f1_macro: 0.06856231190033588
[2m[36m(func pid=144880)[0m f1_weighted: 0.08667814864346966
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.213, 0.383, 0.0, 0.074]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3031716417910448
[2m[36m(func pid=144027)[0m top5: 0.8610074626865671
[2m[36m(func pid=144027)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=144027)[0m f1_macro: 0.2530336384326318
[2m[36m(func pid=144027)[0m f1_weighted: 0.28560788517611213
[2m[36m(func pid=144027)[0m f1_per_class: [0.0, 0.411, 0.524, 0.483, 0.074, 0.231, 0.065, 0.399, 0.159, 0.183]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m top1: 0.2826492537313433
[2m[36m(func pid=144451)[0m top5: 0.8414179104477612
[2m[36m(func pid=144451)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=144451)[0m f1_macro: 0.2246585191807501
[2m[36m(func pid=144451)[0m f1_weighted: 0.2675226963244698
[2m[36m(func pid=144451)[0m f1_per_class: [0.202, 0.057, 0.511, 0.562, 0.059, 0.0, 0.217, 0.415, 0.09, 0.134]
[2m[36m(func pid=144451)[0m 
== Status ==
Current time: 2024-01-07 13:00:18 (running for 00:07:10.42)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.161 |      0.36  |                   69 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.879 |      0.253 |                   69 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.894 |      0.225 |                   70 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.469 |      0.069 |                   67 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.44636194029850745
[2m[36m(func pid=143647)[0m top5: 0.9020522388059702
[2m[36m(func pid=143647)[0m f1_micro: 0.44636194029850745
[2m[36m(func pid=143647)[0m f1_macro: 0.3754508264510908
[2m[36m(func pid=143647)[0m f1_weighted: 0.4768885874759195
[2m[36m(func pid=143647)[0m f1_per_class: [0.479, 0.431, 0.293, 0.534, 0.077, 0.349, 0.545, 0.522, 0.129, 0.395]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.8154 | Steps: 4 | Val loss: 2.7114 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6368 | Steps: 4 | Val loss: 2.7455 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7940 | Steps: 4 | Val loss: 2.3006 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1432 | Steps: 4 | Val loss: 1.6805 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=144880)[0m top1: 0.0960820895522388
[2m[36m(func pid=144880)[0m top5: 0.2579291044776119
[2m[36m(func pid=144880)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=144880)[0m f1_macro: 0.08320673386781656
[2m[36m(func pid=144880)[0m f1_weighted: 0.11985464808767188
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.311, 0.46, 0.0, 0.047]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2887126865671642
[2m[36m(func pid=144027)[0m top5: 0.8614738805970149
[2m[36m(func pid=144027)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=144027)[0m f1_macro: 0.2307483021780862
[2m[36m(func pid=144027)[0m f1_weighted: 0.2600565499788885
[2m[36m(func pid=144027)[0m f1_per_class: [0.043, 0.398, 0.286, 0.453, 0.075, 0.246, 0.009, 0.373, 0.209, 0.214]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 13:00:23 (running for 00:07:15.59)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.617 |      0.375 |                   70 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.637 |      0.231 |                   70 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.794 |      0.23  |                   71 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.815 |      0.083 |                   68 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.2989738805970149
[2m[36m(func pid=144451)[0m top5: 0.855410447761194
[2m[36m(func pid=144451)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=144451)[0m f1_macro: 0.22970715946475745
[2m[36m(func pid=144451)[0m f1_weighted: 0.2754792725091729
[2m[36m(func pid=144451)[0m f1_per_class: [0.209, 0.005, 0.364, 0.552, 0.084, 0.165, 0.219, 0.406, 0.112, 0.181]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.43703358208955223
[2m[36m(func pid=143647)[0m top5: 0.9043843283582089
[2m[36m(func pid=143647)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=143647)[0m f1_macro: 0.37737938960133943
[2m[36m(func pid=143647)[0m f1_weighted: 0.4597974416271078
[2m[36m(func pid=143647)[0m f1_per_class: [0.432, 0.421, 0.369, 0.541, 0.08, 0.329, 0.494, 0.515, 0.145, 0.447]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.8408 | Steps: 4 | Val loss: 2.6590 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3733 | Steps: 4 | Val loss: 2.7322 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.5908 | Steps: 4 | Val loss: 2.3828 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9922 | Steps: 4 | Val loss: 1.6843 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=144880)[0m top1: 0.06623134328358209
[2m[36m(func pid=144880)[0m top5: 0.37826492537313433
[2m[36m(func pid=144880)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=144880)[0m f1_macro: 0.06975098354701983
[2m[36m(func pid=144880)[0m f1_weighted: 0.08524017446536283
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.194, 0.468, 0.0, 0.021]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.28638059701492535
[2m[36m(func pid=144027)[0m top5: 0.8731343283582089
[2m[36m(func pid=144027)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=144027)[0m f1_macro: 0.26605004377041336
[2m[36m(func pid=144027)[0m f1_weighted: 0.2579082963234353
[2m[36m(func pid=144027)[0m f1_per_class: [0.232, 0.4, 0.393, 0.412, 0.093, 0.31, 0.003, 0.353, 0.198, 0.268]
[2m[36m(func pid=144027)[0m 
== Status ==
Current time: 2024-01-07 13:00:28 (running for 00:07:20.99)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.143 |      0.377 |                   71 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.373 |      0.266 |                   71 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.591 |      0.194 |                   72 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.841 |      0.07  |                   69 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.28824626865671643
[2m[36m(func pid=144451)[0m top5: 0.8572761194029851
[2m[36m(func pid=144451)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=144451)[0m f1_macro: 0.1942567040793891
[2m[36m(func pid=144451)[0m f1_weighted: 0.24306356465704068
[2m[36m(func pid=144451)[0m f1_per_class: [0.169, 0.0, 0.0, 0.544, 0.094, 0.332, 0.078, 0.289, 0.19, 0.247]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.4291044776119403
[2m[36m(func pid=143647)[0m top5: 0.9034514925373134
[2m[36m(func pid=143647)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=143647)[0m f1_macro: 0.3867365313435472
[2m[36m(func pid=143647)[0m f1_weighted: 0.44521177362984565
[2m[36m(func pid=143647)[0m f1_per_class: [0.469, 0.479, 0.48, 0.546, 0.076, 0.292, 0.424, 0.475, 0.134, 0.492]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.7651 | Steps: 4 | Val loss: 2.8426 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6064 | Steps: 4 | Val loss: 2.5865 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.4734 | Steps: 4 | Val loss: 2.6537 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9548 | Steps: 4 | Val loss: 1.7305 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=144880)[0m top1: 0.06949626865671642
[2m[36m(func pid=144880)[0m top5: 0.3810634328358209
[2m[36m(func pid=144880)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=144880)[0m f1_macro: 0.07032055954373805
[2m[36m(func pid=144880)[0m f1_weighted: 0.08297641405113626
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.498, 0.0, 0.025]
[2m[36m(func pid=144880)[0m 
== Status ==
Current time: 2024-01-07 13:00:33 (running for 00:07:26.12)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.992 |      0.387 |                   72 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.765 |      0.282 |                   72 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.591 |      0.194 |                   72 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.606 |      0.07  |                   70 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144027)[0m top1: 0.28404850746268656
[2m[36m(func pid=144027)[0m top5: 0.8675373134328358
[2m[36m(func pid=144027)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=144027)[0m f1_macro: 0.2819894996197386
[2m[36m(func pid=144027)[0m f1_weighted: 0.25923613059856354
[2m[36m(func pid=144027)[0m f1_per_class: [0.195, 0.416, 0.458, 0.382, 0.078, 0.333, 0.012, 0.363, 0.206, 0.378]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m top1: 0.2630597014925373
[2m[36m(func pid=144451)[0m top5: 0.8507462686567164
[2m[36m(func pid=144451)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=144451)[0m f1_macro: 0.17002710821714345
[2m[36m(func pid=144451)[0m f1_weighted: 0.21682330965142033
[2m[36m(func pid=144451)[0m f1_per_class: [0.105, 0.005, 0.0, 0.527, 0.114, 0.25, 0.046, 0.262, 0.172, 0.218]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.40625
[2m[36m(func pid=143647)[0m top5: 0.9015858208955224
[2m[36m(func pid=143647)[0m f1_micro: 0.40625
[2m[36m(func pid=143647)[0m f1_macro: 0.37310714882087287
[2m[36m(func pid=143647)[0m f1_weighted: 0.4160973097899525
[2m[36m(func pid=143647)[0m f1_per_class: [0.469, 0.481, 0.579, 0.533, 0.071, 0.275, 0.354, 0.445, 0.112, 0.413]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.1455 | Steps: 4 | Val loss: 2.5471 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4071 | Steps: 4 | Val loss: 3.0621 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5673 | Steps: 4 | Val loss: 2.3872 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3399 | Steps: 4 | Val loss: 1.7850 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:00:39 (running for 00:07:31.34)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.955 |      0.373 |                   73 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.765 |      0.282 |                   72 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  3.473 |      0.17  |                   73 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.145 |      0.077 |                   71 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.07789179104477612
[2m[36m(func pid=144880)[0m top5: 0.40111940298507465
[2m[36m(func pid=144880)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=144880)[0m f1_macro: 0.07667667990776267
[2m[36m(func pid=144880)[0m f1_weighted: 0.10067967120203858
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.019, 0.0, 0.243, 0.48, 0.0, 0.025]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144027)[0m top1: 0.2737873134328358
[2m[36m(func pid=144027)[0m top5: 0.8600746268656716
[2m[36m(func pid=144027)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=144027)[0m f1_macro: 0.28802733028091687
[2m[36m(func pid=144027)[0m f1_weighted: 0.2696218352709705
[2m[36m(func pid=144027)[0m f1_per_class: [0.158, 0.344, 0.55, 0.403, 0.087, 0.354, 0.07, 0.326, 0.167, 0.421]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=144451)[0m top1: 0.25699626865671643
[2m[36m(func pid=144451)[0m top5: 0.8544776119402985
[2m[36m(func pid=144451)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=144451)[0m f1_macro: 0.1937699762378531
[2m[36m(func pid=144451)[0m f1_weighted: 0.22696395059308522
[2m[36m(func pid=144451)[0m f1_per_class: [0.111, 0.016, 0.185, 0.518, 0.105, 0.261, 0.075, 0.253, 0.178, 0.235]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3885261194029851
[2m[36m(func pid=143647)[0m top5: 0.9039179104477612
[2m[36m(func pid=143647)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=143647)[0m f1_macro: 0.3828526363973065
[2m[36m(func pid=143647)[0m f1_weighted: 0.3951338808946415
[2m[36m(func pid=143647)[0m f1_per_class: [0.509, 0.486, 0.688, 0.499, 0.074, 0.298, 0.299, 0.426, 0.138, 0.412]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.8678 | Steps: 4 | Val loss: 2.5642 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9685 | Steps: 4 | Val loss: 2.1922 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2072 | Steps: 4 | Val loss: 2.8496 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9073 | Steps: 4 | Val loss: 1.8042 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 13:00:44 (running for 00:07:36.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.34  |      0.383 |                   74 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.407 |      0.288 |                   73 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.567 |      0.194 |                   74 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.868 |      0.07  |                   72 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.06529850746268656
[2m[36m(func pid=144880)[0m top5: 0.35867537313432835
[2m[36m(func pid=144880)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=144880)[0m f1_macro: 0.06972283239775462
[2m[36m(func pid=144880)[0m f1_weighted: 0.08923896163482559
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.213, 0.437, 0.0, 0.03]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.2630597014925373
[2m[36m(func pid=144451)[0m top5: 0.863339552238806
[2m[36m(func pid=144451)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=144451)[0m f1_macro: 0.21993015867430526
[2m[36m(func pid=144451)[0m f1_weighted: 0.25207019104407963
[2m[36m(func pid=144451)[0m f1_per_class: [0.109, 0.121, 0.289, 0.494, 0.083, 0.236, 0.122, 0.292, 0.149, 0.304]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.34095149253731344
[2m[36m(func pid=144027)[0m top5: 0.8614738805970149
[2m[36m(func pid=144027)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=144027)[0m f1_macro: 0.33336744990606715
[2m[36m(func pid=144027)[0m f1_weighted: 0.3678503867842217
[2m[36m(func pid=144027)[0m f1_per_class: [0.164, 0.401, 0.647, 0.454, 0.128, 0.314, 0.324, 0.385, 0.149, 0.367]
[2m[36m(func pid=144027)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3805970149253731
[2m[36m(func pid=143647)[0m top5: 0.9006529850746269
[2m[36m(func pid=143647)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=143647)[0m f1_macro: 0.37793804442921264
[2m[36m(func pid=143647)[0m f1_weighted: 0.38584361998757144
[2m[36m(func pid=143647)[0m f1_per_class: [0.441, 0.48, 0.733, 0.471, 0.081, 0.334, 0.291, 0.416, 0.127, 0.405]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.8442 | Steps: 4 | Val loss: 2.5184 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 2.1489 | Steps: 4 | Val loss: 2.0834 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=144027)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8482 | Steps: 4 | Val loss: 3.6818 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.9204 | Steps: 4 | Val loss: 1.7942 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:00:50 (running for 00:07:42.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.3385
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING  | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.907 |      0.378 |                   75 |
| train_52b21_00001 | RUNNING  | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  1.207 |      0.333 |                   74 |
| train_52b21_00002 | RUNNING  | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.968 |      0.22  |                   75 |
| train_52b21_00003 | RUNNING  | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.844 |      0.063 |                   73 |
| train_52b21_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144880)[0m top1: 0.054104477611940295
[2m[36m(func pid=144880)[0m top5: 0.44636194029850745
[2m[36m(func pid=144880)[0m f1_micro: 0.054104477611940295
[2m[36m(func pid=144880)[0m f1_macro: 0.06304192575675424
[2m[36m(func pid=144880)[0m f1_weighted: 0.06888658332985059
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.018, 0.006, 0.137, 0.469, 0.0, 0.0]
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=144451)[0m top1: 0.2775186567164179
[2m[36m(func pid=144451)[0m top5: 0.851679104477612
[2m[36m(func pid=144451)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=144451)[0m f1_macro: 0.24039473465349598
[2m[36m(func pid=144451)[0m f1_weighted: 0.29640638381008116
[2m[36m(func pid=144451)[0m f1_per_class: [0.107, 0.33, 0.258, 0.367, 0.071, 0.278, 0.244, 0.358, 0.14, 0.25]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144027)[0m top1: 0.3204291044776119
[2m[36m(func pid=144027)[0m top5: 0.8479477611940298
[2m[36m(func pid=144027)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=144027)[0m f1_macro: 0.3058947722772038
[2m[36m(func pid=144027)[0m f1_weighted: 0.3707555090441872
[2m[36m(func pid=144027)[0m f1_per_class: [0.142, 0.417, 0.645, 0.446, 0.09, 0.231, 0.38, 0.373, 0.085, 0.25]
[2m[36m(func pid=143647)[0m top1: 0.3871268656716418
[2m[36m(func pid=143647)[0m top5: 0.8927238805970149
[2m[36m(func pid=143647)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=143647)[0m f1_macro: 0.3705349758942239
[2m[36m(func pid=143647)[0m f1_weighted: 0.38348673018756807
[2m[36m(func pid=143647)[0m f1_per_class: [0.364, 0.485, 0.688, 0.514, 0.097, 0.364, 0.232, 0.403, 0.174, 0.386]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.8741 | Steps: 4 | Val loss: 2.4937 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.7139 | Steps: 4 | Val loss: 2.0000 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.9615 | Steps: 4 | Val loss: 1.8443 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=144451)[0m top1: 0.3003731343283582
[2m[36m(func pid=144451)[0m top5: 0.8568097014925373
[2m[36m(func pid=144451)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=144451)[0m f1_macro: 0.260835298995337
[2m[36m(func pid=144451)[0m f1_weighted: 0.31990116262650126
[2m[36m(func pid=144451)[0m f1_per_class: [0.081, 0.485, 0.233, 0.202, 0.056, 0.331, 0.35, 0.46, 0.15, 0.262]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=144880)[0m top1: 0.0480410447761194
[2m[36m(func pid=144880)[0m top5: 0.4505597014925373
[2m[36m(func pid=144880)[0m f1_micro: 0.0480410447761194
[2m[36m(func pid=144880)[0m f1_macro: 0.059708125212018424
[2m[36m(func pid=144880)[0m f1_weighted: 0.05289927222792097
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.018, 0.064, 0.065, 0.45, 0.0, 0.0]
[2m[36m(func pid=143647)[0m top1: 0.396455223880597
[2m[36m(func pid=143647)[0m top5: 0.8857276119402985
[2m[36m(func pid=143647)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=143647)[0m f1_macro: 0.3607723222392587
[2m[36m(func pid=143647)[0m f1_weighted: 0.3825578316958692
[2m[36m(func pid=143647)[0m f1_per_class: [0.318, 0.524, 0.579, 0.497, 0.098, 0.376, 0.214, 0.446, 0.188, 0.368]
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 2.6082 | Steps: 4 | Val loss: 1.9175 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:00:55 (running for 00:07:47.73)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.34199999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.92  |      0.371 |                   76 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.714 |      0.261 |                   77 |
| train_52b21_00003 | RUNNING    | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.844 |      0.063 |                   73 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m 
[2m[36m(func pid=162589)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=162589)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=162589)[0m Configuration completed!
[2m[36m(func pid=162589)[0m New optimizer parameters:
[2m[36m(func pid=162589)[0m SGD (
[2m[36m(func pid=162589)[0m Parameter Group 0
[2m[36m(func pid=162589)[0m     dampening: 0
[2m[36m(func pid=162589)[0m     differentiable: False
[2m[36m(func pid=162589)[0m     foreach: None
[2m[36m(func pid=162589)[0m     lr: 0.0001
[2m[36m(func pid=162589)[0m     maximize: False
[2m[36m(func pid=162589)[0m     momentum: 0.9
[2m[36m(func pid=162589)[0m     nesterov: False
[2m[36m(func pid=162589)[0m     weight_decay: 0
[2m[36m(func pid=162589)[0m )
[2m[36m(func pid=162589)[0m 
== Status ==
Current time: 2024-01-07 13:01:00 (running for 00:07:53.27)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.34199999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.961 |      0.361 |                   77 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.608 |      0.273 |                   78 |
| train_52b21_00003 | RUNNING    | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  2.874 |      0.06  |                   74 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.30550373134328357
[2m[36m(func pid=144451)[0m top5: 0.8605410447761194
[2m[36m(func pid=144451)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=144451)[0m f1_macro: 0.2731523409160844
[2m[36m(func pid=144451)[0m f1_weighted: 0.31281742815792235
[2m[36m(func pid=144451)[0m f1_per_class: [0.095, 0.538, 0.301, 0.121, 0.055, 0.321, 0.363, 0.511, 0.142, 0.286]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.0366 | Steps: 4 | Val loss: 1.9004 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144880)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.3514 | Steps: 4 | Val loss: 2.2835 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 2.2373 | Steps: 4 | Val loss: 2.1032 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0499 | Steps: 4 | Val loss: 2.3251 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
[2m[36m(func pid=143647)[0m top1: 0.38992537313432835
[2m[36m(func pid=143647)[0m top5: 0.8852611940298507
[2m[36m(func pid=143647)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=143647)[0m f1_macro: 0.3442809540342151
[2m[36m(func pid=143647)[0m f1_weighted: 0.37350761991224846
[2m[36m(func pid=143647)[0m f1_per_class: [0.289, 0.532, 0.45, 0.505, 0.11, 0.369, 0.175, 0.452, 0.213, 0.347]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144880)[0m top1: 0.09281716417910447
[2m[36m(func pid=144880)[0m top5: 0.7555970149253731
[2m[36m(func pid=144880)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=144880)[0m f1_macro: 0.08004423279877135
[2m[36m(func pid=144880)[0m f1_weighted: 0.10834300445077878
[2m[36m(func pid=144880)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.019, 0.064, 0.247, 0.47, 0.0, 0.0]
[2m[36m(func pid=144451)[0m top1: 0.279384328358209
[2m[36m(func pid=144451)[0m top5: 0.8470149253731343
[2m[36m(func pid=144451)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=144451)[0m f1_macro: 0.25308205329808536
[2m[36m(func pid=144451)[0m f1_weighted: 0.2819641757106012
[2m[36m(func pid=144451)[0m f1_per_class: [0.087, 0.566, 0.289, 0.032, 0.015, 0.258, 0.348, 0.528, 0.158, 0.25]
[2m[36m(func pid=162589)[0m top1: 0.15065298507462688
[2m[36m(func pid=162589)[0m top5: 0.5471082089552238
[2m[36m(func pid=162589)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=162589)[0m f1_macro: 0.04445365486821502
[2m[36m(func pid=162589)[0m f1_weighted: 0.0851841364174793
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.0, 0.0, 0.269, 0.0, 0.0, 0.0, 0.176, 0.0, 0.0]
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.7705 | Steps: 4 | Val loss: 2.1343 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=143647)[0m top1: 0.36473880597014924
[2m[36m(func pid=143647)[0m top5: 0.8577425373134329
[2m[36m(func pid=143647)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=143647)[0m f1_macro: 0.3162085602121243
[2m[36m(func pid=143647)[0m f1_weighted: 0.34400030915005836
[2m[36m(func pid=143647)[0m f1_per_class: [0.263, 0.578, 0.377, 0.465, 0.099, 0.303, 0.118, 0.475, 0.162, 0.321]
== Status ==
Current time: 2024-01-07 13:01:06 (running for 00:07:58.67)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.037 |      0.344 |                   78 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.608 |      0.273 |                   78 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=163372)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=163372)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=163372)[0m Configuration completed!
[2m[36m(func pid=163372)[0m New optimizer parameters:
[2m[36m(func pid=163372)[0m SGD (
[2m[36m(func pid=163372)[0m Parameter Group 0
[2m[36m(func pid=163372)[0m     dampening: 0
[2m[36m(func pid=163372)[0m     differentiable: False
[2m[36m(func pid=163372)[0m     foreach: None
[2m[36m(func pid=163372)[0m     lr: 0.001
[2m[36m(func pid=163372)[0m     maximize: False
[2m[36m(func pid=163372)[0m     momentum: 0.9
[2m[36m(func pid=163372)[0m     nesterov: False
[2m[36m(func pid=163372)[0m     weight_decay: 0
[2m[36m(func pid=163372)[0m )
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:01:15 (running for 00:08:07.79)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.771 |      0.316 |                   79 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.608 |      0.273 |                   78 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.8314 | Steps: 4 | Val loss: 1.9897 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 2.0816 | Steps: 4 | Val loss: 2.1289 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1153 | Steps: 4 | Val loss: 2.3073 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:01:20 (running for 00:08:12.81)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.771 |      0.316 |                   79 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.237 |      0.253 |                   79 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  3.05  |      0.044 |                    1 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |        |            |                      |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0232 | Steps: 4 | Val loss: 2.3313 | Batch size: 32 | lr: 0.001 | Duration: 5.11s
[2m[36m(func pid=143647)[0m top1: 0.36240671641791045
[2m[36m(func pid=143647)[0m top5: 0.8666044776119403
[2m[36m(func pid=143647)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=143647)[0m f1_macro: 0.3161354716408279
[2m[36m(func pid=143647)[0m f1_weighted: 0.33057640422460005
[2m[36m(func pid=143647)[0m f1_per_class: [0.265, 0.572, 0.263, 0.458, 0.09, 0.348, 0.063, 0.438, 0.23, 0.435]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.29990671641791045
[2m[36m(func pid=144451)[0m top5: 0.8568097014925373
[2m[36m(func pid=144451)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=144451)[0m f1_macro: 0.2531083780709379
[2m[36m(func pid=144451)[0m f1_weighted: 0.284286162123313
[2m[36m(func pid=144451)[0m f1_per_class: [0.118, 0.573, 0.282, 0.01, 0.019, 0.256, 0.369, 0.543, 0.178, 0.182]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.20289179104477612
[2m[36m(func pid=162589)[0m top5: 0.5881529850746269
[2m[36m(func pid=162589)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=162589)[0m f1_macro: 0.04105745886161317
[2m[36m(func pid=162589)[0m f1_weighted: 0.10303182246691703
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.0, 0.0, 0.359, 0.0, 0.0, 0.0, 0.052, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.11567164179104478
[2m[36m(func pid=163372)[0m top5: 0.4962686567164179
[2m[36m(func pid=163372)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=163372)[0m f1_macro: 0.05102955539250076
[2m[36m(func pid=163372)[0m f1_weighted: 0.07762387054872052
[2m[36m(func pid=163372)[0m f1_per_class: [0.02, 0.0, 0.034, 0.229, 0.0, 0.0, 0.0, 0.228, 0.0, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.8050 | Steps: 4 | Val loss: 2.0755 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.0723 | Steps: 4 | Val loss: 1.9871 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9409 | Steps: 4 | Val loss: 2.3061 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8457 | Steps: 4 | Val loss: 2.3152 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 13:01:26 (running for 00:08:18.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.831 |      0.316 |                   80 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.805 |      0.265 |                   81 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  3.115 |      0.041 |                    2 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  3.023 |      0.051 |                    1 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.30783582089552236
[2m[36m(func pid=144451)[0m top5: 0.8610074626865671
[2m[36m(func pid=144451)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=144451)[0m f1_macro: 0.26509105948129125
[2m[36m(func pid=144451)[0m f1_weighted: 0.2831788180490436
[2m[36m(func pid=144451)[0m f1_per_class: [0.116, 0.554, 0.421, 0.0, 0.063, 0.271, 0.386, 0.495, 0.176, 0.168]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=143647)[0m top1: 0.34888059701492535
[2m[36m(func pid=143647)[0m top5: 0.8614738805970149
[2m[36m(func pid=143647)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=143647)[0m f1_macro: 0.30341362448998954
[2m[36m(func pid=143647)[0m f1_weighted: 0.3067692692247806
[2m[36m(func pid=143647)[0m f1_per_class: [0.287, 0.583, 0.143, 0.424, 0.098, 0.357, 0.016, 0.371, 0.229, 0.526]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=162589)[0m top1: 0.23647388059701493
[2m[36m(func pid=162589)[0m top5: 0.5904850746268657
[2m[36m(func pid=162589)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=162589)[0m f1_macro: 0.045771774230640896
[2m[36m(func pid=162589)[0m f1_weighted: 0.11795756365819621
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.04, 0.0, 0.393, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.010727611940298507
[2m[36m(func pid=163372)[0m top5: 0.5853544776119403
[2m[36m(func pid=163372)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=163372)[0m f1_macro: 0.006178305859221777
[2m[36m(func pid=163372)[0m f1_weighted: 0.00793279343398199
[2m[36m(func pid=163372)[0m f1_per_class: [0.022, 0.0, 0.013, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.7280 | Steps: 4 | Val loss: 2.1378 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.9947 | Steps: 4 | Val loss: 1.9871 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9544 | Steps: 4 | Val loss: 2.3024 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8371 | Steps: 4 | Val loss: 2.2708 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:01:31 (running for 00:08:24.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.072 |      0.303 |                   81 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.728 |      0.264 |                   82 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.941 |      0.046 |                    3 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.846 |      0.006 |                    2 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.28451492537313433
[2m[36m(func pid=144451)[0m top5: 0.8628731343283582
[2m[36m(func pid=144451)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=144451)[0m f1_macro: 0.2639741879229879
[2m[36m(func pid=144451)[0m f1_weighted: 0.28976731867080135
[2m[36m(func pid=144451)[0m f1_per_class: [0.099, 0.473, 0.444, 0.0, 0.067, 0.287, 0.453, 0.523, 0.085, 0.209]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.25279850746268656
[2m[36m(func pid=162589)[0m top5: 0.5797574626865671
[2m[36m(func pid=162589)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=162589)[0m f1_macro: 0.08646441968035679
[2m[36m(func pid=162589)[0m f1_weighted: 0.13003663312544828
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.09, 0.372, 0.403, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3344216417910448
[2m[36m(func pid=143647)[0m top5: 0.8619402985074627
[2m[36m(func pid=143647)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=143647)[0m f1_macro: 0.28845068988782346
[2m[36m(func pid=143647)[0m f1_weighted: 0.2943651469067053
[2m[36m(func pid=143647)[0m f1_per_class: [0.317, 0.567, 0.137, 0.404, 0.117, 0.323, 0.025, 0.343, 0.193, 0.459]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=163372)[0m top1: 0.07229477611940298
[2m[36m(func pid=163372)[0m top5: 0.6142723880597015
[2m[36m(func pid=163372)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=163372)[0m f1_macro: 0.06961896843087965
[2m[36m(func pid=163372)[0m f1_weighted: 0.06651618300194118
[2m[36m(func pid=163372)[0m f1_per_class: [0.05, 0.0, 0.118, 0.076, 0.0, 0.307, 0.0, 0.146, 0.0, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 2.1050 | Steps: 4 | Val loss: 2.2532 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.9389 | Steps: 4 | Val loss: 2.0116 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8764 | Steps: 4 | Val loss: 2.3003 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:01:37 (running for 00:08:29.63)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.995 |      0.288 |                   82 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.105 |      0.252 |                   83 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.954 |      0.086 |                    4 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.837 |      0.07  |                    3 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=144451)[0m top1: 0.23041044776119404
[2m[36m(func pid=144451)[0m top5: 0.875
[2m[36m(func pid=144451)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=144451)[0m f1_macro: 0.2515760118931563
[2m[36m(func pid=144451)[0m f1_weighted: 0.24825835582617442
[2m[36m(func pid=144451)[0m f1_per_class: [0.113, 0.181, 0.526, 0.0, 0.035, 0.245, 0.488, 0.524, 0.131, 0.272]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7427 | Steps: 4 | Val loss: 2.1817 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=162589)[0m top1: 0.24580223880597016
[2m[36m(func pid=162589)[0m top5: 0.5629664179104478
[2m[36m(func pid=162589)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=162589)[0m f1_macro: 0.06271983094574375
[2m[36m(func pid=162589)[0m f1_weighted: 0.12788950208307456
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.091, 0.137, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=143647)[0m top1: 0.3353544776119403
[2m[36m(func pid=143647)[0m top5: 0.851679104477612
[2m[36m(func pid=143647)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=143647)[0m f1_macro: 0.29484481319652356
[2m[36m(func pid=143647)[0m f1_weighted: 0.2927500395031636
[2m[36m(func pid=143647)[0m f1_per_class: [0.346, 0.564, 0.135, 0.379, 0.116, 0.367, 0.022, 0.362, 0.199, 0.459]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=163372)[0m top1: 0.26026119402985076
[2m[36m(func pid=163372)[0m top5: 0.7854477611940298
[2m[36m(func pid=163372)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=163372)[0m f1_macro: 0.15467697009620404
[2m[36m(func pid=163372)[0m f1_weighted: 0.19460010974194347
[2m[36m(func pid=163372)[0m f1_per_class: [0.122, 0.0, 0.215, 0.414, 0.0, 0.295, 0.068, 0.356, 0.0, 0.077]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7961 | Steps: 4 | Val loss: 2.2824 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.8988 | Steps: 4 | Val loss: 2.0047 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 2.1086 | Steps: 4 | Val loss: 2.3236 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 13:01:42 (running for 00:08:35.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.939 |      0.295 |                   83 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.105 |      0.252 |                   83 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.796 |      0.059 |                    6 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.743 |      0.155 |                    4 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3353544776119403
[2m[36m(func pid=143647)[0m top5: 0.8540111940298507
[2m[36m(func pid=143647)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=143647)[0m f1_macro: 0.30274325411586317
[2m[36m(func pid=143647)[0m f1_weighted: 0.2935848065378679
[2m[36m(func pid=143647)[0m f1_per_class: [0.405, 0.555, 0.142, 0.364, 0.126, 0.379, 0.028, 0.394, 0.22, 0.414]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.21688432835820895
[2m[36m(func pid=144451)[0m top5: 0.8619402985074627
[2m[36m(func pid=144451)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=144451)[0m f1_macro: 0.20858811940099886
[2m[36m(func pid=144451)[0m f1_weighted: 0.22111243456525254
[2m[36m(func pid=144451)[0m f1_per_class: [0.11, 0.128, 0.205, 0.0, 0.04, 0.215, 0.456, 0.463, 0.126, 0.343]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.24253731343283583
[2m[36m(func pid=162589)[0m top5: 0.5853544776119403
[2m[36m(func pid=162589)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=162589)[0m f1_macro: 0.05903239670847037
[2m[36m(func pid=162589)[0m f1_weighted: 0.12442717275460467
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.076, 0.118, 0.397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5461 | Steps: 4 | Val loss: 2.0558 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=163372)[0m top1: 0.300839552238806
[2m[36m(func pid=163372)[0m top5: 0.8493470149253731
[2m[36m(func pid=163372)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=163372)[0m f1_macro: 0.15781862693192888
[2m[36m(func pid=163372)[0m f1_weighted: 0.24100176862636355
[2m[36m(func pid=163372)[0m f1_per_class: [0.173, 0.0, 0.36, 0.297, 0.0, 0.0, 0.457, 0.266, 0.025, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0178 | Steps: 4 | Val loss: 2.2350 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.7631 | Steps: 4 | Val loss: 2.2841 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7564 | Steps: 4 | Val loss: 2.2851 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:01:48 (running for 00:08:40.71)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.018 |      0.285 |                   85 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.109 |      0.209 |                   84 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.796 |      0.059 |                    6 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.546 |      0.158 |                    5 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.314365671641791
[2m[36m(func pid=143647)[0m top5: 0.8297574626865671
[2m[36m(func pid=143647)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=143647)[0m f1_macro: 0.2853834122903317
[2m[36m(func pid=143647)[0m f1_weighted: 0.27506898387755563
[2m[36m(func pid=143647)[0m f1_per_class: [0.409, 0.57, 0.109, 0.304, 0.128, 0.344, 0.025, 0.439, 0.19, 0.336]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=162589)[0m top1: 0.21595149253731344
[2m[36m(func pid=162589)[0m top5: 0.5802238805970149
[2m[36m(func pid=162589)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=162589)[0m f1_macro: 0.05117270858431668
[2m[36m(func pid=162589)[0m f1_weighted: 0.11304145282325556
[2m[36m(func pid=162589)[0m f1_per_class: [0.0, 0.057, 0.086, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=144451)[0m top1: 0.22014925373134328
[2m[36m(func pid=144451)[0m top5: 0.8596082089552238
[2m[36m(func pid=144451)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=144451)[0m f1_macro: 0.209500780398325
[2m[36m(func pid=144451)[0m f1_weighted: 0.22686579680589145
[2m[36m(func pid=144451)[0m f1_per_class: [0.112, 0.192, 0.103, 0.0, 0.043, 0.256, 0.425, 0.441, 0.15, 0.375]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5193 | Steps: 4 | Val loss: 1.9725 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=163372)[0m top1: 0.29524253731343286
[2m[36m(func pid=163372)[0m top5: 0.8838619402985075
[2m[36m(func pid=163372)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=163372)[0m f1_macro: 0.14377757852341544
[2m[36m(func pid=163372)[0m f1_weighted: 0.16010514475668433
[2m[36m(func pid=163372)[0m f1_per_class: [0.11, 0.0, 0.452, 0.0, 0.0, 0.0, 0.435, 0.442, 0.0, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7451 | Steps: 4 | Val loss: 2.2822 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.3037 | Steps: 4 | Val loss: 2.1781 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.9598 | Steps: 4 | Val loss: 2.1587 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:01:53 (running for 00:08:46.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.304 |      0.293 |                   86 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.763 |      0.21  |                   85 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.756 |      0.051 |                    7 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.519 |      0.144 |                    6 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.31669776119402987
[2m[36m(func pid=143647)[0m top5: 0.8568097014925373
[2m[36m(func pid=143647)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=143647)[0m f1_macro: 0.2934955648397217
[2m[36m(func pid=143647)[0m f1_weighted: 0.28773230308664344
[2m[36m(func pid=143647)[0m f1_per_class: [0.442, 0.558, 0.138, 0.345, 0.133, 0.315, 0.046, 0.425, 0.187, 0.345]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=162589)[0m top1: 0.20009328358208955
[2m[36m(func pid=162589)[0m top5: 0.5881529850746269
[2m[36m(func pid=162589)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=162589)[0m f1_macro: 0.055752118490094336
[2m[36m(func pid=162589)[0m f1_weighted: 0.1120227194110553
[2m[36m(func pid=162589)[0m f1_per_class: [0.042, 0.08, 0.088, 0.347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=144451)[0m top1: 0.28078358208955223
[2m[36m(func pid=144451)[0m top5: 0.8624067164179104
[2m[36m(func pid=144451)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=144451)[0m f1_macro: 0.24254761990337775
[2m[36m(func pid=144451)[0m f1_weighted: 0.2690764969201811
[2m[36m(func pid=144451)[0m f1_per_class: [0.108, 0.48, 0.114, 0.0, 0.053, 0.296, 0.386, 0.419, 0.163, 0.407]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4135 | Steps: 4 | Val loss: 1.9771 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=163372)[0m top1: 0.28171641791044777
[2m[36m(func pid=163372)[0m top5: 0.8549440298507462
[2m[36m(func pid=163372)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=163372)[0m f1_macro: 0.16570027936508616
[2m[36m(func pid=163372)[0m f1_weighted: 0.18075609734107118
[2m[36m(func pid=163372)[0m f1_per_class: [0.166, 0.032, 0.444, 0.016, 0.087, 0.0, 0.463, 0.449, 0.0, 0.0]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.9755 | Steps: 4 | Val loss: 1.9916 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.7220 | Steps: 4 | Val loss: 2.0195 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6647 | Steps: 4 | Val loss: 2.2855 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:01:59 (running for 00:08:51.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.304 |      0.293 |                   86 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.96  |      0.243 |                   86 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.665 |      0.058 |                    9 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.414 |      0.166 |                    7 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3306902985074627
[2m[36m(func pid=143647)[0m top5: 0.875
[2m[36m(func pid=143647)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=143647)[0m f1_macro: 0.3073642824502679
[2m[36m(func pid=143647)[0m f1_weighted: 0.30777037969211396
[2m[36m(func pid=143647)[0m f1_per_class: [0.405, 0.531, 0.145, 0.34, 0.127, 0.366, 0.114, 0.395, 0.236, 0.415]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3302238805970149
[2m[36m(func pid=144451)[0m top5: 0.8815298507462687
[2m[36m(func pid=144451)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=144451)[0m f1_macro: 0.272323506083931
[2m[36m(func pid=144451)[0m f1_weighted: 0.3007147432757788
[2m[36m(func pid=144451)[0m f1_per_class: [0.105, 0.537, 0.155, 0.0, 0.072, 0.377, 0.411, 0.499, 0.167, 0.4]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.1646455223880597
[2m[36m(func pid=162589)[0m top5: 0.590018656716418
[2m[36m(func pid=162589)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=162589)[0m f1_macro: 0.0584237555946094
[2m[36m(func pid=162589)[0m f1_weighted: 0.09822084984029628
[2m[36m(func pid=162589)[0m f1_per_class: [0.063, 0.078, 0.134, 0.293, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3857 | Steps: 4 | Val loss: 2.0991 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=163372)[0m top1: 0.26492537313432835
[2m[36m(func pid=163372)[0m top5: 0.8367537313432836
[2m[36m(func pid=163372)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=163372)[0m f1_macro: 0.16583572161593313
[2m[36m(func pid=163372)[0m f1_weighted: 0.27590086750070825
[2m[36m(func pid=163372)[0m f1_per_class: [0.072, 0.192, 0.098, 0.28, 0.061, 0.008, 0.457, 0.419, 0.0, 0.071]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.0008 | Steps: 4 | Val loss: 1.9877 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 2.0253 | Steps: 4 | Val loss: 1.9565 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6393 | Steps: 4 | Val loss: 2.2882 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:02:04 (running for 00:08:57.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.001 |      0.306 |                   88 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.722 |      0.272 |                   87 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.665 |      0.058 |                    9 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.386 |      0.166 |                    8 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.33675373134328357
[2m[36m(func pid=143647)[0m top5: 0.8796641791044776
[2m[36m(func pid=143647)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=143647)[0m f1_macro: 0.3060937666911669
[2m[36m(func pid=143647)[0m f1_weighted: 0.33953340551301137
[2m[36m(func pid=143647)[0m f1_per_class: [0.338, 0.511, 0.13, 0.358, 0.095, 0.345, 0.227, 0.423, 0.203, 0.43]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=162589)[0m top1: 0.13899253731343283
[2m[36m(func pid=162589)[0m top5: 0.5998134328358209
[2m[36m(func pid=162589)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=162589)[0m f1_macro: 0.06235612981961406
[2m[36m(func pid=162589)[0m f1_weighted: 0.09331926316667878
[2m[36m(func pid=162589)[0m f1_per_class: [0.052, 0.087, 0.14, 0.256, 0.0, 0.0, 0.0, 0.089, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3283582089552239
[2m[36m(func pid=144451)[0m top5: 0.8880597014925373
[2m[36m(func pid=144451)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=144451)[0m f1_macro: 0.26214691887959823
[2m[36m(func pid=144451)[0m f1_weighted: 0.29802456975061764
[2m[36m(func pid=144451)[0m f1_per_class: [0.098, 0.53, 0.117, 0.003, 0.0, 0.373, 0.398, 0.534, 0.21, 0.36]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2371 | Steps: 4 | Val loss: 2.0750 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=163372)[0m top1: 0.1553171641791045
[2m[36m(func pid=163372)[0m top5: 0.8428171641791045
[2m[36m(func pid=163372)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=163372)[0m f1_macro: 0.15989421019691477
[2m[36m(func pid=163372)[0m f1_weighted: 0.15912321620412917
[2m[36m(func pid=163372)[0m f1_per_class: [0.125, 0.215, 0.191, 0.148, 0.094, 0.28, 0.062, 0.445, 0.0, 0.04]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.2278 | Steps: 4 | Val loss: 1.9536 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.8410 | Steps: 4 | Val loss: 1.9502 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6200 | Steps: 4 | Val loss: 2.2772 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:02:10 (running for 00:09:02.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.228 |      0.28  |                   89 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.025 |      0.262 |                   88 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.639 |      0.062 |                   10 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.237 |      0.16  |                    9 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.32742537313432835
[2m[36m(func pid=143647)[0m top5: 0.8908582089552238
[2m[36m(func pid=143647)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=143647)[0m f1_macro: 0.2801732407230349
[2m[36m(func pid=143647)[0m f1_weighted: 0.3401027940318565
[2m[36m(func pid=143647)[0m f1_per_class: [0.301, 0.469, 0.121, 0.391, 0.104, 0.322, 0.25, 0.389, 0.164, 0.291]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3199626865671642
[2m[36m(func pid=144451)[0m top5: 0.8931902985074627
[2m[36m(func pid=144451)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=144451)[0m f1_macro: 0.25676767885914054
[2m[36m(func pid=144451)[0m f1_weighted: 0.28520514607341907
[2m[36m(func pid=144451)[0m f1_per_class: [0.105, 0.531, 0.058, 0.05, 0.065, 0.362, 0.313, 0.545, 0.212, 0.328]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.15764925373134328
[2m[36m(func pid=162589)[0m top5: 0.6385261194029851
[2m[36m(func pid=162589)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=162589)[0m f1_macro: 0.10344040071257096
[2m[36m(func pid=162589)[0m f1_weighted: 0.12117007274539975
[2m[36m(func pid=162589)[0m f1_per_class: [0.087, 0.183, 0.222, 0.25, 0.0, 0.0, 0.0, 0.292, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3102 | Steps: 4 | Val loss: 2.0186 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9370 | Steps: 4 | Val loss: 1.9451 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163372)[0m top1: 0.18936567164179105
[2m[36m(func pid=163372)[0m top5: 0.8386194029850746
[2m[36m(func pid=163372)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=163372)[0m f1_macro: 0.21698101629467423
[2m[36m(func pid=163372)[0m f1_weighted: 0.16278309216992742
[2m[36m(func pid=163372)[0m f1_per_class: [0.146, 0.35, 0.667, 0.099, 0.108, 0.409, 0.0, 0.341, 0.0, 0.049]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 2.1716 | Steps: 4 | Val loss: 2.0528 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5813 | Steps: 4 | Val loss: 2.2835 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:02:15 (running for 00:09:08.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.937 |      0.305 |                   90 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.841 |      0.257 |                   89 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.62  |      0.103 |                   11 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.31  |      0.217 |                   10 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3572761194029851
[2m[36m(func pid=143647)[0m top5: 0.886660447761194
[2m[36m(func pid=143647)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=143647)[0m f1_macro: 0.304586191920863
[2m[36m(func pid=143647)[0m f1_weighted: 0.38484380257747924
[2m[36m(func pid=143647)[0m f1_per_class: [0.292, 0.49, 0.109, 0.398, 0.089, 0.3, 0.369, 0.47, 0.217, 0.312]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.31343283582089554
[2m[36m(func pid=144451)[0m top5: 0.886660447761194
[2m[36m(func pid=144451)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=144451)[0m f1_macro: 0.2379695414009774
[2m[36m(func pid=144451)[0m f1_weighted: 0.30518180165868064
[2m[36m(func pid=144451)[0m f1_per_class: [0.098, 0.53, 0.018, 0.202, 0.0, 0.318, 0.27, 0.505, 0.202, 0.237]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.14225746268656717
[2m[36m(func pid=162589)[0m top5: 0.6497201492537313
[2m[36m(func pid=162589)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=162589)[0m f1_macro: 0.09918646603187137
[2m[36m(func pid=162589)[0m f1_weighted: 0.11811198029517977
[2m[36m(func pid=162589)[0m f1_per_class: [0.057, 0.183, 0.125, 0.232, 0.0, 0.008, 0.0, 0.312, 0.0, 0.074]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2800 | Steps: 4 | Val loss: 1.9798 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.1180 | Steps: 4 | Val loss: 2.0050 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 2.0500 | Steps: 4 | Val loss: 1.9891 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=163372)[0m top1: 0.251865671641791
[2m[36m(func pid=163372)[0m top5: 0.8171641791044776
[2m[36m(func pid=163372)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=163372)[0m f1_macro: 0.24190832503174228
[2m[36m(func pid=163372)[0m f1_weighted: 0.22555535078988292
[2m[36m(func pid=163372)[0m f1_per_class: [0.16, 0.42, 0.667, 0.294, 0.096, 0.377, 0.0, 0.335, 0.0, 0.07]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7290 | Steps: 4 | Val loss: 2.2800 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:02:21 (running for 00:09:13.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.118 |      0.327 |                   91 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.172 |      0.238 |                   90 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.581 |      0.099 |                   12 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.28  |      0.242 |                   11 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.386660447761194
[2m[36m(func pid=143647)[0m top5: 0.8768656716417911
[2m[36m(func pid=143647)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=143647)[0m f1_macro: 0.3269738926687987
[2m[36m(func pid=143647)[0m f1_weighted: 0.42976939161053884
[2m[36m(func pid=143647)[0m f1_per_class: [0.393, 0.472, 0.117, 0.427, 0.095, 0.322, 0.478, 0.527, 0.237, 0.202]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.33861940298507465
[2m[36m(func pid=144451)[0m top5: 0.8973880597014925
[2m[36m(func pid=144451)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=144451)[0m f1_macro: 0.2503470962160765
[2m[36m(func pid=144451)[0m f1_weighted: 0.3396473919360822
[2m[36m(func pid=144451)[0m f1_per_class: [0.114, 0.541, 0.0, 0.319, 0.038, 0.316, 0.272, 0.52, 0.15, 0.233]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.16417910447761194
[2m[36m(func pid=162589)[0m top5: 0.6576492537313433
[2m[36m(func pid=162589)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=162589)[0m f1_macro: 0.11588205431705438
[2m[36m(func pid=162589)[0m f1_weighted: 0.1415330094002505
[2m[36m(func pid=162589)[0m f1_per_class: [0.051, 0.2, 0.077, 0.259, 0.0, 0.062, 0.0, 0.439, 0.0, 0.071]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1844 | Steps: 4 | Val loss: 1.9689 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.9273 | Steps: 4 | Val loss: 2.2295 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.8508 | Steps: 4 | Val loss: 2.0310 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=163372)[0m top1: 0.25326492537313433
[2m[36m(func pid=163372)[0m top5: 0.7765858208955224
[2m[36m(func pid=163372)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=163372)[0m f1_macro: 0.26181907448840336
[2m[36m(func pid=163372)[0m f1_weighted: 0.23633559947335328
[2m[36m(func pid=163372)[0m f1_per_class: [0.311, 0.437, 0.537, 0.312, 0.06, 0.259, 0.0, 0.509, 0.124, 0.069]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5388 | Steps: 4 | Val loss: 2.2549 | Batch size: 32 | lr: 0.0001 | Duration: 3.30s
== Status ==
Current time: 2024-01-07 13:02:26 (running for 00:09:19.09)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.927 |      0.328 |                   92 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.05  |      0.25  |                   91 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.729 |      0.116 |                   13 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.184 |      0.262 |                   12 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3978544776119403
[2m[36m(func pid=143647)[0m top5: 0.8777985074626866
[2m[36m(func pid=143647)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=143647)[0m f1_macro: 0.3276486434664871
[2m[36m(func pid=143647)[0m f1_weighted: 0.446972340718359
[2m[36m(func pid=143647)[0m f1_per_class: [0.441, 0.478, 0.146, 0.511, 0.098, 0.257, 0.489, 0.479, 0.201, 0.176]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.34794776119402987
[2m[36m(func pid=144451)[0m top5: 0.8838619402985075
[2m[36m(func pid=144451)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=144451)[0m f1_macro: 0.2598590698110924
[2m[36m(func pid=144451)[0m f1_weighted: 0.35387325887517423
[2m[36m(func pid=144451)[0m f1_per_class: [0.122, 0.554, 0.0, 0.403, 0.047, 0.295, 0.232, 0.545, 0.197, 0.205]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.20242537313432835
[2m[36m(func pid=162589)[0m top5: 0.6781716417910447
[2m[36m(func pid=162589)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=162589)[0m f1_macro: 0.11807309967211137
[2m[36m(func pid=162589)[0m f1_weighted: 0.16228838967670564
[2m[36m(func pid=162589)[0m f1_per_class: [0.056, 0.173, 0.08, 0.33, 0.0, 0.133, 0.0, 0.408, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9971 | Steps: 4 | Val loss: 1.8897 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.9656 | Steps: 4 | Val loss: 3.0797 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=163372)[0m top1: 0.25466417910447764
[2m[36m(func pid=163372)[0m top5: 0.8134328358208955
[2m[36m(func pid=163372)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=163372)[0m f1_macro: 0.22214594102213275
[2m[36m(func pid=163372)[0m f1_weighted: 0.25127050914268034
[2m[36m(func pid=163372)[0m f1_per_class: [0.0, 0.443, 0.3, 0.249, 0.062, 0.321, 0.116, 0.444, 0.138, 0.148]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.5967 | Steps: 4 | Val loss: 1.9711 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5554 | Steps: 4 | Val loss: 2.2512 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:02:32 (running for 00:09:24.64)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.966 |      0.265 |                   93 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.851 |      0.26  |                   92 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.539 |      0.118 |                   14 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.997 |      0.222 |                   13 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.31902985074626866
[2m[36m(func pid=143647)[0m top5: 0.8442164179104478
[2m[36m(func pid=143647)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=143647)[0m f1_macro: 0.2652042644137501
[2m[36m(func pid=143647)[0m f1_weighted: 0.366522837779927
[2m[36m(func pid=143647)[0m f1_per_class: [0.444, 0.441, 0.182, 0.518, 0.097, 0.144, 0.323, 0.318, 0.084, 0.1]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3596082089552239
[2m[36m(func pid=144451)[0m top5: 0.8894589552238806
[2m[36m(func pid=144451)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=144451)[0m f1_macro: 0.26958550577786816
[2m[36m(func pid=144451)[0m f1_weighted: 0.3766464096614834
[2m[36m(func pid=144451)[0m f1_per_class: [0.124, 0.499, 0.097, 0.477, 0.076, 0.305, 0.275, 0.528, 0.149, 0.167]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.20382462686567165
[2m[36m(func pid=162589)[0m top5: 0.6711753731343284
[2m[36m(func pid=162589)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=162589)[0m f1_macro: 0.11135347260916047
[2m[36m(func pid=162589)[0m f1_weighted: 0.15588598550424557
[2m[36m(func pid=162589)[0m f1_per_class: [0.058, 0.128, 0.063, 0.339, 0.0, 0.129, 0.0, 0.396, 0.0, 0.0]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9312 | Steps: 4 | Val loss: 1.8782 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.7232 | Steps: 4 | Val loss: 2.7192 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.6221 | Steps: 4 | Val loss: 2.0680 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=163372)[0m top1: 0.27472014925373134
[2m[36m(func pid=163372)[0m top5: 0.7933768656716418
[2m[36m(func pid=163372)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=163372)[0m f1_macro: 0.21213817044559025
[2m[36m(func pid=163372)[0m f1_weighted: 0.2406551540906127
[2m[36m(func pid=163372)[0m f1_per_class: [0.0, 0.453, 0.171, 0.05, 0.051, 0.263, 0.262, 0.545, 0.177, 0.148]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4966 | Steps: 4 | Val loss: 2.2411 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:02:37 (running for 00:09:30.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.723 |      0.269 |                   94 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.597 |      0.27  |                   93 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.555 |      0.111 |                   15 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.931 |      0.212 |                   14 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.33302238805970147
[2m[36m(func pid=143647)[0m top5: 0.8586753731343284
[2m[36m(func pid=143647)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=143647)[0m f1_macro: 0.26938068680820393
[2m[36m(func pid=143647)[0m f1_weighted: 0.3802718682614292
[2m[36m(func pid=143647)[0m f1_per_class: [0.382, 0.408, 0.183, 0.528, 0.092, 0.153, 0.37, 0.362, 0.092, 0.125]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3278917910447761
[2m[36m(func pid=144451)[0m top5: 0.8782649253731343
[2m[36m(func pid=144451)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=144451)[0m f1_macro: 0.24695317571446776
[2m[36m(func pid=144451)[0m f1_weighted: 0.3497429313309823
[2m[36m(func pid=144451)[0m f1_per_class: [0.126, 0.399, 0.058, 0.486, 0.045, 0.297, 0.24, 0.518, 0.143, 0.157]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.22154850746268656
[2m[36m(func pid=162589)[0m top5: 0.7024253731343284
[2m[36m(func pid=162589)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=162589)[0m f1_macro: 0.13369992572688366
[2m[36m(func pid=162589)[0m f1_weighted: 0.17400576887346375
[2m[36m(func pid=162589)[0m f1_per_class: [0.047, 0.121, 0.055, 0.362, 0.0, 0.193, 0.0, 0.482, 0.0, 0.077]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0131 | Steps: 4 | Val loss: 1.9399 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.3618 | Steps: 4 | Val loss: 1.8706 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 2.1003 | Steps: 4 | Val loss: 1.9725 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=163372)[0m top1: 0.24953358208955223
[2m[36m(func pid=163372)[0m top5: 0.777518656716418
[2m[36m(func pid=163372)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=163372)[0m f1_macro: 0.19833025282989705
[2m[36m(func pid=163372)[0m f1_weighted: 0.19769402102091205
[2m[36m(func pid=163372)[0m f1_per_class: [0.082, 0.446, 0.163, 0.016, 0.053, 0.26, 0.158, 0.51, 0.166, 0.129]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6348 | Steps: 4 | Val loss: 2.2396 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:02:43 (running for 00:09:35.46)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.362 |      0.336 |                   95 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.622 |      0.247 |                   94 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.497 |      0.134 |                   16 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.013 |      0.198 |                   15 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.4253731343283582
[2m[36m(func pid=143647)[0m top5: 0.9006529850746269
[2m[36m(func pid=143647)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=143647)[0m f1_macro: 0.3355518686861718
[2m[36m(func pid=143647)[0m f1_weighted: 0.45321263062092504
[2m[36m(func pid=143647)[0m f1_per_class: [0.355, 0.326, 0.212, 0.549, 0.087, 0.227, 0.564, 0.528, 0.191, 0.315]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.31156716417910446
[2m[36m(func pid=144451)[0m top5: 0.894589552238806
[2m[36m(func pid=144451)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=144451)[0m f1_macro: 0.24655169538390914
[2m[36m(func pid=144451)[0m f1_weighted: 0.31440393903082975
[2m[36m(func pid=144451)[0m f1_per_class: [0.128, 0.218, 0.213, 0.491, 0.053, 0.353, 0.207, 0.458, 0.146, 0.198]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.21828358208955223
[2m[36m(func pid=162589)[0m top5: 0.7159514925373134
[2m[36m(func pid=162589)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=162589)[0m f1_macro: 0.1300158310459643
[2m[36m(func pid=162589)[0m f1_weighted: 0.16456038398343445
[2m[36m(func pid=162589)[0m f1_per_class: [0.039, 0.087, 0.05, 0.365, 0.051, 0.158, 0.0, 0.473, 0.0, 0.077]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7813 | Steps: 4 | Val loss: 2.0114 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.8975 | Steps: 4 | Val loss: 1.7426 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.5204 | Steps: 4 | Val loss: 2.0831 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=163372)[0m top1: 0.23880597014925373
[2m[36m(func pid=163372)[0m top5: 0.7495335820895522
[2m[36m(func pid=163372)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=163372)[0m f1_macro: 0.20723639454487905
[2m[36m(func pid=163372)[0m f1_weighted: 0.1913654715207306
[2m[36m(func pid=163372)[0m f1_per_class: [0.219, 0.471, 0.338, 0.003, 0.045, 0.219, 0.169, 0.359, 0.16, 0.089]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5103 | Steps: 4 | Val loss: 2.2207 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:02:48 (running for 00:09:40.91)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  0.898 |      0.356 |                   96 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  2.1   |      0.247 |                   95 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.635 |      0.13  |                   17 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.781 |      0.207 |                   16 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.4435634328358209
[2m[36m(func pid=143647)[0m top5: 0.9123134328358209
[2m[36m(func pid=143647)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=143647)[0m f1_macro: 0.3558088746727731
[2m[36m(func pid=143647)[0m f1_weighted: 0.46562287535802294
[2m[36m(func pid=143647)[0m f1_per_class: [0.367, 0.4, 0.231, 0.551, 0.08, 0.208, 0.562, 0.534, 0.196, 0.429]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.30363805970149255
[2m[36m(func pid=144451)[0m top5: 0.8847947761194029
[2m[36m(func pid=144451)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=144451)[0m f1_macro: 0.24893722524635348
[2m[36m(func pid=144451)[0m f1_weighted: 0.3097759132702724
[2m[36m(func pid=144451)[0m f1_per_class: [0.101, 0.239, 0.268, 0.476, 0.062, 0.355, 0.19, 0.466, 0.167, 0.165]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.23600746268656717
[2m[36m(func pid=162589)[0m top5: 0.7285447761194029
[2m[36m(func pid=162589)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=162589)[0m f1_macro: 0.13397202323996232
[2m[36m(func pid=162589)[0m f1_weighted: 0.17185131819345442
[2m[36m(func pid=162589)[0m f1_per_class: [0.037, 0.091, 0.061, 0.391, 0.067, 0.156, 0.0, 0.462, 0.0, 0.074]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.0130 | Steps: 4 | Val loss: 1.9175 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.7883 | Steps: 4 | Val loss: 1.8052 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.6622 | Steps: 4 | Val loss: 2.1343 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=163372)[0m top1: 0.30923507462686567
[2m[36m(func pid=163372)[0m top5: 0.7859141791044776
[2m[36m(func pid=163372)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=163372)[0m f1_macro: 0.26598712938340235
[2m[36m(func pid=163372)[0m f1_weighted: 0.2888715307784676
[2m[36m(func pid=163372)[0m f1_per_class: [0.182, 0.48, 0.488, 0.0, 0.049, 0.255, 0.455, 0.489, 0.141, 0.121]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5858 | Steps: 4 | Val loss: 2.2106 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:02:54 (running for 00:09:46.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.788 |      0.359 |                   97 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.52  |      0.249 |                   96 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.51  |      0.134 |                   18 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.013 |      0.266 |                   17 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.4388992537313433
[2m[36m(func pid=143647)[0m top5: 0.9039179104477612
[2m[36m(func pid=143647)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=143647)[0m f1_macro: 0.35927519280163417
[2m[36m(func pid=143647)[0m f1_weighted: 0.47350966949063356
[2m[36m(func pid=143647)[0m f1_per_class: [0.288, 0.496, 0.161, 0.526, 0.08, 0.232, 0.543, 0.549, 0.258, 0.459]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.314365671641791
[2m[36m(func pid=144451)[0m top5: 0.8796641791044776
[2m[36m(func pid=144451)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=144451)[0m f1_macro: 0.2582851666182952
[2m[36m(func pid=144451)[0m f1_weighted: 0.3329743991792672
[2m[36m(func pid=144451)[0m f1_per_class: [0.107, 0.248, 0.247, 0.473, 0.068, 0.342, 0.261, 0.508, 0.196, 0.133]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=162589)[0m top1: 0.25652985074626866
[2m[36m(func pid=162589)[0m top5: 0.7430037313432836
[2m[36m(func pid=162589)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=162589)[0m f1_macro: 0.15249992167396575
[2m[36m(func pid=162589)[0m f1_weighted: 0.19730443461687366
[2m[36m(func pid=162589)[0m f1_per_class: [0.031, 0.14, 0.061, 0.418, 0.109, 0.247, 0.0, 0.447, 0.0, 0.071]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.0204 | Steps: 4 | Val loss: 2.0068 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.1220 | Steps: 4 | Val loss: 1.9661 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.8986 | Steps: 4 | Val loss: 2.2003 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5702 | Steps: 4 | Val loss: 2.1918 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=163372)[0m top1: 0.2667910447761194
[2m[36m(func pid=163372)[0m top5: 0.7719216417910447
[2m[36m(func pid=163372)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=163372)[0m f1_macro: 0.23238716866351
[2m[36m(func pid=163372)[0m f1_weighted: 0.25353889080289943
[2m[36m(func pid=163372)[0m f1_per_class: [0.195, 0.453, 0.417, 0.0, 0.04, 0.142, 0.416, 0.418, 0.077, 0.167]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:02:59 (running for 00:09:52.09)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.122 |      0.344 |                   98 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.662 |      0.258 |                   97 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.586 |      0.152 |                   19 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.02  |      0.232 |                   18 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.40904850746268656
[2m[36m(func pid=143647)[0m top5: 0.9020522388059702
[2m[36m(func pid=143647)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=143647)[0m f1_macro: 0.344487688024253
[2m[36m(func pid=143647)[0m f1_weighted: 0.4487140701648416
[2m[36m(func pid=143647)[0m f1_per_class: [0.243, 0.49, 0.157, 0.503, 0.061, 0.204, 0.503, 0.547, 0.194, 0.542]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=162589)[0m top1: 0.2737873134328358
[2m[36m(func pid=162589)[0m top5: 0.7350746268656716
[2m[36m(func pid=162589)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=162589)[0m f1_macro: 0.16554584187971683
[2m[36m(func pid=162589)[0m f1_weighted: 0.20435962207662242
[2m[36m(func pid=162589)[0m f1_per_class: [0.093, 0.117, 0.095, 0.445, 0.098, 0.242, 0.0, 0.489, 0.0, 0.077]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3283582089552239
[2m[36m(func pid=144451)[0m top5: 0.8698694029850746
[2m[36m(func pid=144451)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=144451)[0m f1_macro: 0.2721732700526076
[2m[36m(func pid=144451)[0m f1_weighted: 0.35687296164128696
[2m[36m(func pid=144451)[0m f1_per_class: [0.112, 0.382, 0.216, 0.447, 0.068, 0.336, 0.284, 0.538, 0.2, 0.139]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7788 | Steps: 4 | Val loss: 2.0584 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8714 | Steps: 4 | Val loss: 2.1399 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5677 | Steps: 4 | Val loss: 2.1852 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.6330 | Steps: 4 | Val loss: 2.3910 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=163372)[0m top1: 0.23041044776119404
[2m[36m(func pid=163372)[0m top5: 0.7630597014925373
[2m[36m(func pid=163372)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=163372)[0m f1_macro: 0.2336991470407614
[2m[36m(func pid=163372)[0m f1_weighted: 0.2093776141738411
[2m[36m(func pid=163372)[0m f1_per_class: [0.256, 0.456, 0.328, 0.0, 0.044, 0.16, 0.245, 0.435, 0.095, 0.317]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m top1: 0.2835820895522388
[2m[36m(func pid=162589)[0m top5: 0.7336753731343284
[2m[36m(func pid=162589)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=162589)[0m f1_macro: 0.17959438308900233
[2m[36m(func pid=162589)[0m f1_weighted: 0.21823356815159167
[2m[36m(func pid=162589)[0m f1_per_class: [0.094, 0.156, 0.103, 0.46, 0.114, 0.253, 0.0, 0.476, 0.065, 0.074]
[2m[36m(func pid=162589)[0m 
== Status ==
Current time: 2024-01-07 13:03:05 (running for 00:09:57.70)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | RUNNING    | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.122 |      0.344 |                   98 |
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.899 |      0.272 |                   98 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.568 |      0.18  |                   21 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.779 |      0.234 |                   19 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.3833955223880597
[2m[36m(func pid=143647)[0m top5: 0.9053171641791045
[2m[36m(func pid=143647)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=143647)[0m f1_macro: 0.3262419452145483
[2m[36m(func pid=143647)[0m f1_weighted: 0.4317591646160657
[2m[36m(func pid=143647)[0m f1_per_class: [0.215, 0.516, 0.115, 0.498, 0.038, 0.151, 0.465, 0.551, 0.123, 0.591]
[2m[36m(func pid=143647)[0m 
[2m[36m(func pid=144451)[0m top1: 0.3278917910447761
[2m[36m(func pid=144451)[0m top5: 0.8530783582089553
[2m[36m(func pid=144451)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=144451)[0m f1_macro: 0.28157416315973804
[2m[36m(func pid=144451)[0m f1_weighted: 0.36402016296993167
[2m[36m(func pid=144451)[0m f1_per_class: [0.156, 0.445, 0.255, 0.378, 0.084, 0.3, 0.347, 0.544, 0.179, 0.127]
[2m[36m(func pid=144451)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5421 | Steps: 4 | Val loss: 2.0099 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=144451)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.6249 | Steps: 4 | Val loss: 2.6941 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=143647)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.2046 | Steps: 4 | Val loss: 2.1320 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6084 | Steps: 4 | Val loss: 2.1762 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=163372)[0m top1: 0.22108208955223882
[2m[36m(func pid=163372)[0m top5: 0.7863805970149254
[2m[36m(func pid=163372)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=163372)[0m f1_macro: 0.2262498303017222
[2m[36m(func pid=163372)[0m f1_weighted: 0.18440318969307226
[2m[36m(func pid=163372)[0m f1_per_class: [0.262, 0.442, 0.353, 0.126, 0.046, 0.285, 0.006, 0.453, 0.061, 0.227]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:03:10 (running for 00:10:03.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.324
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 3 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00002 | RUNNING    | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.633 |      0.282 |                   99 |
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.568 |      0.18  |                   21 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.542 |      0.226 |                   20 |
| train_52b21_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=143647)[0m top1: 0.37080223880597013
[2m[36m(func pid=143647)[0m top5: 0.9095149253731343
[2m[36m(func pid=143647)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=143647)[0m f1_macro: 0.30932682264437006
[2m[36m(func pid=143647)[0m f1_weighted: 0.4154162965836928
[2m[36m(func pid=143647)[0m f1_per_class: [0.213, 0.521, 0.03, 0.495, 0.034, 0.166, 0.413, 0.534, 0.078, 0.609]
[2m[36m(func pid=144451)[0m top1: 0.29244402985074625
[2m[36m(func pid=144451)[0m top5: 0.8311567164179104
[2m[36m(func pid=144451)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=144451)[0m f1_macro: 0.24944151381791618
[2m[36m(func pid=144451)[0m f1_weighted: 0.32513921785677685
[2m[36m(func pid=144451)[0m f1_per_class: [0.157, 0.447, 0.167, 0.295, 0.102, 0.217, 0.338, 0.51, 0.131, 0.131]
[2m[36m(func pid=162589)[0m top1: 0.27658582089552236
[2m[36m(func pid=162589)[0m top5: 0.75
[2m[36m(func pid=162589)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=162589)[0m f1_macro: 0.1897977992174578
[2m[36m(func pid=162589)[0m f1_weighted: 0.2273508928996343
[2m[36m(func pid=162589)[0m f1_per_class: [0.085, 0.236, 0.106, 0.456, 0.087, 0.201, 0.003, 0.482, 0.094, 0.148]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.7340 | Steps: 4 | Val loss: 2.0165 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4435 | Steps: 4 | Val loss: 2.1499 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=163372)[0m top1: 0.261660447761194
[2m[36m(func pid=163372)[0m top5: 0.8073694029850746
[2m[36m(func pid=163372)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=163372)[0m f1_macro: 0.2231327728084073
[2m[36m(func pid=163372)[0m f1_weighted: 0.25181658213962077
[2m[36m(func pid=163372)[0m f1_per_class: [0.205, 0.384, 0.174, 0.412, 0.05, 0.312, 0.0, 0.443, 0.049, 0.202]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m top1: 0.2971082089552239
[2m[36m(func pid=162589)[0m top5: 0.784981343283582
[2m[36m(func pid=162589)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=162589)[0m f1_macro: 0.21227931579380982
[2m[36m(func pid=162589)[0m f1_weighted: 0.2528280207772256
[2m[36m(func pid=162589)[0m f1_per_class: [0.104, 0.252, 0.154, 0.466, 0.095, 0.297, 0.028, 0.504, 0.079, 0.143]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8594 | Steps: 4 | Val loss: 2.0736 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=168552)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168552)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=168552)[0m Configuration completed!
[2m[36m(func pid=168552)[0m New optimizer parameters:
[2m[36m(func pid=168552)[0m SGD (
[2m[36m(func pid=168552)[0m Parameter Group 0
[2m[36m(func pid=168552)[0m     dampening: 0
[2m[36m(func pid=168552)[0m     differentiable: False
[2m[36m(func pid=168552)[0m     foreach: None
[2m[36m(func pid=168552)[0m     lr: 0.01
[2m[36m(func pid=168552)[0m     maximize: False
[2m[36m(func pid=168552)[0m     momentum: 0.9
[2m[36m(func pid=168552)[0m     nesterov: False
[2m[36m(func pid=168552)[0m     weight_decay: 0
[2m[36m(func pid=168552)[0m )
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4770 | Steps: 4 | Val loss: 2.1495 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:03:16 (running for 00:10:08.49)
Memory usage on this node: 20.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.443 |      0.212 |                   23 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.734 |      0.223 |                   21 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168563)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=168563)[0m Configuration completed!
[2m[36m(func pid=168563)[0m New optimizer parameters:
[2m[36m(func pid=168563)[0m SGD (
[2m[36m(func pid=168563)[0m Parameter Group 0
[2m[36m(func pid=168563)[0m     dampening: 0
[2m[36m(func pid=168563)[0m     differentiable: False
[2m[36m(func pid=168563)[0m     foreach: None
[2m[36m(func pid=168563)[0m     lr: 0.1
[2m[36m(func pid=168563)[0m     maximize: False
[2m[36m(func pid=168563)[0m     momentum: 0.9
[2m[36m(func pid=168563)[0m     nesterov: False
[2m[36m(func pid=168563)[0m     weight_decay: 0
[2m[36m(func pid=168563)[0m )
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.26632462686567165
[2m[36m(func pid=163372)[0m top5: 0.8022388059701493
[2m[36m(func pid=163372)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=163372)[0m f1_macro: 0.29195447710227
[2m[36m(func pid=163372)[0m f1_weighted: 0.2593303234759522
[2m[36m(func pid=163372)[0m f1_per_class: [0.258, 0.333, 0.7, 0.438, 0.082, 0.299, 0.0, 0.513, 0.11, 0.186]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:03:21 (running for 00:10:13.89)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.477 |      0.223 |                   24 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.859 |      0.292 |                   22 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |        |            |                      |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |        |            |                      |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m top1: 0.2835820895522388
[2m[36m(func pid=162589)[0m top5: 0.7943097014925373
[2m[36m(func pid=162589)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=162589)[0m f1_macro: 0.22294974267139808
[2m[36m(func pid=162589)[0m f1_weighted: 0.24944819392606762
[2m[36m(func pid=162589)[0m f1_per_class: [0.136, 0.259, 0.216, 0.455, 0.084, 0.244, 0.037, 0.506, 0.086, 0.207]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0271 | Steps: 4 | Val loss: 1.8594 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.8212 | Steps: 4 | Val loss: 3.0661 | Batch size: 32 | lr: 0.01 | Duration: 4.81s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.0990 | Steps: 4 | Val loss: 27602.5781 | Batch size: 32 | lr: 0.1 | Duration: 4.52s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4049 | Steps: 4 | Val loss: 2.1403 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=163372)[0m top1: 0.3204291044776119
[2m[36m(func pid=163372)[0m top5: 0.8246268656716418
[2m[36m(func pid=163372)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=163372)[0m f1_macro: 0.258115440595363
[2m[36m(func pid=163372)[0m f1_weighted: 0.29944383741015385
[2m[36m(func pid=163372)[0m f1_per_class: [0.246, 0.498, 0.143, 0.488, 0.073, 0.323, 0.006, 0.45, 0.106, 0.248]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.011194029850746268
[2m[36m(func pid=168552)[0m top5: 0.5601679104477612
[2m[36m(func pid=168552)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=168552)[0m f1_macro: 0.006617110989184436
[2m[36m(func pid=168552)[0m f1_weighted: 0.0006003146859932538
[2m[36m(func pid=168552)[0m f1_per_class: [0.014, 0.0, 0.052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.17210820895522388
[2m[36m(func pid=168563)[0m top5: 0.5727611940298507
[2m[36m(func pid=168563)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=168563)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=168563)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
== Status ==
Current time: 2024-01-07 13:03:26 (running for 00:10:19.14)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.405 |      0.218 |                   25 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  2.027 |      0.258 |                   23 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.821 |      0.007 |                    1 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  6.099 |      0.029 |                    1 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m top1: 0.26865671641791045
[2m[36m(func pid=162589)[0m top5: 0.8143656716417911
[2m[36m(func pid=162589)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=162589)[0m f1_macro: 0.21781449817193482
[2m[36m(func pid=162589)[0m f1_weighted: 0.24150156818701432
[2m[36m(func pid=162589)[0m f1_per_class: [0.137, 0.205, 0.224, 0.448, 0.071, 0.229, 0.051, 0.492, 0.132, 0.188]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7120 | Steps: 4 | Val loss: 1.7607 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0543 | Steps: 4 | Val loss: 3.5112 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 13.4370 | Steps: 4 | Val loss: 14032083.0000 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4021 | Steps: 4 | Val loss: 2.1315 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163372)[0m top1: 0.34794776119402987
[2m[36m(func pid=163372)[0m top5: 0.8330223880597015
[2m[36m(func pid=163372)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=163372)[0m f1_macro: 0.30237569259881636
[2m[36m(func pid=163372)[0m f1_weighted: 0.342570118807725
[2m[36m(func pid=163372)[0m f1_per_class: [0.31, 0.554, 0.556, 0.404, 0.105, 0.279, 0.215, 0.425, 0.063, 0.112]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.09001865671641791
[2m[36m(func pid=168552)[0m top5: 0.7126865671641791
[2m[36m(func pid=168552)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=168552)[0m f1_macro: 0.05656384446814509
[2m[36m(func pid=168552)[0m f1_weighted: 0.08604532928152339
[2m[36m(func pid=168552)[0m f1_per_class: [0.028, 0.442, 0.065, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2789179104477612
[2m[36m(func pid=168563)[0m top5: 0.7821828358208955
[2m[36m(func pid=168563)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=168563)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=168563)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
== Status ==
Current time: 2024-01-07 13:03:32 (running for 00:10:24.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.402 |      0.195 |                   26 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.712 |      0.302 |                   24 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  3.054 |      0.057 |                    2 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      | 13.437 |      0.044 |                    2 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m top1: 0.26492537313432835
[2m[36m(func pid=162589)[0m top5: 0.8260261194029851
[2m[36m(func pid=162589)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=162589)[0m f1_macro: 0.19529263409677816
[2m[36m(func pid=162589)[0m f1_weighted: 0.23649130161997192
[2m[36m(func pid=162589)[0m f1_per_class: [0.117, 0.158, 0.156, 0.466, 0.061, 0.231, 0.054, 0.492, 0.089, 0.129]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7786 | Steps: 4 | Val loss: 2.5094 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3153 | Steps: 4 | Val loss: 3.9707 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 9.2605 | Steps: 4 | Val loss: 1725619.8750 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4311 | Steps: 4 | Val loss: 2.1164 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=163372)[0m top1: 0.19496268656716417
[2m[36m(func pid=163372)[0m top5: 0.7807835820895522
[2m[36m(func pid=163372)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=163372)[0m f1_macro: 0.14789504671230433
[2m[36m(func pid=163372)[0m f1_weighted: 0.17215335302633006
[2m[36m(func pid=163372)[0m f1_per_class: [0.129, 0.51, 0.353, 0.229, 0.116, 0.0, 0.036, 0.047, 0.028, 0.031]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.08628731343283583
[2m[36m(func pid=168552)[0m top5: 0.7425373134328358
[2m[36m(func pid=168552)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=168552)[0m f1_macro: 0.12599940578830893
[2m[36m(func pid=168552)[0m f1_weighted: 0.09853802104302312
[2m[36m(func pid=168552)[0m f1_per_class: [0.054, 0.287, 0.571, 0.0, 0.189, 0.0, 0.142, 0.017, 0.0, 0.0]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.05783582089552239
[2m[36m(func pid=168563)[0m top5: 0.5149253731343284
[2m[36m(func pid=168563)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=168563)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=168563)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.27005597014925375
[2m[36m(func pid=162589)[0m top5: 0.8376865671641791
[2m[36m(func pid=162589)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=162589)[0m f1_macro: 0.18866549150624554
[2m[36m(func pid=162589)[0m f1_weighted: 0.24595281582153514
[2m[36m(func pid=162589)[0m f1_per_class: [0.105, 0.168, 0.129, 0.482, 0.056, 0.196, 0.085, 0.503, 0.026, 0.138]== Status ==
Current time: 2024-01-07 13:03:37 (running for 00:10:30.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.431 |      0.189 |                   27 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.779 |      0.148 |                   25 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.315 |      0.126 |                    3 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  9.26  |      0.011 |                    3 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6982 | Steps: 4 | Val loss: 2.5168 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8959 | Steps: 4 | Val loss: 8.0161 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 8.7555 | Steps: 4 | Val loss: 71100.1172 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5419 | Steps: 4 | Val loss: 2.1081 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=163372)[0m top1: 0.16930970149253732
[2m[36m(func pid=163372)[0m top5: 0.761660447761194
[2m[36m(func pid=163372)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=163372)[0m f1_macro: 0.10670561042462887
[2m[36m(func pid=163372)[0m f1_weighted: 0.17411678767195377
[2m[36m(func pid=163372)[0m f1_per_class: [0.0, 0.481, 0.135, 0.233, 0.098, 0.0, 0.082, 0.0, 0.0, 0.038]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.08722014925373134
[2m[36m(func pid=168552)[0m top5: 0.6124067164179104
[2m[36m(func pid=168552)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=168552)[0m f1_macro: 0.07838503817293385
[2m[36m(func pid=168552)[0m f1_weighted: 0.09939959733926979
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.301, 0.052, 0.0, 0.156, 0.0, 0.139, 0.035, 0.069, 0.032]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.0065298507462686565
[2m[36m(func pid=168563)[0m top5: 0.5102611940298507
[2m[36m(func pid=168563)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=168563)[0m f1_macro: 0.0013539651837524177
[2m[36m(func pid=168563)[0m f1_weighted: 0.00010104217789197148
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
== Status ==
Current time: 2024-01-07 13:03:42 (running for 00:10:35.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.542 |      0.199 |                   28 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.698 |      0.107 |                   26 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.896 |      0.078 |                    4 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  8.755 |      0.001 |                    4 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m top1: 0.2756529850746269
[2m[36m(func pid=162589)[0m top5: 0.8442164179104478
[2m[36m(func pid=162589)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=162589)[0m f1_macro: 0.1988982463536457
[2m[36m(func pid=162589)[0m f1_weighted: 0.2595098681905523
[2m[36m(func pid=162589)[0m f1_per_class: [0.122, 0.193, 0.132, 0.491, 0.05, 0.137, 0.126, 0.508, 0.024, 0.207]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9670 | Steps: 4 | Val loss: 9.5423 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8848 | Steps: 4 | Val loss: 2.5400 | Batch size: 32 | lr: 0.001 | Duration: 3.34s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 7.3597 | Steps: 4 | Val loss: 8157.7183 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3505 | Steps: 4 | Val loss: 2.0940 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=168552)[0m top1: 0.09794776119402986
[2m[36m(func pid=168552)[0m top5: 0.6259328358208955
[2m[36m(func pid=168552)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=168552)[0m f1_macro: 0.07624836287510554
[2m[36m(func pid=168552)[0m f1_weighted: 0.08282269053053035
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.316, 0.035, 0.0, 0.0, 0.092, 0.0, 0.302, 0.0, 0.018]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.17210820895522388
[2m[36m(func pid=163372)[0m top5: 0.7192164179104478
[2m[36m(func pid=163372)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=163372)[0m f1_macro: 0.1439448386275819
[2m[36m(func pid=163372)[0m f1_weighted: 0.18786329129646576
[2m[36m(func pid=163372)[0m f1_per_class: [0.284, 0.453, 0.168, 0.299, 0.081, 0.0, 0.056, 0.0, 0.056, 0.043]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:03:48 (running for 00:10:40.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.542 |      0.199 |                   28 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.885 |      0.144 |                   27 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.967 |      0.076 |                    5 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  7.36  |      0.001 |                    5 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m top1: 0.28638059701492535
[2m[36m(func pid=162589)[0m top5: 0.8572761194029851
[2m[36m(func pid=162589)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=162589)[0m f1_macro: 0.20857012182488366
[2m[36m(func pid=162589)[0m f1_weighted: 0.28209265305227466
[2m[36m(func pid=162589)[0m f1_per_class: [0.156, 0.224, 0.138, 0.502, 0.046, 0.097, 0.188, 0.485, 0.05, 0.2]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168563)[0m top1: 0.006063432835820896
[2m[36m(func pid=168563)[0m top5: 0.5135261194029851
[2m[36m(func pid=168563)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=168563)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=168563)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.2743 | Steps: 4 | Val loss: 5.3576 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4016 | Steps: 4 | Val loss: 2.4211 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3698 | Steps: 4 | Val loss: 2.0871 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.0120 | Steps: 4 | Val loss: 1799.2079 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=168552)[0m top1: 0.09654850746268656
[2m[36m(func pid=168552)[0m top5: 0.7625932835820896
[2m[36m(func pid=168552)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=168552)[0m f1_macro: 0.08341870652917521
[2m[36m(func pid=168552)[0m f1_weighted: 0.11619947074781839
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.31, 0.0, 0.153, 0.0, 0.0, 0.0, 0.344, 0.0, 0.027]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:03:53 (running for 00:10:45.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.35  |      0.209 |                   29 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.885 |      0.144 |                   27 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  3.274 |      0.083 |                    6 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  4.012 |      0.013 |                    6 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.05830223880597015
[2m[36m(func pid=168563)[0m top5: 0.5606343283582089
[2m[36m(func pid=168563)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=168563)[0m f1_macro: 0.012795514606099564
[2m[36m(func pid=168563)[0m f1_weighted: 0.007083465919811698
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.115, 0.013, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.17350746268656717
[2m[36m(func pid=163372)[0m top5: 0.738339552238806
[2m[36m(func pid=163372)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=163372)[0m f1_macro: 0.15731711377177998
[2m[36m(func pid=163372)[0m f1_weighted: 0.1851549145537429
[2m[36m(func pid=163372)[0m f1_per_class: [0.173, 0.446, 0.0, 0.176, 0.053, 0.021, 0.074, 0.474, 0.079, 0.078]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28777985074626866
[2m[36m(func pid=162589)[0m top5: 0.8684701492537313
[2m[36m(func pid=162589)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=162589)[0m f1_macro: 0.21209422861500551
[2m[36m(func pid=162589)[0m f1_weighted: 0.28616677995409046
[2m[36m(func pid=162589)[0m f1_per_class: [0.174, 0.218, 0.137, 0.501, 0.045, 0.129, 0.198, 0.477, 0.0, 0.242]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7536 | Steps: 4 | Val loss: 2.4464 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 8.7121 | Steps: 4 | Val loss: 104.0979 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5109 | Steps: 4 | Val loss: 2.2213 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3964 | Steps: 4 | Val loss: 2.0795 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=168552)[0m top1: 0.23694029850746268
[2m[36m(func pid=168552)[0m top5: 0.7798507462686567
[2m[36m(func pid=168552)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=168552)[0m f1_macro: 0.1980764137055629
[2m[36m(func pid=168552)[0m f1_weighted: 0.18526410260286297
[2m[36m(func pid=168552)[0m f1_per_class: [0.04, 0.396, 0.632, 0.287, 0.069, 0.0, 0.003, 0.531, 0.0, 0.023]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:03:59 (running for 00:10:51.31)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.37  |      0.212 |                   30 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.402 |      0.157 |                   28 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.754 |      0.198 |                    7 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  8.712 |      0.045 |                    7 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=168563)[0m top1: 0.09048507462686567

[2m[36m(func pid=168563)[0m top5: 0.6693097014925373
[2m[36m(func pid=168563)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=168563)[0m f1_macro: 0.044898980587907394
[2m[36m(func pid=168563)[0m f1_weighted: 0.039876305043817846
[2m[36m(func pid=168563)[0m f1_per_class: [0.042, 0.027, 0.0, 0.0, 0.0, 0.258, 0.0, 0.035, 0.087, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.20755597014925373
[2m[36m(func pid=163372)[0m top5: 0.7854477611940298
[2m[36m(func pid=163372)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=163372)[0m f1_macro: 0.20774391306270562
[2m[36m(func pid=163372)[0m f1_weighted: 0.17998481650215975
[2m[36m(func pid=163372)[0m f1_per_class: [0.19, 0.479, 0.471, 0.16, 0.057, 0.114, 0.012, 0.46, 0.032, 0.101]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m top1: 0.2868470149253731
[2m[36m(func pid=162589)[0m top5: 0.8824626865671642
[2m[36m(func pid=162589)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=162589)[0m f1_macro: 0.2143237866475424
[2m[36m(func pid=162589)[0m f1_weighted: 0.2884970308956843
[2m[36m(func pid=162589)[0m f1_per_class: [0.203, 0.197, 0.15, 0.5, 0.045, 0.111, 0.227, 0.448, 0.026, 0.235]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.0985 | Steps: 4 | Val loss: 6.2845 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 6.9944 | Steps: 4 | Val loss: 81.9859 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2759 | Steps: 4 | Val loss: 2.0781 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6092 | Steps: 4 | Val loss: 2.5078 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:04:04 (running for 00:10:56.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.396 |      0.214 |                   31 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.511 |      0.208 |                   29 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  4.098 |      0.107 |                    8 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  8.712 |      0.045 |                    7 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.1875
[2m[36m(func pid=168552)[0m top5: 0.5153917910447762
[2m[36m(func pid=168552)[0m f1_micro: 0.1875
[2m[36m(func pid=168552)[0m f1_macro: 0.10672710799407352
[2m[36m(func pid=168552)[0m f1_weighted: 0.13499210085590557
[2m[36m(func pid=168552)[0m f1_per_class: [0.171, 0.119, 0.066, 0.377, 0.05, 0.016, 0.0, 0.0, 0.0, 0.267]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28125
[2m[36m(func pid=162589)[0m top5: 0.8824626865671642
[2m[36m(func pid=162589)[0m f1_micro: 0.28125
[2m[36m(func pid=162589)[0m f1_macro: 0.2094840563535541
[2m[36m(func pid=162589)[0m f1_weighted: 0.29204287039378946
[2m[36m(func pid=162589)[0m f1_per_class: [0.162, 0.196, 0.131, 0.496, 0.046, 0.167, 0.231, 0.417, 0.026, 0.222]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168563)[0m top1: 0.04477611940298507
[2m[36m(func pid=168563)[0m top5: 0.7513992537313433
[2m[36m(func pid=168563)[0m f1_micro: 0.04477611940298508
[2m[36m(func pid=168563)[0m f1_macro: 0.019610582000754544
[2m[36m(func pid=168563)[0m f1_weighted: 0.01738434819913172
[2m[36m(func pid=168563)[0m f1_per_class: [0.081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.045, 0.0, 0.07, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.20055970149253732
[2m[36m(func pid=163372)[0m top5: 0.7686567164179104
[2m[36m(func pid=163372)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=163372)[0m f1_macro: 0.18499590028247942
[2m[36m(func pid=163372)[0m f1_weighted: 0.18135412206218574
[2m[36m(func pid=163372)[0m f1_per_class: [0.17, 0.538, 0.267, 0.171, 0.083, 0.026, 0.006, 0.506, 0.019, 0.064]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4081 | Steps: 4 | Val loss: 10.7543 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.8143 | Steps: 4 | Val loss: 25.5408 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4297 | Steps: 4 | Val loss: 2.0841 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.7013 | Steps: 4 | Val loss: 2.5945 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:04:09 (running for 00:11:01.71)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.276 |      0.209 |                   32 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.609 |      0.185 |                   30 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.408 |      0.071 |                    9 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  6.994 |      0.02  |                    8 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.049440298507462684
[2m[36m(func pid=168552)[0m top5: 0.435634328358209
[2m[36m(func pid=168552)[0m f1_micro: 0.049440298507462684
[2m[36m(func pid=168552)[0m f1_macro: 0.07143645479297592
[2m[36m(func pid=168552)[0m f1_weighted: 0.03719885204447208
[2m[36m(func pid=168552)[0m f1_per_class: [0.061, 0.147, 0.0, 0.0, 0.136, 0.039, 0.0, 0.0, 0.059, 0.273]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.08162313432835822
[2m[36m(func pid=168563)[0m top5: 0.7523320895522388
[2m[36m(func pid=168563)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=168563)[0m f1_macro: 0.04057221151026521
[2m[36m(func pid=168563)[0m f1_weighted: 0.019107456811512265
[2m[36m(func pid=168563)[0m f1_per_class: [0.075, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.216, 0.099, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.27472014925373134
[2m[36m(func pid=162589)[0m top5: 0.8773320895522388
[2m[36m(func pid=162589)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=162589)[0m f1_macro: 0.20212062324188937
[2m[36m(func pid=162589)[0m f1_weighted: 0.29644900519651035
[2m[36m(func pid=162589)[0m f1_per_class: [0.094, 0.226, 0.114, 0.483, 0.047, 0.185, 0.244, 0.406, 0.0, 0.222]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.24347014925373134
[2m[36m(func pid=163372)[0m top5: 0.7957089552238806
[2m[36m(func pid=163372)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=163372)[0m f1_macro: 0.2655992450722892
[2m[36m(func pid=163372)[0m f1_weighted: 0.2539523199800591
[2m[36m(func pid=163372)[0m f1_per_class: [0.244, 0.476, 0.833, 0.475, 0.104, 0.03, 0.0, 0.4, 0.047, 0.047]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3176 | Steps: 4 | Val loss: 10.2352 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.2559 | Steps: 4 | Val loss: 8.5767 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4081 | Steps: 4 | Val loss: 2.0668 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.5084 | Steps: 4 | Val loss: 2.3402 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:04:14 (running for 00:11:07.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.43  |      0.202 |                   33 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.701 |      0.266 |                   31 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.318 |      0.098 |                   10 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  4.814 |      0.041 |                    9 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.10354477611940298
[2m[36m(func pid=168552)[0m top5: 0.5489738805970149
[2m[36m(func pid=168552)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=168552)[0m f1_macro: 0.09848784105321869
[2m[36m(func pid=168552)[0m f1_weighted: 0.10060115693502145
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.454, 0.025, 0.0, 0.107, 0.142, 0.0, 0.013, 0.075, 0.169]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.09748134328358209
[2m[36m(func pid=168563)[0m top5: 0.6986940298507462
[2m[36m(func pid=168563)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=168563)[0m f1_macro: 0.03574554138116558
[2m[36m(func pid=168563)[0m f1_weighted: 0.041260966525609497
[2m[36m(func pid=168563)[0m f1_per_class: [0.067, 0.0, 0.0, 0.041, 0.0, 0.249, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.2826492537313433
[2m[36m(func pid=162589)[0m top5: 0.8805970149253731
[2m[36m(func pid=162589)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=162589)[0m f1_macro: 0.21324721628724652
[2m[36m(func pid=162589)[0m f1_weighted: 0.30062326484961693
[2m[36m(func pid=162589)[0m f1_per_class: [0.147, 0.241, 0.121, 0.49, 0.048, 0.234, 0.22, 0.387, 0.027, 0.216]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.28171641791044777
[2m[36m(func pid=163372)[0m top5: 0.8264925373134329
[2m[36m(func pid=163372)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=163372)[0m f1_macro: 0.2199692013326183
[2m[36m(func pid=163372)[0m f1_weighted: 0.27572782374645727
[2m[36m(func pid=163372)[0m f1_per_class: [0.254, 0.397, 0.171, 0.561, 0.113, 0.097, 0.006, 0.508, 0.027, 0.065]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8387 | Steps: 4 | Val loss: 18.2521 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.8274 | Steps: 4 | Val loss: 2.4941 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3407 | Steps: 4 | Val loss: 2.0542 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5136 | Steps: 4 | Val loss: 1.9754 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:04:20 (running for 00:11:12.64)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.408 |      0.213 |                   34 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.508 |      0.22  |                   32 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.839 |      0.054 |                   11 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  3.256 |      0.036 |                   10 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.08069029850746269
[2m[36m(func pid=168552)[0m top5: 0.5764925373134329
[2m[36m(func pid=168552)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=168552)[0m f1_macro: 0.05368746464070671
[2m[36m(func pid=168552)[0m f1_weighted: 0.08050135083103256
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.432, 0.022, 0.016, 0.0, 0.0, 0.0, 0.0, 0.035, 0.032]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.12080223880597014
[2m[36m(func pid=168563)[0m top5: 0.6581156716417911
[2m[36m(func pid=168563)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=168563)[0m f1_macro: 0.0841475676758466
[2m[36m(func pid=168563)[0m f1_weighted: 0.0850942530256057
[2m[36m(func pid=168563)[0m f1_per_class: [0.058, 0.0, 0.0, 0.115, 0.0, 0.232, 0.0, 0.436, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.29197761194029853
[2m[36m(func pid=162589)[0m top5: 0.8889925373134329
[2m[36m(func pid=162589)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=162589)[0m f1_macro: 0.21655553248867707
[2m[36m(func pid=162589)[0m f1_weighted: 0.31178095323259053
[2m[36m(func pid=162589)[0m f1_per_class: [0.138, 0.305, 0.132, 0.482, 0.051, 0.253, 0.224, 0.399, 0.0, 0.182]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.314365671641791
[2m[36m(func pid=163372)[0m top5: 0.847481343283582
[2m[36m(func pid=163372)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=163372)[0m f1_macro: 0.23185725889191683
[2m[36m(func pid=163372)[0m f1_weighted: 0.306728700797698
[2m[36m(func pid=163372)[0m f1_per_class: [0.19, 0.329, 0.146, 0.546, 0.102, 0.236, 0.112, 0.523, 0.0, 0.133]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5352 | Steps: 4 | Val loss: 14.3490 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.7478 | Steps: 4 | Val loss: 2.0838 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4301 | Steps: 4 | Val loss: 2.0628 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3340 | Steps: 4 | Val loss: 1.9458 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:04:25 (running for 00:11:17.97)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.341 |      0.217 |                   35 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.514 |      0.232 |                   33 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.535 |      0.046 |                   12 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  3.827 |      0.084 |                   11 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.07975746268656717
[2m[36m(func pid=168552)[0m top5: 0.5713619402985075
[2m[36m(func pid=168552)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=168552)[0m f1_macro: 0.045998752038843196
[2m[36m(func pid=168552)[0m f1_weighted: 0.07476551400948665
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.433, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.3978544776119403
[2m[36m(func pid=168563)[0m top5: 0.6301305970149254
[2m[36m(func pid=168563)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=168563)[0m f1_macro: 0.16915989057494252
[2m[36m(func pid=168563)[0m f1_weighted: 0.33654822540942564
[2m[36m(func pid=168563)[0m f1_per_class: [0.02, 0.0, 0.0, 0.511, 0.0, 0.15, 0.492, 0.519, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28078358208955223
[2m[36m(func pid=162589)[0m top5: 0.8833955223880597
[2m[36m(func pid=162589)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=162589)[0m f1_macro: 0.2190166860162618
[2m[36m(func pid=162589)[0m f1_weighted: 0.30945144027995036
[2m[36m(func pid=162589)[0m f1_per_class: [0.099, 0.38, 0.121, 0.428, 0.048, 0.234, 0.216, 0.456, 0.075, 0.133]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.33908582089552236
[2m[36m(func pid=163372)[0m top5: 0.8577425373134329
[2m[36m(func pid=163372)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=163372)[0m f1_macro: 0.28015760084639457
[2m[36m(func pid=163372)[0m f1_weighted: 0.35587593351813
[2m[36m(func pid=163372)[0m f1_per_class: [0.227, 0.411, 0.392, 0.536, 0.12, 0.246, 0.228, 0.523, 0.0, 0.118]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.2750 | Steps: 4 | Val loss: 6.1842 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.1213 | Steps: 4 | Val loss: 2.4653 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3041 | Steps: 4 | Val loss: 2.0579 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.6753 | Steps: 4 | Val loss: 1.8739 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=168552)[0m top1: 0.134794776119403
[2m[36m(func pid=168552)[0m top5: 0.4986007462686567
[2m[36m(func pid=168552)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=168552)[0m f1_macro: 0.12108377161442715
[2m[36m(func pid=168552)[0m f1_weighted: 0.08027420649336149
[2m[36m(func pid=168552)[0m f1_per_class: [0.209, 0.404, 0.556, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.033]
== Status ==
Current time: 2024-01-07 13:04:30 (running for 00:11:23.18)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.43  |      0.219 |                   36 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.334 |      0.28  |                   34 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.275 |      0.121 |                   13 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  3.748 |      0.169 |                   12 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.09468283582089553
[2m[36m(func pid=168563)[0m top5: 0.5890858208955224
[2m[36m(func pid=168563)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=168563)[0m f1_macro: 0.0704274328981992
[2m[36m(func pid=168563)[0m f1_weighted: 0.06585349872249723
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.066, 0.0, 0.211, 0.0, 0.397, 0.0, 0.03]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28078358208955223
[2m[36m(func pid=162589)[0m top5: 0.8847947761194029
[2m[36m(func pid=162589)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=162589)[0m f1_macro: 0.2195164844596103
[2m[36m(func pid=162589)[0m f1_weighted: 0.3006404798019195
[2m[36m(func pid=162589)[0m f1_per_class: [0.096, 0.405, 0.121, 0.422, 0.053, 0.271, 0.165, 0.454, 0.071, 0.138]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.34654850746268656
[2m[36m(func pid=163372)[0m top5: 0.8708022388059702
[2m[36m(func pid=163372)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=163372)[0m f1_macro: 0.3189796974954712
[2m[36m(func pid=163372)[0m f1_weighted: 0.3807180436885527
[2m[36m(func pid=163372)[0m f1_per_class: [0.224, 0.426, 0.615, 0.474, 0.09, 0.311, 0.327, 0.524, 0.022, 0.177]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9782 | Steps: 4 | Val loss: 4.0587 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9301 | Steps: 4 | Val loss: 2.3177 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3061 | Steps: 4 | Val loss: 2.0555 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.1580 | Steps: 4 | Val loss: 1.9814 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:04:36 (running for 00:11:28.52)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.304 |      0.22  |                   37 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.675 |      0.319 |                   35 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.978 |      0.163 |                   14 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  4.121 |      0.07  |                   13 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.271455223880597
[2m[36m(func pid=168552)[0m top5: 0.7607276119402985
[2m[36m(func pid=168552)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=168552)[0m f1_macro: 0.16257040552500832
[2m[36m(func pid=168552)[0m f1_weighted: 0.22069748254192662
[2m[36m(func pid=168552)[0m f1_per_class: [0.039, 0.372, 0.0, 0.363, 0.0, 0.223, 0.0, 0.411, 0.125, 0.092]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.0708955223880597
[2m[36m(func pid=168563)[0m top5: 0.5317164179104478
[2m[36m(func pid=168563)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=168563)[0m f1_macro: 0.0656925180295014
[2m[36m(func pid=168563)[0m f1_weighted: 0.04777818382555491
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.141, 0.012, 0.478, 0.0, 0.025]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28031716417910446
[2m[36m(func pid=162589)[0m top5: 0.8861940298507462
[2m[36m(func pid=162589)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=162589)[0m f1_macro: 0.2253950257869961
[2m[36m(func pid=162589)[0m f1_weighted: 0.3020270015840073
[2m[36m(func pid=162589)[0m f1_per_class: [0.119, 0.408, 0.122, 0.422, 0.055, 0.273, 0.164, 0.425, 0.127, 0.138]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3041044776119403
[2m[36m(func pid=163372)[0m top5: 0.8596082089552238
[2m[36m(func pid=163372)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=163372)[0m f1_macro: 0.3208337614178408
[2m[36m(func pid=163372)[0m f1_weighted: 0.3357245284538726
[2m[36m(func pid=163372)[0m f1_per_class: [0.224, 0.532, 0.667, 0.378, 0.067, 0.261, 0.224, 0.443, 0.118, 0.295]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3922 | Steps: 4 | Val loss: 3.4170 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8199 | Steps: 4 | Val loss: 2.2711 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3034 | Steps: 4 | Val loss: 2.0245 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.3723 | Steps: 4 | Val loss: 1.9320 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:04:41 (running for 00:11:33.95)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.306 |      0.225 |                   38 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.158 |      0.321 |                   36 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.392 |      0.173 |                   15 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.93  |      0.066 |                   14 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.28031716417910446
[2m[36m(func pid=168552)[0m top5: 0.8353544776119403
[2m[36m(func pid=168552)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=168552)[0m f1_macro: 0.1726851427189008
[2m[36m(func pid=168552)[0m f1_weighted: 0.24653860332365363
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.399, 0.0, 0.128, 0.085, 0.116, 0.34, 0.388, 0.068, 0.203]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.1166044776119403
[2m[36m(func pid=168563)[0m top5: 0.5345149253731343
[2m[36m(func pid=168563)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=168563)[0m f1_macro: 0.08727218328669208
[2m[36m(func pid=168563)[0m f1_weighted: 0.13977473665888668
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.375, 0.465, 0.0, 0.025]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.27705223880597013
[2m[36m(func pid=162589)[0m top5: 0.9048507462686567
[2m[36m(func pid=162589)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=162589)[0m f1_macro: 0.22118329101061374
[2m[36m(func pid=162589)[0m f1_weighted: 0.28831237499336115
[2m[36m(func pid=162589)[0m f1_per_class: [0.168, 0.347, 0.132, 0.435, 0.065, 0.305, 0.143, 0.355, 0.069, 0.194]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3148320895522388
[2m[36m(func pid=163372)[0m top5: 0.855410447761194
[2m[36m(func pid=163372)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=163372)[0m f1_macro: 0.28306560038794804
[2m[36m(func pid=163372)[0m f1_weighted: 0.29103261564282085
[2m[36m(func pid=163372)[0m f1_per_class: [0.263, 0.577, 0.293, 0.277, 0.059, 0.301, 0.117, 0.496, 0.161, 0.286]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4883 | Steps: 4 | Val loss: 2.2829 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7017 | Steps: 4 | Val loss: 2.9951 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2553 | Steps: 4 | Val loss: 2.0098 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8929 | Steps: 4 | Val loss: 1.9653 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:04:46 (running for 00:11:39.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.303 |      0.221 |                   39 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.372 |      0.283 |                   37 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.488 |      0.194 |                   16 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.82  |      0.087 |                   15 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.27425373134328357
[2m[36m(func pid=168552)[0m top5: 0.8722014925373134
[2m[36m(func pid=168552)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=168552)[0m f1_macro: 0.19440012061983727
[2m[36m(func pid=168552)[0m f1_weighted: 0.22540954932000926
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.418, 0.0, 0.243, 0.117, 0.294, 0.053, 0.54, 0.039, 0.241]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.02005597014925373
[2m[36m(func pid=168563)[0m top5: 0.4962686567164179
[2m[36m(func pid=168563)[0m f1_micro: 0.02005597014925373
[2m[36m(func pid=168563)[0m f1_macro: 0.013728765577469729
[2m[36m(func pid=168563)[0m f1_weighted: 0.017490367580845
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.043, 0.062, 0.0, 0.025]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.291044776119403
[2m[36m(func pid=162589)[0m top5: 0.9109141791044776
[2m[36m(func pid=162589)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=162589)[0m f1_macro: 0.22542120827173867
[2m[36m(func pid=162589)[0m f1_weighted: 0.30198199883548227
[2m[36m(func pid=162589)[0m f1_per_class: [0.171, 0.31, 0.144, 0.461, 0.062, 0.295, 0.185, 0.385, 0.058, 0.182]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.322294776119403
[2m[36m(func pid=163372)[0m top5: 0.8442164179104478
[2m[36m(func pid=163372)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=163372)[0m f1_macro: 0.2951682615666078
[2m[36m(func pid=163372)[0m f1_weighted: 0.27925933253708346
[2m[36m(func pid=163372)[0m f1_per_class: [0.284, 0.563, 0.48, 0.248, 0.065, 0.299, 0.115, 0.466, 0.175, 0.258]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.1373 | Steps: 4 | Val loss: 2.7694 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5942 | Steps: 4 | Val loss: 2.3844 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3949 | Steps: 4 | Val loss: 2.0193 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=168552)[0m top1: 0.28078358208955223
[2m[36m(func pid=168552)[0m top5: 0.9249067164179104
[2m[36m(func pid=168552)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=168552)[0m f1_macro: 0.189365623933672
[2m[36m(func pid=168552)[0m f1_weighted: 0.2461669856340897
[2m[36m(func pid=168552)[0m f1_per_class: [0.059, 0.473, 0.0, 0.345, 0.175, 0.0, 0.143, 0.262, 0.146, 0.291]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:04:52 (running for 00:11:44.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.255 |      0.225 |                   40 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.893 |      0.295 |                   38 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.137 |      0.189 |                   17 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.702 |      0.014 |                   16 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.0777 | Steps: 4 | Val loss: 2.0116 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=168563)[0m top1: 0.05923507462686567
[2m[36m(func pid=168563)[0m top5: 0.4906716417910448
[2m[36m(func pid=168563)[0m f1_micro: 0.05923507462686567
[2m[36m(func pid=168563)[0m f1_macro: 0.05551698123339488
[2m[36m(func pid=168563)[0m f1_weighted: 0.03628686479992789
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.054, 0.009, 0.468, 0.0, 0.024]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.29384328358208955
[2m[36m(func pid=162589)[0m top5: 0.9001865671641791
[2m[36m(func pid=162589)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=162589)[0m f1_macro: 0.2349382478887015
[2m[36m(func pid=162589)[0m f1_weighted: 0.3158391273155675
[2m[36m(func pid=162589)[0m f1_per_class: [0.16, 0.329, 0.145, 0.445, 0.06, 0.277, 0.229, 0.461, 0.058, 0.188]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m top1: 0.31529850746268656
[2m[36m(func pid=163372)[0m top5: 0.8372201492537313
[2m[36m(func pid=163372)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=163372)[0m f1_macro: 0.3371223982884879
[2m[36m(func pid=163372)[0m f1_weighted: 0.2955072311502372
[2m[36m(func pid=163372)[0m f1_per_class: [0.336, 0.564, 0.88, 0.316, 0.092, 0.255, 0.107, 0.523, 0.121, 0.177]
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6234 | Steps: 4 | Val loss: 2.7091 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7735 | Steps: 4 | Val loss: 2.2695 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2079 | Steps: 4 | Val loss: 2.0117 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:04:57 (running for 00:11:49.72)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.395 |      0.235 |                   41 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.078 |      0.337 |                   39 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.623 |      0.166 |                   18 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.594 |      0.056 |                   17 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.24580223880597016
[2m[36m(func pid=168552)[0m top5: 0.835820895522388
[2m[36m(func pid=168552)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=168552)[0m f1_macro: 0.1659907010067346
[2m[36m(func pid=168552)[0m f1_weighted: 0.1872615139603856
[2m[36m(func pid=168552)[0m f1_per_class: [0.176, 0.52, 0.0, 0.197, 0.164, 0.024, 0.039, 0.332, 0.092, 0.116]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5697 | Steps: 4 | Val loss: 1.8759 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=168563)[0m top1: 0.09888059701492537
[2m[36m(func pid=168563)[0m top5: 0.5466417910447762
[2m[36m(func pid=168563)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=168563)[0m f1_macro: 0.08208557723036017
[2m[36m(func pid=168563)[0m f1_weighted: 0.11440416055035725
[2m[36m(func pid=168563)[0m f1_per_class: [0.057, 0.0, 0.0, 0.124, 0.0, 0.092, 0.156, 0.35, 0.041, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.30223880597014924
[2m[36m(func pid=162589)[0m top5: 0.8992537313432836
[2m[36m(func pid=162589)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=162589)[0m f1_macro: 0.246788151202629
[2m[36m(func pid=162589)[0m f1_weighted: 0.3169913638846382
[2m[36m(func pid=162589)[0m f1_per_class: [0.205, 0.383, 0.194, 0.436, 0.063, 0.29, 0.202, 0.461, 0.047, 0.188]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.0557 | Steps: 4 | Val loss: 3.5089 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163372)[0m top1: 0.32882462686567165
[2m[36m(func pid=163372)[0m top5: 0.8624067164179104
[2m[36m(func pid=163372)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=163372)[0m f1_macro: 0.34735516952334644
[2m[36m(func pid=163372)[0m f1_weighted: 0.2949602980293041
[2m[36m(func pid=163372)[0m f1_per_class: [0.317, 0.542, 0.818, 0.38, 0.089, 0.344, 0.034, 0.395, 0.23, 0.325]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4091 | Steps: 4 | Val loss: 2.2573 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3084 | Steps: 4 | Val loss: 2.0083 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=168552)[0m top1: 0.3358208955223881
[2m[36m(func pid=168552)[0m top5: 0.8404850746268657
[2m[36m(func pid=168552)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=168552)[0m f1_macro: 0.18492223475389155
[2m[36m(func pid=168552)[0m f1_weighted: 0.28203217331954694
[2m[36m(func pid=168552)[0m f1_per_class: [0.115, 0.475, 0.0, 0.0, 0.197, 0.102, 0.606, 0.0, 0.0, 0.354]
== Status ==
Current time: 2024-01-07 13:05:02 (running for 00:11:55.00)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.208 |      0.247 |                   42 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.57  |      0.347 |                   40 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  3.056 |      0.185 |                   19 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.774 |      0.082 |                   18 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.2062 | Steps: 4 | Val loss: 2.3978 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=168563)[0m top1: 0.30550373134328357
[2m[36m(func pid=168563)[0m top5: 0.7154850746268657
[2m[36m(func pid=168563)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=168563)[0m f1_macro: 0.1180222229968472
[2m[36m(func pid=168563)[0m f1_weighted: 0.22641516434319128
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.521, 0.0, 0.18, 0.142, 0.287, 0.051, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m top1: 0.28451492537313433
[2m[36m(func pid=162589)[0m top5: 0.8805970149253731
[2m[36m(func pid=162589)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=162589)[0m f1_macro: 0.23758786074226937
[2m[36m(func pid=162589)[0m f1_weighted: 0.27911435571690785
[2m[36m(func pid=162589)[0m f1_per_class: [0.221, 0.408, 0.237, 0.39, 0.071, 0.322, 0.104, 0.381, 0.049, 0.194]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7343 | Steps: 4 | Val loss: 3.5401 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=163372)[0m top1: 0.29850746268656714
[2m[36m(func pid=163372)[0m top5: 0.8274253731343284
[2m[36m(func pid=163372)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=163372)[0m f1_macro: 0.2953531111384678
[2m[36m(func pid=163372)[0m f1_weighted: 0.3033545088470533
[2m[36m(func pid=163372)[0m f1_per_class: [0.27, 0.586, 0.815, 0.47, 0.127, 0.088, 0.084, 0.391, 0.038, 0.084]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4104 | Steps: 4 | Val loss: 2.1108 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2334 | Steps: 4 | Val loss: 2.0017 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:05:07 (running for 00:12:00.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.308 |      0.238 |                   43 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.206 |      0.295 |                   41 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.734 |      0.138 |                   20 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.409 |      0.118 |                   19 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.240205223880597
[2m[36m(func pid=168552)[0m top5: 0.8194962686567164
[2m[36m(func pid=168552)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=168552)[0m f1_macro: 0.13811581281549068
[2m[36m(func pid=168552)[0m f1_weighted: 0.273395353528334
[2m[36m(func pid=168552)[0m f1_per_class: [0.095, 0.097, 0.0, 0.415, 0.11, 0.106, 0.415, 0.0, 0.027, 0.116]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.10261194029850747
[2m[36m(func pid=168563)[0m top5: 0.789179104477612
[2m[36m(func pid=168563)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=168563)[0m f1_macro: 0.08175975037142963
[2m[36m(func pid=168563)[0m f1_weighted: 0.07753202322089613
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.01, 0.021, 0.251, 0.062, 0.474, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.4055 | Steps: 4 | Val loss: 2.0310 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=162589)[0m top1: 0.2630597014925373
[2m[36m(func pid=162589)[0m top5: 0.863339552238806
[2m[36m(func pid=162589)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=162589)[0m f1_macro: 0.22431829349257715
[2m[36m(func pid=162589)[0m f1_weighted: 0.2486721401814155
[2m[36m(func pid=162589)[0m f1_per_class: [0.219, 0.392, 0.319, 0.374, 0.066, 0.328, 0.039, 0.324, 0.0, 0.182]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4728 | Steps: 4 | Val loss: 3.0729 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163372)[0m top1: 0.3460820895522388
[2m[36m(func pid=163372)[0m top5: 0.847481343283582
[2m[36m(func pid=163372)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=163372)[0m f1_macro: 0.27917329542465963
[2m[36m(func pid=163372)[0m f1_weighted: 0.361128211251024
[2m[36m(func pid=163372)[0m f1_per_class: [0.197, 0.551, 0.216, 0.46, 0.115, 0.223, 0.238, 0.52, 0.104, 0.167]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3848 | Steps: 4 | Val loss: 2.1143 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2401 | Steps: 4 | Val loss: 2.0033 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=168552)[0m top1: 0.25699626865671643
[2m[36m(func pid=168552)[0m top5: 0.8283582089552238
[2m[36m(func pid=168552)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=168552)[0m f1_macro: 0.20149323436071565
[2m[36m(func pid=168552)[0m f1_weighted: 0.26725235156973315
[2m[36m(func pid=168552)[0m f1_per_class: [0.248, 0.0, 0.0, 0.488, 0.186, 0.201, 0.232, 0.5, 0.08, 0.08]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:05:13 (running for 00:12:05.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.233 |      0.224 |                   44 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.405 |      0.279 |                   42 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.473 |      0.201 |                   21 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.41  |      0.082 |                   20 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.2490671641791045
[2m[36m(func pid=168563)[0m top5: 0.8050373134328358
[2m[36m(func pid=168563)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=168563)[0m f1_macro: 0.1250906545902731
[2m[36m(func pid=168563)[0m f1_weighted: 0.18446644727674777
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.0, 0.434, 0.023, 0.305, 0.0, 0.488, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.3122 | Steps: 4 | Val loss: 2.0586 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=162589)[0m top1: 0.26026119402985076
[2m[36m(func pid=162589)[0m top5: 0.8591417910447762
[2m[36m(func pid=162589)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=162589)[0m f1_macro: 0.22088373079528428
[2m[36m(func pid=162589)[0m f1_weighted: 0.24221701799163053
[2m[36m(func pid=162589)[0m f1_per_class: [0.208, 0.407, 0.282, 0.381, 0.076, 0.302, 0.012, 0.296, 0.051, 0.192]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.0604 | Steps: 4 | Val loss: 2.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=163372)[0m top1: 0.32322761194029853
[2m[36m(func pid=163372)[0m top5: 0.8568097014925373
[2m[36m(func pid=163372)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=163372)[0m f1_macro: 0.26518216111644677
[2m[36m(func pid=163372)[0m f1_weighted: 0.3258737104444679
[2m[36m(func pid=163372)[0m f1_per_class: [0.043, 0.445, 0.379, 0.323, 0.092, 0.265, 0.303, 0.51, 0.089, 0.202]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7486 | Steps: 4 | Val loss: 1.9876 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 13:05:18 (running for 00:12:10.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.24  |      0.221 |                   45 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.312 |      0.265 |                   43 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.06  |      0.187 |                   22 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.385 |      0.125 |                   21 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2429 | Steps: 4 | Val loss: 2.0113 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=168552)[0m top1: 0.25279850746268656
[2m[36m(func pid=168552)[0m top5: 0.8297574626865671
[2m[36m(func pid=168552)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=168552)[0m f1_macro: 0.1865481708477635
[2m[36m(func pid=168552)[0m f1_weighted: 0.21562397815366544
[2m[36m(func pid=168552)[0m f1_per_class: [0.181, 0.0, 0.0, 0.449, 0.087, 0.324, 0.065, 0.411, 0.086, 0.263]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.34654850746268656
[2m[36m(func pid=168563)[0m top5: 0.7551305970149254
[2m[36m(func pid=168563)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=168563)[0m f1_macro: 0.19161584386687625
[2m[36m(func pid=168563)[0m f1_weighted: 0.2695978049980107
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.4, 0.535, 0.038, 0.262, 0.201, 0.48, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2905 | Steps: 4 | Val loss: 1.8547 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=162589)[0m top1: 0.2621268656716418
[2m[36m(func pid=162589)[0m top5: 0.84375
[2m[36m(func pid=162589)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=162589)[0m f1_macro: 0.22426541695516816
[2m[36m(func pid=162589)[0m f1_weighted: 0.24160623582333196
[2m[36m(func pid=162589)[0m f1_per_class: [0.196, 0.407, 0.297, 0.373, 0.073, 0.324, 0.006, 0.319, 0.048, 0.2]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0003 | Steps: 4 | Val loss: 2.1831 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7670 | Steps: 4 | Val loss: 2.3246 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=163372)[0m top1: 0.37779850746268656
[2m[36m(func pid=163372)[0m top5: 0.8736007462686567
[2m[36m(func pid=163372)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=163372)[0m f1_macro: 0.3137395860131708
[2m[36m(func pid=163372)[0m f1_weighted: 0.3829233161936093
[2m[36m(func pid=163372)[0m f1_per_class: [0.169, 0.371, 0.387, 0.537, 0.09, 0.372, 0.278, 0.499, 0.166, 0.269]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.27052238805970147
[2m[36m(func pid=168552)[0m top5: 0.8115671641791045
[2m[36m(func pid=168552)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=168552)[0m f1_macro: 0.1939219137925256
[2m[36m(func pid=168552)[0m f1_weighted: 0.2410504378102648
[2m[36m(func pid=168552)[0m f1_per_class: [0.198, 0.388, 0.0, 0.443, 0.122, 0.016, 0.037, 0.486, 0.098, 0.152]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:05:23 (running for 00:12:16.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.243 |      0.224 |                   46 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.291 |      0.314 |                   44 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2     |      0.194 |                   23 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.749 |      0.192 |                   22 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3210 | Steps: 4 | Val loss: 2.0048 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=168563)[0m top1: 0.07975746268656717
[2m[36m(func pid=168563)[0m top5: 0.683768656716418
[2m[36m(func pid=168563)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=168563)[0m f1_macro: 0.05812018827774231
[2m[36m(func pid=168563)[0m f1_weighted: 0.08688063017170135
[2m[36m(func pid=168563)[0m f1_per_class: [0.046, 0.0, 0.091, 0.007, 0.042, 0.082, 0.238, 0.016, 0.06, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7801 | Steps: 4 | Val loss: 2.2218 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=162589)[0m top1: 0.2756529850746269
[2m[36m(func pid=162589)[0m top5: 0.8460820895522388
[2m[36m(func pid=162589)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=162589)[0m f1_macro: 0.23740796581876614
[2m[36m(func pid=162589)[0m f1_weighted: 0.25670665348496097
[2m[36m(func pid=162589)[0m f1_per_class: [0.221, 0.424, 0.286, 0.407, 0.076, 0.325, 0.009, 0.313, 0.08, 0.233]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5943 | Steps: 4 | Val loss: 2.0781 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.2392 | Steps: 4 | Val loss: 2.3340 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=163372)[0m top1: 0.2826492537313433
[2m[36m(func pid=163372)[0m top5: 0.8390858208955224
[2m[36m(func pid=163372)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=163372)[0m f1_macro: 0.2516233623033077
[2m[36m(func pid=163372)[0m f1_weighted: 0.2811435555738771
[2m[36m(func pid=163372)[0m f1_per_class: [0.202, 0.166, 0.522, 0.536, 0.1, 0.272, 0.13, 0.353, 0.087, 0.148]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:05:29 (running for 00:12:21.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.321 |      0.237 |                   47 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.78  |      0.252 |                   45 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.594 |      0.215 |                   24 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.767 |      0.058 |                   23 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1535 | Steps: 4 | Val loss: 1.9973 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=168552)[0m top1: 0.34375
[2m[36m(func pid=168552)[0m top5: 0.8927238805970149
[2m[36m(func pid=168552)[0m f1_micro: 0.34375
[2m[36m(func pid=168552)[0m f1_macro: 0.21453553260973018
[2m[36m(func pid=168552)[0m f1_weighted: 0.355402203749361
[2m[36m(func pid=168552)[0m f1_per_class: [0.044, 0.324, 0.421, 0.521, 0.077, 0.008, 0.483, 0.0, 0.119, 0.149]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.07229477611940298
[2m[36m(func pid=168563)[0m top5: 0.4944029850746269
[2m[36m(func pid=168563)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=168563)[0m f1_macro: 0.07588629126759176
[2m[36m(func pid=168563)[0m f1_weighted: 0.048764787595817864
[2m[36m(func pid=168563)[0m f1_per_class: [0.064, 0.0, 0.0, 0.0, 0.031, 0.029, 0.04, 0.517, 0.062, 0.017]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5177 | Steps: 4 | Val loss: 3.3434 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=162589)[0m top1: 0.29850746268656714
[2m[36m(func pid=162589)[0m top5: 0.8507462686567164
[2m[36m(func pid=162589)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=162589)[0m f1_macro: 0.25157383579174875
[2m[36m(func pid=162589)[0m f1_weighted: 0.26987922144294807
[2m[36m(func pid=162589)[0m f1_per_class: [0.242, 0.467, 0.301, 0.405, 0.087, 0.352, 0.006, 0.356, 0.132, 0.167]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6196 | Steps: 4 | Val loss: 3.3413 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7037 | Steps: 4 | Val loss: 2.3808 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=163372)[0m top1: 0.16277985074626866
[2m[36m(func pid=163372)[0m top5: 0.7569962686567164
[2m[36m(func pid=163372)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=163372)[0m f1_macro: 0.15886934106669798
[2m[36m(func pid=163372)[0m f1_weighted: 0.16629637658354376
[2m[36m(func pid=163372)[0m f1_per_class: [0.149, 0.14, 0.609, 0.447, 0.096, 0.037, 0.012, 0.0, 0.043, 0.055]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1802 | Steps: 4 | Val loss: 1.9962 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 13:05:34 (running for 00:12:26.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.153 |      0.252 |                   48 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.518 |      0.159 |                   46 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.62  |      0.162 |                   25 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  3.239 |      0.076 |                   24 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.23414179104477612
[2m[36m(func pid=168552)[0m top5: 0.7924440298507462
[2m[36m(func pid=168552)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=168552)[0m f1_macro: 0.16240529252450484
[2m[36m(func pid=168552)[0m f1_weighted: 0.2593433333248364
[2m[36m(func pid=168552)[0m f1_per_class: [0.074, 0.0, 0.087, 0.402, 0.06, 0.241, 0.368, 0.0, 0.14, 0.252]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.06996268656716417
[2m[36m(func pid=168563)[0m top5: 0.4556902985074627
[2m[36m(func pid=168563)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=168563)[0m f1_macro: 0.10629017534189866
[2m[36m(func pid=168563)[0m f1_weighted: 0.04502386243669869
[2m[36m(func pid=168563)[0m f1_per_class: [0.064, 0.044, 0.333, 0.0, 0.0, 0.015, 0.0, 0.52, 0.061, 0.026]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5419 | Steps: 4 | Val loss: 2.0371 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=162589)[0m top1: 0.3003731343283582
[2m[36m(func pid=162589)[0m top5: 0.8451492537313433
[2m[36m(func pid=162589)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=162589)[0m f1_macro: 0.255441913306242
[2m[36m(func pid=162589)[0m f1_weighted: 0.26859739284652767
[2m[36m(func pid=162589)[0m f1_per_class: [0.246, 0.485, 0.367, 0.391, 0.081, 0.335, 0.006, 0.38, 0.143, 0.121]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5683 | Steps: 4 | Val loss: 2.9635 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5312 | Steps: 4 | Val loss: 2.3182 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=163372)[0m top1: 0.3148320895522388
[2m[36m(func pid=163372)[0m top5: 0.867070895522388
[2m[36m(func pid=163372)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=163372)[0m f1_macro: 0.30933088872631403
[2m[36m(func pid=163372)[0m f1_weighted: 0.3055765290550737
[2m[36m(func pid=163372)[0m f1_per_class: [0.286, 0.342, 0.453, 0.552, 0.06, 0.257, 0.045, 0.52, 0.192, 0.388]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.2462686567164179
[2m[36m(func pid=168552)[0m top5: 0.7761194029850746
[2m[36m(func pid=168552)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=168552)[0m f1_macro: 0.22561899484993186
[2m[36m(func pid=168552)[0m f1_weighted: 0.23224132574106351
[2m[36m(func pid=168552)[0m f1_per_class: [0.154, 0.0, 0.156, 0.43, 0.049, 0.362, 0.106, 0.396, 0.224, 0.379]
== Status ==
Current time: 2024-01-07 13:05:39 (running for 00:12:32.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.18  |      0.255 |                   49 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.542 |      0.309 |                   47 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.568 |      0.226 |                   26 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.704 |      0.106 |                   25 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2221 | Steps: 4 | Val loss: 2.0102 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=168563)[0m top1: 0.03824626865671642
[2m[36m(func pid=168563)[0m top5: 0.5359141791044776
[2m[36m(func pid=168563)[0m f1_micro: 0.03824626865671642
[2m[36m(func pid=168563)[0m f1_macro: 0.056425090586502746
[2m[36m(func pid=168563)[0m f1_weighted: 0.03171943738004355
[2m[36m(func pid=168563)[0m f1_per_class: [0.026, 0.0, 0.021, 0.0, 0.0, 0.044, 0.0, 0.418, 0.055, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2979 | Steps: 4 | Val loss: 1.8949 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=162589)[0m top1: 0.3041044776119403
[2m[36m(func pid=162589)[0m top5: 0.8306902985074627
[2m[36m(func pid=162589)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=162589)[0m f1_macro: 0.2591297362845562
[2m[36m(func pid=162589)[0m f1_weighted: 0.26863654380034874
[2m[36m(func pid=162589)[0m f1_per_class: [0.231, 0.511, 0.361, 0.381, 0.089, 0.314, 0.003, 0.409, 0.135, 0.158]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.1880 | Steps: 4 | Val loss: 3.0795 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4518 | Steps: 4 | Val loss: 2.2159 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=163372)[0m top1: 0.3302238805970149
[2m[36m(func pid=163372)[0m top5: 0.8824626865671642
[2m[36m(func pid=163372)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=163372)[0m f1_macro: 0.28329547878046746
[2m[36m(func pid=163372)[0m f1_weighted: 0.32736836466046076
[2m[36m(func pid=163372)[0m f1_per_class: [0.157, 0.484, 0.282, 0.496, 0.06, 0.283, 0.123, 0.343, 0.219, 0.386]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1334 | Steps: 4 | Val loss: 2.0123 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:05:45 (running for 00:12:37.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.222 |      0.259 |                   50 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.298 |      0.283 |                   48 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  3.188 |      0.146 |                   27 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.531 |      0.056 |                   26 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.1609141791044776
[2m[36m(func pid=168552)[0m top5: 0.8036380597014925
[2m[36m(func pid=168552)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=168552)[0m f1_macro: 0.14614396356399867
[2m[36m(func pid=168552)[0m f1_weighted: 0.14198411880900316
[2m[36m(func pid=168552)[0m f1_per_class: [0.204, 0.087, 0.0, 0.229, 0.04, 0.304, 0.0, 0.307, 0.133, 0.157]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.06949626865671642
[2m[36m(func pid=168563)[0m top5: 0.6674440298507462
[2m[36m(func pid=168563)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=168563)[0m f1_macro: 0.06563139275342972
[2m[36m(func pid=168563)[0m f1_weighted: 0.07591292959533158
[2m[36m(func pid=168563)[0m f1_per_class: [0.017, 0.0, 0.021, 0.0, 0.0, 0.167, 0.132, 0.257, 0.062, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9829 | Steps: 4 | Val loss: 2.2716 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=162589)[0m top1: 0.30223880597014924
[2m[36m(func pid=162589)[0m top5: 0.8227611940298507
[2m[36m(func pid=162589)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=162589)[0m f1_macro: 0.2666427593092537
[2m[36m(func pid=162589)[0m f1_weighted: 0.2579469820056675
[2m[36m(func pid=162589)[0m f1_per_class: [0.241, 0.518, 0.4, 0.329, 0.095, 0.317, 0.003, 0.428, 0.151, 0.186]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4510 | Steps: 4 | Val loss: 3.8009 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3919 | Steps: 4 | Val loss: 2.1337 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=163372)[0m top1: 0.33675373134328357
[2m[36m(func pid=163372)[0m top5: 0.8381529850746269
[2m[36m(func pid=163372)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=163372)[0m f1_macro: 0.27055971950951646
[2m[36m(func pid=163372)[0m f1_weighted: 0.3655817509218024
[2m[36m(func pid=163372)[0m f1_per_class: [0.197, 0.558, 0.286, 0.457, 0.09, 0.21, 0.28, 0.402, 0.11, 0.116]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:05:50 (running for 00:12:42.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.133 |      0.267 |                   51 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.983 |      0.271 |                   49 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.451 |      0.168 |                   28 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.452 |      0.066 |                   27 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.24067164179104478
[2m[36m(func pid=168552)[0m top5: 0.8549440298507462
[2m[36m(func pid=168552)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=168552)[0m f1_macro: 0.16816134128417104
[2m[36m(func pid=168552)[0m f1_weighted: 0.17055880441228977
[2m[36m(func pid=168552)[0m f1_per_class: [0.143, 0.466, 0.0, 0.003, 0.009, 0.289, 0.082, 0.43, 0.053, 0.207]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1005 | Steps: 4 | Val loss: 2.0074 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=168563)[0m top1: 0.1394589552238806
[2m[36m(func pid=168563)[0m top5: 0.7276119402985075
[2m[36m(func pid=168563)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=168563)[0m f1_macro: 0.10905107939222496
[2m[36m(func pid=168563)[0m f1_weighted: 0.16457824716368594
[2m[36m(func pid=168563)[0m f1_per_class: [0.032, 0.0, 0.021, 0.0, 0.045, 0.189, 0.397, 0.407, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.3057 | Steps: 4 | Val loss: 1.8088 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=162589)[0m top1: 0.2868470149253731
[2m[36m(func pid=162589)[0m top5: 0.8288246268656716
[2m[36m(func pid=162589)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=162589)[0m f1_macro: 0.24400451146809923
[2m[36m(func pid=162589)[0m f1_weighted: 0.23401773518345764
[2m[36m(func pid=162589)[0m f1_per_class: [0.224, 0.499, 0.253, 0.252, 0.098, 0.323, 0.003, 0.448, 0.15, 0.19]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4427 | Steps: 4 | Val loss: 1.9653 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.1762 | Steps: 4 | Val loss: 2.1024 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=163372)[0m top1: 0.40111940298507465
[2m[36m(func pid=163372)[0m top5: 0.8628731343283582
[2m[36m(func pid=163372)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=163372)[0m f1_macro: 0.3171325399813924
[2m[36m(func pid=163372)[0m f1_weighted: 0.4248324211831293
[2m[36m(func pid=163372)[0m f1_per_class: [0.227, 0.535, 0.312, 0.379, 0.082, 0.273, 0.511, 0.505, 0.144, 0.203]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:05:56 (running for 00:12:48.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.1   |      0.244 |                   52 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.306 |      0.317 |                   50 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.443 |      0.264 |                   29 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.392 |      0.109 |                   28 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.35494402985074625
[2m[36m(func pid=168552)[0m top5: 0.9029850746268657
[2m[36m(func pid=168552)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=168552)[0m f1_macro: 0.26429888850144645
[2m[36m(func pid=168552)[0m f1_weighted: 0.3709646029697851
[2m[36m(func pid=168552)[0m f1_per_class: [0.236, 0.274, 0.392, 0.374, 0.048, 0.047, 0.593, 0.432, 0.099, 0.148]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.0790 | Steps: 4 | Val loss: 1.9952 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=168563)[0m top1: 0.10680970149253731
[2m[36m(func pid=168563)[0m top5: 0.7742537313432836
[2m[36m(func pid=168563)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=168563)[0m f1_macro: 0.10412754365588676
[2m[36m(func pid=168563)[0m f1_weighted: 0.11344493739380355
[2m[36m(func pid=168563)[0m f1_per_class: [0.031, 0.105, 0.025, 0.0, 0.034, 0.216, 0.138, 0.491, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.2208 | Steps: 4 | Val loss: 1.9979 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=162589)[0m top1: 0.30130597014925375
[2m[36m(func pid=162589)[0m top5: 0.832089552238806
[2m[36m(func pid=162589)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=162589)[0m f1_macro: 0.2581544135481381
[2m[36m(func pid=162589)[0m f1_weighted: 0.2386717212632849
[2m[36m(func pid=162589)[0m f1_per_class: [0.227, 0.521, 0.293, 0.251, 0.118, 0.337, 0.0, 0.429, 0.17, 0.235]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9892 | Steps: 4 | Val loss: 1.9632 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4772 | Steps: 4 | Val loss: 2.0813 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=163372)[0m top1: 0.36473880597014924
[2m[36m(func pid=163372)[0m top5: 0.8372201492537313
[2m[36m(func pid=163372)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=163372)[0m f1_macro: 0.3535618010700327
[2m[36m(func pid=163372)[0m f1_weighted: 0.40225175086110054
[2m[36m(func pid=163372)[0m f1_per_class: [0.248, 0.562, 0.8, 0.309, 0.09, 0.318, 0.469, 0.459, 0.128, 0.152]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:06:01 (running for 00:12:53.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.079 |      0.258 |                   53 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.221 |      0.354 |                   51 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.989 |      0.287 |                   30 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.176 |      0.104 |                   29 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.2366 | Steps: 4 | Val loss: 1.9826 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=168552)[0m top1: 0.3666044776119403
[2m[36m(func pid=168552)[0m top5: 0.8530783582089553
[2m[36m(func pid=168552)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=168552)[0m f1_macro: 0.286735240025876
[2m[36m(func pid=168552)[0m f1_weighted: 0.35496200726550187
[2m[36m(func pid=168552)[0m f1_per_class: [0.083, 0.0, 0.571, 0.533, 0.051, 0.0, 0.542, 0.474, 0.212, 0.4]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.19776119402985073
[2m[36m(func pid=168563)[0m top5: 0.8157649253731343
[2m[36m(func pid=168563)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=168563)[0m f1_macro: 0.13422627846600754
[2m[36m(func pid=168563)[0m f1_weighted: 0.14021738674665643
[2m[36m(func pid=168563)[0m f1_per_class: [0.012, 0.398, 0.108, 0.0, 0.023, 0.314, 0.028, 0.459, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3253 | Steps: 4 | Val loss: 1.8520 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=162589)[0m top1: 0.28638059701492535
[2m[36m(func pid=162589)[0m top5: 0.8278917910447762
[2m[36m(func pid=162589)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=162589)[0m f1_macro: 0.243851851659398
[2m[36m(func pid=162589)[0m f1_weighted: 0.22862392188013708
[2m[36m(func pid=162589)[0m f1_per_class: [0.25, 0.483, 0.278, 0.249, 0.102, 0.352, 0.0, 0.361, 0.147, 0.216]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9509 | Steps: 4 | Val loss: 2.5217 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5247 | Steps: 4 | Val loss: 2.0846 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=163372)[0m top1: 0.3773320895522388
[2m[36m(func pid=163372)[0m top5: 0.8647388059701493
[2m[36m(func pid=163372)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=163372)[0m f1_macro: 0.3711765929844348
[2m[36m(func pid=163372)[0m f1_weighted: 0.3921170387652396
[2m[36m(func pid=163372)[0m f1_per_class: [0.226, 0.582, 0.783, 0.359, 0.089, 0.323, 0.363, 0.455, 0.183, 0.348]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:06:07 (running for 00:12:59.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.237 |      0.244 |                   54 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.325 |      0.371 |                   52 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.951 |      0.265 |                   31 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.477 |      0.134 |                   30 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.26259328358208955
[2m[36m(func pid=168552)[0m top5: 0.7178171641791045
[2m[36m(func pid=168552)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=168552)[0m f1_macro: 0.2648974178995859
[2m[36m(func pid=168552)[0m f1_weighted: 0.2192293802523497
[2m[36m(func pid=168552)[0m f1_per_class: [0.147, 0.0, 0.815, 0.523, 0.078, 0.232, 0.0, 0.528, 0.183, 0.143]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.16417910447761194
[2m[36m(func pid=168563)[0m top5: 0.7943097014925373
[2m[36m(func pid=168563)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=168563)[0m f1_macro: 0.1385477703698448
[2m[36m(func pid=168563)[0m f1_weighted: 0.1282537165835148
[2m[36m(func pid=168563)[0m f1_per_class: [0.068, 0.0, 0.293, 0.041, 0.033, 0.361, 0.159, 0.431, 0.0, 0.0]
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0358 | Steps: 4 | Val loss: 1.9924 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.6870 | Steps: 4 | Val loss: 1.9875 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=162589)[0m top1: 0.2835820895522388
[2m[36m(func pid=162589)[0m top5: 0.8232276119402985
[2m[36m(func pid=162589)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=162589)[0m f1_macro: 0.23374326145778293
[2m[36m(func pid=162589)[0m f1_weighted: 0.22722693820669854
[2m[36m(func pid=162589)[0m f1_per_class: [0.251, 0.478, 0.234, 0.252, 0.097, 0.333, 0.0, 0.419, 0.096, 0.178]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7225 | Steps: 4 | Val loss: 2.3395 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6480 | Steps: 4 | Val loss: 2.6652 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=163372)[0m top1: 0.3316231343283582
[2m[36m(func pid=163372)[0m top5: 0.8661380597014925
[2m[36m(func pid=163372)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=163372)[0m f1_macro: 0.3316947856376417
[2m[36m(func pid=163372)[0m f1_weighted: 0.2934083423816226
[2m[36m(func pid=163372)[0m f1_per_class: [0.286, 0.538, 0.88, 0.295, 0.078, 0.325, 0.135, 0.375, 0.138, 0.267]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:06:12 (running for 00:13:04.65)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.036 |      0.234 |                   55 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.687 |      0.332 |                   53 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.951 |      0.265 |                   31 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.723 |      0.103 |                   32 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.12173507462686567
[2m[36m(func pid=168563)[0m top5: 0.7215485074626866
[2m[36m(func pid=168563)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=168563)[0m f1_macro: 0.10274866675601899
[2m[36m(func pid=168563)[0m f1_weighted: 0.10170033166984524
[2m[36m(func pid=168563)[0m f1_per_class: [0.067, 0.0, 0.0, 0.031, 0.047, 0.32, 0.094, 0.462, 0.0, 0.008]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.23367537313432835
[2m[36m(func pid=168552)[0m top5: 0.7765858208955224
[2m[36m(func pid=168552)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=168552)[0m f1_macro: 0.23667690199917932
[2m[36m(func pid=168552)[0m f1_weighted: 0.2040825249677005
[2m[36m(func pid=168552)[0m f1_per_class: [0.136, 0.0, 0.667, 0.482, 0.082, 0.234, 0.0, 0.55, 0.052, 0.164]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0719 | Steps: 4 | Val loss: 1.9802 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162589)[0m top1: 0.28171641791044777
[2m[36m(func pid=162589)[0m top5: 0.8306902985074627
[2m[36m(func pid=162589)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=162589)[0m f1_macro: 0.2325530444677805
[2m[36m(func pid=162589)[0m f1_weighted: 0.2302979441913928
[2m[36m(func pid=162589)[0m f1_per_class: [0.22, 0.47, 0.224, 0.27, 0.1, 0.351, 0.0, 0.371, 0.101, 0.217]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4946 | Steps: 4 | Val loss: 1.8308 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3521 | Steps: 4 | Val loss: 2.3606 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7562 | Steps: 4 | Val loss: 2.0422 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:06:17 (running for 00:13:09.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.072 |      0.233 |                   56 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.495 |      0.346 |                   54 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.648 |      0.237 |                   32 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.723 |      0.103 |                   32 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m top1: 0.36380597014925375
[2m[36m(func pid=163372)[0m top5: 0.8656716417910447
[2m[36m(func pid=163372)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=163372)[0m f1_macro: 0.34593118623868324
[2m[36m(func pid=163372)[0m f1_weighted: 0.3363304061721977
[2m[36m(func pid=163372)[0m f1_per_class: [0.302, 0.551, 0.6, 0.384, 0.094, 0.349, 0.157, 0.444, 0.222, 0.356]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.28591417910447764
[2m[36m(func pid=168552)[0m top5: 0.851679104477612
[2m[36m(func pid=168552)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=168552)[0m f1_macro: 0.25192273429189194
[2m[36m(func pid=168552)[0m f1_weighted: 0.2657276104036765
[2m[36m(func pid=168552)[0m f1_per_class: [0.213, 0.391, 0.154, 0.451, 0.071, 0.326, 0.0, 0.385, 0.054, 0.475]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.13992537313432835
[2m[36m(func pid=168563)[0m top5: 0.7714552238805971
[2m[36m(func pid=168563)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=168563)[0m f1_macro: 0.11604785212047668
[2m[36m(func pid=168563)[0m f1_weighted: 0.10749956206742911
[2m[36m(func pid=168563)[0m f1_per_class: [0.067, 0.0, 0.143, 0.037, 0.0, 0.338, 0.096, 0.48, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2333 | Steps: 4 | Val loss: 1.9937 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=162589)[0m top1: 0.27425373134328357
[2m[36m(func pid=162589)[0m top5: 0.835820895522388
[2m[36m(func pid=162589)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=162589)[0m f1_macro: 0.2187549522590324
[2m[36m(func pid=162589)[0m f1_weighted: 0.23594058314814365
[2m[36m(func pid=162589)[0m f1_per_class: [0.158, 0.474, 0.147, 0.298, 0.088, 0.31, 0.0, 0.433, 0.12, 0.158]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.0433 | Steps: 4 | Val loss: 1.9751 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.6030 | Steps: 4 | Val loss: 2.2342 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.3627 | Steps: 4 | Val loss: 2.1602 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:06:22 (running for 00:13:15.21)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.233 |      0.219 |                   57 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.043 |      0.314 |                   55 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.756 |      0.252 |                   33 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.352 |      0.116 |                   33 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m top1: 0.33488805970149255
[2m[36m(func pid=163372)[0m top5: 0.8652052238805971
[2m[36m(func pid=163372)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=163372)[0m f1_macro: 0.313683794330105
[2m[36m(func pid=163372)[0m f1_weighted: 0.33024156116554043
[2m[36m(func pid=163372)[0m f1_per_class: [0.338, 0.552, 0.4, 0.475, 0.112, 0.281, 0.083, 0.465, 0.169, 0.261]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m top1: 0.17537313432835822
[2m[36m(func pid=168563)[0m top5: 0.7723880597014925
[2m[36m(func pid=168563)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=168563)[0m f1_macro: 0.12429797216463179
[2m[36m(func pid=168563)[0m f1_weighted: 0.12806688993733167
[2m[36m(func pid=168563)[0m f1_per_class: [0.07, 0.0, 0.178, 0.003, 0.0, 0.35, 0.202, 0.404, 0.037, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.21735074626865672
[2m[36m(func pid=168552)[0m top5: 0.8115671641791045
[2m[36m(func pid=168552)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=168552)[0m f1_macro: 0.20776140455424524
[2m[36m(func pid=168552)[0m f1_weighted: 0.19659782819288568
[2m[36m(func pid=168552)[0m f1_per_class: [0.191, 0.402, 0.147, 0.02, 0.07, 0.213, 0.207, 0.379, 0.16, 0.288]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.0332 | Steps: 4 | Val loss: 1.9931 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3066 | Steps: 4 | Val loss: 1.9784 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=162589)[0m top1: 0.26492537313432835
[2m[36m(func pid=162589)[0m top5: 0.8325559701492538
[2m[36m(func pid=162589)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=162589)[0m f1_macro: 0.2325140327490553
[2m[36m(func pid=162589)[0m f1_weighted: 0.23294367359909937
[2m[36m(func pid=162589)[0m f1_per_class: [0.182, 0.455, 0.186, 0.293, 0.074, 0.3, 0.003, 0.427, 0.132, 0.273]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8866 | Steps: 4 | Val loss: 3.7693 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3595 | Steps: 4 | Val loss: 2.2509 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=163372)[0m top1: 0.33675373134328357
[2m[36m(func pid=163372)[0m top5: 0.871268656716418
[2m[36m(func pid=163372)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=163372)[0m f1_macro: 0.3082677335532517
[2m[36m(func pid=163372)[0m f1_weighted: 0.35791404150751305
[2m[36m(func pid=163372)[0m f1_per_class: [0.358, 0.484, 0.296, 0.511, 0.131, 0.263, 0.192, 0.46, 0.162, 0.225]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:06:28 (running for 00:13:20.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.033 |      0.233 |                   58 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.307 |      0.308 |                   56 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.603 |      0.208 |                   34 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.363 |      0.124 |                   34 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.23973880597014927
[2m[36m(func pid=168552)[0m top5: 0.7327425373134329
[2m[36m(func pid=168552)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=168552)[0m f1_macro: 0.19309694783864845
[2m[36m(func pid=168552)[0m f1_weighted: 0.26918155382707737
[2m[36m(func pid=168552)[0m f1_per_class: [0.039, 0.285, 0.0, 0.023, 0.061, 0.296, 0.482, 0.474, 0.206, 0.065]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.0914179104477612
[2m[36m(func pid=168563)[0m top5: 0.8064365671641791
[2m[36m(func pid=168563)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=168563)[0m f1_macro: 0.08949070066342696
[2m[36m(func pid=168563)[0m f1_weighted: 0.08440764657725303
[2m[36m(func pid=168563)[0m f1_per_class: [0.036, 0.061, 0.046, 0.007, 0.037, 0.152, 0.09, 0.446, 0.02, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0216 | Steps: 4 | Val loss: 2.0064 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.8018 | Steps: 4 | Val loss: 2.0399 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=162589)[0m top1: 0.261660447761194
[2m[36m(func pid=162589)[0m top5: 0.8306902985074627
[2m[36m(func pid=162589)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=162589)[0m f1_macro: 0.2246233737204973
[2m[36m(func pid=162589)[0m f1_weighted: 0.23228715728271834
[2m[36m(func pid=162589)[0m f1_per_class: [0.19, 0.454, 0.202, 0.301, 0.074, 0.292, 0.0, 0.434, 0.118, 0.182]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2445 | Steps: 4 | Val loss: 3.2226 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1306 | Steps: 4 | Val loss: 2.3972 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 13:06:33 (running for 00:13:26.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.022 |      0.225 |                   59 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.307 |      0.308 |                   56 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.244 |      0.248 |                   36 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.359 |      0.089 |                   35 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.28404850746268656
[2m[36m(func pid=168552)[0m top5: 0.8264925373134329
[2m[36m(func pid=168552)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=168552)[0m f1_macro: 0.24752299990435497
[2m[36m(func pid=168552)[0m f1_weighted: 0.324136391766995
[2m[36m(func pid=168552)[0m f1_per_class: [0.159, 0.279, 0.05, 0.175, 0.061, 0.232, 0.528, 0.523, 0.183, 0.286]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.33488805970149255
[2m[36m(func pid=163372)[0m top5: 0.867070895522388
[2m[36m(func pid=163372)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=163372)[0m f1_macro: 0.29243022344698105
[2m[36m(func pid=163372)[0m f1_weighted: 0.3721460399241817
[2m[36m(func pid=163372)[0m f1_per_class: [0.157, 0.434, 0.066, 0.414, 0.112, 0.388, 0.315, 0.489, 0.206, 0.341]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m top1: 0.08955223880597014
[2m[36m(func pid=168563)[0m top5: 0.7961753731343284
[2m[36m(func pid=168563)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=168563)[0m f1_macro: 0.08866400424251678
[2m[36m(func pid=168563)[0m f1_weighted: 0.0749908616417109
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.202, 0.039, 0.0, 0.03, 0.07, 0.009, 0.455, 0.081, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1633 | Steps: 4 | Val loss: 2.0143 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162589)[0m top1: 0.2583955223880597
[2m[36m(func pid=162589)[0m top5: 0.8236940298507462
[2m[36m(func pid=162589)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=162589)[0m f1_macro: 0.24008330786738669
[2m[36m(func pid=162589)[0m f1_weighted: 0.23381249515814345
[2m[36m(func pid=162589)[0m f1_per_class: [0.199, 0.452, 0.328, 0.306, 0.07, 0.292, 0.0, 0.442, 0.071, 0.24]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7428 | Steps: 4 | Val loss: 3.5680 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4341 | Steps: 4 | Val loss: 2.2948 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0040 | Steps: 4 | Val loss: 2.0710 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:06:39 (running for 00:13:31.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.163 |      0.24  |                   60 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.802 |      0.292 |                   57 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.743 |      0.173 |                   37 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.131 |      0.089 |                   36 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.21175373134328357
[2m[36m(func pid=168552)[0m top5: 0.9001865671641791
[2m[36m(func pid=168552)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=168552)[0m f1_macro: 0.17321626240206073
[2m[36m(func pid=168552)[0m f1_weighted: 0.22513703307275423
[2m[36m(func pid=168552)[0m f1_per_class: [0.186, 0.375, 0.061, 0.252, 0.064, 0.128, 0.167, 0.332, 0.0, 0.167]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.11007462686567164
[2m[36m(func pid=168563)[0m top5: 0.7994402985074627
[2m[36m(func pid=168563)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=168563)[0m f1_macro: 0.08645232454651554
[2m[36m(func pid=168563)[0m f1_weighted: 0.11112213541788217
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.005, 0.053, 0.0, 0.035, 0.0, 0.279, 0.41, 0.082, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.31576492537313433
[2m[36m(func pid=163372)[0m top5: 0.8722014925373134
[2m[36m(func pid=163372)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=163372)[0m f1_macro: 0.27893260698417727
[2m[36m(func pid=163372)[0m f1_weighted: 0.3388092659210777
[2m[36m(func pid=163372)[0m f1_per_class: [0.184, 0.439, 0.069, 0.427, 0.1, 0.383, 0.204, 0.409, 0.206, 0.368]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1101 | Steps: 4 | Val loss: 2.0182 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=162589)[0m top1: 0.25419776119402987
[2m[36m(func pid=162589)[0m top5: 0.8232276119402985
[2m[36m(func pid=162589)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=162589)[0m f1_macro: 0.24110410639537508
[2m[36m(func pid=162589)[0m f1_weighted: 0.2320912207769256
[2m[36m(func pid=162589)[0m f1_per_class: [0.182, 0.459, 0.272, 0.287, 0.073, 0.283, 0.003, 0.461, 0.117, 0.275]
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.0037 | Steps: 4 | Val loss: 2.5931 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4983 | Steps: 4 | Val loss: 2.2786 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.2104 | Steps: 4 | Val loss: 2.1688 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 13:06:44 (running for 00:13:37.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.11  |      0.241 |                   61 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.004 |      0.279 |                   58 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.743 |      0.173 |                   37 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.498 |      0.073 |                   38 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.26725746268656714
[2m[36m(func pid=168552)[0m top5: 0.8675373134328358
[2m[36m(func pid=168552)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=168552)[0m f1_macro: 0.1816315768661522
[2m[36m(func pid=168552)[0m f1_weighted: 0.2160828516155779
[2m[36m(func pid=168552)[0m f1_per_class: [0.167, 0.218, 0.136, 0.523, 0.085, 0.046, 0.0, 0.292, 0.075, 0.275]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.08768656716417911
[2m[36m(func pid=168563)[0m top5: 0.800839552238806
[2m[36m(func pid=168563)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=168563)[0m f1_macro: 0.07308418960627044
[2m[36m(func pid=168563)[0m f1_weighted: 0.07057517933282846
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.055, 0.0, 0.0, 0.097, 0.115, 0.384, 0.08, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3045708955223881
[2m[36m(func pid=163372)[0m top5: 0.8465485074626866
[2m[36m(func pid=163372)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=163372)[0m f1_macro: 0.2547734051281857
[2m[36m(func pid=163372)[0m f1_weighted: 0.3240856317832061
[2m[36m(func pid=163372)[0m f1_per_class: [0.18, 0.454, 0.08, 0.438, 0.089, 0.318, 0.162, 0.451, 0.181, 0.194]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1198 | Steps: 4 | Val loss: 2.0008 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=162589)[0m top1: 0.26725746268656714
[2m[36m(func pid=162589)[0m top5: 0.8250932835820896
[2m[36m(func pid=162589)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=162589)[0m f1_macro: 0.24291045103282283
[2m[36m(func pid=162589)[0m f1_weighted: 0.2448205853526526
[2m[36m(func pid=162589)[0m f1_per_class: [0.208, 0.455, 0.262, 0.331, 0.076, 0.317, 0.0, 0.431, 0.119, 0.231]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2595 | Steps: 4 | Val loss: 2.2849 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6993 | Steps: 4 | Val loss: 2.6370 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.0937 | Steps: 4 | Val loss: 2.2478 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:06:50 (running for 00:13:42.51)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.12  |      0.243 |                   62 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.21  |      0.255 |                   59 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.004 |      0.182 |                   38 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.259 |      0.077 |                   39 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.07882462686567164
[2m[36m(func pid=168563)[0m top5: 0.7747201492537313
[2m[36m(func pid=168563)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=168563)[0m f1_macro: 0.07652058889306743
[2m[36m(func pid=168563)[0m f1_weighted: 0.0588206645363623
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.055, 0.0, 0.0, 0.115, 0.054, 0.459, 0.083, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.26492537313432835
[2m[36m(func pid=168552)[0m top5: 0.832089552238806
[2m[36m(func pid=168552)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=168552)[0m f1_macro: 0.2323117628140937
[2m[36m(func pid=168552)[0m f1_weighted: 0.2697042117528479
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.205, 0.629, 0.489, 0.059, 0.045, 0.191, 0.479, 0.069, 0.158]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1254 | Steps: 4 | Val loss: 1.9703 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163372)[0m top1: 0.3050373134328358
[2m[36m(func pid=163372)[0m top5: 0.8763992537313433
[2m[36m(func pid=163372)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=163372)[0m f1_macro: 0.26985915874145017
[2m[36m(func pid=163372)[0m f1_weighted: 0.324507384315267
[2m[36m(func pid=163372)[0m f1_per_class: [0.22, 0.397, 0.27, 0.51, 0.063, 0.242, 0.154, 0.411, 0.234, 0.198]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m top1: 0.27845149253731344
[2m[36m(func pid=162589)[0m top5: 0.835820895522388
[2m[36m(func pid=162589)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=162589)[0m f1_macro: 0.25150295304616443
[2m[36m(func pid=162589)[0m f1_weighted: 0.2563822547648759
[2m[36m(func pid=162589)[0m f1_per_class: [0.229, 0.454, 0.229, 0.37, 0.075, 0.321, 0.0, 0.409, 0.143, 0.286]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2014 | Steps: 4 | Val loss: 1.8397 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1351 | Steps: 4 | Val loss: 2.1906 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8977 | Steps: 4 | Val loss: 2.2184 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:06:55 (running for 00:13:47.93)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.125 |      0.252 |                   63 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.094 |      0.27  |                   60 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.699 |      0.232 |                   39 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.135 |      0.112 |                   40 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.1553171641791045
[2m[36m(func pid=168563)[0m top5: 0.7681902985074627
[2m[36m(func pid=168563)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=168563)[0m f1_macro: 0.11157181194938362
[2m[36m(func pid=168563)[0m f1_weighted: 0.1529198769950685
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.049, 0.339, 0.0, 0.142, 0.042, 0.457, 0.087, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.3712686567164179
[2m[36m(func pid=168552)[0m top5: 0.9207089552238806
[2m[36m(func pid=168552)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=168552)[0m f1_macro: 0.24585307272852233
[2m[36m(func pid=168552)[0m f1_weighted: 0.3607376055561508
[2m[36m(func pid=168552)[0m f1_per_class: [0.095, 0.382, 0.0, 0.223, 0.091, 0.278, 0.577, 0.301, 0.144, 0.368]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.9900 | Steps: 4 | Val loss: 1.9490 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163372)[0m top1: 0.30830223880597013
[2m[36m(func pid=163372)[0m top5: 0.8875932835820896
[2m[36m(func pid=163372)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=163372)[0m f1_macro: 0.304981255759955
[2m[36m(func pid=163372)[0m f1_weighted: 0.31295426594889897
[2m[36m(func pid=163372)[0m f1_per_class: [0.25, 0.408, 0.55, 0.534, 0.056, 0.221, 0.089, 0.365, 0.243, 0.333]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.2839 | Steps: 4 | Val loss: 2.0961 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=162589)[0m top1: 0.28777985074626866
[2m[36m(func pid=162589)[0m top5: 0.8432835820895522
[2m[36m(func pid=162589)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=162589)[0m f1_macro: 0.25884922024592044
[2m[36m(func pid=162589)[0m f1_weighted: 0.2645405187076485
[2m[36m(func pid=162589)[0m f1_per_class: [0.261, 0.451, 0.256, 0.398, 0.076, 0.326, 0.0, 0.401, 0.142, 0.278]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.1325 | Steps: 4 | Val loss: 2.4205 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1755 | Steps: 4 | Val loss: 1.7377 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 13:07:01 (running for 00:13:53.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.99  |      0.259 |                   64 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.898 |      0.305 |                   61 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.201 |      0.246 |                   40 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.284 |      0.121 |                   41 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.19402985074626866
[2m[36m(func pid=168563)[0m top5: 0.8017723880597015
[2m[36m(func pid=168563)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=168563)[0m f1_macro: 0.12138536322324747
[2m[36m(func pid=168563)[0m f1_weighted: 0.1695079820370127
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.068, 0.419, 0.047, 0.159, 0.019, 0.471, 0.032, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.32322761194029853
[2m[36m(func pid=168552)[0m top5: 0.9174440298507462
[2m[36m(func pid=168552)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=168552)[0m f1_macro: 0.24076337643301096
[2m[36m(func pid=168552)[0m f1_weighted: 0.32610292386221124
[2m[36m(func pid=168552)[0m f1_per_class: [0.122, 0.416, 0.0, 0.113, 0.0, 0.434, 0.478, 0.378, 0.057, 0.411]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.43236940298507465
[2m[36m(func pid=163372)[0m top5: 0.9160447761194029
[2m[36m(func pid=163372)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=163372)[0m f1_macro: 0.40542059324215185
[2m[36m(func pid=163372)[0m f1_weighted: 0.4538269739660183
[2m[36m(func pid=163372)[0m f1_per_class: [0.254, 0.448, 0.786, 0.573, 0.075, 0.347, 0.417, 0.533, 0.203, 0.418]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0007 | Steps: 4 | Val loss: 1.9493 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.0203 | Steps: 4 | Val loss: 2.0402 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=162589)[0m top1: 0.2896455223880597
[2m[36m(func pid=162589)[0m top5: 0.8414179104477612
[2m[36m(func pid=162589)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=162589)[0m f1_macro: 0.2742349482808512
[2m[36m(func pid=162589)[0m f1_weighted: 0.2675641112851897
[2m[36m(func pid=162589)[0m f1_per_class: [0.221, 0.473, 0.386, 0.394, 0.075, 0.308, 0.003, 0.421, 0.137, 0.324]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8169 | Steps: 4 | Val loss: 7.6948 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8401 | Steps: 4 | Val loss: 1.6702 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:07:06 (running for 00:13:58.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.001 |      0.274 |                   65 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.176 |      0.405 |                   62 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.132 |      0.241 |                   41 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.02  |      0.135 |                   42 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.12220149253731344
[2m[36m(func pid=168552)[0m top5: 0.8302238805970149
[2m[36m(func pid=168552)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=168552)[0m f1_macro: 0.12970652987893289
[2m[36m(func pid=168552)[0m f1_weighted: 0.1031628919969579
[2m[36m(func pid=168552)[0m f1_per_class: [0.077, 0.209, 0.0, 0.0, 0.063, 0.102, 0.093, 0.333, 0.072, 0.347]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2080223880597015
[2m[36m(func pid=168563)[0m top5: 0.8083022388059702
[2m[36m(func pid=168563)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=168563)[0m f1_macro: 0.1347619391344466
[2m[36m(func pid=168563)[0m f1_weighted: 0.22538984343191354
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.0, 0.064, 0.409, 0.041, 0.115, 0.235, 0.461, 0.022, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9978 | Steps: 4 | Val loss: 1.9411 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=163372)[0m top1: 0.45848880597014924
[2m[36m(func pid=163372)[0m top5: 0.9155783582089553
[2m[36m(func pid=163372)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=163372)[0m f1_macro: 0.3962072409253562
[2m[36m(func pid=163372)[0m f1_weighted: 0.4874458490645419
[2m[36m(func pid=163372)[0m f1_per_class: [0.361, 0.505, 0.5, 0.553, 0.115, 0.303, 0.53, 0.552, 0.195, 0.348]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0546 | Steps: 4 | Val loss: 5.3773 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5122 | Steps: 4 | Val loss: 2.0745 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=162589)[0m top1: 0.279384328358209
[2m[36m(func pid=162589)[0m top5: 0.8395522388059702
[2m[36m(func pid=162589)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=162589)[0m f1_macro: 0.27458128437956225
[2m[36m(func pid=162589)[0m f1_weighted: 0.24967748371008772
[2m[36m(func pid=162589)[0m f1_per_class: [0.242, 0.479, 0.465, 0.318, 0.067, 0.301, 0.006, 0.449, 0.157, 0.263]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.0262 | Steps: 4 | Val loss: 2.0098 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=168552)[0m top1: 0.2103544776119403
[2m[36m(func pid=168552)[0m top5: 0.7835820895522388
[2m[36m(func pid=168552)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=168552)[0m f1_macro: 0.16910170658844897
[2m[36m(func pid=168552)[0m f1_weighted: 0.23728767409212975
[2m[36m(func pid=168552)[0m f1_per_class: [0.08, 0.26, 0.0, 0.007, 0.048, 0.0, 0.517, 0.46, 0.195, 0.125]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:07:11 (running for 00:14:04.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.998 |      0.275 |                   66 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.84  |      0.396 |                   63 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.055 |      0.169 |                   43 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.02  |      0.135 |                   42 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.24486940298507462
[2m[36m(func pid=168563)[0m top5: 0.7887126865671642
[2m[36m(func pid=168563)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=168563)[0m f1_macro: 0.13451486873771104
[2m[36m(func pid=168563)[0m f1_weighted: 0.28189414938372936
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.092, 0.058, 0.408, 0.038, 0.0, 0.451, 0.298, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.9151 | Steps: 4 | Val loss: 1.9363 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=163372)[0m top1: 0.3689365671641791
[2m[36m(func pid=163372)[0m top5: 0.8955223880597015
[2m[36m(func pid=163372)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=163372)[0m f1_macro: 0.3119985420652197
[2m[36m(func pid=163372)[0m f1_weighted: 0.4055460488991337
[2m[36m(func pid=163372)[0m f1_per_class: [0.171, 0.547, 0.132, 0.378, 0.116, 0.315, 0.42, 0.493, 0.229, 0.32]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2739 | Steps: 4 | Val loss: 4.7627 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3293 | Steps: 4 | Val loss: 2.0428 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=162589)[0m top1: 0.2751865671641791
[2m[36m(func pid=162589)[0m top5: 0.8484141791044776
[2m[36m(func pid=162589)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=162589)[0m f1_macro: 0.275556957619731
[2m[36m(func pid=162589)[0m f1_weighted: 0.23958387063300932
[2m[36m(func pid=162589)[0m f1_per_class: [0.21, 0.479, 0.545, 0.283, 0.072, 0.306, 0.009, 0.429, 0.129, 0.294]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9931 | Steps: 4 | Val loss: 2.6485 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:07:16 (running for 00:14:09.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.915 |      0.276 |                   67 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.026 |      0.312 |                   64 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.274 |      0.09  |                   44 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.512 |      0.135 |                   43 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.1259328358208955
[2m[36m(func pid=168552)[0m top5: 0.6805037313432836
[2m[36m(func pid=168552)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=168552)[0m f1_macro: 0.08968871417630948
[2m[36m(func pid=168552)[0m f1_weighted: 0.13571835016283978
[2m[36m(func pid=168552)[0m f1_per_class: [0.108, 0.305, 0.0, 0.02, 0.057, 0.0, 0.239, 0.0, 0.086, 0.082]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.20149253731343283
[2m[36m(func pid=168563)[0m top5: 0.7831156716417911
[2m[36m(func pid=168563)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=168563)[0m f1_macro: 0.18228601595889068
[2m[36m(func pid=168563)[0m f1_weighted: 0.2562186480948646
[2m[36m(func pid=168563)[0m f1_per_class: [0.091, 0.329, 0.367, 0.208, 0.021, 0.0, 0.379, 0.42, 0.0, 0.009]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.9765 | Steps: 4 | Val loss: 1.9278 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=163372)[0m top1: 0.26865671641791045
[2m[36m(func pid=163372)[0m top5: 0.8605410447761194
[2m[36m(func pid=163372)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=163372)[0m f1_macro: 0.24561379237362962
[2m[36m(func pid=163372)[0m f1_weighted: 0.30788446025230054
[2m[36m(func pid=163372)[0m f1_per_class: [0.178, 0.558, 0.066, 0.223, 0.128, 0.209, 0.281, 0.538, 0.114, 0.16]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2001 | Steps: 4 | Val loss: 3.4165 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4284 | Steps: 4 | Val loss: 1.9860 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=162589)[0m top1: 0.2728544776119403
[2m[36m(func pid=162589)[0m top5: 0.8488805970149254
[2m[36m(func pid=162589)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=162589)[0m f1_macro: 0.24799860643104576
[2m[36m(func pid=162589)[0m f1_weighted: 0.2326467556200645
[2m[36m(func pid=162589)[0m f1_per_class: [0.203, 0.481, 0.439, 0.273, 0.077, 0.306, 0.003, 0.412, 0.157, 0.129]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7859 | Steps: 4 | Val loss: 4.2985 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 13:07:22 (running for 00:14:14.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.976 |      0.248 |                   68 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.993 |      0.246 |                   65 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.2   |      0.184 |                   45 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.329 |      0.182 |                   44 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.16977611940298507
[2m[36m(func pid=168552)[0m top5: 0.7919776119402985
[2m[36m(func pid=168552)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=168552)[0m f1_macro: 0.18356371001139993
[2m[36m(func pid=168552)[0m f1_weighted: 0.177935213717434
[2m[36m(func pid=168552)[0m f1_per_class: [0.148, 0.233, 0.049, 0.223, 0.062, 0.167, 0.057, 0.486, 0.145, 0.267]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.16884328358208955
[2m[36m(func pid=168563)[0m top5: 0.8036380597014925
[2m[36m(func pid=168563)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=168563)[0m f1_macro: 0.13197816734829007
[2m[36m(func pid=168563)[0m f1_weighted: 0.20668471297506297
[2m[36m(func pid=168563)[0m f1_per_class: [0.078, 0.136, 0.155, 0.075, 0.025, 0.0, 0.466, 0.356, 0.0, 0.03]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0424 | Steps: 4 | Val loss: 1.9500 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=163372)[0m top1: 0.19962686567164178
[2m[36m(func pid=163372)[0m top5: 0.8013059701492538
[2m[36m(func pid=163372)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=163372)[0m f1_macro: 0.15022799828368388
[2m[36m(func pid=163372)[0m f1_weighted: 0.1868802091321924
[2m[36m(func pid=163372)[0m f1_per_class: [0.25, 0.556, 0.124, 0.237, 0.155, 0.03, 0.04, 0.0, 0.075, 0.036]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7207 | Steps: 4 | Val loss: 2.4952 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.8689 | Steps: 4 | Val loss: 2.0057 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=162589)[0m top1: 0.2733208955223881
[2m[36m(func pid=162589)[0m top5: 0.8488805970149254
[2m[36m(func pid=162589)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=162589)[0m f1_macro: 0.2481816544141615
[2m[36m(func pid=162589)[0m f1_weighted: 0.2361406380910899
[2m[36m(func pid=162589)[0m f1_per_class: [0.19, 0.494, 0.4, 0.259, 0.076, 0.299, 0.015, 0.462, 0.149, 0.138]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0455 | Steps: 4 | Val loss: 1.8909 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 13:07:27 (running for 00:14:19.83)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.042 |      0.248 |                   69 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.786 |      0.15  |                   66 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.721 |      0.223 |                   46 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.428 |      0.132 |                   45 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.2873134328358209
[2m[36m(func pid=168552)[0m top5: 0.8736007462686567
[2m[36m(func pid=168552)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=168552)[0m f1_macro: 0.22332594332852027
[2m[36m(func pid=168552)[0m f1_weighted: 0.26936878561938293
[2m[36m(func pid=168552)[0m f1_per_class: [0.151, 0.358, 0.154, 0.433, 0.16, 0.008, 0.18, 0.354, 0.076, 0.359]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.12779850746268656
[2m[36m(func pid=168563)[0m top5: 0.7957089552238806
[2m[36m(func pid=168563)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=168563)[0m f1_macro: 0.11780923144895325
[2m[36m(func pid=168563)[0m f1_weighted: 0.1134593868553022
[2m[36m(func pid=168563)[0m f1_per_class: [0.073, 0.031, 0.094, 0.0, 0.049, 0.289, 0.159, 0.398, 0.055, 0.03]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0071 | Steps: 4 | Val loss: 1.9378 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=163372)[0m top1: 0.37033582089552236
[2m[36m(func pid=163372)[0m top5: 0.902518656716418
[2m[36m(func pid=163372)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=163372)[0m f1_macro: 0.3418195771431018
[2m[36m(func pid=163372)[0m f1_weighted: 0.3795079721223194
[2m[36m(func pid=163372)[0m f1_per_class: [0.31, 0.502, 0.4, 0.334, 0.083, 0.398, 0.358, 0.451, 0.255, 0.327]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9105 | Steps: 4 | Val loss: 2.5369 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0805 | Steps: 4 | Val loss: 2.2088 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=162589)[0m top1: 0.28218283582089554
[2m[36m(func pid=162589)[0m top5: 0.8549440298507462
[2m[36m(func pid=162589)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=162589)[0m f1_macro: 0.2542988989157745
[2m[36m(func pid=162589)[0m f1_weighted: 0.2512506768778846
[2m[36m(func pid=162589)[0m f1_per_class: [0.193, 0.49, 0.361, 0.293, 0.082, 0.307, 0.034, 0.467, 0.124, 0.194]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9148 | Steps: 4 | Val loss: 2.0609 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=168552)[0m top1: 0.2751865671641791
[2m[36m(func pid=168552)[0m top5: 0.8600746268656716
[2m[36m(func pid=168552)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=168552)[0m f1_macro: 0.23421284548896013
[2m[36m(func pid=168552)[0m f1_weighted: 0.23755948515395742
[2m[36m(func pid=168552)[0m f1_per_class: [0.208, 0.393, 0.375, 0.504, 0.119, 0.008, 0.0, 0.238, 0.064, 0.432]
== Status ==
Current time: 2024-01-07 13:07:32 (running for 00:14:25.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.007 |      0.254 |                   70 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.045 |      0.342 |                   67 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.911 |      0.234 |                   47 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.869 |      0.118 |                   46 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.09888059701492537
[2m[36m(func pid=168563)[0m top5: 0.7789179104477612
[2m[36m(func pid=168563)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=168563)[0m f1_macro: 0.12028215903159725
[2m[36m(func pid=168563)[0m f1_weighted: 0.07969225244805977
[2m[36m(func pid=168563)[0m f1_per_class: [0.083, 0.042, 0.253, 0.0, 0.0, 0.191, 0.057, 0.482, 0.072, 0.024]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.9890 | Steps: 4 | Val loss: 1.9230 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=163372)[0m top1: 0.3381529850746269
[2m[36m(func pid=163372)[0m top5: 0.8973880597014925
[2m[36m(func pid=163372)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=163372)[0m f1_macro: 0.3967042387752426
[2m[36m(func pid=163372)[0m f1_weighted: 0.35265870074485484
[2m[36m(func pid=163372)[0m f1_per_class: [0.315, 0.465, 0.889, 0.341, 0.06, 0.355, 0.287, 0.452, 0.163, 0.64]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7347 | Steps: 4 | Val loss: 2.3917 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.5109 | Steps: 4 | Val loss: 2.3124 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=162589)[0m top1: 0.2989738805970149
[2m[36m(func pid=162589)[0m top5: 0.8502798507462687
[2m[36m(func pid=162589)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=162589)[0m f1_macro: 0.2633114531960391
[2m[36m(func pid=162589)[0m f1_weighted: 0.2684858902549321
[2m[36m(func pid=162589)[0m f1_per_class: [0.224, 0.511, 0.344, 0.33, 0.082, 0.303, 0.039, 0.501, 0.111, 0.188]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0943 | Steps: 4 | Val loss: 2.2263 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:07:38 (running for 00:14:30.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.989 |      0.263 |                   71 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.915 |      0.397 |                   68 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.735 |      0.185 |                   48 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.08  |      0.12  |                   47 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.29151119402985076
[2m[36m(func pid=168552)[0m top5: 0.8409514925373134
[2m[36m(func pid=168552)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=168552)[0m f1_macro: 0.18521446809622139
[2m[36m(func pid=168552)[0m f1_weighted: 0.28498401217665237
[2m[36m(func pid=168552)[0m f1_per_class: [0.158, 0.397, 0.0, 0.464, 0.112, 0.016, 0.202, 0.312, 0.043, 0.148]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.09748134328358209
[2m[36m(func pid=168563)[0m top5: 0.7504664179104478
[2m[36m(func pid=168563)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=168563)[0m f1_macro: 0.1260351626255946
[2m[36m(func pid=168563)[0m f1_weighted: 0.09338765101696427
[2m[36m(func pid=168563)[0m f1_per_class: [0.088, 0.222, 0.31, 0.0, 0.0, 0.074, 0.048, 0.479, 0.023, 0.017]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.9033 | Steps: 4 | Val loss: 1.9071 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=163372)[0m top1: 0.32322761194029853
[2m[36m(func pid=163372)[0m top5: 0.886660447761194
[2m[36m(func pid=163372)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=163372)[0m f1_macro: 0.3878142470484557
[2m[36m(func pid=163372)[0m f1_weighted: 0.36986136220278865
[2m[36m(func pid=163372)[0m f1_per_class: [0.351, 0.423, 0.8, 0.387, 0.047, 0.291, 0.339, 0.49, 0.241, 0.508]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2822 | Steps: 4 | Val loss: 2.4205 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3933 | Steps: 4 | Val loss: 2.3327 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=162589)[0m top1: 0.30363805970149255
[2m[36m(func pid=162589)[0m top5: 0.8586753731343284
[2m[36m(func pid=162589)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=162589)[0m f1_macro: 0.26280724851049847
[2m[36m(func pid=162589)[0m f1_weighted: 0.272482118410645
[2m[36m(func pid=162589)[0m f1_per_class: [0.226, 0.506, 0.392, 0.362, 0.084, 0.308, 0.028, 0.499, 0.091, 0.133]
[2m[36m(func pid=162589)[0m 
[2m[36m(func pid=168552)[0m top1: 0.3516791044776119
[2m[36m(func pid=168552)[0m top5: 0.8512126865671642
[2m[36m(func pid=168552)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=168552)[0m f1_macro: 0.19076301821054584
[2m[36m(func pid=168552)[0m f1_weighted: 0.3836105720782911
[2m[36m(func pid=168552)[0m f1_per_class: [0.14, 0.499, 0.0, 0.434, 0.054, 0.179, 0.502, 0.0, 0.099, 0.0]
[2m[36m(func pid=168552)[0m 
== Status ==
Current time: 2024-01-07 13:07:43 (running for 00:14:35.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.903 |      0.263 |                   72 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.094 |      0.388 |                   69 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.282 |      0.191 |                   49 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.511 |      0.126 |                   48 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2121 | Steps: 4 | Val loss: 2.2379 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=168563)[0m top1: 0.06576492537313433
[2m[36m(func pid=168563)[0m top5: 0.7360074626865671
[2m[36m(func pid=168563)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=168563)[0m f1_macro: 0.0944082869035178
[2m[36m(func pid=168563)[0m f1_weighted: 0.0697762576959002
[2m[36m(func pid=168563)[0m f1_per_class: [0.075, 0.073, 0.273, 0.0, 0.0, 0.0, 0.102, 0.406, 0.0, 0.015]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.0257 | Steps: 4 | Val loss: 1.9055 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=163372)[0m top1: 0.30363805970149255
[2m[36m(func pid=163372)[0m top5: 0.8773320895522388
[2m[36m(func pid=163372)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=163372)[0m f1_macro: 0.34381604273345096
[2m[36m(func pid=163372)[0m f1_weighted: 0.3574651588031902
[2m[36m(func pid=163372)[0m f1_per_class: [0.272, 0.408, 0.558, 0.415, 0.04, 0.249, 0.309, 0.507, 0.196, 0.484]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.6555 | Steps: 4 | Val loss: 2.0129 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2968 | Steps: 4 | Val loss: 2.3203 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=162589)[0m top1: 0.3087686567164179
[2m[36m(func pid=162589)[0m top5: 0.8493470149253731
[2m[36m(func pid=162589)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=162589)[0m f1_macro: 0.26242654696331147
[2m[36m(func pid=162589)[0m f1_weighted: 0.27104770938664
[2m[36m(func pid=162589)[0m f1_per_class: [0.225, 0.54, 0.393, 0.346, 0.089, 0.297, 0.025, 0.495, 0.073, 0.143]
[2m[36m(func pid=162589)[0m 
== Status ==
Current time: 2024-01-07 13:07:49 (running for 00:14:41.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  2.026 |      0.262 |                   73 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.212 |      0.344 |                   70 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.655 |      0.199 |                   50 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.393 |      0.094 |                   49 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.2957089552238806
[2m[36m(func pid=168552)[0m top5: 0.8675373134328358
[2m[36m(func pid=168552)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=168552)[0m f1_macro: 0.199195767951198
[2m[36m(func pid=168552)[0m f1_weighted: 0.30304443460019986
[2m[36m(func pid=168552)[0m f1_per_class: [0.167, 0.416, 0.0, 0.222, 0.056, 0.12, 0.421, 0.348, 0.165, 0.077]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1754 | Steps: 4 | Val loss: 2.2892 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=168563)[0m top1: 0.06949626865671642
[2m[36m(func pid=168563)[0m top5: 0.7705223880597015
[2m[36m(func pid=168563)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=168563)[0m f1_macro: 0.08210195368538739
[2m[36m(func pid=168563)[0m f1_weighted: 0.07649296967782464
[2m[36m(func pid=168563)[0m f1_per_class: [0.082, 0.0, 0.077, 0.0, 0.0, 0.09, 0.141, 0.333, 0.074, 0.023]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.8114 | Steps: 4 | Val loss: 1.8969 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=163372)[0m top1: 0.292910447761194
[2m[36m(func pid=163372)[0m top5: 0.8661380597014925
[2m[36m(func pid=163372)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=163372)[0m f1_macro: 0.2733449779221298
[2m[36m(func pid=163372)[0m f1_weighted: 0.3223113007194593
[2m[36m(func pid=163372)[0m f1_per_class: [0.289, 0.362, 0.203, 0.478, 0.062, 0.269, 0.189, 0.373, 0.212, 0.296]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.7260 | Steps: 4 | Val loss: 2.2802 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1458 | Steps: 4 | Val loss: 2.2954 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=162589)[0m top1: 0.31669776119402987
[2m[36m(func pid=162589)[0m top5: 0.8498134328358209
[2m[36m(func pid=162589)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=162589)[0m f1_macro: 0.2661015058475976
[2m[36m(func pid=162589)[0m f1_weighted: 0.2823702696280465
[2m[36m(func pid=162589)[0m f1_per_class: [0.217, 0.547, 0.4, 0.362, 0.09, 0.306, 0.042, 0.481, 0.077, 0.138]
[2m[36m(func pid=162589)[0m 
== Status ==
Current time: 2024-01-07 13:07:54 (running for 00:14:46.84)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.324
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00004 | RUNNING    | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.811 |      0.266 |                   74 |
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.175 |      0.273 |                   71 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.726 |      0.158 |                   51 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.297 |      0.082 |                   50 |
| train_52b21_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.18470149253731344
[2m[36m(func pid=168552)[0m top5: 0.8222947761194029
[2m[36m(func pid=168552)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=168552)[0m f1_macro: 0.15787164948900165
[2m[36m(func pid=168552)[0m f1_weighted: 0.15558151492367397
[2m[36m(func pid=168552)[0m f1_per_class: [0.0, 0.305, 0.0, 0.145, 0.041, 0.293, 0.0, 0.396, 0.067, 0.333]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3290 | Steps: 4 | Val loss: 4.1938 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=168563)[0m top1: 0.11007462686567164
[2m[36m(func pid=168563)[0m top5: 0.7574626865671642
[2m[36m(func pid=168563)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=168563)[0m f1_macro: 0.10208361541311291
[2m[36m(func pid=168563)[0m f1_weighted: 0.079069184344404
[2m[36m(func pid=168563)[0m f1_per_class: [0.09, 0.0, 0.103, 0.0, 0.03, 0.282, 0.065, 0.423, 0.0, 0.027]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=162589)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8339 | Steps: 4 | Val loss: 1.8969 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=163372)[0m top1: 0.2196828358208955
[2m[36m(func pid=163372)[0m top5: 0.8190298507462687
[2m[36m(func pid=163372)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=163372)[0m f1_macro: 0.19203331848571795
[2m[36m(func pid=163372)[0m f1_weighted: 0.22714362245660405
[2m[36m(func pid=163372)[0m f1_per_class: [0.394, 0.496, 0.233, 0.433, 0.147, 0.046, 0.003, 0.0, 0.1, 0.068]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.7916 | Steps: 4 | Val loss: 2.0866 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1725 | Steps: 4 | Val loss: 2.2388 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=162589)[0m top1: 0.30783582089552236
[2m[36m(func pid=162589)[0m top5: 0.8535447761194029
[2m[36m(func pid=162589)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=162589)[0m f1_macro: 0.27476630538150004
[2m[36m(func pid=162589)[0m f1_weighted: 0.27719178566350317
[2m[36m(func pid=162589)[0m f1_per_class: [0.213, 0.528, 0.435, 0.344, 0.083, 0.311, 0.046, 0.471, 0.123, 0.194]
[2m[36m(func pid=168552)[0m top1: 0.269589552238806
[2m[36m(func pid=168552)[0m top5: 0.8409514925373134
[2m[36m(func pid=168552)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=168552)[0m f1_macro: 0.22181924442101889
[2m[36m(func pid=168552)[0m f1_weighted: 0.29117251714939824
[2m[36m(func pid=168552)[0m f1_per_class: [0.105, 0.32, 0.0, 0.301, 0.049, 0.308, 0.284, 0.408, 0.05, 0.394]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8350 | Steps: 4 | Val loss: 2.0485 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=168563)[0m top1: 0.11847014925373134
[2m[36m(func pid=168563)[0m top5: 0.761660447761194
[2m[36m(func pid=168563)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=168563)[0m f1_macro: 0.1125274883177875
[2m[36m(func pid=168563)[0m f1_weighted: 0.07636786516477953
[2m[36m(func pid=168563)[0m f1_per_class: [0.091, 0.0, 0.106, 0.036, 0.057, 0.289, 0.0, 0.522, 0.0, 0.025]
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8060 | Steps: 4 | Val loss: 2.0585 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=163372)[0m top1: 0.3521455223880597
[2m[36m(func pid=163372)[0m top5: 0.8731343283582089
[2m[36m(func pid=163372)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=163372)[0m f1_macro: 0.34052189835390123
[2m[36m(func pid=163372)[0m f1_weighted: 0.35306083298697943
[2m[36m(func pid=163372)[0m f1_per_class: [0.505, 0.56, 0.5, 0.334, 0.137, 0.212, 0.304, 0.457, 0.159, 0.238]
[2m[36m(func pid=168552)[0m top1: 0.3180970149253731
[2m[36m(func pid=168552)[0m top5: 0.8479477611940298
[2m[36m(func pid=168552)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=168552)[0m f1_macro: 0.23713015740454982
[2m[36m(func pid=168552)[0m f1_weighted: 0.3292257798883831
[2m[36m(func pid=168552)[0m f1_per_class: [0.101, 0.442, 0.0, 0.261, 0.083, 0.098, 0.448, 0.44, 0.077, 0.421]
== Status ==
Current time: 2024-01-07 13:07:59 (running for 00:14:52.26)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.306
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.329 |      0.192 |                   72 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.792 |      0.222 |                   52 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.146 |      0.102 |                   51 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=181384)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=181384)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=181384)[0m Configuration completed!
[2m[36m(func pid=181384)[0m New optimizer parameters:
[2m[36m(func pid=181384)[0m SGD (
[2m[36m(func pid=181384)[0m Parameter Group 0
[2m[36m(func pid=181384)[0m     dampening: 0
[2m[36m(func pid=181384)[0m     differentiable: False
[2m[36m(func pid=181384)[0m     foreach: None
[2m[36m(func pid=181384)[0m     lr: 0.0001
[2m[36m(func pid=181384)[0m     maximize: False
[2m[36m(func pid=181384)[0m     momentum: 0.99
[2m[36m(func pid=181384)[0m     nesterov: False
[2m[36m(func pid=181384)[0m     weight_decay: 0.0001
[2m[36m(func pid=181384)[0m )
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:08:06 (running for 00:14:59.14)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.306
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.329 |      0.192 |                   72 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.806 |      0.237 |                   53 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.146 |      0.102 |                   51 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.5640 | Steps: 4 | Val loss: 2.1412 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9106 | Steps: 4 | Val loss: 1.9277 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3376 | Steps: 4 | Val loss: 1.8426 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1411 | Steps: 4 | Val loss: 2.3270 | Batch size: 32 | lr: 0.0001 | Duration: 5.01s
== Status ==
Current time: 2024-01-07 13:08:11 (running for 00:15:04.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.306
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.835 |      0.341 |                   73 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.806 |      0.237 |                   53 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.172 |      0.113 |                   52 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.43470149253731344
[2m[36m(func pid=168552)[0m top5: 0.8521455223880597
[2m[36m(func pid=168552)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=168552)[0m f1_macro: 0.3579851001294136
[2m[36m(func pid=168552)[0m f1_weighted: 0.44106389485868164
[2m[36m(func pid=168552)[0m f1_per_class: [0.129, 0.536, 0.783, 0.469, 0.158, 0.305, 0.468, 0.519, 0.127, 0.087]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.1296641791044776
[2m[36m(func pid=168563)[0m top5: 0.7714552238805971
[2m[36m(func pid=168563)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=168563)[0m f1_macro: 0.1348247145475216
[2m[36m(func pid=168563)[0m f1_weighted: 0.09719299362234333
[2m[36m(func pid=168563)[0m f1_per_class: [0.105, 0.104, 0.226, 0.047, 0.043, 0.286, 0.0, 0.509, 0.0, 0.028]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3824626865671642
[2m[36m(func pid=163372)[0m top5: 0.8777985074626866
[2m[36m(func pid=163372)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=163372)[0m f1_macro: 0.3404460766381757
[2m[36m(func pid=163372)[0m f1_weighted: 0.362165488431262
[2m[36m(func pid=163372)[0m f1_per_class: [0.407, 0.545, 0.414, 0.263, 0.154, 0.231, 0.402, 0.446, 0.209, 0.333]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.14832089552238806
[2m[36m(func pid=181384)[0m top5: 0.5303171641791045
[2m[36m(func pid=181384)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=181384)[0m f1_macro: 0.043893360567880756
[2m[36m(func pid=181384)[0m f1_weighted: 0.08402334011933175
[2m[36m(func pid=181384)[0m f1_per_class: [0.0, 0.0, 0.0, 0.265, 0.0, 0.0, 0.0, 0.174, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.5605 | Steps: 4 | Val loss: 1.9384 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8550 | Steps: 4 | Val loss: 1.7859 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.8841 | Steps: 4 | Val loss: 2.0123 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9831 | Steps: 4 | Val loss: 2.3231 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 13:08:17 (running for 00:15:10.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.306
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.911 |      0.34  |                   74 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.338 |      0.358 |                   54 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.561 |      0.19  |                   54 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  3.141 |      0.044 |                    1 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.42490671641791045
[2m[36m(func pid=168552)[0m top5: 0.8852611940298507
[2m[36m(func pid=168552)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=168552)[0m f1_macro: 0.2733033557098741
[2m[36m(func pid=168552)[0m f1_weighted: 0.43979350263414874
[2m[36m(func pid=168552)[0m f1_per_class: [0.191, 0.555, 0.0, 0.475, 0.114, 0.165, 0.526, 0.437, 0.14, 0.13]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.28078358208955223
[2m[36m(func pid=168563)[0m top5: 0.804570895522388
[2m[36m(func pid=168563)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=168563)[0m f1_macro: 0.19033874865522354
[2m[36m(func pid=168563)[0m f1_weighted: 0.2940454811705585
[2m[36m(func pid=168563)[0m f1_per_class: [0.093, 0.402, 0.224, 0.124, 0.0, 0.0, 0.528, 0.508, 0.0, 0.023]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.4193097014925373
[2m[36m(func pid=163372)[0m top5: 0.875
[2m[36m(func pid=163372)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=163372)[0m f1_macro: 0.31879613949200114
[2m[36m(func pid=163372)[0m f1_weighted: 0.43332318535886355
[2m[36m(func pid=163372)[0m f1_per_class: [0.273, 0.563, 0.224, 0.368, 0.115, 0.298, 0.524, 0.484, 0.189, 0.151]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.18936567164179105
[2m[36m(func pid=181384)[0m top5: 0.5471082089552238
[2m[36m(func pid=181384)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=181384)[0m f1_macro: 0.03950083469024136
[2m[36m(func pid=181384)[0m f1_weighted: 0.09784362366499548
[2m[36m(func pid=181384)[0m f1_per_class: [0.0, 0.0, 0.0, 0.343, 0.0, 0.0, 0.0, 0.035, 0.0, 0.017]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7911 | Steps: 4 | Val loss: 2.0736 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2637 | Steps: 4 | Val loss: 1.9179 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.0594 | Steps: 4 | Val loss: 2.2583 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9404 | Steps: 4 | Val loss: 2.3322 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:08:23 (running for 00:15:15.49)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.884 |      0.319 |                   75 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.791 |      0.271 |                   56 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.561 |      0.19  |                   54 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.983 |      0.04  |                    2 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.34048507462686567
[2m[36m(func pid=168552)[0m top5: 0.8390858208955224
[2m[36m(func pid=168552)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=168552)[0m f1_macro: 0.2713894787786309
[2m[36m(func pid=168552)[0m f1_weighted: 0.3480928244788122
[2m[36m(func pid=168552)[0m f1_per_class: [0.175, 0.465, 0.143, 0.107, 0.078, 0.363, 0.513, 0.511, 0.23, 0.129]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.31203358208955223
[2m[36m(func pid=168563)[0m top5: 0.7877798507462687
[2m[36m(func pid=168563)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=168563)[0m f1_macro: 0.17817268655282723
[2m[36m(func pid=168563)[0m f1_weighted: 0.29683862676749423
[2m[36m(func pid=168563)[0m f1_per_class: [0.091, 0.447, 0.117, 0.07, 0.0, 0.0, 0.572, 0.466, 0.0, 0.018]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.35261194029850745
[2m[36m(func pid=163372)[0m top5: 0.8927238805970149
[2m[36m(func pid=163372)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=163372)[0m f1_macro: 0.29520795895394447
[2m[36m(func pid=163372)[0m f1_weighted: 0.38830599196934995
[2m[36m(func pid=163372)[0m f1_per_class: [0.222, 0.563, 0.1, 0.415, 0.108, 0.302, 0.323, 0.502, 0.223, 0.193]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2103544776119403
[2m[36m(func pid=181384)[0m top5: 0.5471082089552238
[2m[36m(func pid=181384)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=181384)[0m f1_macro: 0.07733580684860959
[2m[36m(func pid=181384)[0m f1_weighted: 0.10709028038988508
[2m[36m(func pid=181384)[0m f1_per_class: [0.0, 0.011, 0.375, 0.364, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5217 | Steps: 4 | Val loss: 1.9851 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.9779 | Steps: 4 | Val loss: 2.7010 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.0474 | Steps: 4 | Val loss: 2.2138 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8726 | Steps: 4 | Val loss: 2.3369 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:08:28 (running for 00:15:21.02)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.059 |      0.295 |                   76 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.791 |      0.271 |                   56 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.522 |      0.131 |                   56 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.94  |      0.077 |                    3 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.30223880597014924
[2m[36m(func pid=168552)[0m top5: 0.7523320895522388
[2m[36m(func pid=168552)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=168552)[0m f1_macro: 0.24651105397284603
[2m[36m(func pid=168552)[0m f1_weighted: 0.3062410308306892
[2m[36m(func pid=168552)[0m f1_per_class: [0.162, 0.44, 0.111, 0.0, 0.056, 0.285, 0.523, 0.504, 0.192, 0.192]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2826492537313433
[2m[36m(func pid=168563)[0m top5: 0.7737873134328358
[2m[36m(func pid=168563)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=168563)[0m f1_macro: 0.1310477866887118
[2m[36m(func pid=168563)[0m f1_weighted: 0.26492597955382496
[2m[36m(func pid=168563)[0m f1_per_class: [0.065, 0.416, 0.061, 0.063, 0.041, 0.0, 0.563, 0.101, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3498134328358209
[2m[36m(func pid=163372)[0m top5: 0.8997201492537313
[2m[36m(func pid=163372)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=163372)[0m f1_macro: 0.29991229902770994
[2m[36m(func pid=163372)[0m f1_weighted: 0.3568189812673136
[2m[36m(func pid=163372)[0m f1_per_class: [0.304, 0.532, 0.108, 0.499, 0.091, 0.307, 0.144, 0.508, 0.25, 0.257]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.1935634328358209
[2m[36m(func pid=181384)[0m top5: 0.554570895522388
[2m[36m(func pid=181384)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=181384)[0m f1_macro: 0.05459901910479574
[2m[36m(func pid=181384)[0m f1_weighted: 0.11019824111453377
[2m[36m(func pid=181384)[0m f1_per_class: [0.074, 0.016, 0.048, 0.371, 0.0, 0.0, 0.0, 0.037, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3814 | Steps: 4 | Val loss: 1.9316 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.6510 | Steps: 4 | Val loss: 2.6441 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.1424 | Steps: 4 | Val loss: 1.9098 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8258 | Steps: 4 | Val loss: 2.3376 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:08:34 (running for 00:15:26.38)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.047 |      0.3   |                   77 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.651 |      0.249 |                   58 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.522 |      0.131 |                   56 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.873 |      0.055 |                    4 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.30970149253731344
[2m[36m(func pid=168552)[0m top5: 0.738339552238806
[2m[36m(func pid=168552)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=168552)[0m f1_macro: 0.24923520826124523
[2m[36m(func pid=168552)[0m f1_weighted: 0.30949798710324533
[2m[36m(func pid=168552)[0m f1_per_class: [0.142, 0.427, 0.115, 0.0, 0.058, 0.261, 0.555, 0.496, 0.146, 0.294]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.31203358208955223
[2m[36m(func pid=168563)[0m top5: 0.7826492537313433
[2m[36m(func pid=168563)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=168563)[0m f1_macro: 0.13881036352183088
[2m[36m(func pid=168563)[0m f1_weighted: 0.26690639122223697
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.436, 0.074, 0.016, 0.039, 0.0, 0.589, 0.144, 0.09, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.37173507462686567
[2m[36m(func pid=163372)[0m top5: 0.9076492537313433
[2m[36m(func pid=163372)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=163372)[0m f1_macro: 0.325049970133433
[2m[36m(func pid=163372)[0m f1_weighted: 0.3468642515557371
[2m[36m(func pid=163372)[0m f1_per_class: [0.335, 0.491, 0.338, 0.548, 0.081, 0.377, 0.065, 0.43, 0.28, 0.306]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.12826492537313433
[2m[36m(func pid=181384)[0m top5: 0.570429104477612
[2m[36m(func pid=181384)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=181384)[0m f1_macro: 0.04097238344706399
[2m[36m(func pid=181384)[0m f1_weighted: 0.0858634003843273
[2m[36m(func pid=181384)[0m f1_per_class: [0.074, 0.0, 0.034, 0.302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4775 | Steps: 4 | Val loss: 2.9324 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2535 | Steps: 4 | Val loss: 2.0032 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.9740 | Steps: 4 | Val loss: 1.9624 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7796 | Steps: 4 | Val loss: 2.3301 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:08:39 (running for 00:15:31.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.142 |      0.325 |                   78 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.651 |      0.249 |                   58 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.253 |      0.176 |                   58 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.826 |      0.041 |                    5 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.2080223880597015
[2m[36m(func pid=168552)[0m top5: 0.784981343283582
[2m[36m(func pid=168552)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=168552)[0m f1_macro: 0.14887424947644184
[2m[36m(func pid=168552)[0m f1_weighted: 0.21857299312141956
[2m[36m(func pid=168552)[0m f1_per_class: [0.13, 0.501, 0.067, 0.01, 0.074, 0.195, 0.337, 0.0, 0.048, 0.126]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2490671641791045
[2m[36m(func pid=168563)[0m top5: 0.7854477611940298
[2m[36m(func pid=168563)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=168563)[0m f1_macro: 0.17633117178669114
[2m[36m(func pid=168563)[0m f1_weighted: 0.1724054991452759
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.439, 0.393, 0.0, 0.038, 0.278, 0.118, 0.436, 0.061, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3833955223880597
[2m[36m(func pid=163372)[0m top5: 0.8978544776119403
[2m[36m(func pid=163372)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=163372)[0m f1_macro: 0.3439388731081957
[2m[36m(func pid=163372)[0m f1_weighted: 0.383740923759005
[2m[36m(func pid=163372)[0m f1_per_class: [0.495, 0.526, 0.5, 0.545, 0.074, 0.298, 0.194, 0.479, 0.186, 0.143]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.08069029850746269
[2m[36m(func pid=181384)[0m top5: 0.5960820895522388
[2m[36m(func pid=181384)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=181384)[0m f1_macro: 0.048539696471077386
[2m[36m(func pid=181384)[0m f1_weighted: 0.06894368667557647
[2m[36m(func pid=181384)[0m f1_per_class: [0.065, 0.0, 0.032, 0.2, 0.0, 0.014, 0.0, 0.175, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2174 | Steps: 4 | Val loss: 2.1621 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3344 | Steps: 4 | Val loss: 2.1619 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.8595 | Steps: 4 | Val loss: 1.9009 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7069 | Steps: 4 | Val loss: 2.3358 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=168563)[0m top1: 0.24953358208955223
[2m[36m(func pid=168563)[0m top5: 0.7723880597014925
[2m[36m(func pid=168563)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=168563)[0m f1_macro: 0.19249703033048457
[2m[36m(func pid=168563)[0m f1_weighted: 0.14734729575087174
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.448, 0.606, 0.0, 0.041, 0.308, 0.006, 0.49, 0.026, 0.0]
== Status ==
Current time: 2024-01-07 13:08:44 (running for 00:15:37.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.974 |      0.344 |                   79 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.478 |      0.149 |                   59 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.217 |      0.192 |                   59 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.78  |      0.049 |                    6 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.3381529850746269
[2m[36m(func pid=168552)[0m top5: 0.8647388059701493
[2m[36m(func pid=168552)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=168552)[0m f1_macro: 0.24971914911652626
[2m[36m(func pid=168552)[0m f1_weighted: 0.3626655247213412
[2m[36m(func pid=168552)[0m f1_per_class: [0.174, 0.529, 0.145, 0.198, 0.052, 0.208, 0.565, 0.148, 0.244, 0.234]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.37546641791044777
[2m[36m(func pid=163372)[0m top5: 0.8969216417910447
[2m[36m(func pid=163372)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=163372)[0m f1_macro: 0.3727073137821424
[2m[36m(func pid=163372)[0m f1_weighted: 0.3573537993524811
[2m[36m(func pid=163372)[0m f1_per_class: [0.442, 0.557, 0.629, 0.513, 0.049, 0.317, 0.095, 0.442, 0.336, 0.349]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.08208955223880597
[2m[36m(func pid=181384)[0m top5: 0.5942164179104478
[2m[36m(func pid=181384)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=181384)[0m f1_macro: 0.054579872620842705
[2m[36m(func pid=181384)[0m f1_weighted: 0.06100494501320724
[2m[36m(func pid=181384)[0m f1_per_class: [0.055, 0.01, 0.054, 0.081, 0.0, 0.269, 0.0, 0.077, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2195 | Steps: 4 | Val loss: 2.2035 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4673 | Steps: 4 | Val loss: 2.1148 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.5776 | Steps: 4 | Val loss: 1.9706 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8237 | Steps: 4 | Val loss: 2.3335 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=168563)[0m top1: 0.2178171641791045
[2m[36m(func pid=168563)[0m top5: 0.7901119402985075
[2m[36m(func pid=168563)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=168563)[0m f1_macro: 0.13855079035725876
[2m[36m(func pid=168563)[0m f1_weighted: 0.14019791281079094
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.438, 0.111, 0.003, 0.039, 0.309, 0.0, 0.469, 0.018, 0.0]
[2m[36m(func pid=168563)[0m 
== Status ==
Current time: 2024-01-07 13:08:50 (running for 00:15:42.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.859 |      0.373 |                   80 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.334 |      0.25  |                   60 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.219 |      0.139 |                   60 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.707 |      0.055 |                    7 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.30783582089552236
[2m[36m(func pid=168552)[0m top5: 0.8885261194029851
[2m[36m(func pid=168552)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=168552)[0m f1_macro: 0.26298693113284577
[2m[36m(func pid=168552)[0m f1_weighted: 0.32177752914410684
[2m[36m(func pid=168552)[0m f1_per_class: [0.205, 0.511, 0.115, 0.419, 0.059, 0.053, 0.241, 0.379, 0.199, 0.448]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.34095149253731344
[2m[36m(func pid=163372)[0m top5: 0.8903917910447762
[2m[36m(func pid=163372)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=163372)[0m f1_macro: 0.3612426469680186
[2m[36m(func pid=163372)[0m f1_weighted: 0.32333548270692625
[2m[36m(func pid=163372)[0m f1_per_class: [0.367, 0.553, 0.774, 0.444, 0.059, 0.28, 0.087, 0.358, 0.235, 0.456]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.10960820895522388
[2m[36m(func pid=181384)[0m top5: 0.5988805970149254
[2m[36m(func pid=181384)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.0778630695309266
[2m[36m(func pid=181384)[0m f1_weighted: 0.04966595883239614
[2m[36m(func pid=181384)[0m f1_per_class: [0.057, 0.016, 0.212, 0.006, 0.098, 0.343, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1746 | Steps: 4 | Val loss: 2.0875 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2721 | Steps: 4 | Val loss: 2.3417 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7811 | Steps: 4 | Val loss: 2.0710 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5264 | Steps: 4 | Val loss: 2.3146 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:08:55 (running for 00:15:48.02)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.578 |      0.361 |                   81 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.467 |      0.263 |                   61 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.175 |      0.154 |                   61 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.824 |      0.078 |                    8 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.30550373134328357
[2m[36m(func pid=168552)[0m top5: 0.8740671641791045
[2m[36m(func pid=168552)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=168552)[0m f1_macro: 0.26871808841431327
[2m[36m(func pid=168552)[0m f1_weighted: 0.28680702269542746
[2m[36m(func pid=168552)[0m f1_per_class: [0.206, 0.464, 0.157, 0.503, 0.062, 0.216, 0.0, 0.413, 0.219, 0.448]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.19402985074626866
[2m[36m(func pid=168563)[0m top5: 0.7957089552238806
[2m[36m(func pid=168563)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=168563)[0m f1_macro: 0.15443489190567247
[2m[36m(func pid=168563)[0m f1_weighted: 0.1452533329499212
[2m[36m(func pid=168563)[0m f1_per_class: [0.058, 0.444, 0.088, 0.007, 0.044, 0.286, 0.003, 0.465, 0.126, 0.024]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.32649253731343286
[2m[36m(func pid=163372)[0m top5: 0.8773320895522388
[2m[36m(func pid=163372)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=163372)[0m f1_macro: 0.3428181618478912
[2m[36m(func pid=163372)[0m f1_weighted: 0.3041923425509172
[2m[36m(func pid=163372)[0m f1_per_class: [0.25, 0.535, 0.686, 0.325, 0.074, 0.33, 0.12, 0.439, 0.224, 0.444]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.11893656716417911
[2m[36m(func pid=181384)[0m top5: 0.605410447761194
[2m[36m(func pid=181384)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=181384)[0m f1_macro: 0.08904601592984791
[2m[36m(func pid=181384)[0m f1_weighted: 0.04935979100660378
[2m[36m(func pid=181384)[0m f1_per_class: [0.061, 0.027, 0.333, 0.0, 0.089, 0.333, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0054 | Steps: 4 | Val loss: 1.9849 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3253 | Steps: 4 | Val loss: 2.8473 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.9375 | Steps: 4 | Val loss: 2.1525 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5766 | Steps: 4 | Val loss: 2.3131 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:09:01 (running for 00:15:53.42)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.781 |      0.343 |                   82 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  2.272 |      0.269 |                   62 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.005 |      0.165 |                   62 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.526 |      0.089 |                    9 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.25093283582089554
[2m[36m(func pid=168552)[0m top5: 0.875
[2m[36m(func pid=168552)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=168552)[0m f1_macro: 0.25140238088231237
[2m[36m(func pid=168552)[0m f1_weighted: 0.2842807253538985
[2m[36m(func pid=168552)[0m f1_per_class: [0.177, 0.312, 0.556, 0.43, 0.044, 0.232, 0.217, 0.0, 0.266, 0.28]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2042910447761194
[2m[36m(func pid=168563)[0m top5: 0.7971082089552238
[2m[36m(func pid=168563)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=168563)[0m f1_macro: 0.1650351809791619
[2m[36m(func pid=168563)[0m f1_weighted: 0.1593016755862585
[2m[36m(func pid=168563)[0m f1_per_class: [0.06, 0.417, 0.267, 0.003, 0.039, 0.312, 0.071, 0.458, 0.0, 0.024]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3283582089552239
[2m[36m(func pid=163372)[0m top5: 0.8726679104477612
[2m[36m(func pid=163372)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=163372)[0m f1_macro: 0.30922040043693305
[2m[36m(func pid=163372)[0m f1_weighted: 0.3346480938518084
[2m[36m(func pid=163372)[0m f1_per_class: [0.273, 0.528, 0.436, 0.335, 0.079, 0.3, 0.262, 0.295, 0.243, 0.34]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.11986940298507463
[2m[36m(func pid=181384)[0m top5: 0.6557835820895522
[2m[36m(func pid=181384)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=181384)[0m f1_macro: 0.08151197896563575
[2m[36m(func pid=181384)[0m f1_weighted: 0.05219457916374382
[2m[36m(func pid=181384)[0m f1_per_class: [0.061, 0.042, 0.224, 0.0, 0.077, 0.318, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8317 | Steps: 4 | Val loss: 2.7250 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2472 | Steps: 4 | Val loss: 2.0086 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6201 | Steps: 4 | Val loss: 2.1307 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4896 | Steps: 4 | Val loss: 2.3186 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:09:06 (running for 00:15:58.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.938 |      0.309 |                   83 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.325 |      0.251 |                   63 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.247 |      0.216 |                   63 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.577 |      0.082 |                   10 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.19169776119402984
[2m[36m(func pid=168563)[0m top5: 0.7975746268656716
[2m[36m(func pid=168563)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=168563)[0m f1_macro: 0.21620726441278576
[2m[36m(func pid=168563)[0m f1_weighted: 0.16046976854168543
[2m[36m(func pid=168563)[0m f1_per_class: [0.118, 0.34, 0.759, 0.007, 0.041, 0.329, 0.095, 0.46, 0.0, 0.014]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.2453358208955224
[2m[36m(func pid=168552)[0m top5: 0.8624067164179104
[2m[36m(func pid=168552)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=168552)[0m f1_macro: 0.16954806505097725
[2m[36m(func pid=168552)[0m f1_weighted: 0.2386085075288845
[2m[36m(func pid=168552)[0m f1_per_class: [0.176, 0.415, 0.0, 0.305, 0.073, 0.28, 0.132, 0.0, 0.143, 0.171]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3521455223880597
[2m[36m(func pid=163372)[0m top5: 0.8857276119402985
[2m[36m(func pid=163372)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=163372)[0m f1_macro: 0.3232011481326065
[2m[36m(func pid=163372)[0m f1_weighted: 0.3725928578017436
[2m[36m(func pid=163372)[0m f1_per_class: [0.341, 0.515, 0.511, 0.441, 0.067, 0.284, 0.309, 0.24, 0.269, 0.255]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.11707089552238806
[2m[36m(func pid=181384)[0m top5: 0.6940298507462687
[2m[36m(func pid=181384)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=181384)[0m f1_macro: 0.09120191387443977
[2m[36m(func pid=181384)[0m f1_weighted: 0.04907719847638231
[2m[36m(func pid=181384)[0m f1_per_class: [0.059, 0.032, 0.19, 0.0, 0.174, 0.302, 0.0, 0.078, 0.0, 0.077]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.0747 | Steps: 4 | Val loss: 2.0867 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6434 | Steps: 4 | Val loss: 2.4849 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.1548 | Steps: 4 | Val loss: 2.0127 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 13:09:11 (running for 00:16:04.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.62  |      0.323 |                   84 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.832 |      0.17  |                   64 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.075 |      0.147 |                   64 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.49  |      0.091 |                   11 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.14458955223880596
[2m[36m(func pid=168563)[0m top5: 0.7915111940298507
[2m[36m(func pid=168563)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=168563)[0m f1_macro: 0.14674722515788238
[2m[36m(func pid=168563)[0m f1_weighted: 0.1416767792758093
[2m[36m(func pid=168563)[0m f1_per_class: [0.079, 0.209, 0.235, 0.029, 0.038, 0.292, 0.116, 0.453, 0.0, 0.016]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.4970 | Steps: 4 | Val loss: 2.3190 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=168552)[0m top1: 0.27052238805970147
[2m[36m(func pid=168552)[0m top5: 0.835820895522388
[2m[36m(func pid=168552)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=168552)[0m f1_macro: 0.2232941552555936
[2m[36m(func pid=168552)[0m f1_weighted: 0.291348984778289
[2m[36m(func pid=168552)[0m f1_per_class: [0.178, 0.381, 0.143, 0.34, 0.084, 0.245, 0.229, 0.446, 0.065, 0.122]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m top1: 0.37406716417910446
[2m[36m(func pid=163372)[0m top5: 0.9006529850746269
[2m[36m(func pid=163372)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=163372)[0m f1_macro: 0.35572515541167343
[2m[36m(func pid=163372)[0m f1_weighted: 0.3723726517706527
[2m[36m(func pid=163372)[0m f1_per_class: [0.345, 0.529, 0.462, 0.513, 0.038, 0.317, 0.165, 0.503, 0.26, 0.425]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.11567164179104478
[2m[36m(func pid=181384)[0m top5: 0.7075559701492538
[2m[36m(func pid=181384)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=181384)[0m f1_macro: 0.09080714985363059
[2m[36m(func pid=181384)[0m f1_weighted: 0.05004880891065142
[2m[36m(func pid=181384)[0m f1_per_class: [0.057, 0.011, 0.14, 0.0, 0.15, 0.292, 0.003, 0.161, 0.019, 0.077]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2410 | Steps: 4 | Val loss: 2.1824 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9943 | Steps: 4 | Val loss: 3.0052 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.7715 | Steps: 4 | Val loss: 2.1414 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:09:17 (running for 00:16:09.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.155 |      0.356 |                   85 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.643 |      0.223 |                   65 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.241 |      0.125 |                   65 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.497 |      0.091 |                   12 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.12546641791044777
[2m[36m(func pid=168563)[0m top5: 0.7504664179104478
[2m[36m(func pid=168563)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=168563)[0m f1_macro: 0.12518436614789302
[2m[36m(func pid=168563)[0m f1_weighted: 0.14155028371844455
[2m[36m(func pid=168563)[0m f1_per_class: [0.059, 0.251, 0.124, 0.026, 0.024, 0.181, 0.143, 0.444, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.18889925373134328
[2m[36m(func pid=168552)[0m top5: 0.7649253731343284
[2m[36m(func pid=168552)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=168552)[0m f1_macro: 0.15543295574395394
[2m[36m(func pid=168552)[0m f1_weighted: 0.1922998665243633
[2m[36m(func pid=168552)[0m f1_per_class: [0.212, 0.285, 0.0, 0.374, 0.071, 0.071, 0.0, 0.388, 0.073, 0.079]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6769 | Steps: 4 | Val loss: 2.3228 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=163372)[0m top1: 0.3493470149253731
[2m[36m(func pid=163372)[0m top5: 0.8880597014925373
[2m[36m(func pid=163372)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=163372)[0m f1_macro: 0.28901618046820066
[2m[36m(func pid=163372)[0m f1_weighted: 0.34723358634392565
[2m[36m(func pid=163372)[0m f1_per_class: [0.286, 0.494, 0.14, 0.5, 0.023, 0.301, 0.149, 0.494, 0.128, 0.376]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.1226679104477612
[2m[36m(func pid=181384)[0m top5: 0.7084888059701493
[2m[36m(func pid=181384)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=181384)[0m f1_macro: 0.09927015755447684
[2m[36m(func pid=181384)[0m f1_weighted: 0.0761098705703137
[2m[36m(func pid=181384)[0m f1_per_class: [0.043, 0.073, 0.076, 0.0, 0.138, 0.294, 0.047, 0.187, 0.058, 0.077]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2546 | Steps: 4 | Val loss: 2.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8075 | Steps: 4 | Val loss: 2.3594 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.8504 | Steps: 4 | Val loss: 2.1230 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 13:09:22 (running for 00:16:14.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.771 |      0.289 |                   86 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.994 |      0.155 |                   66 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.255 |      0.115 |                   66 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.677 |      0.099 |                   13 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.14692164179104478
[2m[36m(func pid=168563)[0m top5: 0.7509328358208955
[2m[36m(func pid=168563)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=168563)[0m f1_macro: 0.11533750935756559
[2m[36m(func pid=168563)[0m f1_weighted: 0.16019932428862194
[2m[36m(func pid=168563)[0m f1_per_class: [0.073, 0.405, 0.113, 0.013, 0.028, 0.198, 0.18, 0.144, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=168552)[0m top1: 0.25046641791044777
[2m[36m(func pid=168552)[0m top5: 0.8470149253731343
[2m[36m(func pid=168552)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=168552)[0m f1_macro: 0.2313075581286424
[2m[36m(func pid=168552)[0m f1_weighted: 0.28478300979791193
[2m[36m(func pid=168552)[0m f1_per_class: [0.325, 0.369, 0.143, 0.337, 0.079, 0.172, 0.251, 0.312, 0.127, 0.196]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4657 | Steps: 4 | Val loss: 2.2330 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=163372)[0m top1: 0.37919776119402987
[2m[36m(func pid=163372)[0m top5: 0.8815298507462687
[2m[36m(func pid=163372)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=163372)[0m f1_macro: 0.34842806313789415
[2m[36m(func pid=163372)[0m f1_weighted: 0.3448679293944646
[2m[36m(func pid=163372)[0m f1_per_class: [0.387, 0.534, 0.611, 0.474, 0.011, 0.333, 0.12, 0.44, 0.146, 0.429]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.19962686567164178
[2m[36m(func pid=181384)[0m top5: 0.7122201492537313
[2m[36m(func pid=181384)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=181384)[0m f1_macro: 0.14022107969299138
[2m[36m(func pid=181384)[0m f1_weighted: 0.18481829477754608
[2m[36m(func pid=181384)[0m f1_per_class: [0.064, 0.165, 0.086, 0.0, 0.147, 0.334, 0.344, 0.169, 0.094, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6363 | Steps: 4 | Val loss: 2.0246 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4628 | Steps: 4 | Val loss: 2.1058 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.9594 | Steps: 4 | Val loss: 2.0285 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=168552)[0m top1: 0.3148320895522388
[2m[36m(func pid=168552)[0m top5: 0.8582089552238806
[2m[36m(func pid=168552)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=168552)[0m f1_macro: 0.2673052953052087
[2m[36m(func pid=168552)[0m f1_weighted: 0.34947065968886964
[2m[36m(func pid=168552)[0m f1_per_class: [0.198, 0.354, 0.211, 0.276, 0.083, 0.266, 0.496, 0.314, 0.173, 0.302]
== Status ==
Current time: 2024-01-07 13:09:28 (running for 00:16:20.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.85  |      0.348 |                   87 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.463 |      0.267 |                   68 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.255 |      0.115 |                   66 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.466 |      0.14  |                   14 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.20335820895522388
[2m[36m(func pid=168563)[0m top5: 0.8069029850746269
[2m[36m(func pid=168563)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=168563)[0m f1_macro: 0.176495952343996
[2m[36m(func pid=168563)[0m f1_weighted: 0.18796541895493404
[2m[36m(func pid=168563)[0m f1_per_class: [0.126, 0.455, 0.264, 0.003, 0.031, 0.274, 0.157, 0.456, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4351 | Steps: 4 | Val loss: 2.1817 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=163372)[0m top1: 0.34375
[2m[36m(func pid=163372)[0m top5: 0.8922574626865671
[2m[36m(func pid=163372)[0m f1_micro: 0.34375
[2m[36m(func pid=163372)[0m f1_macro: 0.2894310913134913
[2m[36m(func pid=163372)[0m f1_weighted: 0.316389569177517
[2m[36m(func pid=163372)[0m f1_per_class: [0.246, 0.553, 0.267, 0.374, 0.086, 0.312, 0.145, 0.327, 0.235, 0.349]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.261660447761194
[2m[36m(func pid=181384)[0m top5: 0.7243470149253731
[2m[36m(func pid=181384)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=181384)[0m f1_macro: 0.16901105008060577
[2m[36m(func pid=181384)[0m f1_weighted: 0.2591804753812355
[2m[36m(func pid=181384)[0m f1_per_class: [0.075, 0.275, 0.085, 0.0, 0.115, 0.337, 0.526, 0.189, 0.089, 0.0]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3029 | Steps: 4 | Val loss: 1.9138 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0911 | Steps: 4 | Val loss: 2.0558 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:09:33 (running for 00:16:25.68)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.959 |      0.289 |                   88 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.303 |      0.345 |                   69 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.636 |      0.176 |                   67 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.435 |      0.169 |                   15 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.39972014925373134
[2m[36m(func pid=168552)[0m top5: 0.8815298507462687
[2m[36m(func pid=168552)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=168552)[0m f1_macro: 0.34511408575469077
[2m[36m(func pid=168552)[0m f1_weighted: 0.4384973172651193
[2m[36m(func pid=168552)[0m f1_per_class: [0.189, 0.491, 0.485, 0.423, 0.107, 0.313, 0.535, 0.398, 0.195, 0.316]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.2725 | Steps: 4 | Val loss: 1.8991 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=168563)[0m top1: 0.20289179104477612
[2m[36m(func pid=168563)[0m top5: 0.789179104477612
[2m[36m(func pid=168563)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=168563)[0m f1_macro: 0.16003411493613742
[2m[36m(func pid=168563)[0m f1_weighted: 0.1748753031131571
[2m[36m(func pid=168563)[0m f1_per_class: [0.102, 0.466, 0.144, 0.016, 0.031, 0.247, 0.103, 0.472, 0.018, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7384 | Steps: 4 | Val loss: 2.1567 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=163372)[0m top1: 0.38199626865671643
[2m[36m(func pid=163372)[0m top5: 0.8885261194029851
[2m[36m(func pid=163372)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=163372)[0m f1_macro: 0.3070892667705853
[2m[36m(func pid=163372)[0m f1_weighted: 0.3952079902774963
[2m[36m(func pid=163372)[0m f1_per_class: [0.289, 0.525, 0.0, 0.367, 0.119, 0.352, 0.396, 0.449, 0.226, 0.348]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2868470149253731
[2m[36m(func pid=181384)[0m top5: 0.7691231343283582
[2m[36m(func pid=181384)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=181384)[0m f1_macro: 0.17581474717309042
[2m[36m(func pid=181384)[0m f1_weighted: 0.2769039444419762
[2m[36m(func pid=181384)[0m f1_per_class: [0.065, 0.341, 0.096, 0.0, 0.095, 0.31, 0.572, 0.09, 0.113, 0.077]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1095 | Steps: 4 | Val loss: 2.0564 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0272 | Steps: 4 | Val loss: 1.9991 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:09:38 (running for 00:16:31.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.273 |      0.307 |                   89 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.11  |      0.285 |                   70 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.091 |      0.16  |                   68 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.738 |      0.176 |                   16 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.3773320895522388
[2m[36m(func pid=168552)[0m top5: 0.882929104477612
[2m[36m(func pid=168552)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=168552)[0m f1_macro: 0.2845467403635483
[2m[36m(func pid=168552)[0m f1_weighted: 0.4125406826952029
[2m[36m(func pid=168552)[0m f1_per_class: [0.18, 0.525, 0.051, 0.41, 0.081, 0.308, 0.466, 0.344, 0.178, 0.303]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.21315298507462688
[2m[36m(func pid=168563)[0m top5: 0.7980410447761194
[2m[36m(func pid=168563)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=168563)[0m f1_macro: 0.17308880088262277
[2m[36m(func pid=168563)[0m f1_weighted: 0.18911393428427692
[2m[36m(func pid=168563)[0m f1_per_class: [0.115, 0.459, 0.375, 0.016, 0.032, 0.254, 0.19, 0.233, 0.059, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.7604 | Steps: 4 | Val loss: 2.0992 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3895 | Steps: 4 | Val loss: 2.0855 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
[2m[36m(func pid=163372)[0m top1: 0.3493470149253731
[2m[36m(func pid=163372)[0m top5: 0.8847947761194029
[2m[36m(func pid=163372)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=163372)[0m f1_macro: 0.31857448409503475
[2m[36m(func pid=163372)[0m f1_weighted: 0.3652938228407051
[2m[36m(func pid=163372)[0m f1_per_class: [0.46, 0.475, 0.5, 0.512, 0.101, 0.334, 0.254, 0.09, 0.145, 0.315]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2404 | Steps: 4 | Val loss: 2.8749 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=181384)[0m top1: 0.3148320895522388
[2m[36m(func pid=181384)[0m top5: 0.8306902985074627
[2m[36m(func pid=181384)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.18364106798238483
[2m[36m(func pid=181384)[0m f1_weighted: 0.28094445544555546
[2m[36m(func pid=181384)[0m f1_per_class: [0.151, 0.382, 0.158, 0.0, 0.085, 0.217, 0.595, 0.076, 0.1, 0.074]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3748 | Steps: 4 | Val loss: 1.9445 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:09:44 (running for 00:16:36.66)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.76  |      0.319 |                   90 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.24  |      0.213 |                   71 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.027 |      0.173 |                   69 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.39  |      0.184 |                   17 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.2439365671641791
[2m[36m(func pid=168552)[0m top5: 0.8027052238805971
[2m[36m(func pid=168552)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=168552)[0m f1_macro: 0.21315824308564962
[2m[36m(func pid=168552)[0m f1_weighted: 0.26470226590422563
[2m[36m(func pid=168552)[0m f1_per_class: [0.172, 0.23, 0.194, 0.431, 0.119, 0.212, 0.164, 0.358, 0.126, 0.125]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.2294776119402985
[2m[36m(func pid=168563)[0m top5: 0.8246268656716418
[2m[36m(func pid=168563)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=168563)[0m f1_macro: 0.22877090615399828
[2m[36m(func pid=168563)[0m f1_weighted: 0.20705931192977192
[2m[36m(func pid=168563)[0m f1_per_class: [0.124, 0.476, 0.733, 0.036, 0.036, 0.265, 0.183, 0.356, 0.08, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9329 | Steps: 4 | Val loss: 1.8808 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2846 | Steps: 4 | Val loss: 2.0186 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4115 | Steps: 4 | Val loss: 3.1482 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=163372)[0m top1: 0.4281716417910448
[2m[36m(func pid=163372)[0m top5: 0.8969216417910447
[2m[36m(func pid=163372)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=163372)[0m f1_macro: 0.3801077452961844
[2m[36m(func pid=163372)[0m f1_weighted: 0.44831775478581304
[2m[36m(func pid=163372)[0m f1_per_class: [0.433, 0.381, 0.564, 0.547, 0.1, 0.39, 0.463, 0.4, 0.248, 0.275]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3260261194029851
[2m[36m(func pid=181384)[0m top5: 0.8717350746268657
[2m[36m(func pid=181384)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=181384)[0m f1_macro: 0.20881698109985267
[2m[36m(func pid=181384)[0m f1_weighted: 0.27542611726938193
[2m[36m(func pid=181384)[0m f1_per_class: [0.191, 0.405, 0.289, 0.0, 0.077, 0.089, 0.597, 0.076, 0.129, 0.235]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.9879 | Steps: 4 | Val loss: 2.0048 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:09:49 (running for 00:16:42.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.933 |      0.38  |                   91 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.412 |      0.215 |                   72 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.375 |      0.229 |                   70 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.285 |      0.209 |                   18 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.23274253731343283
[2m[36m(func pid=168552)[0m top5: 0.8362873134328358
[2m[36m(func pid=168552)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=168552)[0m f1_macro: 0.2147914692732446
[2m[36m(func pid=168552)[0m f1_weighted: 0.25448390982629526
[2m[36m(func pid=168552)[0m f1_per_class: [0.171, 0.068, 0.273, 0.414, 0.078, 0.172, 0.229, 0.468, 0.164, 0.112]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.21828358208955223
[2m[36m(func pid=168563)[0m top5: 0.7943097014925373
[2m[36m(func pid=168563)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=168563)[0m f1_macro: 0.1975824609591431
[2m[36m(func pid=168563)[0m f1_weighted: 0.1837873723323768
[2m[36m(func pid=168563)[0m f1_per_class: [0.128, 0.461, 0.522, 0.038, 0.03, 0.105, 0.157, 0.456, 0.078, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.8672 | Steps: 4 | Val loss: 1.8891 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2741 | Steps: 4 | Val loss: 1.9717 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9612 | Steps: 4 | Val loss: 3.8868 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1433 | Steps: 4 | Val loss: 2.0189 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=163372)[0m top1: 0.3941231343283582
[2m[36m(func pid=163372)[0m top5: 0.9015858208955224
[2m[36m(func pid=163372)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=163372)[0m f1_macro: 0.33260657633004476
[2m[36m(func pid=163372)[0m f1_weighted: 0.428019678323256
[2m[36m(func pid=163372)[0m f1_per_class: [0.303, 0.365, 0.145, 0.498, 0.083, 0.39, 0.446, 0.507, 0.22, 0.368]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.332089552238806
[2m[36m(func pid=181384)[0m top5: 0.8941231343283582
[2m[36m(func pid=181384)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=181384)[0m f1_macro: 0.27036790190027166
[2m[36m(func pid=181384)[0m f1_weighted: 0.27940119051825085
[2m[36m(func pid=181384)[0m f1_per_class: [0.296, 0.416, 0.512, 0.0, 0.076, 0.024, 0.573, 0.256, 0.151, 0.4]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:09:55 (running for 00:16:47.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.867 |      0.333 |                   92 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  0.961 |      0.178 |                   73 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  1.988 |      0.198 |                   71 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.274 |      0.27  |                   19 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.19542910447761194
[2m[36m(func pid=168552)[0m top5: 0.8288246268656716
[2m[36m(func pid=168552)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=168552)[0m f1_macro: 0.17771922448976216
[2m[36m(func pid=168552)[0m f1_weighted: 0.23523623673460647
[2m[36m(func pid=168552)[0m f1_per_class: [0.182, 0.436, 0.097, 0.239, 0.056, 0.115, 0.185, 0.247, 0.166, 0.054]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.23973880597014927
[2m[36m(func pid=168563)[0m top5: 0.7845149253731343
[2m[36m(func pid=168563)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=168563)[0m f1_macro: 0.15971436624892948
[2m[36m(func pid=168563)[0m f1_weighted: 0.2322922006322697
[2m[36m(func pid=168563)[0m f1_per_class: [0.034, 0.443, 0.117, 0.007, 0.034, 0.0, 0.411, 0.478, 0.074, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.8502 | Steps: 4 | Val loss: 1.9467 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2761 | Steps: 4 | Val loss: 1.9426 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.4660 | Steps: 4 | Val loss: 2.3584 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.4749 | Steps: 4 | Val loss: 2.1211 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=181384)[0m top1: 0.31763059701492535
[2m[36m(func pid=181384)[0m top5: 0.8875932835820896
[2m[36m(func pid=181384)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=181384)[0m f1_macro: 0.2858180716113077
[2m[36m(func pid=181384)[0m f1_weighted: 0.2708159045984213
[2m[36m(func pid=181384)[0m f1_per_class: [0.288, 0.396, 0.667, 0.0, 0.066, 0.0, 0.543, 0.36, 0.146, 0.391]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3726679104477612
[2m[36m(func pid=163372)[0m top5: 0.9011194029850746
[2m[36m(func pid=163372)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=163372)[0m f1_macro: 0.31161969893649255
[2m[36m(func pid=163372)[0m f1_weighted: 0.40677334556853545
[2m[36m(func pid=163372)[0m f1_per_class: [0.28, 0.541, 0.1, 0.43, 0.096, 0.341, 0.376, 0.407, 0.267, 0.278]
[2m[36m(func pid=163372)[0m 
== Status ==
Current time: 2024-01-07 13:10:00 (running for 00:16:53.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.31575
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.85  |      0.312 |                   93 |
| train_52b21_00006 | RUNNING    | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.466 |      0.291 |                   74 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.143 |      0.16  |                   72 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.276 |      0.286 |                   20 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168552)[0m top1: 0.34654850746268656
[2m[36m(func pid=168552)[0m top5: 0.9015858208955224
[2m[36m(func pid=168552)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=168552)[0m f1_macro: 0.29123265098482054
[2m[36m(func pid=168552)[0m f1_weighted: 0.37426462138402317
[2m[36m(func pid=168552)[0m f1_per_class: [0.209, 0.435, 0.107, 0.205, 0.063, 0.383, 0.519, 0.504, 0.164, 0.324]
[2m[36m(func pid=168552)[0m 
[2m[36m(func pid=168563)[0m top1: 0.1837686567164179
[2m[36m(func pid=168563)[0m top5: 0.7569962686567164
[2m[36m(func pid=168563)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=168563)[0m f1_macro: 0.16395537895143286
[2m[36m(func pid=168563)[0m f1_weighted: 0.2031280501044621
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.247, 0.686, 0.0, 0.027, 0.0, 0.493, 0.124, 0.062, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.6963 | Steps: 4 | Val loss: 1.8898 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1473 | Steps: 4 | Val loss: 1.9169 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.7028 | Steps: 4 | Val loss: 2.1150 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=168552)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.1231 | Steps: 4 | Val loss: 2.7996 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181384)[0m top1: 0.300839552238806
[2m[36m(func pid=181384)[0m top5: 0.8922574626865671
[2m[36m(func pid=181384)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=181384)[0m f1_macro: 0.2845775413142752
[2m[36m(func pid=181384)[0m f1_weighted: 0.2574303786628392
[2m[36m(func pid=181384)[0m f1_per_class: [0.319, 0.388, 0.727, 0.007, 0.065, 0.008, 0.48, 0.44, 0.147, 0.265]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=163372)[0m top1: 0.3843283582089552
[2m[36m(func pid=163372)[0m top5: 0.9090485074626866
[2m[36m(func pid=163372)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=163372)[0m f1_macro: 0.2786311975972847
[2m[36m(func pid=163372)[0m f1_weighted: 0.3949312786278369
[2m[36m(func pid=163372)[0m f1_per_class: [0.338, 0.551, 0.216, 0.449, 0.059, 0.324, 0.39, 0.076, 0.188, 0.196]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=168552)[0m top1: 0.23833955223880596
[2m[36m(func pid=168552)[0m top5: 0.8535447761194029
[2m[36m(func pid=168552)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=168552)[0m f1_macro: 0.20493955923125845
[2m[36m(func pid=168552)[0m f1_weighted: 0.21452228554930225
[2m[36m(func pid=168552)[0m f1_per_class: [0.247, 0.368, 0.0, 0.283, 0.06, 0.326, 0.009, 0.351, 0.071, 0.333]
== Status ==
Current time: 2024-01-07 13:10:06 (running for 00:16:58.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.3125
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.696 |      0.279 |                   94 |
| train_52b21_00007 | RUNNING    | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.475 |      0.164 |                   73 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.147 |      0.285 |                   21 |
| train_52b21_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=168563)[0m top1: 0.1553171641791045
[2m[36m(func pid=168563)[0m top5: 0.7821828358208955
[2m[36m(func pid=168563)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=168563)[0m f1_macro: 0.17709792465229243
[2m[36m(func pid=168563)[0m f1_weighted: 0.1812228998807475
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.14, 0.632, 0.0, 0.023, 0.15, 0.367, 0.459, 0.0, 0.0]
[2m[36m(func pid=168563)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.7793 | Steps: 4 | Val loss: 1.7276 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2327 | Steps: 4 | Val loss: 1.9066 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=168563)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.4004 | Steps: 4 | Val loss: 2.2354 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=163372)[0m top1: 0.42490671641791045
[2m[36m(func pid=163372)[0m top5: 0.9216417910447762
[2m[36m(func pid=163372)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=163372)[0m f1_macro: 0.39743736477862374
[2m[36m(func pid=163372)[0m f1_weighted: 0.44058413813078623
[2m[36m(func pid=163372)[0m f1_per_class: [0.479, 0.544, 0.511, 0.465, 0.076, 0.37, 0.407, 0.514, 0.154, 0.455]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2751865671641791
[2m[36m(func pid=181384)[0m top5: 0.8885261194029851
[2m[36m(func pid=181384)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.24992174726235108
[2m[36m(func pid=181384)[0m f1_weighted: 0.2310771460705104
[2m[36m(func pid=181384)[0m f1_per_class: [0.299, 0.392, 0.632, 0.01, 0.069, 0.0, 0.401, 0.442, 0.115, 0.14]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=168563)[0m top1: 0.13899253731343283
[2m[36m(func pid=168563)[0m top5: 0.7779850746268657
[2m[36m(func pid=168563)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=168563)[0m f1_macro: 0.13810433555929866
[2m[36m(func pid=168563)[0m f1_weighted: 0.0997489217987241
[2m[36m(func pid=168563)[0m f1_per_class: [0.0, 0.241, 0.4, 0.0, 0.032, 0.263, 0.003, 0.423, 0.0, 0.019]
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.5194 | Steps: 4 | Val loss: 1.8417 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5742 | Steps: 4 | Val loss: 1.9036 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 13:10:11 (running for 00:17:04.05)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.30925
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.779 |      0.397 |                   95 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.233 |      0.25  |                   22 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=186833)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=186833)[0m Configuration completed!
[2m[36m(func pid=186833)[0m New optimizer parameters:
[2m[36m(func pid=186833)[0m SGD (
[2m[36m(func pid=186833)[0m Parameter Group 0
[2m[36m(func pid=186833)[0m     dampening: 0
[2m[36m(func pid=186833)[0m     differentiable: False
[2m[36m(func pid=186833)[0m     foreach: None
[2m[36m(func pid=186833)[0m     lr: 0.001
[2m[36m(func pid=186833)[0m     maximize: False
[2m[36m(func pid=186833)[0m     momentum: 0.99
[2m[36m(func pid=186833)[0m     nesterov: False
[2m[36m(func pid=186833)[0m     weight_decay: 0.0001
[2m[36m(func pid=186833)[0m )
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=163372)[0m top1: 0.38759328358208955
[2m[36m(func pid=163372)[0m top5: 0.9123134328358209
[2m[36m(func pid=163372)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=163372)[0m f1_macro: 0.3939906820879822
[2m[36m(func pid=163372)[0m f1_weighted: 0.4145822104952506
[2m[36m(func pid=163372)[0m f1_per_class: [0.355, 0.501, 0.828, 0.458, 0.063, 0.333, 0.38, 0.479, 0.115, 0.429]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2332089552238806
[2m[36m(func pid=181384)[0m top5: 0.886660447761194
[2m[36m(func pid=181384)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=181384)[0m f1_macro: 0.22438975544946516
[2m[36m(func pid=181384)[0m f1_weighted: 0.18362009429094558
[2m[36m(func pid=181384)[0m f1_per_class: [0.286, 0.409, 0.632, 0.048, 0.071, 0.0, 0.21, 0.403, 0.083, 0.103]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9118 | Steps: 4 | Val loss: 2.0562 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.2014 | Steps: 4 | Val loss: 1.8872 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0999 | Steps: 4 | Val loss: 2.3217 | Batch size: 32 | lr: 0.001 | Duration: 4.88s
[2m[36m(func pid=163372)[0m top1: 0.36986940298507465
[2m[36m(func pid=163372)[0m top5: 0.8950559701492538
[2m[36m(func pid=163372)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=163372)[0m f1_macro: 0.381331916418953
[2m[36m(func pid=163372)[0m f1_weighted: 0.4025550496258994
[2m[36m(func pid=163372)[0m f1_per_class: [0.3, 0.545, 0.88, 0.403, 0.058, 0.32, 0.385, 0.389, 0.208, 0.326]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=181384)[0m top1: 0.21828358208955223
[2m[36m(func pid=181384)[0m top5: 0.8824626865671642
[2m[36m(func pid=181384)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=181384)[0m f1_macro: 0.215154221974301
[2m[36m(func pid=181384)[0m f1_weighted: 0.16700818732757997
[2m[36m(func pid=181384)[0m f1_per_class: [0.286, 0.434, 0.632, 0.145, 0.071, 0.0, 0.054, 0.387, 0.069, 0.074]
[2m[36m(func pid=186833)[0m top1: 0.09794776119402986
[2m[36m(func pid=186833)[0m top5: 0.5517723880597015
[2m[36m(func pid=186833)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=186833)[0m f1_macro: 0.041760788611527266
[2m[36m(func pid=186833)[0m f1_weighted: 0.06520506944043121
[2m[36m(func pid=186833)[0m f1_per_class: [0.064, 0.005, 0.034, 0.202, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:10:21 (running for 00:17:13.62)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.912 |      0.381 |                   97 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.574 |      0.224 |                   23 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=187437)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=187437)[0m Configuration completed!
[2m[36m(func pid=187437)[0m New optimizer parameters:
[2m[36m(func pid=187437)[0m SGD (
[2m[36m(func pid=187437)[0m Parameter Group 0
[2m[36m(func pid=187437)[0m     dampening: 0
[2m[36m(func pid=187437)[0m     differentiable: False
[2m[36m(func pid=187437)[0m     foreach: None
[2m[36m(func pid=187437)[0m     lr: 0.01
[2m[36m(func pid=187437)[0m     maximize: False
[2m[36m(func pid=187437)[0m     momentum: 0.99
[2m[36m(func pid=187437)[0m     nesterov: False
[2m[36m(func pid=187437)[0m     weight_decay: 0.0001
[2m[36m(func pid=187437)[0m )
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.7968 | Steps: 4 | Val loss: 2.1971 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.2677 | Steps: 4 | Val loss: 1.8688 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7320 | Steps: 4 | Val loss: 2.2931 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 13:10:27 (running for 00:17:19.36)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.797 |      0.324 |                   98 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.201 |      0.215 |                   24 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  3.1   |      0.042 |                    1 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m top1: 0.3003731343283582
[2m[36m(func pid=163372)[0m top5: 0.882929104477612
[2m[36m(func pid=163372)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=163372)[0m f1_macro: 0.32423857146601187
[2m[36m(func pid=163372)[0m f1_weighted: 0.32563517887444754
[2m[36m(func pid=163372)[0m f1_per_class: [0.237, 0.399, 0.667, 0.433, 0.054, 0.263, 0.202, 0.443, 0.211, 0.333]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9102 | Steps: 4 | Val loss: 3.7884 | Batch size: 32 | lr: 0.01 | Duration: 4.81s
[2m[36m(func pid=186833)[0m top1: 0.006063432835820896
[2m[36m(func pid=186833)[0m top5: 0.6884328358208955
[2m[36m(func pid=186833)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=186833)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=186833)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=181384)[0m top1: 0.24440298507462688
[2m[36m(func pid=181384)[0m top5: 0.8736007462686567
[2m[36m(func pid=181384)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=181384)[0m f1_macro: 0.227363209232572
[2m[36m(func pid=181384)[0m f1_weighted: 0.204433732473388
[2m[36m(func pid=181384)[0m f1_per_class: [0.292, 0.464, 0.636, 0.296, 0.067, 0.008, 0.022, 0.391, 0.024, 0.074]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.9356 | Steps: 4 | Val loss: 2.2728 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=187437)[0m top1: 0.006063432835820896
[2m[36m(func pid=187437)[0m top5: 0.6268656716417911
[2m[36m(func pid=187437)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=187437)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=187437)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9257 | Steps: 4 | Val loss: 2.3055 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2284 | Steps: 4 | Val loss: 1.8470 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:10:32 (running for 00:17:24.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00005 | RUNNING    | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  0.936 |      0.275 |                   99 |
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.268 |      0.227 |                   25 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.732 |      0.001 |                    2 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.91  |      0.001 |                    1 |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=163372)[0m top1: 0.2593283582089552
[2m[36m(func pid=163372)[0m top5: 0.8596082089552238
[2m[36m(func pid=163372)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=163372)[0m f1_macro: 0.27532652071163194
[2m[36m(func pid=163372)[0m f1_weighted: 0.2607362159974077
[2m[36m(func pid=163372)[0m f1_per_class: [0.363, 0.381, 0.429, 0.385, 0.061, 0.227, 0.068, 0.4, 0.133, 0.306]
[2m[36m(func pid=163372)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.5232 | Steps: 4 | Val loss: 10.1710 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=181384)[0m top1: 0.27098880597014924
[2m[36m(func pid=181384)[0m top5: 0.878731343283582
[2m[36m(func pid=181384)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=181384)[0m f1_macro: 0.2582092255244074
[2m[36m(func pid=181384)[0m f1_weighted: 0.24840604309605635
[2m[36m(func pid=181384)[0m f1_per_class: [0.325, 0.455, 0.696, 0.426, 0.064, 0.068, 0.025, 0.383, 0.048, 0.093]
[2m[36m(func pid=186833)[0m top1: 0.10727611940298508
[2m[36m(func pid=186833)[0m top5: 0.5909514925373134
[2m[36m(func pid=186833)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=186833)[0m f1_macro: 0.07607500993108345
[2m[36m(func pid=186833)[0m f1_weighted: 0.1324556002031023
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.0, 0.016, 0.0, 0.146, 0.257, 0.342, 0.0, 0.0, 0.0]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=163372)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.0792 | Steps: 4 | Val loss: 2.0331 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=187437)[0m top1: 0.022388059701492536
[2m[36m(func pid=187437)[0m top5: 0.617070895522388
[2m[36m(func pid=187437)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=187437)[0m f1_macro: 0.006331261176002428
[2m[36m(func pid=187437)[0m f1_weighted: 0.004550496408964126
[2m[36m(func pid=187437)[0m f1_per_class: [0.042, 0.021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9928 | Steps: 4 | Val loss: 1.8408 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8418 | Steps: 4 | Val loss: 2.2453 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=163372)[0m top1: 0.3414179104477612
[2m[36m(func pid=163372)[0m top5: 0.8908582089552238
[2m[36m(func pid=163372)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=163372)[0m f1_macro: 0.3442036347012339
[2m[36m(func pid=163372)[0m f1_weighted: 0.36744742921334267
[2m[36m(func pid=163372)[0m f1_per_class: [0.381, 0.322, 0.558, 0.492, 0.057, 0.351, 0.291, 0.444, 0.196, 0.35]
== Status ==
Current time: 2024-01-07 13:10:38 (running for 00:17:30.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 3 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.228 |      0.258 |                   26 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.926 |      0.076 |                    3 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.523 |      0.006 |                    2 |
| train_52b21_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.2483 | Steps: 4 | Val loss: 5.8612 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=181384)[0m top1: 0.29384328358208955
[2m[36m(func pid=181384)[0m top5: 0.8708022388059702
[2m[36m(func pid=181384)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=181384)[0m f1_macro: 0.277322712524194
[2m[36m(func pid=181384)[0m f1_weighted: 0.27942167265736384
[2m[36m(func pid=181384)[0m f1_per_class: [0.351, 0.4, 0.667, 0.484, 0.07, 0.248, 0.036, 0.391, 0.026, 0.101]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.2621268656716418
[2m[36m(func pid=186833)[0m top5: 0.6399253731343284
[2m[36m(func pid=186833)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=186833)[0m f1_macro: 0.14536961347286756
[2m[36m(func pid=186833)[0m f1_weighted: 0.230186612699205
[2m[36m(func pid=186833)[0m f1_per_class: [0.096, 0.263, 0.247, 0.0, 0.131, 0.065, 0.572, 0.0, 0.08, 0.0]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m top1: 0.11753731343283583
[2m[36m(func pid=187437)[0m top5: 0.7066231343283582
[2m[36m(func pid=187437)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=187437)[0m f1_macro: 0.07007270743011536
[2m[36m(func pid=187437)[0m f1_weighted: 0.0751988943573662
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.245, 0.055, 0.003, 0.195, 0.0, 0.093, 0.0, 0.056, 0.053]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0347 | Steps: 4 | Val loss: 1.8388 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7311 | Steps: 4 | Val loss: 2.1065 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0927 | Steps: 4 | Val loss: 7.9316 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=181384)[0m top1: 0.2980410447761194
[2m[36m(func pid=181384)[0m top5: 0.8698694029850746
[2m[36m(func pid=181384)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=181384)[0m f1_macro: 0.2640078648946106
[2m[36m(func pid=181384)[0m f1_weighted: 0.2887066913519566
[2m[36m(func pid=181384)[0m f1_per_class: [0.35, 0.371, 0.478, 0.499, 0.08, 0.279, 0.059, 0.41, 0.026, 0.088]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.30783582089552236
[2m[36m(func pid=186833)[0m top5: 0.7308768656716418
[2m[36m(func pid=186833)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=186833)[0m f1_macro: 0.12824243958295034
[2m[36m(func pid=186833)[0m f1_weighted: 0.24205037051518893
[2m[36m(func pid=186833)[0m f1_per_class: [0.145, 0.361, 0.0, 0.0, 0.126, 0.0, 0.583, 0.0, 0.068, 0.0]
== Status ==
Current time: 2024-01-07 13:10:46 (running for 00:17:38.29)
Memory usage on this node: 23.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.035 |      0.264 |                   28 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.842 |      0.145 |                    4 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.248 |      0.07  |                    3 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=549)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=549)[0m Configuration completed!
[2m[36m(func pid=549)[0m New optimizer parameters:
[2m[36m(func pid=549)[0m SGD (
[2m[36m(func pid=549)[0m Parameter Group 0
[2m[36m(func pid=549)[0m     dampening: 0
[2m[36m(func pid=549)[0m     differentiable: False
[2m[36m(func pid=549)[0m     foreach: None
[2m[36m(func pid=549)[0m     lr: 0.1
[2m[36m(func pid=549)[0m     maximize: False
[2m[36m(func pid=549)[0m     momentum: 0.99
[2m[36m(func pid=549)[0m     nesterov: False
[2m[36m(func pid=549)[0m     weight_decay: 0.0001
[2m[36m(func pid=549)[0m )
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=187437)[0m top1: 0.06902985074626866
[2m[36m(func pid=187437)[0m top5: 0.6049440298507462
[2m[36m(func pid=187437)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=187437)[0m f1_macro: 0.03183363581577131
[2m[36m(func pid=187437)[0m f1_weighted: 0.0220561203988062
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.0, 0.044, 0.034, 0.0, 0.0, 0.0, 0.203, 0.0, 0.037]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.8802 | Steps: 4 | Val loss: 1.8264 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6385 | Steps: 4 | Val loss: 2.0015 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.6050 | Steps: 4 | Val loss: 22.8442 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.8433 | Steps: 4 | Val loss: 812.7524 | Batch size: 32 | lr: 0.1 | Duration: 4.49s
== Status ==
Current time: 2024-01-07 13:10:51 (running for 00:17:43.95)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.88  |      0.267 |                   29 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.731 |      0.128 |                    5 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.093 |      0.032 |                    4 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.32136194029850745
[2m[36m(func pid=181384)[0m top5: 0.8722014925373134
[2m[36m(func pid=181384)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=181384)[0m f1_macro: 0.26698308544578764
[2m[36m(func pid=181384)[0m f1_weighted: 0.32131503501693154
[2m[36m(func pid=181384)[0m f1_per_class: [0.293, 0.369, 0.429, 0.513, 0.087, 0.312, 0.148, 0.425, 0.0, 0.094]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.35027985074626866
[2m[36m(func pid=186833)[0m top5: 0.8493470149253731
[2m[36m(func pid=186833)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=186833)[0m f1_macro: 0.19710218445269895
[2m[36m(func pid=186833)[0m f1_weighted: 0.2759222873275512
[2m[36m(func pid=186833)[0m f1_per_class: [0.146, 0.468, 0.556, 0.017, 0.109, 0.0, 0.608, 0.0, 0.068, 0.0]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m top1: 0.07276119402985075
[2m[36m(func pid=187437)[0m top5: 0.6777052238805971
[2m[36m(func pid=187437)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=187437)[0m f1_macro: 0.04670960673796449
[2m[36m(func pid=187437)[0m f1_weighted: 0.027387411175294057
[2m[36m(func pid=187437)[0m f1_per_class: [0.02, 0.092, 0.043, 0.0, 0.143, 0.0, 0.0, 0.169, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.020522388059701493
[2m[36m(func pid=549)[0m top5: 0.45988805970149255
[2m[36m(func pid=549)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=549)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=549)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=549)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.0807 | Steps: 4 | Val loss: 1.8039 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.1576 | Steps: 4 | Val loss: 15.4389 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3532 | Steps: 4 | Val loss: 2.0163 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 9.6839 | Steps: 4 | Val loss: 3022086.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:10:57 (running for 00:17:49.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.081 |      0.271 |                   30 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.639 |      0.197 |                    6 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.605 |      0.047 |                    5 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.843 |      0.004 |                    1 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.33348880597014924
[2m[36m(func pid=181384)[0m top5: 0.8745335820895522
[2m[36m(func pid=181384)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=181384)[0m f1_macro: 0.2709040438356422
[2m[36m(func pid=181384)[0m f1_weighted: 0.3421488537743877
[2m[36m(func pid=181384)[0m f1_per_class: [0.274, 0.406, 0.407, 0.514, 0.093, 0.271, 0.21, 0.44, 0.0, 0.094]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.3694029850746269
[2m[36m(func pid=186833)[0m top5: 0.8502798507462687
[2m[36m(func pid=186833)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=186833)[0m f1_macro: 0.15848550876580053
[2m[36m(func pid=186833)[0m f1_weighted: 0.35884828736501434
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.462, 0.081, 0.384, 0.084, 0.0, 0.575, 0.0, 0.0, 0.0]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m top1: 0.04057835820895522
[2m[36m(func pid=187437)[0m top5: 0.5527052238805971
[2m[36m(func pid=187437)[0m f1_micro: 0.04057835820895522
[2m[36m(func pid=187437)[0m f1_macro: 0.05804436725827167
[2m[36m(func pid=187437)[0m f1_weighted: 0.010899666412198993
[2m[36m(func pid=187437)[0m f1_per_class: [0.034, 0.011, 0.24, 0.0, 0.191, 0.0, 0.0, 0.081, 0.023, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.020522388059701493
[2m[36m(func pid=549)[0m top5: 0.5237873134328358
[2m[36m(func pid=549)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=549)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=549)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=549)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0067 | Steps: 4 | Val loss: 1.7741 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2556 | Steps: 4 | Val loss: 2.1560 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.3573 | Steps: 4 | Val loss: 11.8975 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 14.7261 | Steps: 4 | Val loss: 831023.3125 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 13:11:02 (running for 00:17:55.13)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.007 |      0.273 |                   31 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.353 |      0.158 |                    7 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.158 |      0.058 |                    6 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  9.684 |      0.004 |                    2 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.3498134328358209
[2m[36m(func pid=181384)[0m top5: 0.8801305970149254
[2m[36m(func pid=181384)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=181384)[0m f1_macro: 0.27298489635941053
[2m[36m(func pid=181384)[0m f1_weighted: 0.3627127321096816
[2m[36m(func pid=181384)[0m f1_per_class: [0.302, 0.397, 0.333, 0.526, 0.103, 0.246, 0.282, 0.44, 0.0, 0.101]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.20289179104477612
[2m[36m(func pid=186833)[0m top5: 0.8456156716417911
[2m[36m(func pid=186833)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=186833)[0m f1_macro: 0.12561936674529733
[2m[36m(func pid=186833)[0m f1_weighted: 0.19596775517478332
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.405, 0.073, 0.195, 0.069, 0.0, 0.199, 0.172, 0.0, 0.143]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m top1: 0.034981343283582086
[2m[36m(func pid=187437)[0m top5: 0.5340485074626866
[2m[36m(func pid=187437)[0m f1_micro: 0.034981343283582086
[2m[36m(func pid=187437)[0m f1_macro: 0.033370569610153986
[2m[36m(func pid=187437)[0m f1_weighted: 0.033591941458933405
[2m[36m(func pid=187437)[0m f1_per_class: [0.025, 0.021, 0.0, 0.0, 0.125, 0.0, 0.089, 0.0, 0.046, 0.027]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.006063432835820896
[2m[36m(func pid=549)[0m top5: 0.5093283582089553
[2m[36m(func pid=549)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=549)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=549)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.7397 | Steps: 4 | Val loss: 1.7470 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.0461 | Steps: 4 | Val loss: 56.0991 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1711 | Steps: 4 | Val loss: 2.3030 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 7.5046 | Steps: 4 | Val loss: 49506.4414 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:11:08 (running for 00:18:00.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.74  |      0.282 |                   32 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.256 |      0.126 |                    8 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.357 |      0.033 |                    7 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 | 14.726 |      0.001 |                    3 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.38386194029850745
[2m[36m(func pid=181384)[0m top5: 0.8880597014925373
[2m[36m(func pid=181384)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=181384)[0m f1_macro: 0.28206158383588675
[2m[36m(func pid=181384)[0m f1_weighted: 0.40364465574938374
[2m[36m(func pid=181384)[0m f1_per_class: [0.275, 0.38, 0.293, 0.529, 0.105, 0.229, 0.43, 0.459, 0.0, 0.12]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.00792910447761194
[2m[36m(func pid=187437)[0m top5: 0.3087686567164179
[2m[36m(func pid=187437)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=187437)[0m f1_macro: 0.0037375305089827207
[2m[36m(func pid=187437)[0m f1_weighted: 0.0028218565251751783
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.015, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m top1: 0.20522388059701493
[2m[36m(func pid=186833)[0m top5: 0.8083022388059702
[2m[36m(func pid=186833)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=186833)[0m f1_macro: 0.12202182802530837
[2m[36m(func pid=186833)[0m f1_weighted: 0.1592712853528673
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.447, 0.077, 0.228, 0.083, 0.0, 0.0, 0.285, 0.0, 0.1]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.0065298507462686565
[2m[36m(func pid=549)[0m top5: 0.5097947761194029
[2m[36m(func pid=549)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=549)[0m f1_macro: 0.00174558007767306
[2m[36m(func pid=549)[0m f1_weighted: 0.0010009621625022272
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.005, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8424 | Steps: 4 | Val loss: 1.7376 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.3831 | Steps: 4 | Val loss: 143.7488 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2257 | Steps: 4 | Val loss: 2.6123 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 6.6862 | Steps: 4 | Val loss: 29479.3770 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 13:11:13 (running for 00:18:06.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.842 |      0.304 |                   33 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.171 |      0.122 |                    9 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.046 |      0.004 |                    8 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  7.505 |      0.002 |                    4 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.4155783582089552
[2m[36m(func pid=181384)[0m top5: 0.8871268656716418
[2m[36m(func pid=181384)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=181384)[0m f1_macro: 0.30410896235380513
[2m[36m(func pid=181384)[0m f1_weighted: 0.4448293349449178
[2m[36m(func pid=181384)[0m f1_per_class: [0.266, 0.43, 0.304, 0.533, 0.109, 0.227, 0.526, 0.52, 0.0, 0.127]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.018656716417910446
[2m[36m(func pid=187437)[0m top5: 0.7038246268656716
[2m[36m(func pid=187437)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=187437)[0m f1_macro: 0.035211624753990124
[2m[36m(func pid=187437)[0m f1_weighted: 0.021411104086192897
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.082, 0.013, 0.019, 0.211, 0.0, 0.0, 0.0, 0.0, 0.027]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m top1: 0.18703358208955223
[2m[36m(func pid=186833)[0m top5: 0.789179104477612
[2m[36m(func pid=186833)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=186833)[0m f1_macro: 0.14914167740038065
[2m[36m(func pid=186833)[0m f1_weighted: 0.17755155317849525
[2m[36m(func pid=186833)[0m f1_per_class: [0.082, 0.423, 0.071, 0.29, 0.245, 0.0, 0.0, 0.334, 0.0, 0.046]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.17210820895522388
[2m[36m(func pid=549)[0m top5: 0.574160447761194
[2m[36m(func pid=549)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=549)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=549)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8053 | Steps: 4 | Val loss: 1.6988 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.6955 | Steps: 4 | Val loss: 100.1943 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0327 | Steps: 4 | Val loss: 2.7277 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.8194 | Steps: 4 | Val loss: 4690.0269 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:11:19 (running for 00:18:11.83)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.805 |      0.318 |                   34 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.226 |      0.149 |                   10 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.383 |      0.035 |                    9 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  6.686 |      0.029 |                    5 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.42630597014925375
[2m[36m(func pid=181384)[0m top5: 0.8894589552238806
[2m[36m(func pid=181384)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=181384)[0m f1_macro: 0.3181731727187861
[2m[36m(func pid=181384)[0m f1_weighted: 0.4535796775697433
[2m[36m(func pid=181384)[0m f1_per_class: [0.259, 0.463, 0.324, 0.532, 0.091, 0.246, 0.521, 0.552, 0.0, 0.194]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.1515858208955224
[2m[36m(func pid=186833)[0m top5: 0.7742537313432836
[2m[36m(func pid=186833)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=186833)[0m f1_macro: 0.17038122742160783
[2m[36m(func pid=186833)[0m f1_weighted: 0.15461185825971482
[2m[36m(func pid=186833)[0m f1_per_class: [0.109, 0.331, 0.358, 0.237, 0.2, 0.008, 0.0, 0.422, 0.0, 0.04]
[2m[36m(func pid=187437)[0m top1: 0.021455223880597014
[2m[36m(func pid=187437)[0m top5: 0.6063432835820896
[2m[36m(func pid=187437)[0m f1_micro: 0.021455223880597014
[2m[36m(func pid=187437)[0m f1_macro: 0.0253410222390552
[2m[36m(func pid=187437)[0m f1_weighted: 0.0270485411367164
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.027, 0.014, 0.049, 0.138, 0.0, 0.026, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.23694029850746268
[2m[36m(func pid=549)[0m top5: 0.7238805970149254
[2m[36m(func pid=549)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=549)[0m f1_macro: 0.041440518859873696
[2m[36m(func pid=549)[0m f1_weighted: 0.11269768031323157
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.011, 0.404, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1791 | Steps: 4 | Val loss: 1.6882 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.6700 | Steps: 4 | Val loss: 609.0831 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1967 | Steps: 4 | Val loss: 2.5976 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0425 | Steps: 4 | Val loss: 2540.6472 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:11:25 (running for 00:18:17.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.179 |      0.319 |                   35 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.033 |      0.17  |                   11 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.695 |      0.025 |                   10 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.819 |      0.041 |                    6 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.408115671641791
[2m[36m(func pid=181384)[0m top5: 0.8861940298507462
[2m[36m(func pid=181384)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.31862272872498165
[2m[36m(func pid=181384)[0m f1_weighted: 0.42877087347205645
[2m[36m(func pid=181384)[0m f1_per_class: [0.298, 0.397, 0.444, 0.502, 0.076, 0.231, 0.512, 0.514, 0.0, 0.211]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.0065298507462686565
[2m[36m(func pid=187437)[0m top5: 0.3199626865671642
[2m[36m(func pid=187437)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=187437)[0m f1_macro: 0.0019588084504552285
[2m[36m(func pid=187437)[0m f1_weighted: 0.0001877875274911573
[2m[36m(func pid=187437)[0m f1_per_class: [0.005, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m top1: 0.16557835820895522
[2m[36m(func pid=186833)[0m top5: 0.7831156716417911
[2m[36m(func pid=186833)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=186833)[0m f1_macro: 0.22537374741672464
[2m[36m(func pid=186833)[0m f1_weighted: 0.18044379524198018
[2m[36m(func pid=186833)[0m f1_per_class: [0.108, 0.324, 0.667, 0.291, 0.323, 0.096, 0.0, 0.404, 0.0, 0.042]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.026119402985074626
[2m[36m(func pid=549)[0m top5: 0.6427238805970149
[2m[36m(func pid=549)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=549)[0m f1_macro: 0.008032481472835218
[2m[36m(func pid=549)[0m f1_weighted: 0.011683234587147208
[2m[36m(func pid=549)[0m f1_per_class: [0.041, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.6296 | Steps: 4 | Val loss: 1.6867 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.2437 | Steps: 4 | Val loss: 503.9095 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7903 | Steps: 4 | Val loss: 1.8182 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 5.5056 | Steps: 4 | Val loss: 1192.2855 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 13:11:30 (running for 00:18:22.83)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.179 |      0.319 |                   35 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.197 |      0.225 |                   12 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.244 |      0.023 |                   12 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.043 |      0.008 |                    7 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.0228544776119403
[2m[36m(func pid=187437)[0m top5: 0.37826492537313433
[2m[36m(func pid=187437)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=187437)[0m f1_macro: 0.02268509842410729
[2m[36m(func pid=187437)[0m f1_weighted: 0.0065514519395795945
[2m[36m(func pid=187437)[0m f1_per_class: [0.038, 0.0, 0.0, 0.016, 0.172, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.4141791044776119
[2m[36m(func pid=181384)[0m top5: 0.8931902985074627
[2m[36m(func pid=181384)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=181384)[0m f1_macro: 0.3237844807878378
[2m[36m(func pid=181384)[0m f1_weighted: 0.44506234243681947
[2m[36m(func pid=181384)[0m f1_per_class: [0.251, 0.439, 0.387, 0.494, 0.08, 0.284, 0.524, 0.564, 0.0, 0.215]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.3521455223880597
[2m[36m(func pid=186833)[0m top5: 0.8367537313432836
[2m[36m(func pid=186833)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=186833)[0m f1_macro: 0.32304560130648813
[2m[36m(func pid=186833)[0m f1_weighted: 0.36835362040821895
[2m[36m(func pid=186833)[0m f1_per_class: [0.164, 0.487, 0.667, 0.42, 0.235, 0.431, 0.29, 0.341, 0.024, 0.171]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.008395522388059701
[2m[36m(func pid=549)[0m top5: 0.5139925373134329
[2m[36m(func pid=549)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=549)[0m f1_macro: 0.007054349931463243
[2m[36m(func pid=549)[0m f1_weighted: 0.0021136439724210023
[2m[36m(func pid=549)[0m f1_per_class: [0.054, 0.0, 0.013, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.5941 | Steps: 4 | Val loss: 192.1503 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8672 | Steps: 4 | Val loss: 1.7006 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.1808 | Steps: 4 | Val loss: 1.8233 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.5353 | Steps: 4 | Val loss: 2471.4426 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=187437)[0m top1: 0.2733208955223881
[2m[36m(func pid=187437)[0m top5: 0.6152052238805971
[2m[36m(func pid=187437)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=187437)[0m f1_macro: 0.05554641181531953
[2m[36m(func pid=187437)[0m f1_weighted: 0.12612657839982494
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.0, 0.0, 0.443, 0.097, 0.016, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:11:36 (running for 00:18:28.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.63  |      0.324 |                   36 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.79  |      0.323 |                   13 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.594 |      0.056 |                   13 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  5.506 |      0.007 |                    8 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.4048507462686567
[2m[36m(func pid=181384)[0m top5: 0.8922574626865671
[2m[36m(func pid=181384)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=181384)[0m f1_macro: 0.30998282937760935
[2m[36m(func pid=181384)[0m f1_weighted: 0.43103007089620693
[2m[36m(func pid=181384)[0m f1_per_class: [0.249, 0.433, 0.338, 0.481, 0.082, 0.272, 0.505, 0.533, 0.0, 0.208]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.40718283582089554
[2m[36m(func pid=186833)[0m top5: 0.8456156716417911
[2m[36m(func pid=186833)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=186833)[0m f1_macro: 0.31330509893601166
[2m[36m(func pid=186833)[0m f1_weighted: 0.4042989654708147
[2m[36m(func pid=186833)[0m f1_per_class: [0.085, 0.507, 0.632, 0.299, 0.213, 0.335, 0.598, 0.038, 0.127, 0.3]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.006063432835820896
[2m[36m(func pid=549)[0m top5: 0.5629664179104478
[2m[36m(func pid=549)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=549)[0m f1_macro: 0.0012206572769953052
[2m[36m(func pid=549)[0m f1_weighted: 7.401373414617056e-05
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.2938 | Steps: 4 | Val loss: 79.1797 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0157 | Steps: 4 | Val loss: 2.0397 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.4867 | Steps: 4 | Val loss: 1.7091 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 4.6706 | Steps: 4 | Val loss: 1677.0093 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:11:41 (running for 00:18:33.82)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.867 |      0.31  |                   37 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.181 |      0.313 |                   14 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.594 |      0.056 |                   13 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.535 |      0.001 |                    9 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m top1: 0.3978544776119403
[2m[36m(func pid=186833)[0m top5: 0.8442164179104478
[2m[36m(func pid=186833)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=186833)[0m f1_macro: 0.27898673600691437
[2m[36m(func pid=186833)[0m f1_weighted: 0.35645700535717345
[2m[36m(func pid=186833)[0m f1_per_class: [0.0, 0.508, 0.625, 0.171, 0.219, 0.213, 0.613, 0.014, 0.132, 0.294]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m top1: 0.14272388059701493
[2m[36m(func pid=187437)[0m top5: 0.6333955223880597
[2m[36m(func pid=187437)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=187437)[0m f1_macro: 0.09039151088977895
[2m[36m(func pid=187437)[0m f1_weighted: 0.16921832934199063
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.308, 0.023, 0.352, 0.0, 0.145, 0.0, 0.011, 0.0, 0.065]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3969216417910448
[2m[36m(func pid=181384)[0m top5: 0.894589552238806
[2m[36m(func pid=181384)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=181384)[0m f1_macro: 0.3244851615079966
[2m[36m(func pid=181384)[0m f1_weighted: 0.42615334212438527
[2m[36m(func pid=181384)[0m f1_per_class: [0.256, 0.45, 0.393, 0.449, 0.076, 0.306, 0.489, 0.54, 0.0, 0.286]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.006063432835820896
[2m[36m(func pid=549)[0m top5: 0.5680970149253731
[2m[36m(func pid=549)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=549)[0m f1_macro: 0.0012464046021093001
[2m[36m(func pid=549)[0m f1_weighted: 7.557490591147808e-05
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6162 | Steps: 4 | Val loss: 2.4933 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.1739 | Steps: 4 | Val loss: 68.3213 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4728 | Steps: 4 | Val loss: 1.7288 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.0107 | Steps: 4 | Val loss: 228.0885 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:11:46 (running for 00:18:39.23)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.487 |      0.324 |                   38 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.016 |      0.279 |                   15 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.174 |      0.067 |                   15 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.671 |      0.001 |                   10 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.10960820895522388
[2m[36m(func pid=187437)[0m top5: 0.5881529850746269
[2m[36m(func pid=187437)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=187437)[0m f1_macro: 0.0674705289359909
[2m[36m(func pid=187437)[0m f1_weighted: 0.11227450547380657
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.452, 0.026, 0.083, 0.0, 0.032, 0.021, 0.011, 0.0, 0.05]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m top1: 0.24113805970149255
[2m[36m(func pid=186833)[0m top5: 0.7831156716417911
[2m[36m(func pid=186833)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=186833)[0m f1_macro: 0.21160845515443388
[2m[36m(func pid=186833)[0m f1_weighted: 0.25023503871281727
[2m[36m(func pid=186833)[0m f1_per_class: [0.085, 0.44, 0.25, 0.259, 0.22, 0.214, 0.186, 0.234, 0.064, 0.164]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m top1: 0.010727611940298507
[2m[36m(func pid=549)[0m top5: 0.35867537313432835
[2m[36m(func pid=549)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=549)[0m f1_macro: 0.005496131420584469
[2m[36m(func pid=549)[0m f1_weighted: 0.0014046300315514355
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.36800373134328357
[2m[36m(func pid=181384)[0m top5: 0.886660447761194
[2m[36m(func pid=181384)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=181384)[0m f1_macro: 0.3063731737143711
[2m[36m(func pid=181384)[0m f1_weighted: 0.3942097185023948
[2m[36m(func pid=181384)[0m f1_per_class: [0.24, 0.461, 0.381, 0.426, 0.076, 0.327, 0.402, 0.48, 0.0, 0.269]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.6210 | Steps: 4 | Val loss: 17.6144 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.9269 | Steps: 4 | Val loss: 3.8358 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 4.0373 | Steps: 4 | Val loss: 23.1925 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7345 | Steps: 4 | Val loss: 1.7792 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:11:52 (running for 00:18:44.62)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.473 |      0.306 |                   39 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.616 |      0.212 |                   16 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.174 |      0.067 |                   15 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.011 |      0.005 |                   11 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.18796641791044777
[2m[36m(func pid=187437)[0m top5: 0.5750932835820896
[2m[36m(func pid=187437)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=187437)[0m f1_macro: 0.09330946090733214
[2m[36m(func pid=187437)[0m f1_weighted: 0.1771879051482205
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.407, 0.0, 0.09, 0.018, 0.0, 0.26, 0.036, 0.028, 0.093]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.03871268656716418
[2m[36m(func pid=549)[0m top5: 0.49720149253731344
[2m[36m(func pid=549)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=549)[0m f1_macro: 0.02135792149055838
[2m[36m(func pid=549)[0m f1_weighted: 0.006296082872038847
[2m[36m(func pid=549)[0m f1_per_class: [0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.056, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.18097014925373134
[2m[36m(func pid=186833)[0m top5: 0.7658582089552238
[2m[36m(func pid=186833)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=186833)[0m f1_macro: 0.168462563043791
[2m[36m(func pid=186833)[0m f1_weighted: 0.1532872521825911
[2m[36m(func pid=186833)[0m f1_per_class: [0.286, 0.425, 0.13, 0.14, 0.176, 0.178, 0.0, 0.188, 0.0, 0.162]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3400186567164179
[2m[36m(func pid=181384)[0m top5: 0.8805970149253731
[2m[36m(func pid=181384)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=181384)[0m f1_macro: 0.27819010704102
[2m[36m(func pid=181384)[0m f1_weighted: 0.35769748525635736
[2m[36m(func pid=181384)[0m f1_per_class: [0.237, 0.505, 0.304, 0.419, 0.085, 0.294, 0.281, 0.452, 0.025, 0.18]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 4.0606 | Steps: 4 | Val loss: 7.7898 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.3914 | Steps: 4 | Val loss: 14.2381 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7362 | Steps: 4 | Val loss: 1.8882 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4739 | Steps: 4 | Val loss: 4.0785 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:11:57 (running for 00:18:50.08)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.734 |      0.278 |                   40 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.927 |      0.168 |                   17 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.621 |      0.093 |                   16 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.037 |      0.021 |                   12 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.20009328358208955
[2m[36m(func pid=187437)[0m top5: 0.7276119402985075
[2m[36m(func pid=187437)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=187437)[0m f1_macro: 0.1263356684109532
[2m[36m(func pid=187437)[0m f1_weighted: 0.2176749876101519
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.38, 0.0, 0.162, 0.042, 0.0, 0.312, 0.136, 0.15, 0.082]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.07136194029850747
[2m[36m(func pid=549)[0m top5: 0.7737873134328358
[2m[36m(func pid=549)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=549)[0m f1_macro: 0.03215974625406358
[2m[36m(func pid=549)[0m f1_weighted: 0.014652839890932905
[2m[36m(func pid=549)[0m f1_per_class: [0.146, 0.0, 0.0, 0.0, 0.0, 0.011, 0.006, 0.137, 0.022, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.30923507462686567
[2m[36m(func pid=181384)[0m top5: 0.8610074626865671
[2m[36m(func pid=181384)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=181384)[0m f1_macro: 0.25815445981263213
[2m[36m(func pid=181384)[0m f1_weighted: 0.32277717813163664
[2m[36m(func pid=181384)[0m f1_per_class: [0.232, 0.519, 0.289, 0.395, 0.086, 0.231, 0.201, 0.479, 0.022, 0.126]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.17863805970149255
[2m[36m(func pid=186833)[0m top5: 0.7467350746268657
[2m[36m(func pid=186833)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=186833)[0m f1_macro: 0.19480104897513026
[2m[36m(func pid=186833)[0m f1_weighted: 0.1529533865389309
[2m[36m(func pid=186833)[0m f1_per_class: [0.234, 0.418, 0.556, 0.159, 0.133, 0.122, 0.0, 0.2, 0.02, 0.107]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.6168 | Steps: 4 | Val loss: 9.5926 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7532 | Steps: 4 | Val loss: 19.0471 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5078 | Steps: 4 | Val loss: 1.9608 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5990 | Steps: 4 | Val loss: 4.0512 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:12:03 (running for 00:18:55.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.736 |      0.258 |                   41 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.474 |      0.195 |                   18 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.061 |      0.126 |                   17 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.391 |      0.032 |                   13 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.12313432835820895
[2m[36m(func pid=187437)[0m top5: 0.7621268656716418
[2m[36m(func pid=187437)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=187437)[0m f1_macro: 0.0824494626672431
[2m[36m(func pid=187437)[0m f1_weighted: 0.09451731065837739
[2m[36m(func pid=187437)[0m f1_per_class: [0.044, 0.227, 0.0, 0.124, 0.046, 0.0, 0.0, 0.277, 0.105, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.07975746268656717
[2m[36m(func pid=549)[0m top5: 0.8642723880597015
[2m[36m(func pid=549)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=549)[0m f1_macro: 0.0418556333607268
[2m[36m(func pid=549)[0m f1_weighted: 0.025535789585027925
[2m[36m(func pid=549)[0m f1_per_class: [0.067, 0.0, 0.077, 0.003, 0.0, 0.074, 0.012, 0.185, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.302705223880597
[2m[36m(func pid=181384)[0m top5: 0.8512126865671642
[2m[36m(func pid=181384)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=181384)[0m f1_macro: 0.2576214261484465
[2m[36m(func pid=181384)[0m f1_weighted: 0.3107320071310377
[2m[36m(func pid=181384)[0m f1_per_class: [0.21, 0.559, 0.255, 0.39, 0.064, 0.226, 0.134, 0.478, 0.133, 0.126]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.17630597014925373
[2m[36m(func pid=186833)[0m top5: 0.7719216417910447
[2m[36m(func pid=186833)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=186833)[0m f1_macro: 0.1401074155356298
[2m[36m(func pid=186833)[0m f1_weighted: 0.15193002822993562
[2m[36m(func pid=186833)[0m f1_per_class: [0.227, 0.378, 0.0, 0.153, 0.096, 0.223, 0.0, 0.206, 0.0, 0.118]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2038 | Steps: 4 | Val loss: 9.1409 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.7683 | Steps: 4 | Val loss: 57.3902 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0710 | Steps: 4 | Val loss: 2.0218 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5583 | Steps: 4 | Val loss: 3.0613 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 13:12:08 (running for 00:19:01.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.508 |      0.258 |                   42 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.599 |      0.14  |                   19 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.617 |      0.082 |                   18 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.753 |      0.042 |                   14 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.07276119402985075
[2m[36m(func pid=187437)[0m top5: 0.6291977611940298
[2m[36m(func pid=187437)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=187437)[0m f1_macro: 0.06389638119256527
[2m[36m(func pid=187437)[0m f1_weighted: 0.045937062928426996
[2m[36m(func pid=187437)[0m f1_per_class: [0.023, 0.114, 0.0, 0.01, 0.073, 0.0, 0.0, 0.351, 0.068, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.15205223880597016
[2m[36m(func pid=549)[0m top5: 0.8414179104477612
[2m[36m(func pid=549)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=549)[0m f1_macro: 0.08788656226219733
[2m[36m(func pid=549)[0m f1_weighted: 0.10570987703034129
[2m[36m(func pid=549)[0m f1_per_class: [0.031, 0.407, 0.056, 0.01, 0.0, 0.154, 0.006, 0.215, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.28777985074626866
[2m[36m(func pid=181384)[0m top5: 0.8381529850746269
[2m[36m(func pid=181384)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=181384)[0m f1_macro: 0.23721487782016415
[2m[36m(func pid=181384)[0m f1_weighted: 0.27810190912284083
[2m[36m(func pid=181384)[0m f1_per_class: [0.223, 0.573, 0.211, 0.372, 0.067, 0.202, 0.049, 0.461, 0.12, 0.094]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.22994402985074627
[2m[36m(func pid=186833)[0m top5: 0.8083022388059702
[2m[36m(func pid=186833)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=186833)[0m f1_macro: 0.17967197925193273
[2m[36m(func pid=186833)[0m f1_weighted: 0.2031768827585619
[2m[36m(func pid=186833)[0m f1_per_class: [0.216, 0.449, 0.0, 0.216, 0.081, 0.366, 0.0, 0.248, 0.085, 0.135]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 8.5743 | Steps: 4 | Val loss: 13.4412 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9172 | Steps: 4 | Val loss: 16.1650 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4568 | Steps: 4 | Val loss: 2.4560 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5562 | Steps: 4 | Val loss: 1.9104 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:12:14 (running for 00:19:06.47)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  2.071 |      0.237 |                   43 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.558 |      0.18  |                   20 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  8.574 |      0.101 |                   20 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.768 |      0.088 |                   15 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1142723880597015
[2m[36m(func pid=187437)[0m top5: 0.5335820895522388
[2m[36m(func pid=187437)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=187437)[0m f1_macro: 0.10068963449690624
[2m[36m(func pid=187437)[0m f1_weighted: 0.09472223886180758
[2m[36m(func pid=187437)[0m f1_per_class: [0.012, 0.185, 0.003, 0.01, 0.079, 0.34, 0.0, 0.319, 0.06, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.26492537313432835
[2m[36m(func pid=549)[0m top5: 0.8936567164179104
[2m[36m(func pid=549)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=549)[0m f1_macro: 0.13134479611972433
[2m[36m(func pid=549)[0m f1_weighted: 0.1959078332761169
[2m[36m(func pid=549)[0m f1_per_class: [0.094, 0.398, 0.119, 0.0, 0.0, 0.008, 0.348, 0.346, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.27705223880597013
[2m[36m(func pid=186833)[0m top5: 0.8316231343283582
[2m[36m(func pid=186833)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=186833)[0m f1_macro: 0.24101558516038496
[2m[36m(func pid=186833)[0m f1_weighted: 0.25895453822299186
[2m[36m(func pid=186833)[0m f1_per_class: [0.151, 0.494, 0.333, 0.276, 0.087, 0.35, 0.097, 0.282, 0.088, 0.252]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2933768656716418
[2m[36m(func pid=181384)[0m top5: 0.8558768656716418
[2m[36m(func pid=181384)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=181384)[0m f1_macro: 0.24386194013366946
[2m[36m(func pid=181384)[0m f1_weighted: 0.2584497355831847
[2m[36m(func pid=181384)[0m f1_per_class: [0.242, 0.559, 0.231, 0.329, 0.085, 0.255, 0.006, 0.449, 0.148, 0.135]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.6170 | Steps: 4 | Val loss: 9.6613 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.8356 | Steps: 4 | Val loss: 13.5829 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.4466 | Steps: 4 | Val loss: 1.8731 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.9016 | Steps: 4 | Val loss: 2.9110 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 13:12:19 (running for 00:19:11.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.556 |      0.244 |                   44 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.457 |      0.241 |                   21 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.617 |      0.119 |                   21 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.917 |      0.131 |                   16 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17583955223880596
[2m[36m(func pid=187437)[0m top5: 0.6040111940298507
[2m[36m(func pid=187437)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=187437)[0m f1_macro: 0.11869160286929811
[2m[36m(func pid=187437)[0m f1_weighted: 0.14691012253094002
[2m[36m(func pid=187437)[0m f1_per_class: [0.036, 0.477, 0.0, 0.07, 0.025, 0.312, 0.0, 0.118, 0.0, 0.148]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.35074626865671643
[2m[36m(func pid=549)[0m top5: 0.8927238805970149
[2m[36m(func pid=549)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=549)[0m f1_macro: 0.12021410837747651
[2m[36m(func pid=549)[0m f1_weighted: 0.23422459436266957
[2m[36m(func pid=549)[0m f1_per_class: [0.08, 0.403, 0.124, 0.0, 0.0, 0.0, 0.533, 0.062, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.26399253731343286
[2m[36m(func pid=186833)[0m top5: 0.8306902985074627
[2m[36m(func pid=186833)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=186833)[0m f1_macro: 0.21722287400938342
[2m[36m(func pid=186833)[0m f1_weighted: 0.26323126627515936
[2m[36m(func pid=186833)[0m f1_per_class: [0.154, 0.448, 0.137, 0.306, 0.134, 0.145, 0.193, 0.275, 0.067, 0.314]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.28591417910447764
[2m[36m(func pid=181384)[0m top5: 0.8619402985074627
[2m[36m(func pid=181384)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=181384)[0m f1_macro: 0.26494733278210686
[2m[36m(func pid=181384)[0m f1_weighted: 0.23985551826445514
[2m[36m(func pid=181384)[0m f1_per_class: [0.288, 0.515, 0.407, 0.268, 0.082, 0.299, 0.003, 0.424, 0.165, 0.199]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 6.6393 | Steps: 4 | Val loss: 6.4803 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6831 | Steps: 4 | Val loss: 7.1433 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.8919 | Steps: 4 | Val loss: 2.7567 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7102 | Steps: 4 | Val loss: 1.8779 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:12:25 (running for 00:19:17.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.447 |      0.265 |                   45 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.902 |      0.217 |                   22 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  6.639 |      0.104 |                   22 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.836 |      0.12  |                   17 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2042910447761194
[2m[36m(func pid=187437)[0m top5: 0.5666977611940298
[2m[36m(func pid=187437)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=187437)[0m f1_macro: 0.10377329192546583
[2m[36m(func pid=187437)[0m f1_weighted: 0.1645925535366645
[2m[36m(func pid=187437)[0m f1_per_class: [0.075, 0.456, 0.0, 0.192, 0.0, 0.267, 0.0, 0.0, 0.0, 0.048]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.19869402985074627
[2m[36m(func pid=549)[0m top5: 0.8931902985074627
[2m[36m(func pid=549)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=549)[0m f1_macro: 0.10845965776360889
[2m[36m(func pid=549)[0m f1_weighted: 0.16112259719946037
[2m[36m(func pid=549)[0m f1_per_class: [0.095, 0.016, 0.081, 0.0, 0.022, 0.0, 0.441, 0.419, 0.0, 0.011]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2835820895522388
[2m[36m(func pid=181384)[0m top5: 0.8624067164179104
[2m[36m(func pid=181384)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.26269737575506336
[2m[36m(func pid=181384)[0m f1_weighted: 0.22704340755462518
[2m[36m(func pid=181384)[0m f1_per_class: [0.29, 0.511, 0.364, 0.216, 0.078, 0.313, 0.006, 0.408, 0.169, 0.272]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.3558768656716418
[2m[36m(func pid=186833)[0m top5: 0.8292910447761194
[2m[36m(func pid=186833)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=186833)[0m f1_macro: 0.25890723886564665
[2m[36m(func pid=186833)[0m f1_weighted: 0.36342063482733955
[2m[36m(func pid=186833)[0m f1_per_class: [0.095, 0.421, 0.115, 0.321, 0.134, 0.103, 0.505, 0.492, 0.106, 0.296]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.7060 | Steps: 4 | Val loss: 4.9923 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7331 | Steps: 4 | Val loss: 8.2364 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.8453 | Steps: 4 | Val loss: 1.9133 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.6752 | Steps: 4 | Val loss: 2.8485 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:12:30 (running for 00:19:22.60)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.71  |      0.263 |                   46 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.892 |      0.259 |                   23 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.706 |      0.098 |                   23 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.683 |      0.108 |                   18 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1791044776119403
[2m[36m(func pid=187437)[0m top5: 0.5904850746268657
[2m[36m(func pid=187437)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=187437)[0m f1_macro: 0.09818214213494322
[2m[36m(func pid=187437)[0m f1_weighted: 0.14353948201634043
[2m[36m(func pid=187437)[0m f1_per_class: [0.082, 0.422, 0.0, 0.158, 0.143, 0.0, 0.072, 0.0, 0.066, 0.039]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.24860074626865672
[2m[36m(func pid=549)[0m top5: 0.8442164179104478
[2m[36m(func pid=549)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=549)[0m f1_macro: 0.10372182076650541
[2m[36m(func pid=549)[0m f1_weighted: 0.16619858604895008
[2m[36m(func pid=549)[0m f1_per_class: [0.218, 0.005, 0.0, 0.52, 0.0, 0.0, 0.0, 0.266, 0.0, 0.028]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.27425373134328357
[2m[36m(func pid=181384)[0m top5: 0.8563432835820896
[2m[36m(func pid=181384)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=181384)[0m f1_macro: 0.251008007330028
[2m[36m(func pid=181384)[0m f1_weighted: 0.21342877884188008
[2m[36m(func pid=181384)[0m f1_per_class: [0.25, 0.484, 0.32, 0.179, 0.079, 0.326, 0.009, 0.401, 0.184, 0.278]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.29197761194029853
[2m[36m(func pid=186833)[0m top5: 0.8283582089552238
[2m[36m(func pid=186833)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=186833)[0m f1_macro: 0.2378631762568574
[2m[36m(func pid=186833)[0m f1_weighted: 0.29774866154606977
[2m[36m(func pid=186833)[0m f1_per_class: [0.277, 0.217, 0.161, 0.44, 0.117, 0.069, 0.314, 0.355, 0.14, 0.288]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.6309 | Steps: 4 | Val loss: 5.2260 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.4597 | Steps: 4 | Val loss: 6.6447 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.0929 | Steps: 4 | Val loss: 3.2728 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6249 | Steps: 4 | Val loss: 1.9113 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:12:35 (running for 00:19:27.93)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.845 |      0.251 |                   47 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.675 |      0.238 |                   24 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.631 |      0.115 |                   24 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.733 |      0.104 |                   19 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.18703358208955223
[2m[36m(func pid=187437)[0m top5: 0.5872201492537313
[2m[36m(func pid=187437)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=187437)[0m f1_macro: 0.11476224824874222
[2m[36m(func pid=187437)[0m f1_weighted: 0.15529772487129545
[2m[36m(func pid=187437)[0m f1_per_class: [0.056, 0.424, 0.19, 0.184, 0.141, 0.0, 0.088, 0.0, 0.024, 0.039]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.2667910447761194
[2m[36m(func pid=549)[0m top5: 0.8292910447761194
[2m[36m(func pid=549)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=549)[0m f1_macro: 0.10072478687114403
[2m[36m(func pid=549)[0m f1_weighted: 0.16952906701901596
[2m[36m(func pid=549)[0m f1_per_class: [0.123, 0.005, 0.062, 0.542, 0.0, 0.0, 0.0, 0.247, 0.0, 0.028]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.20522388059701493
[2m[36m(func pid=186833)[0m top5: 0.7985074626865671
[2m[36m(func pid=186833)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=186833)[0m f1_macro: 0.1762432917624876
[2m[36m(func pid=186833)[0m f1_weighted: 0.18822685461517616
[2m[36m(func pid=186833)[0m f1_per_class: [0.154, 0.143, 0.186, 0.349, 0.077, 0.037, 0.106, 0.348, 0.06, 0.304]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.27425373134328357
[2m[36m(func pid=181384)[0m top5: 0.8638059701492538
[2m[36m(func pid=181384)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=181384)[0m f1_macro: 0.2578339407816811
[2m[36m(func pid=181384)[0m f1_weighted: 0.2223313468034448
[2m[36m(func pid=181384)[0m f1_per_class: [0.255, 0.476, 0.375, 0.214, 0.085, 0.321, 0.015, 0.374, 0.207, 0.256]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 5.2631 | Steps: 4 | Val loss: 6.7933 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9355 | Steps: 4 | Val loss: 6.4413 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5915 | Steps: 4 | Val loss: 4.4531 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1389 | Steps: 4 | Val loss: 1.9283 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 13:12:41 (running for 00:19:33.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.625 |      0.258 |                   48 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.093 |      0.176 |                   25 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  5.263 |      0.119 |                   25 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.46  |      0.101 |                   20 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2080223880597015
[2m[36m(func pid=187437)[0m top5: 0.5503731343283582
[2m[36m(func pid=187437)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=187437)[0m f1_macro: 0.11863932621095621
[2m[36m(func pid=187437)[0m f1_weighted: 0.19392842284995188
[2m[36m(func pid=187437)[0m f1_per_class: [0.058, 0.432, 0.19, 0.385, 0.048, 0.0, 0.031, 0.0, 0.0, 0.043]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06763059701492537
[2m[36m(func pid=549)[0m top5: 0.7803171641791045
[2m[36m(func pid=549)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=549)[0m f1_macro: 0.04666127088411451
[2m[36m(func pid=549)[0m f1_weighted: 0.018745307393612926
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.005, 0.164, 0.01, 0.021, 0.0, 0.0, 0.234, 0.0, 0.032]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2835820895522388
[2m[36m(func pid=181384)[0m top5: 0.8605410447761194
[2m[36m(func pid=181384)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.26132588910642496
[2m[36m(func pid=181384)[0m f1_weighted: 0.23632377144850938
[2m[36m(func pid=181384)[0m f1_per_class: [0.25, 0.498, 0.353, 0.236, 0.081, 0.305, 0.027, 0.423, 0.204, 0.236]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m top1: 0.18097014925373134
[2m[36m(func pid=186833)[0m top5: 0.7630597014925373
[2m[36m(func pid=186833)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=186833)[0m f1_macro: 0.1666615635536212
[2m[36m(func pid=186833)[0m f1_weighted: 0.19375224547885012
[2m[36m(func pid=186833)[0m f1_per_class: [0.103, 0.13, 0.0, 0.252, 0.086, 0.239, 0.134, 0.433, 0.125, 0.165]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.9656 | Steps: 4 | Val loss: 5.9805 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5595 | Steps: 4 | Val loss: 7.5959 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5778 | Steps: 4 | Val loss: 2.0138 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3459 | Steps: 4 | Val loss: 3.6696 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:12:46 (running for 00:19:38.66)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.139 |      0.261 |                   49 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.591 |      0.167 |                   26 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.966 |      0.147 |                   26 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.935 |      0.047 |                   21 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2593283582089552
[2m[36m(func pid=187437)[0m top5: 0.5382462686567164
[2m[36m(func pid=187437)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=187437)[0m f1_macro: 0.14700439807433643
[2m[36m(func pid=187437)[0m f1_weighted: 0.23889541448020202
[2m[36m(func pid=187437)[0m f1_per_class: [0.08, 0.39, 0.087, 0.474, 0.01, 0.0, 0.071, 0.261, 0.0, 0.097]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06669776119402986
[2m[36m(func pid=549)[0m top5: 0.7094216417910447
[2m[36m(func pid=549)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=549)[0m f1_macro: 0.05674665227549715
[2m[36m(func pid=549)[0m f1_weighted: 0.015389841500717307
[2m[36m(func pid=549)[0m f1_per_class: [0.1, 0.0, 0.233, 0.0, 0.025, 0.0, 0.0, 0.201, 0.0, 0.008]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.1958955223880597
[2m[36m(func pid=186833)[0m top5: 0.824160447761194
[2m[36m(func pid=186833)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=186833)[0m f1_macro: 0.18697339533115934
[2m[36m(func pid=186833)[0m f1_weighted: 0.2032932313951663
[2m[36m(func pid=186833)[0m f1_per_class: [0.109, 0.25, 0.178, 0.271, 0.099, 0.178, 0.11, 0.384, 0.081, 0.209]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.2789179104477612
[2m[36m(func pid=181384)[0m top5: 0.8526119402985075
[2m[36m(func pid=181384)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=181384)[0m f1_macro: 0.25902846473315944
[2m[36m(func pid=181384)[0m f1_weighted: 0.24746215847216
[2m[36m(func pid=181384)[0m f1_per_class: [0.317, 0.501, 0.364, 0.292, 0.078, 0.258, 0.03, 0.434, 0.152, 0.165]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.3986 | Steps: 4 | Val loss: 3.8278 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4966 | Steps: 4 | Val loss: 8.3797 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6660 | Steps: 4 | Val loss: 2.9204 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.3881 | Steps: 4 | Val loss: 2.1580 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=187437)[0m top1: 0.28125
[2m[36m(func pid=187437)[0m top5: 0.7574626865671642
[2m[36m(func pid=187437)[0m f1_micro: 0.28125
[2m[36m(func pid=187437)[0m f1_macro: 0.19086739100543562
[2m[36m(func pid=187437)[0m f1_weighted: 0.2697184384426164
[2m[36m(func pid=187437)[0m f1_per_class: [0.1, 0.456, 0.24, 0.482, 0.011, 0.0, 0.089, 0.441, 0.0, 0.089]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:12:51 (running for 00:19:44.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.578 |      0.259 |                   50 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.346 |      0.187 |                   27 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.399 |      0.191 |                   27 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.56  |      0.057 |                   22 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=549)[0m top1: 0.06576492537313433
[2m[36m(func pid=549)[0m top5: 0.550839552238806
[2m[36m(func pid=549)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=549)[0m f1_macro: 0.03811280417312212
[2m[36m(func pid=549)[0m f1_weighted: 0.01419857329829452
[2m[36m(func pid=549)[0m f1_per_class: [0.152, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.185, 0.0, 0.014]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.259794776119403
[2m[36m(func pid=186833)[0m top5: 0.8484141791044776
[2m[36m(func pid=186833)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=186833)[0m f1_macro: 0.2544958938924759
[2m[36m(func pid=186833)[0m f1_weighted: 0.2674542010062404
[2m[36m(func pid=186833)[0m f1_per_class: [0.153, 0.41, 0.438, 0.269, 0.07, 0.206, 0.211, 0.423, 0.026, 0.34]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.28451492537313433
[2m[36m(func pid=181384)[0m top5: 0.8530783582089553
[2m[36m(func pid=181384)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=181384)[0m f1_macro: 0.27234185758587476
[2m[36m(func pid=181384)[0m f1_weighted: 0.28081045926869763
[2m[36m(func pid=181384)[0m f1_per_class: [0.326, 0.491, 0.444, 0.362, 0.084, 0.216, 0.095, 0.455, 0.137, 0.112]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.9190 | Steps: 4 | Val loss: 3.7943 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.9052 | Steps: 4 | Val loss: 7.8715 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5106 | Steps: 4 | Val loss: 3.1386 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:12:56 (running for 00:19:49.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.388 |      0.272 |                   51 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.666 |      0.254 |                   28 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.919 |      0.157 |                   28 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.497 |      0.038 |                   23 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3647 | Steps: 4 | Val loss: 2.5960 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=187437)[0m top1: 0.27005597014925375
[2m[36m(func pid=187437)[0m top5: 0.7285447761194029
[2m[36m(func pid=187437)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=187437)[0m f1_macro: 0.15677079104075495
[2m[36m(func pid=187437)[0m f1_weighted: 0.23798851093755927
[2m[36m(func pid=187437)[0m f1_per_class: [0.081, 0.444, 0.085, 0.461, 0.003, 0.0, 0.009, 0.485, 0.0, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.07555970149253731
[2m[36m(func pid=549)[0m top5: 0.48507462686567165
[2m[36m(func pid=549)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=549)[0m f1_macro: 0.03321343094727356
[2m[36m(func pid=549)[0m f1_weighted: 0.015003422136931932
[2m[36m(func pid=549)[0m f1_per_class: [0.054, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.088, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.3050373134328358
[2m[36m(func pid=186833)[0m top5: 0.8488805970149254
[2m[36m(func pid=186833)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=186833)[0m f1_macro: 0.27752758492106955
[2m[36m(func pid=186833)[0m f1_weighted: 0.2890517544875325
[2m[36m(func pid=186833)[0m f1_per_class: [0.168, 0.433, 0.522, 0.184, 0.168, 0.298, 0.306, 0.456, 0.052, 0.189]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.28638059701492535
[2m[36m(func pid=181384)[0m top5: 0.8213619402985075
[2m[36m(func pid=181384)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=181384)[0m f1_macro: 0.3093748484329587
[2m[36m(func pid=181384)[0m f1_weighted: 0.31237732060331636
[2m[36m(func pid=181384)[0m f1_per_class: [0.364, 0.487, 0.75, 0.465, 0.078, 0.127, 0.127, 0.498, 0.121, 0.076]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.6271 | Steps: 4 | Val loss: 3.6410 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5006 | Steps: 4 | Val loss: 4.3077 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6436 | Steps: 4 | Val loss: 6.7442 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4227 | Steps: 4 | Val loss: 2.8468 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=187437)[0m top1: 0.27705223880597013
[2m[36m(func pid=187437)[0m top5: 0.7798507462686567
[2m[36m(func pid=187437)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=187437)[0m f1_macro: 0.18779270799284947
[2m[36m(func pid=187437)[0m f1_weighted: 0.2419026807764757
[2m[36m(func pid=187437)[0m f1_per_class: [0.109, 0.464, 0.0, 0.352, 0.005, 0.283, 0.0, 0.447, 0.047, 0.171]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:13:02 (running for 00:19:54.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.365 |      0.309 |                   52 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.511 |      0.278 |                   29 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.627 |      0.188 |                   29 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.905 |      0.033 |                   24 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=549)[0m top1: 0.07789179104477612
[2m[36m(func pid=549)[0m top5: 0.49580223880597013
[2m[36m(func pid=549)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=549)[0m f1_macro: 0.06199839762148066
[2m[36m(func pid=549)[0m f1_weighted: 0.01788971630055517
[2m[36m(func pid=549)[0m f1_per_class: [0.038, 0.0, 0.286, 0.0, 0.0, 0.0, 0.0, 0.225, 0.071, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.18703358208955223
[2m[36m(func pid=186833)[0m top5: 0.7308768656716418
[2m[36m(func pid=186833)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=186833)[0m f1_macro: 0.21803791464827707
[2m[36m(func pid=186833)[0m f1_weighted: 0.16224930639151372
[2m[36m(func pid=186833)[0m f1_per_class: [0.136, 0.435, 0.609, 0.169, 0.359, 0.045, 0.015, 0.35, 0.019, 0.043]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.271455223880597
[2m[36m(func pid=181384)[0m top5: 0.8059701492537313
[2m[36m(func pid=181384)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=181384)[0m f1_macro: 0.2956786564113008
[2m[36m(func pid=181384)[0m f1_weighted: 0.30666389325587806
[2m[36m(func pid=181384)[0m f1_per_class: [0.339, 0.416, 0.769, 0.515, 0.069, 0.082, 0.134, 0.431, 0.135, 0.068]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7488 | Steps: 4 | Val loss: 3.1753 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5723 | Steps: 4 | Val loss: 3.6668 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.3488 | Steps: 4 | Val loss: 4.8014 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:13:07 (running for 00:20:00.15)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.423 |      0.296 |                   53 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.644 |      0.218 |                   30 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.749 |      0.153 |                   30 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.501 |      0.062 |                   25 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4678 | Steps: 4 | Val loss: 2.1415 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=187437)[0m top1: 0.2555970149253731
[2m[36m(func pid=187437)[0m top5: 0.7751865671641791
[2m[36m(func pid=187437)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=187437)[0m f1_macro: 0.15299685720224748
[2m[36m(func pid=187437)[0m f1_weighted: 0.1780092088992235
[2m[36m(func pid=187437)[0m f1_per_class: [0.038, 0.411, 0.0, 0.112, 0.021, 0.396, 0.0, 0.471, 0.081, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.2621268656716418
[2m[36m(func pid=549)[0m top5: 0.5046641791044776
[2m[36m(func pid=549)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=549)[0m f1_macro: 0.15780391977699929
[2m[36m(func pid=549)[0m f1_weighted: 0.19524573065866302
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.471, 0.0, 0.0, 0.0, 0.54, 0.539, 0.0, 0.029]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.21175373134328357
[2m[36m(func pid=186833)[0m top5: 0.7672574626865671
[2m[36m(func pid=186833)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=186833)[0m f1_macro: 0.21680284257544408
[2m[36m(func pid=186833)[0m f1_weighted: 0.22282009126190097
[2m[36m(func pid=186833)[0m f1_per_class: [0.136, 0.479, 0.344, 0.358, 0.311, 0.0, 0.024, 0.418, 0.033, 0.065]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3148320895522388
[2m[36m(func pid=181384)[0m top5: 0.8717350746268657
[2m[36m(func pid=181384)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.342892414468288
[2m[36m(func pid=181384)[0m f1_weighted: 0.33219186429677994
[2m[36m(func pid=181384)[0m f1_per_class: [0.408, 0.364, 0.815, 0.548, 0.059, 0.207, 0.133, 0.511, 0.252, 0.131]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.2192 | Steps: 4 | Val loss: 3.3946 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3850 | Steps: 4 | Val loss: 3.8007 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.5938 | Steps: 4 | Val loss: 4.2373 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=187437)[0m top1: 0.20942164179104478
[2m[36m(func pid=187437)[0m top5: 0.7667910447761194
[2m[36m(func pid=187437)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=187437)[0m f1_macro: 0.13142226109479047
[2m[36m(func pid=187437)[0m f1_weighted: 0.12683363490572874
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.325, 0.041, 0.013, 0.08, 0.349, 0.0, 0.398, 0.109, 0.0]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:13:13 (running for 00:20:05.45)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.468 |      0.343 |                   54 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.349 |      0.217 |                   31 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.219 |      0.131 |                   31 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.572 |      0.158 |                   26 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3518 | Steps: 4 | Val loss: 1.9493 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=549)[0m top1: 0.27238805970149255
[2m[36m(func pid=549)[0m top5: 0.503731343283582
[2m[36m(func pid=549)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=549)[0m f1_macro: 0.1412865166879167
[2m[36m(func pid=549)[0m f1_weighted: 0.19386304980886057
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.571, 0.0, 0.0, 0.067, 0.581, 0.161, 0.0, 0.033]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.1623134328358209
[2m[36m(func pid=186833)[0m top5: 0.7658582089552238
[2m[36m(func pid=186833)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=186833)[0m f1_macro: 0.1875453975910119
[2m[36m(func pid=186833)[0m f1_weighted: 0.17434771685097308
[2m[36m(func pid=186833)[0m f1_per_class: [0.125, 0.376, 0.112, 0.162, 0.246, 0.152, 0.047, 0.41, 0.062, 0.183]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3400186567164179
[2m[36m(func pid=181384)[0m top5: 0.8899253731343284
[2m[36m(func pid=181384)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=181384)[0m f1_macro: 0.3665619981583916
[2m[36m(func pid=181384)[0m f1_weighted: 0.3571801972689291
[2m[36m(func pid=181384)[0m f1_per_class: [0.422, 0.378, 0.846, 0.547, 0.057, 0.267, 0.185, 0.495, 0.254, 0.213]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5239 | Steps: 4 | Val loss: 3.4553 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6984 | Steps: 4 | Val loss: 4.7032 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.0030 | Steps: 4 | Val loss: 6.1568 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:13:18 (running for 00:20:10.87)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.352 |      0.367 |                   55 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.594 |      0.188 |                   32 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.524 |      0.108 |                   32 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.385 |      0.141 |                   27 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1935634328358209
[2m[36m(func pid=187437)[0m top5: 0.7663246268656716
[2m[36m(func pid=187437)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=187437)[0m f1_macro: 0.10832190448601202
[2m[36m(func pid=187437)[0m f1_weighted: 0.11229960966289483
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.309, 0.068, 0.01, 0.0, 0.325, 0.0, 0.262, 0.109, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3339 | Steps: 4 | Val loss: 1.9084 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=549)[0m top1: 0.11613805970149253
[2m[36m(func pid=549)[0m top5: 0.5032649253731343
[2m[36m(func pid=549)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=549)[0m f1_macro: 0.0964461789038745
[2m[36m(func pid=549)[0m f1_weighted: 0.07417272906665803
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.489, 0.0, 0.0, 0.061, 0.168, 0.247, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.11520522388059702
[2m[36m(func pid=186833)[0m top5: 0.7248134328358209
[2m[36m(func pid=186833)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=186833)[0m f1_macro: 0.12676997552813507
[2m[36m(func pid=186833)[0m f1_weighted: 0.1284626705673482
[2m[36m(func pid=186833)[0m f1_per_class: [0.109, 0.332, 0.083, 0.139, 0.188, 0.114, 0.018, 0.126, 0.037, 0.122]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3614738805970149
[2m[36m(func pid=181384)[0m top5: 0.8908582089552238
[2m[36m(func pid=181384)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=181384)[0m f1_macro: 0.3756199252855183
[2m[36m(func pid=181384)[0m f1_weighted: 0.3837879859461697
[2m[36m(func pid=181384)[0m f1_per_class: [0.437, 0.418, 0.733, 0.537, 0.06, 0.301, 0.245, 0.511, 0.25, 0.265]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2435 | Steps: 4 | Val loss: 3.0323 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4107 | Steps: 4 | Val loss: 4.2777 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8981 | Steps: 4 | Val loss: 5.1776 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:13:23 (running for 00:20:16.10)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.334 |      0.376 |                   56 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.003 |      0.127 |                   33 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.244 |      0.083 |                   33 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.698 |      0.096 |                   28 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.15578358208955223
[2m[36m(func pid=187437)[0m top5: 0.7574626865671642
[2m[36m(func pid=187437)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=187437)[0m f1_macro: 0.08310012384415924
[2m[36m(func pid=187437)[0m f1_weighted: 0.09264636712365311
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.332, 0.062, 0.0, 0.0, 0.208, 0.0, 0.15, 0.079, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5683 | Steps: 4 | Val loss: 1.9375 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=549)[0m top1: 0.0830223880597015
[2m[36m(func pid=549)[0m top5: 0.47901119402985076
[2m[36m(func pid=549)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=549)[0m f1_macro: 0.09844215824642152
[2m[36m(func pid=549)[0m f1_weighted: 0.029707933730852825
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.571, 0.0, 0.015, 0.092, 0.0, 0.222, 0.084, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.1837686567164179
[2m[36m(func pid=186833)[0m top5: 0.7406716417910447
[2m[36m(func pid=186833)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=186833)[0m f1_macro: 0.1889918373229213
[2m[36m(func pid=186833)[0m f1_weighted: 0.21503999405006935
[2m[36m(func pid=186833)[0m f1_per_class: [0.097, 0.254, 0.022, 0.184, 0.136, 0.271, 0.18, 0.475, 0.068, 0.202]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3670708955223881
[2m[36m(func pid=181384)[0m top5: 0.882929104477612
[2m[36m(func pid=181384)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=181384)[0m f1_macro: 0.35852787977255923
[2m[36m(func pid=181384)[0m f1_weighted: 0.3970912369710455
[2m[36m(func pid=181384)[0m f1_per_class: [0.396, 0.462, 0.55, 0.518, 0.067, 0.277, 0.291, 0.545, 0.26, 0.219]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 5.5217 | Steps: 4 | Val loss: 2.8412 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.5024 | Steps: 4 | Val loss: 4.1488 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2935 | Steps: 4 | Val loss: 4.7641 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:13:28 (running for 00:20:21.24)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.568 |      0.359 |                   57 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.898 |      0.189 |                   34 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  5.522 |      0.11  |                   34 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.411 |      0.098 |                   29 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17210820895522388
[2m[36m(func pid=187437)[0m top5: 0.7821828358208955
[2m[36m(func pid=187437)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=187437)[0m f1_macro: 0.10996306496033506
[2m[36m(func pid=187437)[0m f1_weighted: 0.12564581897572938
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.372, 0.074, 0.0, 0.0, 0.165, 0.068, 0.343, 0.059, 0.018]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.6966 | Steps: 4 | Val loss: 2.0068 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=549)[0m top1: 0.07555970149253731
[2m[36m(func pid=549)[0m top5: 0.47527985074626866
[2m[36m(func pid=549)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=549)[0m f1_macro: 0.07541103080041303
[2m[36m(func pid=549)[0m f1_weighted: 0.027280600382266176
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.333, 0.0, 0.015, 0.067, 0.0, 0.253, 0.086, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m top1: 0.20242537313432835
[2m[36m(func pid=186833)[0m top5: 0.7854477611940298
[2m[36m(func pid=186833)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=186833)[0m f1_macro: 0.16317094023739695
[2m[36m(func pid=186833)[0m f1_weighted: 0.2132798240243176
[2m[36m(func pid=186833)[0m f1_per_class: [0.116, 0.118, 0.0, 0.202, 0.089, 0.197, 0.282, 0.381, 0.093, 0.153]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 4.8990 | Steps: 4 | Val loss: 2.1253 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=181384)[0m top1: 0.34841417910447764
[2m[36m(func pid=181384)[0m top5: 0.8796641791044776
[2m[36m(func pid=181384)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=181384)[0m f1_macro: 0.3665568751307909
[2m[36m(func pid=181384)[0m f1_weighted: 0.3851940924409986
[2m[36m(func pid=181384)[0m f1_per_class: [0.442, 0.464, 0.667, 0.452, 0.061, 0.282, 0.31, 0.551, 0.19, 0.246]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.7584 | Steps: 4 | Val loss: 3.7628 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.3052 | Steps: 4 | Val loss: 5.7054 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 13:13:34 (running for 00:20:26.61)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.697 |      0.367 |                   58 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.294 |      0.163 |                   35 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.899 |      0.195 |                   35 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.502 |      0.075 |                   30 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.27705223880597013
[2m[36m(func pid=187437)[0m top5: 0.8171641791044776
[2m[36m(func pid=187437)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=187437)[0m f1_macro: 0.19472745248418655
[2m[36m(func pid=187437)[0m f1_weighted: 0.22974200002798803
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.411, 0.262, 0.0, 0.066, 0.171, 0.358, 0.451, 0.089, 0.139]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.07136194029850747
[2m[36m(func pid=549)[0m top5: 0.7565298507462687
[2m[36m(func pid=549)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=549)[0m f1_macro: 0.0727611702947904
[2m[36m(func pid=549)[0m f1_weighted: 0.02118473011674608
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.343, 0.0, 0.018, 0.0, 0.0, 0.276, 0.09, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0969 | Steps: 4 | Val loss: 1.8765 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186833)[0m top1: 0.2103544776119403
[2m[36m(func pid=186833)[0m top5: 0.7691231343283582
[2m[36m(func pid=186833)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=186833)[0m f1_macro: 0.17475212329674072
[2m[36m(func pid=186833)[0m f1_weighted: 0.24293219405933722
[2m[36m(func pid=186833)[0m f1_per_class: [0.125, 0.172, 0.0, 0.129, 0.055, 0.192, 0.404, 0.486, 0.085, 0.099]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.6187 | Steps: 4 | Val loss: 2.2268 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=181384)[0m top1: 0.376865671641791
[2m[36m(func pid=181384)[0m top5: 0.8889925373134329
[2m[36m(func pid=181384)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.3707403218807678
[2m[36m(func pid=181384)[0m f1_weighted: 0.40560611674244934
[2m[36m(func pid=181384)[0m f1_per_class: [0.392, 0.467, 0.55, 0.4, 0.073, 0.344, 0.4, 0.544, 0.262, 0.276]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.1748 | Steps: 4 | Val loss: 4.7981 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8034 | Steps: 4 | Val loss: 4.8233 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:13:39 (running for 00:20:32.03)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.097 |      0.371 |                   59 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.305 |      0.175 |                   36 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.619 |      0.145 |                   36 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.758 |      0.073 |                   31 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2234141791044776
[2m[36m(func pid=187437)[0m top5: 0.8111007462686567
[2m[36m(func pid=187437)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=187437)[0m f1_macro: 0.14482344892276114
[2m[36m(func pid=187437)[0m f1_weighted: 0.17467075772176477
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.368, 0.276, 0.0, 0.057, 0.0, 0.291, 0.369, 0.0, 0.087]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06856343283582089
[2m[36m(func pid=549)[0m top5: 0.7714552238805971
[2m[36m(func pid=549)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=549)[0m f1_macro: 0.058150839633265686
[2m[36m(func pid=549)[0m f1_weighted: 0.018836772762881614
[2m[36m(func pid=549)[0m f1_per_class: [0.038, 0.0, 0.2, 0.0, 0.017, 0.0, 0.0, 0.238, 0.089, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3802 | Steps: 4 | Val loss: 1.7887 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=186833)[0m top1: 0.22434701492537312
[2m[36m(func pid=186833)[0m top5: 0.7975746268656716
[2m[36m(func pid=186833)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=186833)[0m f1_macro: 0.18336248102157723
[2m[36m(func pid=186833)[0m f1_weighted: 0.24963586618099992
[2m[36m(func pid=186833)[0m f1_per_class: [0.163, 0.41, 0.0, 0.117, 0.023, 0.201, 0.303, 0.459, 0.06, 0.099]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.3726 | Steps: 4 | Val loss: 2.3565 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=181384)[0m top1: 0.4039179104477612
[2m[36m(func pid=181384)[0m top5: 0.8917910447761194
[2m[36m(func pid=181384)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=181384)[0m f1_macro: 0.3670606012983349
[2m[36m(func pid=181384)[0m f1_weighted: 0.42915322526128535
[2m[36m(func pid=181384)[0m f1_per_class: [0.396, 0.468, 0.375, 0.375, 0.077, 0.367, 0.495, 0.556, 0.224, 0.337]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.2440 | Steps: 4 | Val loss: 5.2657 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3384 | Steps: 4 | Val loss: 4.1035 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:13:45 (running for 00:20:37.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.38  |      0.367 |                   60 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.803 |      0.183 |                   37 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.373 |      0.126 |                   37 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.175 |      0.058 |                   32 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1865671641791045
[2m[36m(func pid=187437)[0m top5: 0.8101679104477612
[2m[36m(func pid=187437)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=187437)[0m f1_macro: 0.12633203064456242
[2m[36m(func pid=187437)[0m f1_weighted: 0.1620245742035507
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.285, 0.191, 0.01, 0.041, 0.008, 0.288, 0.366, 0.0, 0.074]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.16277985074626866
[2m[36m(func pid=549)[0m top5: 0.7924440298507462
[2m[36m(func pid=549)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=549)[0m f1_macro: 0.09374629573738516
[2m[36m(func pid=549)[0m f1_weighted: 0.09804352085304419
[2m[36m(func pid=549)[0m f1_per_class: [0.035, 0.436, 0.087, 0.007, 0.022, 0.037, 0.0, 0.228, 0.063, 0.024]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.0544 | Steps: 4 | Val loss: 1.7745 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=186833)[0m top1: 0.23600746268656717
[2m[36m(func pid=186833)[0m top5: 0.8255597014925373
[2m[36m(func pid=186833)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=186833)[0m f1_macro: 0.16854793224773706
[2m[36m(func pid=186833)[0m f1_weighted: 0.20446246285671107
[2m[36m(func pid=186833)[0m f1_per_class: [0.253, 0.504, 0.0, 0.05, 0.011, 0.14, 0.193, 0.361, 0.091, 0.082]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.1245 | Steps: 4 | Val loss: 3.0393 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=181384)[0m top1: 0.4048507462686567
[2m[36m(func pid=181384)[0m top5: 0.8903917910447762
[2m[36m(func pid=181384)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=181384)[0m f1_macro: 0.3502661832119414
[2m[36m(func pid=181384)[0m f1_weighted: 0.4301396769654276
[2m[36m(func pid=181384)[0m f1_per_class: [0.348, 0.474, 0.273, 0.38, 0.088, 0.367, 0.499, 0.551, 0.209, 0.314]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.0792 | Steps: 4 | Val loss: 4.1559 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:13:50 (running for 00:20:42.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.054 |      0.35  |                   61 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.338 |      0.169 |                   38 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.125 |      0.098 |                   38 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.244 |      0.094 |                   33 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3240 | Steps: 4 | Val loss: 4.2089 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=187437)[0m top1: 0.15904850746268656
[2m[36m(func pid=187437)[0m top5: 0.8064365671641791
[2m[36m(func pid=187437)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=187437)[0m f1_macro: 0.09817948086307354
[2m[36m(func pid=187437)[0m f1_weighted: 0.15056461622720202
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.215, 0.0, 0.05, 0.029, 0.007, 0.261, 0.345, 0.0, 0.075]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.18703358208955223
[2m[36m(func pid=549)[0m top5: 0.784981343283582
[2m[36m(func pid=549)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=549)[0m f1_macro: 0.1236520946092546
[2m[36m(func pid=549)[0m f1_weighted: 0.1283376088191338
[2m[36m(func pid=549)[0m f1_per_class: [0.039, 0.425, 0.387, 0.01, 0.028, 0.0, 0.121, 0.227, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3565 | Steps: 4 | Val loss: 1.8163 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=186833)[0m top1: 0.25466417910447764
[2m[36m(func pid=186833)[0m top5: 0.8386194029850746
[2m[36m(func pid=186833)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=186833)[0m f1_macro: 0.1833353748479171
[2m[36m(func pid=186833)[0m f1_weighted: 0.23630987227058559
[2m[36m(func pid=186833)[0m f1_per_class: [0.205, 0.524, 0.0, 0.062, 0.045, 0.14, 0.27, 0.411, 0.095, 0.082]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.7624 | Steps: 4 | Val loss: 3.1547 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=181384)[0m top1: 0.39225746268656714
[2m[36m(func pid=181384)[0m top5: 0.8871268656716418
[2m[36m(func pid=181384)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=181384)[0m f1_macro: 0.33370094092163793
[2m[36m(func pid=181384)[0m f1_weighted: 0.4223898286628383
[2m[36m(func pid=181384)[0m f1_per_class: [0.327, 0.505, 0.174, 0.374, 0.089, 0.348, 0.47, 0.562, 0.214, 0.275]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8487 | Steps: 4 | Val loss: 2.8057 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=187437)[0m top1: 0.13292910447761194
[2m[36m(func pid=187437)[0m top5: 0.7863805970149254
[2m[36m(func pid=187437)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=187437)[0m f1_macro: 0.08245932519630744
[2m[36m(func pid=187437)[0m f1_weighted: 0.12552742891053348
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.136, 0.0, 0.048, 0.025, 0.0, 0.232, 0.328, 0.0, 0.056]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:13:56 (running for 00:20:48.33)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.356 |      0.334 |                   62 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.324 |      0.183 |                   39 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.762 |      0.082 |                   39 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.079 |      0.124 |                   34 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3226 | Steps: 4 | Val loss: 3.7281 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=549)[0m top1: 0.2896455223880597
[2m[36m(func pid=549)[0m top5: 0.7966417910447762
[2m[36m(func pid=549)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=549)[0m f1_macro: 0.17163564025375289
[2m[36m(func pid=549)[0m f1_weighted: 0.22479544025477385
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.425, 0.457, 0.035, 0.033, 0.0, 0.394, 0.373, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0855 | Steps: 4 | Val loss: 1.8201 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186833)[0m top1: 0.26026119402985076
[2m[36m(func pid=186833)[0m top5: 0.8675373134328358
[2m[36m(func pid=186833)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=186833)[0m f1_macro: 0.21202358268518298
[2m[36m(func pid=186833)[0m f1_weighted: 0.3112463980193077
[2m[36m(func pid=186833)[0m f1_per_class: [0.247, 0.49, 0.0, 0.337, 0.069, 0.22, 0.273, 0.286, 0.089, 0.11]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.7303 | Steps: 4 | Val loss: 2.8356 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=181384)[0m top1: 0.3903917910447761
[2m[36m(func pid=181384)[0m top5: 0.8894589552238806
[2m[36m(func pid=181384)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=181384)[0m f1_macro: 0.33410261019725024
[2m[36m(func pid=181384)[0m f1_weighted: 0.42066260214714113
[2m[36m(func pid=181384)[0m f1_per_class: [0.327, 0.531, 0.171, 0.393, 0.092, 0.33, 0.441, 0.552, 0.198, 0.306]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.4391 | Steps: 4 | Val loss: 2.1872 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:14:01 (running for 00:20:53.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.085 |      0.334 |                   63 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.323 |      0.212 |                   40 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.73  |      0.103 |                   40 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.849 |      0.172 |                   35 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.3591 | Steps: 4 | Val loss: 3.0871 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187437)[0m top1: 0.1609141791044776
[2m[36m(func pid=187437)[0m top5: 0.75
[2m[36m(func pid=187437)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=187437)[0m f1_macro: 0.10254070929361972
[2m[36m(func pid=187437)[0m f1_weighted: 0.1647419554177221
[2m[36m(func pid=187437)[0m f1_per_class: [0.059, 0.126, 0.0, 0.138, 0.036, 0.0, 0.278, 0.337, 0.0, 0.051]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.43796641791044777
[2m[36m(func pid=549)[0m top5: 0.8101679104477612
[2m[36m(func pid=549)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=549)[0m f1_macro: 0.1853554747513499
[2m[36m(func pid=549)[0m f1_weighted: 0.32977395500698004
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.706, 0.546, 0.021, 0.0, 0.581, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.1242 | Steps: 4 | Val loss: 1.8858 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=186833)[0m top1: 0.240205223880597
[2m[36m(func pid=186833)[0m top5: 0.8586753731343284
[2m[36m(func pid=186833)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=186833)[0m f1_macro: 0.14819972716753926
[2m[36m(func pid=186833)[0m f1_weighted: 0.22465057681118783
[2m[36m(func pid=186833)[0m f1_per_class: [0.302, 0.056, 0.0, 0.534, 0.083, 0.117, 0.131, 0.046, 0.086, 0.128]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.1384 | Steps: 4 | Val loss: 2.7423 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=181384)[0m top1: 0.37593283582089554
[2m[36m(func pid=181384)[0m top5: 0.8777985074626866
[2m[36m(func pid=181384)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=181384)[0m f1_macro: 0.3190351404061616
[2m[36m(func pid=181384)[0m f1_weighted: 0.40539713557057927
[2m[36m(func pid=181384)[0m f1_per_class: [0.299, 0.561, 0.152, 0.389, 0.105, 0.286, 0.397, 0.555, 0.186, 0.259]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 5.5823 | Steps: 4 | Val loss: 3.3620 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3214 | Steps: 4 | Val loss: 2.2742 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:14:06 (running for 00:20:59.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.124 |      0.319 |                   64 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.359 |      0.148 |                   41 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.138 |      0.159 |                   41 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.439 |      0.185 |                   36 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.25093283582089554
[2m[36m(func pid=187437)[0m top5: 0.7583955223880597
[2m[36m(func pid=187437)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=187437)[0m f1_macro: 0.1588854168050256
[2m[36m(func pid=187437)[0m f1_weighted: 0.25930447817746627
[2m[36m(func pid=187437)[0m f1_per_class: [0.131, 0.138, 0.031, 0.345, 0.071, 0.0, 0.375, 0.402, 0.0, 0.097]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.44216417910447764
[2m[36m(func pid=549)[0m top5: 0.8036380597014925
[2m[36m(func pid=549)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=549)[0m f1_macro: 0.17572596925860343
[2m[36m(func pid=549)[0m f1_weighted: 0.3383125176749529
[2m[36m(func pid=549)[0m f1_per_class: [0.039, 0.0, 0.545, 0.555, 0.0, 0.0, 0.601, 0.0, 0.0, 0.016]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.1860 | Steps: 4 | Val loss: 1.9465 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=186833)[0m top1: 0.30830223880597013
[2m[36m(func pid=186833)[0m top5: 0.8899253731343284
[2m[36m(func pid=186833)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=186833)[0m f1_macro: 0.23791617093334866
[2m[36m(func pid=186833)[0m f1_weighted: 0.3066944000226088
[2m[36m(func pid=186833)[0m f1_per_class: [0.329, 0.036, 0.121, 0.531, 0.073, 0.26, 0.284, 0.392, 0.135, 0.217]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 3.1855 | Steps: 4 | Val loss: 2.2329 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=181384)[0m top1: 0.35634328358208955
[2m[36m(func pid=181384)[0m top5: 0.8843283582089553
[2m[36m(func pid=181384)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=181384)[0m f1_macro: 0.3073026257812581
[2m[36m(func pid=181384)[0m f1_weighted: 0.38676905910869747
[2m[36m(func pid=181384)[0m f1_per_class: [0.255, 0.551, 0.171, 0.397, 0.106, 0.282, 0.343, 0.522, 0.19, 0.255]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.3552 | Steps: 4 | Val loss: 3.4665 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=187437)[0m top1: 0.30223880597014924
[2m[36m(func pid=187437)[0m top5: 0.8092350746268657
[2m[36m(func pid=187437)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=187437)[0m f1_macro: 0.20341168538347917
[2m[36m(func pid=187437)[0m f1_weighted: 0.3018201307232746
[2m[36m(func pid=187437)[0m f1_per_class: [0.129, 0.254, 0.167, 0.32, 0.103, 0.0, 0.457, 0.458, 0.0, 0.145]
== Status ==
Current time: 2024-01-07 13:14:12 (running for 00:21:04.37)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.186 |      0.307 |                   65 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.321 |      0.238 |                   42 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.185 |      0.203 |                   42 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  5.582 |      0.176 |                   37 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=187437)[0m 

[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4969 | Steps: 4 | Val loss: 2.2194 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=549)[0m top1: 0.42117537313432835
[2m[36m(func pid=549)[0m top5: 0.816231343283582
[2m[36m(func pid=549)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=549)[0m f1_macro: 0.1579523838520916
[2m[36m(func pid=549)[0m f1_weighted: 0.3365115706811602
[2m[36m(func pid=549)[0m f1_per_class: [0.036, 0.0, 0.375, 0.559, 0.0, 0.0, 0.595, 0.0, 0.0, 0.014]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1679 | Steps: 4 | Val loss: 1.8224 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=186833)[0m top1: 0.2957089552238806
[2m[36m(func pid=186833)[0m top5: 0.9127798507462687
[2m[36m(func pid=186833)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=186833)[0m f1_macro: 0.27543097263489436
[2m[36m(func pid=186833)[0m f1_weighted: 0.3177907648366184
[2m[36m(func pid=186833)[0m f1_per_class: [0.228, 0.318, 0.216, 0.378, 0.078, 0.339, 0.256, 0.467, 0.157, 0.318]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4580 | Steps: 4 | Val loss: 2.4959 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=181384)[0m top1: 0.3675373134328358
[2m[36m(func pid=181384)[0m top5: 0.8927238805970149
[2m[36m(func pid=181384)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=181384)[0m f1_macro: 0.3109779682883262
[2m[36m(func pid=181384)[0m f1_weighted: 0.3925647886864099
[2m[36m(func pid=181384)[0m f1_per_class: [0.281, 0.564, 0.159, 0.396, 0.102, 0.319, 0.352, 0.47, 0.174, 0.294]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.6547 | Steps: 4 | Val loss: 2.3236 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:14:17 (running for 00:21:09.71)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.168 |      0.311 |                   66 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.497 |      0.275 |                   43 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.458 |      0.259 |                   43 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.355 |      0.158 |                   38 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.3474813432835821
[2m[36m(func pid=187437)[0m top5: 0.8152985074626866
[2m[36m(func pid=187437)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=187437)[0m f1_macro: 0.2587373218448976
[2m[36m(func pid=187437)[0m f1_weighted: 0.3540626132817963
[2m[36m(func pid=187437)[0m f1_per_class: [0.097, 0.442, 0.343, 0.221, 0.043, 0.168, 0.53, 0.55, 0.026, 0.167]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2229 | Steps: 4 | Val loss: 3.2864 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=549)[0m top1: 0.2947761194029851
[2m[36m(func pid=549)[0m top5: 0.7789179104477612
[2m[36m(func pid=549)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=549)[0m f1_macro: 0.14702540765516856
[2m[36m(func pid=549)[0m f1_weighted: 0.22877345887083445
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.434, 0.522, 0.0, 0.0, 0.0, 0.506, 0.0, 0.0, 0.008]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2437 | Steps: 4 | Val loss: 1.8661 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=186833)[0m top1: 0.24580223880597016
[2m[36m(func pid=186833)[0m top5: 0.878731343283582
[2m[36m(func pid=186833)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=186833)[0m f1_macro: 0.24392390516395288
[2m[36m(func pid=186833)[0m f1_weighted: 0.2557052325149282
[2m[36m(func pid=186833)[0m f1_per_class: [0.198, 0.509, 0.328, 0.062, 0.083, 0.209, 0.297, 0.444, 0.117, 0.192]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 4.2194 | Steps: 4 | Val loss: 2.9962 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.6439 | Steps: 4 | Val loss: 2.4949 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=181384)[0m top1: 0.35401119402985076
[2m[36m(func pid=181384)[0m top5: 0.8913246268656716
[2m[36m(func pid=181384)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=181384)[0m f1_macro: 0.3167133670083291
[2m[36m(func pid=181384)[0m f1_weighted: 0.3694780654509601
[2m[36m(func pid=181384)[0m f1_per_class: [0.31, 0.556, 0.286, 0.398, 0.099, 0.302, 0.282, 0.451, 0.167, 0.316]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:14:22 (running for 00:21:15.28)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.244 |      0.317 |                   67 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.223 |      0.244 |                   44 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.219 |      0.256 |                   44 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.655 |      0.147 |                   39 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.35261194029850745
[2m[36m(func pid=187437)[0m top5: 0.820429104477612
[2m[36m(func pid=187437)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=187437)[0m f1_macro: 0.25561756637294064
[2m[36m(func pid=187437)[0m f1_weighted: 0.3710167551964853
[2m[36m(func pid=187437)[0m f1_per_class: [0.098, 0.46, 0.25, 0.207, 0.0, 0.329, 0.533, 0.54, 0.062, 0.077]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.0009 | Steps: 4 | Val loss: 4.9682 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=549)[0m top1: 0.17863805970149255
[2m[36m(func pid=549)[0m top5: 0.6291977611940298
[2m[36m(func pid=549)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=549)[0m f1_macro: 0.11920618000253949
[2m[36m(func pid=549)[0m f1_weighted: 0.07037998141107808
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.374, 0.8, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0155 | Steps: 4 | Val loss: 1.8834 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=186833)[0m top1: 0.19402985074626866
[2m[36m(func pid=186833)[0m top5: 0.7681902985074627
[2m[36m(func pid=186833)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=186833)[0m f1_macro: 0.2013091647511105
[2m[36m(func pid=186833)[0m f1_weighted: 0.2014004917045821
[2m[36m(func pid=186833)[0m f1_per_class: [0.138, 0.482, 0.281, 0.023, 0.135, 0.149, 0.219, 0.341, 0.081, 0.165]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7873 | Steps: 4 | Val loss: 2.7091 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=181384)[0m top1: 0.3582089552238806
[2m[36m(func pid=181384)[0m top5: 0.8889925373134329
[2m[36m(func pid=181384)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=181384)[0m f1_macro: 0.3112478203675313
[2m[36m(func pid=181384)[0m f1_weighted: 0.3689511888674113
[2m[36m(func pid=181384)[0m f1_per_class: [0.257, 0.583, 0.263, 0.396, 0.123, 0.302, 0.27, 0.45, 0.182, 0.286]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.5181 | Steps: 4 | Val loss: 2.5143 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:14:28 (running for 00:21:20.62)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.016 |      0.311 |                   68 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  2.001 |      0.201 |                   45 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.787 |      0.256 |                   45 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.644 |      0.119 |                   40 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.3362873134328358
[2m[36m(func pid=187437)[0m top5: 0.8111007462686567
[2m[36m(func pid=187437)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=187437)[0m f1_macro: 0.25642499167196176
[2m[36m(func pid=187437)[0m f1_weighted: 0.3430737135566651
[2m[36m(func pid=187437)[0m f1_per_class: [0.105, 0.479, 0.338, 0.156, 0.0, 0.361, 0.467, 0.497, 0.083, 0.077]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3256 | Steps: 4 | Val loss: 5.2447 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=549)[0m top1: 0.2178171641791045
[2m[36m(func pid=549)[0m top5: 0.7709888059701493
[2m[36m(func pid=549)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=549)[0m f1_macro: 0.1461353708947694
[2m[36m(func pid=549)[0m f1_weighted: 0.10781254675868776
[2m[36m(func pid=549)[0m f1_per_class: [0.109, 0.43, 0.667, 0.0, 0.0, 0.238, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1698 | Steps: 4 | Val loss: 1.7243 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 4.5873 | Steps: 4 | Val loss: 2.9754 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=186833)[0m top1: 0.16977611940298507
[2m[36m(func pid=186833)[0m top5: 0.7439365671641791
[2m[36m(func pid=186833)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=186833)[0m f1_macro: 0.17250431691649226
[2m[36m(func pid=186833)[0m f1_weighted: 0.17139370036191348
[2m[36m(func pid=186833)[0m f1_per_class: [0.124, 0.428, 0.211, 0.02, 0.157, 0.108, 0.18, 0.292, 0.083, 0.121]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3736007462686567
[2m[36m(func pid=181384)[0m top5: 0.8964552238805971
[2m[36m(func pid=181384)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=181384)[0m f1_macro: 0.3207867677043485
[2m[36m(func pid=181384)[0m f1_weighted: 0.37705523637719135
[2m[36m(func pid=181384)[0m f1_per_class: [0.273, 0.578, 0.286, 0.412, 0.132, 0.348, 0.273, 0.398, 0.205, 0.304]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.5399 | Steps: 4 | Val loss: 2.8169 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:14:33 (running for 00:21:26.00)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.17  |      0.321 |                   69 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.326 |      0.173 |                   46 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.587 |      0.197 |                   46 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.518 |      0.146 |                   41 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2416044776119403
[2m[36m(func pid=187437)[0m top5: 0.7915111940298507
[2m[36m(func pid=187437)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=187437)[0m f1_macro: 0.19664769231485052
[2m[36m(func pid=187437)[0m f1_weighted: 0.21855668538398562
[2m[36m(func pid=187437)[0m f1_per_class: [0.102, 0.455, 0.32, 0.069, 0.0, 0.3, 0.179, 0.456, 0.085, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.3811 | Steps: 4 | Val loss: 4.2140 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=549)[0m top1: 0.10261194029850747
[2m[36m(func pid=549)[0m top5: 0.7686567164179104
[2m[36m(func pid=549)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=549)[0m f1_macro: 0.10488430423400705
[2m[36m(func pid=549)[0m f1_weighted: 0.037867710435449904
[2m[36m(func pid=549)[0m f1_per_class: [0.059, 0.0, 0.632, 0.0, 0.04, 0.281, 0.0, 0.0, 0.0, 0.038]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1788 | Steps: 4 | Val loss: 1.7931 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 4.2007 | Steps: 4 | Val loss: 2.5943 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=186833)[0m top1: 0.18796641791044777
[2m[36m(func pid=186833)[0m top5: 0.8036380597014925
[2m[36m(func pid=186833)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=186833)[0m f1_macro: 0.1771332507617999
[2m[36m(func pid=186833)[0m f1_weighted: 0.20300924738757314
[2m[36m(func pid=186833)[0m f1_per_class: [0.162, 0.354, 0.122, 0.075, 0.133, 0.13, 0.257, 0.359, 0.087, 0.092]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3460820895522388
[2m[36m(func pid=181384)[0m top5: 0.8964552238805971
[2m[36m(func pid=181384)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=181384)[0m f1_macro: 0.3020588050423691
[2m[36m(func pid=181384)[0m f1_weighted: 0.3278157304544538
[2m[36m(func pid=181384)[0m f1_per_class: [0.284, 0.579, 0.343, 0.382, 0.114, 0.273, 0.17, 0.347, 0.216, 0.311]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 5.1476 | Steps: 4 | Val loss: 3.5688 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=187437)[0m top1: 0.20569029850746268
[2m[36m(func pid=187437)[0m top5: 0.7793843283582089
[2m[36m(func pid=187437)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=187437)[0m f1_macro: 0.18513690592095383
[2m[36m(func pid=187437)[0m f1_weighted: 0.15826767082360027
[2m[36m(func pid=187437)[0m f1_per_class: [0.091, 0.441, 0.429, 0.02, 0.0, 0.281, 0.037, 0.444, 0.108, 0.0]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:14:39 (running for 00:21:31.32)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.179 |      0.302 |                   70 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.381 |      0.177 |                   47 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.201 |      0.185 |                   47 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  5.54  |      0.105 |                   42 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1686 | Steps: 4 | Val loss: 3.3573 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=549)[0m top1: 0.08675373134328358
[2m[36m(func pid=549)[0m top5: 0.5862873134328358
[2m[36m(func pid=549)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=549)[0m f1_macro: 0.10862573256075884
[2m[36m(func pid=549)[0m f1_weighted: 0.029669649605454426
[2m[36m(func pid=549)[0m f1_per_class: [0.043, 0.0, 0.667, 0.0, 0.0, 0.083, 0.0, 0.226, 0.068, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.4218 | Steps: 4 | Val loss: 1.8388 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 5.0096 | Steps: 4 | Val loss: 2.4687 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=186833)[0m top1: 0.2196828358208955
[2m[36m(func pid=186833)[0m top5: 0.8586753731343284
[2m[36m(func pid=186833)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=186833)[0m f1_macro: 0.19049012521136516
[2m[36m(func pid=186833)[0m f1_weighted: 0.24305875446965233
[2m[36m(func pid=186833)[0m f1_per_class: [0.16, 0.314, 0.086, 0.117, 0.136, 0.146, 0.361, 0.384, 0.115, 0.085]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3316231343283582
[2m[36m(func pid=181384)[0m top5: 0.8936567164179104
[2m[36m(func pid=181384)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=181384)[0m f1_macro: 0.27360441681361103
[2m[36m(func pid=181384)[0m f1_weighted: 0.3063616615229805
[2m[36m(func pid=181384)[0m f1_per_class: [0.267, 0.571, 0.301, 0.381, 0.106, 0.169, 0.15, 0.341, 0.221, 0.229]
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 4.2058 | Steps: 4 | Val loss: 5.7609 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:14:44 (running for 00:21:36.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.422 |      0.274 |                   71 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.169 |      0.19  |                   48 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  5.01  |      0.167 |                   48 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  5.148 |      0.109 |                   43 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.19076492537313433
[2m[36m(func pid=187437)[0m top5: 0.7705223880597015
[2m[36m(func pid=187437)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=187437)[0m f1_macro: 0.16731584355769297
[2m[36m(func pid=187437)[0m f1_weighted: 0.1415667527820689
[2m[36m(func pid=187437)[0m f1_per_class: [0.091, 0.435, 0.308, 0.0, 0.0, 0.279, 0.006, 0.441, 0.114, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.07649253731343283
[2m[36m(func pid=549)[0m top5: 0.5788246268656716
[2m[36m(func pid=549)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=549)[0m f1_macro: 0.0594721764633269
[2m[36m(func pid=549)[0m f1_weighted: 0.01658466437557707
[2m[36m(func pid=549)[0m f1_per_class: [0.036, 0.0, 0.286, 0.0, 0.0, 0.0, 0.0, 0.205, 0.067, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5852 | Steps: 4 | Val loss: 3.1177 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1947 | Steps: 4 | Val loss: 1.8446 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.1807 | Steps: 4 | Val loss: 2.9083 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=186833)[0m top1: 0.24253731343283583
[2m[36m(func pid=186833)[0m top5: 0.8614738805970149
[2m[36m(func pid=186833)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=186833)[0m f1_macro: 0.2080794830448788
[2m[36m(func pid=186833)[0m f1_weighted: 0.26376365185227024
[2m[36m(func pid=186833)[0m f1_per_class: [0.229, 0.346, 0.133, 0.2, 0.138, 0.105, 0.351, 0.354, 0.1, 0.125]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.2139 | Steps: 4 | Val loss: 7.3949 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=181384)[0m top1: 0.3381529850746269
[2m[36m(func pid=181384)[0m top5: 0.8885261194029851
[2m[36m(func pid=181384)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=181384)[0m f1_macro: 0.29087639900200396
[2m[36m(func pid=181384)[0m f1_weighted: 0.3164229592466414
[2m[36m(func pid=181384)[0m f1_per_class: [0.235, 0.565, 0.353, 0.357, 0.091, 0.212, 0.185, 0.365, 0.234, 0.311]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:14:49 (running for 00:21:42.14)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.195 |      0.291 |                   72 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.585 |      0.208 |                   49 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.181 |      0.15  |                   49 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  4.206 |      0.059 |                   44 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1865671641791045
[2m[36m(func pid=187437)[0m top5: 0.7541977611940298
[2m[36m(func pid=187437)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=187437)[0m f1_macro: 0.15028857200038614
[2m[36m(func pid=187437)[0m f1_weighted: 0.14327487811322143
[2m[36m(func pid=187437)[0m f1_per_class: [0.09, 0.443, 0.087, 0.006, 0.034, 0.284, 0.003, 0.44, 0.116, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.25046641791044777
[2m[36m(func pid=549)[0m top5: 0.4724813432835821
[2m[36m(func pid=549)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=549)[0m f1_macro: 0.0696550704641766
[2m[36m(func pid=549)[0m f1_weighted: 0.1802045043580756
[2m[36m(func pid=549)[0m f1_per_class: [0.033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.595, 0.0, 0.068, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0474 | Steps: 4 | Val loss: 3.0225 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0166 | Steps: 4 | Val loss: 1.8106 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.5898 | Steps: 4 | Val loss: 3.2134 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=186833)[0m top1: 0.302705223880597
[2m[36m(func pid=186833)[0m top5: 0.8642723880597015
[2m[36m(func pid=186833)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=186833)[0m f1_macro: 0.25492071562617935
[2m[36m(func pid=186833)[0m f1_weighted: 0.3298329651811623
[2m[36m(func pid=186833)[0m f1_per_class: [0.233, 0.328, 0.11, 0.307, 0.101, 0.227, 0.419, 0.378, 0.174, 0.272]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 7.0470 | Steps: 4 | Val loss: 5.9845 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=181384)[0m top1: 0.3628731343283582
[2m[36m(func pid=181384)[0m top5: 0.8964552238805971
[2m[36m(func pid=181384)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=181384)[0m f1_macro: 0.3177558382330284
[2m[36m(func pid=181384)[0m f1_weighted: 0.35663972257704946
[2m[36m(func pid=181384)[0m f1_per_class: [0.243, 0.567, 0.286, 0.381, 0.138, 0.308, 0.253, 0.384, 0.235, 0.382]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:14:55 (running for 00:21:47.40)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.017 |      0.318 |                   73 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.047 |      0.255 |                   50 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.59  |      0.149 |                   50 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.214 |      0.07  |                   45 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17490671641791045
[2m[36m(func pid=187437)[0m top5: 0.7332089552238806
[2m[36m(func pid=187437)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=187437)[0m f1_macro: 0.14919010698274793
[2m[36m(func pid=187437)[0m f1_weighted: 0.1394270739383506
[2m[36m(func pid=187437)[0m f1_per_class: [0.086, 0.429, 0.0, 0.006, 0.127, 0.274, 0.0, 0.444, 0.126, 0.0]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.08069029850746269
[2m[36m(func pid=549)[0m top5: 0.5830223880597015
[2m[36m(func pid=549)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=549)[0m f1_macro: 0.03746449336781067
[2m[36m(func pid=549)[0m f1_weighted: 0.03178784164423163
[2m[36m(func pid=549)[0m f1_per_class: [0.038, 0.0, 0.0, 0.0, 0.0, 0.249, 0.0, 0.0, 0.071, 0.016]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.3646 | Steps: 4 | Val loss: 3.7722 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9459 | Steps: 4 | Val loss: 1.9065 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.7212 | Steps: 4 | Val loss: 2.9208 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=186833)[0m top1: 0.29011194029850745
[2m[36m(func pid=186833)[0m top5: 0.8311567164179104
[2m[36m(func pid=186833)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=186833)[0m f1_macro: 0.25024799203562176
[2m[36m(func pid=186833)[0m f1_weighted: 0.33411467783025045
[2m[36m(func pid=186833)[0m f1_per_class: [0.191, 0.348, 0.0, 0.355, 0.104, 0.333, 0.34, 0.401, 0.137, 0.293]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.1567 | Steps: 4 | Val loss: 4.7106 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=181384)[0m top1: 0.3903917910447761
[2m[36m(func pid=181384)[0m top5: 0.8903917910447762
[2m[36m(func pid=181384)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=181384)[0m f1_macro: 0.34627800679325577
[2m[36m(func pid=181384)[0m f1_weighted: 0.39538383231118485
[2m[36m(func pid=181384)[0m f1_per_class: [0.292, 0.585, 0.444, 0.422, 0.137, 0.275, 0.331, 0.459, 0.209, 0.308]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.16791044776119404
[2m[36m(func pid=187437)[0m top5: 0.7877798507462687
[2m[36m(func pid=187437)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=187437)[0m f1_macro: 0.16152463072743276
[2m[36m(func pid=187437)[0m f1_weighted: 0.12973235004097408
[2m[36m(func pid=187437)[0m f1_per_class: [0.09, 0.314, 0.143, 0.003, 0.098, 0.328, 0.0, 0.515, 0.124, 0.0]
[2m[36m(func pid=187437)[0m 
== Status ==
Current time: 2024-01-07 13:15:00 (running for 00:21:52.99)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.946 |      0.346 |                   74 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.365 |      0.25  |                   51 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.721 |      0.162 |                   51 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  7.047 |      0.037 |                   46 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=549)[0m top1: 0.08255597014925373
[2m[36m(func pid=549)[0m top5: 0.5541044776119403
[2m[36m(func pid=549)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=549)[0m f1_macro: 0.06921450989723195
[2m[36m(func pid=549)[0m f1_weighted: 0.01992127207384844
[2m[36m(func pid=549)[0m f1_per_class: [0.042, 0.0, 0.323, 0.0, 0.0, 0.0, 0.0, 0.253, 0.075, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5018 | Steps: 4 | Val loss: 6.3692 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0067 | Steps: 4 | Val loss: 2.1089 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4260 | Steps: 4 | Val loss: 2.5087 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=186833)[0m top1: 0.24673507462686567
[2m[36m(func pid=186833)[0m top5: 0.7756529850746269
[2m[36m(func pid=186833)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=186833)[0m f1_macro: 0.2068456810846083
[2m[36m(func pid=186833)[0m f1_weighted: 0.2739426783001577
[2m[36m(func pid=186833)[0m f1_per_class: [0.141, 0.527, 0.0, 0.288, 0.088, 0.3, 0.137, 0.337, 0.09, 0.16]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.9700 | Steps: 4 | Val loss: 3.2940 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=181384)[0m top1: 0.37033582089552236
[2m[36m(func pid=181384)[0m top5: 0.8782649253731343
[2m[36m(func pid=181384)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=181384)[0m f1_macro: 0.32100749181015553
[2m[36m(func pid=181384)[0m f1_weighted: 0.38477524328283724
[2m[36m(func pid=181384)[0m f1_per_class: [0.262, 0.592, 0.414, 0.453, 0.132, 0.248, 0.284, 0.471, 0.162, 0.194]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:06 (running for 00:21:58.37)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.007 |      0.321 |                   75 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.502 |      0.207 |                   52 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.426 |      0.201 |                   52 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.157 |      0.069 |                   47 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.1767723880597015
[2m[36m(func pid=187437)[0m top5: 0.7756529850746269
[2m[36m(func pid=187437)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=187437)[0m f1_macro: 0.2008906291227361
[2m[36m(func pid=187437)[0m f1_weighted: 0.1346340327721827
[2m[36m(func pid=187437)[0m f1_per_class: [0.096, 0.317, 0.421, 0.0, 0.087, 0.339, 0.0, 0.527, 0.145, 0.077]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06996268656716417
[2m[36m(func pid=549)[0m top5: 0.5601679104477612
[2m[36m(func pid=549)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=549)[0m f1_macro: 0.08968485544951257
[2m[36m(func pid=549)[0m f1_weighted: 0.034542407438563925
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.293, 0.0, 0.005, 0.0, 0.0, 0.521, 0.079, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8078 | Steps: 4 | Val loss: 6.0786 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.5664 | Steps: 4 | Val loss: 2.1700 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4531 | Steps: 4 | Val loss: 2.4770 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=186833)[0m top1: 0.2574626865671642
[2m[36m(func pid=186833)[0m top5: 0.7793843283582089
[2m[36m(func pid=186833)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=186833)[0m f1_macro: 0.22474539874558333
[2m[36m(func pid=186833)[0m f1_weighted: 0.2552751568858091
[2m[36m(func pid=186833)[0m f1_per_class: [0.149, 0.535, 0.0, 0.208, 0.12, 0.4, 0.081, 0.433, 0.121, 0.2]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7848 | Steps: 4 | Val loss: 2.3425 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=181384)[0m top1: 0.376865671641791
[2m[36m(func pid=181384)[0m top5: 0.8815298507462687
[2m[36m(func pid=181384)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.3416331082601246
[2m[36m(func pid=181384)[0m f1_weighted: 0.40002048105475263
[2m[36m(func pid=181384)[0m f1_per_class: [0.267, 0.562, 0.632, 0.502, 0.121, 0.256, 0.303, 0.46, 0.154, 0.159]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:11 (running for 00:22:03.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.566 |      0.342 |                   76 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.808 |      0.225 |                   53 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.453 |      0.231 |                   53 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.97  |      0.09  |                   48 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17723880597014927
[2m[36m(func pid=187437)[0m top5: 0.7509328358208955
[2m[36m(func pid=187437)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=187437)[0m f1_macro: 0.23082154005499475
[2m[36m(func pid=187437)[0m f1_weighted: 0.13067708755213434
[2m[36m(func pid=187437)[0m f1_per_class: [0.096, 0.275, 0.769, 0.0, 0.088, 0.365, 0.0, 0.494, 0.144, 0.077]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.28125
[2m[36m(func pid=549)[0m top5: 0.5648320895522388
[2m[36m(func pid=549)[0m f1_micro: 0.28125
[2m[36m(func pid=549)[0m f1_macro: 0.0869939843951637
[2m[36m(func pid=549)[0m f1_weighted: 0.12775570765363556
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.421, 0.449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.5958 | Steps: 4 | Val loss: 3.3409 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.5097 | Steps: 4 | Val loss: 2.2808 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7441 | Steps: 4 | Val loss: 2.5520 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.4700 | Steps: 4 | Val loss: 2.3416 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=186833)[0m top1: 0.3111007462686567
[2m[36m(func pid=186833)[0m top5: 0.8582089552238806
[2m[36m(func pid=186833)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=186833)[0m f1_macro: 0.25086390400899894
[2m[36m(func pid=186833)[0m f1_weighted: 0.3008138510315382
[2m[36m(func pid=186833)[0m f1_per_class: [0.204, 0.551, 0.0, 0.337, 0.1, 0.427, 0.08, 0.46, 0.176, 0.174]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3605410447761194
[2m[36m(func pid=181384)[0m top5: 0.8782649253731343
[2m[36m(func pid=181384)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=181384)[0m f1_macro: 0.34516795883685225
[2m[36m(func pid=181384)[0m f1_weighted: 0.3918005850918471
[2m[36m(func pid=181384)[0m f1_per_class: [0.271, 0.519, 0.774, 0.502, 0.102, 0.242, 0.308, 0.438, 0.162, 0.135]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:16 (running for 00:22:09.26)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.51  |      0.345 |                   77 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.596 |      0.251 |                   54 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.744 |      0.222 |                   54 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.785 |      0.087 |                   49 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17397388059701493
[2m[36m(func pid=187437)[0m top5: 0.7327425373134329
[2m[36m(func pid=187437)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=187437)[0m f1_macro: 0.2219534729645764
[2m[36m(func pid=187437)[0m f1_weighted: 0.13109489834350577
[2m[36m(func pid=187437)[0m f1_per_class: [0.106, 0.274, 0.5, 0.0, 0.071, 0.357, 0.0, 0.508, 0.153, 0.25]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.28171641791044777
[2m[36m(func pid=549)[0m top5: 0.5219216417910447
[2m[36m(func pid=549)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=549)[0m f1_macro: 0.08887905604719763
[2m[36m(func pid=549)[0m f1_weighted: 0.12511509102716506
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.45, 0.439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.7989 | Steps: 4 | Val loss: 2.2984 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.1563 | Steps: 4 | Val loss: 2.2744 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0036 | Steps: 4 | Val loss: 2.4244 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.7410 | Steps: 4 | Val loss: 2.1428 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=186833)[0m top1: 0.33722014925373134
[2m[36m(func pid=186833)[0m top5: 0.9146455223880597
[2m[36m(func pid=186833)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=186833)[0m f1_macro: 0.27424094424674283
[2m[36m(func pid=186833)[0m f1_weighted: 0.3189201867963959
[2m[36m(func pid=186833)[0m f1_per_class: [0.29, 0.563, 0.109, 0.419, 0.04, 0.34, 0.074, 0.466, 0.24, 0.2]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3614738805970149
[2m[36m(func pid=181384)[0m top5: 0.8722014925373134
[2m[36m(func pid=181384)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=181384)[0m f1_macro: 0.3422265987303544
[2m[36m(func pid=181384)[0m f1_weighted: 0.3972583484156647
[2m[36m(func pid=181384)[0m f1_per_class: [0.247, 0.512, 0.75, 0.531, 0.101, 0.242, 0.305, 0.437, 0.167, 0.131]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:22 (running for 00:22:14.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.156 |      0.342 |                   78 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.799 |      0.274 |                   55 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.004 |      0.217 |                   55 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.47  |      0.089 |                   50 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17350746268656717
[2m[36m(func pid=187437)[0m top5: 0.7290111940298507
[2m[36m(func pid=187437)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=187437)[0m f1_macro: 0.21701629673706346
[2m[36m(func pid=187437)[0m f1_weighted: 0.12952792549993927
[2m[36m(func pid=187437)[0m f1_per_class: [0.118, 0.29, 0.571, 0.0, 0.068, 0.345, 0.0, 0.474, 0.127, 0.175]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.283115671641791
[2m[36m(func pid=549)[0m top5: 0.7728544776119403
[2m[36m(func pid=549)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=549)[0m f1_macro: 0.0877104377104377
[2m[36m(func pid=549)[0m f1_weighted: 0.12557634805769133
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.436, 0.441, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3427 | Steps: 4 | Val loss: 2.0090 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.5548 | Steps: 4 | Val loss: 2.2168 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.5564 | Steps: 4 | Val loss: 2.3007 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.8119 | Steps: 4 | Val loss: 2.6917 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=186833)[0m top1: 0.3773320895522388
[2m[36m(func pid=186833)[0m top5: 0.9183768656716418
[2m[36m(func pid=186833)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=186833)[0m f1_macro: 0.31024415980161313
[2m[36m(func pid=186833)[0m f1_weighted: 0.36819277518474525
[2m[36m(func pid=186833)[0m f1_per_class: [0.315, 0.544, 0.244, 0.476, 0.058, 0.34, 0.194, 0.453, 0.228, 0.25]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=181384)[0m top1: 0.36986940298507465
[2m[36m(func pid=181384)[0m top5: 0.8656716417910447
[2m[36m(func pid=181384)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=181384)[0m f1_macro: 0.33187936949436103
[2m[36m(func pid=181384)[0m f1_weighted: 0.3980894030583795
[2m[36m(func pid=181384)[0m f1_per_class: [0.244, 0.509, 0.615, 0.54, 0.112, 0.222, 0.307, 0.455, 0.165, 0.15]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:27 (running for 00:22:20.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.555 |      0.332 |                   79 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.343 |      0.31  |                   56 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  3.556 |      0.229 |                   56 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.741 |      0.088 |                   51 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.16651119402985073
[2m[36m(func pid=187437)[0m top5: 0.7411380597014925
[2m[36m(func pid=187437)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=187437)[0m f1_macro: 0.22884309686849572
[2m[36m(func pid=187437)[0m f1_weighted: 0.13085027091044318
[2m[36m(func pid=187437)[0m f1_per_class: [0.122, 0.291, 0.815, 0.0, 0.067, 0.306, 0.024, 0.451, 0.095, 0.118]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.28031716417910446
[2m[36m(func pid=549)[0m top5: 0.7728544776119403
[2m[36m(func pid=549)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=549)[0m f1_macro: 0.07571841324379198
[2m[36m(func pid=549)[0m f1_weighted: 0.1250276441663306
[2m[36m(func pid=549)[0m f1_per_class: [0.0, 0.0, 0.316, 0.441, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7414 | Steps: 4 | Val loss: 2.1868 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.1484 | Steps: 4 | Val loss: 1.9806 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1716 | Steps: 4 | Val loss: 2.1955 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=186833)[0m top1: 0.36613805970149255
[2m[36m(func pid=186833)[0m top5: 0.898320895522388
[2m[36m(func pid=186833)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=186833)[0m f1_macro: 0.29532131548148594
[2m[36m(func pid=186833)[0m f1_weighted: 0.3691243619757375
[2m[36m(func pid=186833)[0m f1_per_class: [0.247, 0.476, 0.381, 0.494, 0.091, 0.16, 0.301, 0.414, 0.23, 0.159]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 3.7214 | Steps: 4 | Val loss: 5.6973 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=181384)[0m top1: 0.39738805970149255
[2m[36m(func pid=181384)[0m top5: 0.8815298507462687
[2m[36m(func pid=181384)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=181384)[0m f1_macro: 0.3602614833379415
[2m[36m(func pid=181384)[0m f1_weighted: 0.42797340795246885
[2m[36m(func pid=181384)[0m f1_per_class: [0.257, 0.481, 0.706, 0.542, 0.094, 0.268, 0.392, 0.486, 0.175, 0.201]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:33 (running for 00:22:25.42)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.148 |      0.36  |                   80 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  0.741 |      0.295 |                   57 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.172 |      0.249 |                   57 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.812 |      0.076 |                   52 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.20988805970149255
[2m[36m(func pid=187437)[0m top5: 0.7607276119402985
[2m[36m(func pid=187437)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=187437)[0m f1_macro: 0.24866370944181182
[2m[36m(func pid=187437)[0m f1_weighted: 0.2277565660234295
[2m[36m(func pid=187437)[0m f1_per_class: [0.121, 0.328, 0.774, 0.0, 0.076, 0.141, 0.384, 0.509, 0.082, 0.071]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.2971082089552239
[2m[36m(func pid=549)[0m top5: 0.8782649253731343
[2m[36m(func pid=549)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=549)[0m f1_macro: 0.07651872966149925
[2m[36m(func pid=549)[0m f1_weighted: 0.14033268991984282
[2m[36m(func pid=549)[0m f1_per_class: [0.071, 0.0, 0.233, 0.0, 0.0, 0.0, 0.461, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0356 | Steps: 4 | Val loss: 2.9352 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.9224 | Steps: 4 | Val loss: 1.9264 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.7153 | Steps: 4 | Val loss: 2.1652 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=186833)[0m top1: 0.302705223880597
[2m[36m(func pid=186833)[0m top5: 0.8428171641791045
[2m[36m(func pid=186833)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=186833)[0m f1_macro: 0.24674034464991723
[2m[36m(func pid=186833)[0m f1_weighted: 0.32360512025764393
[2m[36m(func pid=186833)[0m f1_per_class: [0.17, 0.353, 0.359, 0.466, 0.114, 0.062, 0.302, 0.395, 0.158, 0.087]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.0183 | Steps: 4 | Val loss: 7.8149 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=181384)[0m top1: 0.41277985074626866
[2m[36m(func pid=181384)[0m top5: 0.8810634328358209
[2m[36m(func pid=181384)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=181384)[0m f1_macro: 0.3701032098675513
[2m[36m(func pid=181384)[0m f1_weighted: 0.4348678876556662
[2m[36m(func pid=181384)[0m f1_per_class: [0.269, 0.499, 0.686, 0.551, 0.102, 0.289, 0.381, 0.513, 0.174, 0.237]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:38 (running for 00:22:30.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.922 |      0.37  |                   81 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.036 |      0.247 |                   58 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.715 |      0.249 |                   58 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.721 |      0.077 |                   53 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.24067164179104478
[2m[36m(func pid=187437)[0m top5: 0.7868470149253731
[2m[36m(func pid=187437)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=187437)[0m f1_macro: 0.24851064468401574
[2m[36m(func pid=187437)[0m f1_weighted: 0.2517329616970131
[2m[36m(func pid=187437)[0m f1_per_class: [0.131, 0.371, 0.774, 0.0, 0.084, 0.0, 0.498, 0.495, 0.066, 0.067]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.29850746268656714
[2m[36m(func pid=549)[0m top5: 0.6166044776119403
[2m[36m(func pid=549)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=549)[0m f1_macro: 0.07614334047444875
[2m[36m(func pid=549)[0m f1_weighted: 0.14186502994550187
[2m[36m(func pid=549)[0m f1_per_class: [0.156, 0.0, 0.143, 0.0, 0.0, 0.0, 0.462, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.1055 | Steps: 4 | Val loss: 3.6642 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.0362 | Steps: 4 | Val loss: 1.9222 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9048 | Steps: 4 | Val loss: 2.2493 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=186833)[0m top1: 0.24673507462686567
[2m[36m(func pid=186833)[0m top5: 0.804570895522388
[2m[36m(func pid=186833)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=186833)[0m f1_macro: 0.20592416707744493
[2m[36m(func pid=186833)[0m f1_weighted: 0.27485778262807165
[2m[36m(func pid=186833)[0m f1_per_class: [0.147, 0.272, 0.2, 0.441, 0.127, 0.04, 0.223, 0.422, 0.113, 0.074]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.3130 | Steps: 4 | Val loss: 5.0463 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=181384)[0m top1: 0.40951492537313433
[2m[36m(func pid=181384)[0m top5: 0.8815298507462687
[2m[36m(func pid=181384)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=181384)[0m f1_macro: 0.37077686031529467
[2m[36m(func pid=181384)[0m f1_weighted: 0.43188117053909597
[2m[36m(func pid=181384)[0m f1_per_class: [0.277, 0.498, 0.686, 0.538, 0.107, 0.304, 0.38, 0.496, 0.177, 0.244]
[2m[36m(func pid=181384)[0m 
== Status ==
Current time: 2024-01-07 13:15:43 (running for 00:22:36.04)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.036 |      0.371 |                   82 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.106 |      0.206 |                   59 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.905 |      0.191 |                   59 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.018 |      0.076 |                   54 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.17537313432835822
[2m[36m(func pid=187437)[0m top5: 0.7826492537313433
[2m[36m(func pid=187437)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=187437)[0m f1_macro: 0.19102977716978498
[2m[36m(func pid=187437)[0m f1_weighted: 0.18366930026506414
[2m[36m(func pid=187437)[0m f1_per_class: [0.123, 0.374, 0.774, 0.0, 0.092, 0.0, 0.35, 0.062, 0.082, 0.052]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.2989738805970149
[2m[36m(func pid=549)[0m top5: 0.6623134328358209
[2m[36m(func pid=549)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=549)[0m f1_macro: 0.08467387876501833
[2m[36m(func pid=549)[0m f1_weighted: 0.1417388615703229
[2m[36m(func pid=549)[0m f1_per_class: [0.136, 0.0, 0.25, 0.0, 0.0, 0.0, 0.461, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.4833 | Steps: 4 | Val loss: 3.4658 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.7254 | Steps: 4 | Val loss: 1.8737 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.8889 | Steps: 4 | Val loss: 2.3339 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=186833)[0m top1: 0.22994402985074627
[2m[36m(func pid=186833)[0m top5: 0.8218283582089553
[2m[36m(func pid=186833)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=186833)[0m f1_macro: 0.20955533065572082
[2m[36m(func pid=186833)[0m f1_weighted: 0.26413638966804626
[2m[36m(func pid=186833)[0m f1_per_class: [0.151, 0.237, 0.247, 0.311, 0.099, 0.071, 0.311, 0.432, 0.124, 0.112]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.4415 | Steps: 4 | Val loss: 4.7958 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 13:15:48 (running for 00:22:41.13)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.725 |      0.387 |                   83 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.483 |      0.21  |                   60 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.905 |      0.191 |                   59 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.313 |      0.085 |                   55 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.416044776119403
[2m[36m(func pid=181384)[0m top5: 0.8903917910447762
[2m[36m(func pid=181384)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=181384)[0m f1_macro: 0.38654739252366144
[2m[36m(func pid=181384)[0m f1_weighted: 0.4364200779839126
[2m[36m(func pid=181384)[0m f1_per_class: [0.284, 0.504, 0.71, 0.525, 0.101, 0.355, 0.377, 0.498, 0.208, 0.302]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.15438432835820895
[2m[36m(func pid=187437)[0m top5: 0.7784514925373134
[2m[36m(func pid=187437)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=187437)[0m f1_macro: 0.17348201414726777
[2m[36m(func pid=187437)[0m f1_weighted: 0.16067824835559139
[2m[36m(func pid=187437)[0m f1_per_class: [0.132, 0.355, 0.741, 0.0, 0.082, 0.0, 0.297, 0.0, 0.077, 0.051]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.29990671641791045
[2m[36m(func pid=549)[0m top5: 0.6609141791044776
[2m[36m(func pid=549)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=549)[0m f1_macro: 0.08669114361944416
[2m[36m(func pid=549)[0m f1_weighted: 0.1434878797105923
[2m[36m(func pid=549)[0m f1_per_class: [0.172, 0.005, 0.227, 0.0, 0.0, 0.0, 0.462, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.7246 | Steps: 4 | Val loss: 3.3140 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2010 | Steps: 4 | Val loss: 1.7783 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1209 | Steps: 4 | Val loss: 2.2610 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 3.4404 | Steps: 4 | Val loss: 4.3514 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=186833)[0m top1: 0.23833955223880596
[2m[36m(func pid=186833)[0m top5: 0.835820895522388
[2m[36m(func pid=186833)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=186833)[0m f1_macro: 0.21558415022820282
[2m[36m(func pid=186833)[0m f1_weighted: 0.2744746702051297
[2m[36m(func pid=186833)[0m f1_per_class: [0.151, 0.204, 0.175, 0.268, 0.091, 0.075, 0.392, 0.487, 0.117, 0.195]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:15:54 (running for 00:22:46.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.201 |      0.398 |                   84 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.725 |      0.216 |                   61 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.889 |      0.173 |                   60 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.441 |      0.087 |                   56 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.4300373134328358
[2m[36m(func pid=181384)[0m top5: 0.8992537313432836
[2m[36m(func pid=181384)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=181384)[0m f1_macro: 0.3980765343235179
[2m[36m(func pid=181384)[0m f1_weighted: 0.4477396833213438
[2m[36m(func pid=181384)[0m f1_per_class: [0.304, 0.485, 0.667, 0.524, 0.1, 0.381, 0.412, 0.488, 0.255, 0.366]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.16511194029850745
[2m[36m(func pid=187437)[0m top5: 0.7999067164179104
[2m[36m(func pid=187437)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=187437)[0m f1_macro: 0.17630479468382515
[2m[36m(func pid=187437)[0m f1_weighted: 0.17691719614242205
[2m[36m(func pid=187437)[0m f1_per_class: [0.139, 0.327, 0.72, 0.023, 0.074, 0.0, 0.347, 0.0, 0.076, 0.058]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.11800373134328358
[2m[36m(func pid=549)[0m top5: 0.659981343283582
[2m[36m(func pid=549)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=549)[0m f1_macro: 0.058157086639202736
[2m[36m(func pid=549)[0m f1_weighted: 0.027968289431632657
[2m[36m(func pid=549)[0m f1_per_class: [0.129, 0.0, 0.244, 0.0, 0.0, 0.209, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.0527 | Steps: 4 | Val loss: 3.4522 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.0107 | Steps: 4 | Val loss: 1.7869 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0734 | Steps: 4 | Val loss: 2.2044 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.4562 | Steps: 4 | Val loss: 5.2331 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=186833)[0m top1: 0.22667910447761194
[2m[36m(func pid=186833)[0m top5: 0.8339552238805971
[2m[36m(func pid=186833)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=186833)[0m f1_macro: 0.21527504459581398
[2m[36m(func pid=186833)[0m f1_weighted: 0.26566289069740695
[2m[36m(func pid=186833)[0m f1_per_class: [0.189, 0.163, 0.18, 0.347, 0.074, 0.131, 0.294, 0.457, 0.114, 0.203]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:15:59 (running for 00:22:51.77)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.011 |      0.39  |                   85 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.053 |      0.215 |                   62 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.121 |      0.176 |                   61 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.44  |      0.058 |                   57 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.4221082089552239
[2m[36m(func pid=181384)[0m top5: 0.9001865671641791
[2m[36m(func pid=181384)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=181384)[0m f1_macro: 0.3904790401231447
[2m[36m(func pid=181384)[0m f1_weighted: 0.43654328533842396
[2m[36m(func pid=181384)[0m f1_per_class: [0.28, 0.499, 0.588, 0.499, 0.109, 0.35, 0.402, 0.475, 0.274, 0.429]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.18889925373134328
[2m[36m(func pid=187437)[0m top5: 0.8120335820895522
[2m[36m(func pid=187437)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=187437)[0m f1_macro: 0.191040944701867
[2m[36m(func pid=187437)[0m f1_weighted: 0.2181773504420749
[2m[36m(func pid=187437)[0m f1_per_class: [0.144, 0.303, 0.72, 0.16, 0.075, 0.0, 0.37, 0.0, 0.078, 0.06]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06389925373134328
[2m[36m(func pid=549)[0m top5: 0.5032649253731343
[2m[36m(func pid=549)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=549)[0m f1_macro: 0.052629138377546014
[2m[36m(func pid=549)[0m f1_weighted: 0.011998542527005902
[2m[36m(func pid=549)[0m f1_per_class: [0.205, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.9548 | Steps: 4 | Val loss: 3.5704 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.1176 | Steps: 4 | Val loss: 1.8163 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.4179 | Steps: 4 | Val loss: 2.0822 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=186833)[0m top1: 0.21082089552238806
[2m[36m(func pid=186833)[0m top5: 0.8339552238805971
[2m[36m(func pid=186833)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=186833)[0m f1_macro: 0.20539279634652416
[2m[36m(func pid=186833)[0m f1_weighted: 0.24303281391745057
[2m[36m(func pid=186833)[0m f1_per_class: [0.148, 0.234, 0.117, 0.355, 0.073, 0.157, 0.16, 0.473, 0.122, 0.216]
[2m[36m(func pid=186833)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.2925 | Steps: 4 | Val loss: 4.0910 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:16:04 (running for 00:22:57.21)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.118 |      0.367 |                   86 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  0.955 |      0.205 |                   63 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.073 |      0.191 |                   62 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.456 |      0.053 |                   58 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.4001865671641791
[2m[36m(func pid=181384)[0m top5: 0.9001865671641791
[2m[36m(func pid=181384)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.366884248812299
[2m[36m(func pid=181384)[0m f1_weighted: 0.4135450923292519
[2m[36m(func pid=181384)[0m f1_per_class: [0.282, 0.488, 0.533, 0.496, 0.11, 0.364, 0.343, 0.433, 0.244, 0.375]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.25886194029850745
[2m[36m(func pid=187437)[0m top5: 0.8330223880597015
[2m[36m(func pid=187437)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=187437)[0m f1_macro: 0.2176461194550397
[2m[36m(func pid=187437)[0m f1_weighted: 0.2849205976132525
[2m[36m(func pid=187437)[0m f1_per_class: [0.136, 0.163, 0.692, 0.382, 0.098, 0.0, 0.449, 0.078, 0.105, 0.074]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.0625
[2m[36m(func pid=549)[0m top5: 0.5055970149253731
[2m[36m(func pid=549)[0m f1_micro: 0.0625
[2m[36m(func pid=549)[0m f1_macro: 0.05095901504900151
[2m[36m(func pid=549)[0m f1_weighted: 0.0106886433712848
[2m[36m(func pid=549)[0m f1_per_class: [0.125, 0.0, 0.273, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.0581 | Steps: 4 | Val loss: 3.8600 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.9780 | Steps: 4 | Val loss: 1.8701 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.4630 | Steps: 4 | Val loss: 2.0984 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.8630 | Steps: 4 | Val loss: 4.1406 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=186833)[0m top1: 0.21082089552238806
[2m[36m(func pid=186833)[0m top5: 0.8236940298507462
[2m[36m(func pid=186833)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=186833)[0m f1_macro: 0.1873979758620999
[2m[36m(func pid=186833)[0m f1_weighted: 0.2290504231704647
[2m[36m(func pid=186833)[0m f1_per_class: [0.107, 0.435, 0.105, 0.357, 0.073, 0.134, 0.039, 0.317, 0.107, 0.199]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:10 (running for 00:23:02.70)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.978 |      0.371 |                   87 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.058 |      0.187 |                   64 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.418 |      0.218 |                   63 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.293 |      0.051 |                   59 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.3969216417910448
[2m[36m(func pid=181384)[0m top5: 0.8913246268656716
[2m[36m(func pid=181384)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=181384)[0m f1_macro: 0.37075851422124745
[2m[36m(func pid=181384)[0m f1_weighted: 0.4084749121861739
[2m[36m(func pid=181384)[0m f1_per_class: [0.281, 0.539, 0.545, 0.473, 0.095, 0.318, 0.329, 0.457, 0.25, 0.419]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.3148320895522388
[2m[36m(func pid=187437)[0m top5: 0.8250932835820896
[2m[36m(func pid=187437)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=187437)[0m f1_macro: 0.26833136900556165
[2m[36m(func pid=187437)[0m f1_weighted: 0.32847491169260057
[2m[36m(func pid=187437)[0m f1_per_class: [0.128, 0.144, 0.643, 0.438, 0.188, 0.059, 0.466, 0.398, 0.123, 0.097]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06203358208955224
[2m[36m(func pid=549)[0m top5: 0.23973880597014927
[2m[36m(func pid=549)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=549)[0m f1_macro: 0.04538932018645268
[2m[36m(func pid=549)[0m f1_weighted: 0.010290975540375233
[2m[36m(func pid=549)[0m f1_per_class: [0.119, 0.0, 0.222, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.5426 | Steps: 4 | Val loss: 3.4285 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.8252 | Steps: 4 | Val loss: 1.8877 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0389 | Steps: 4 | Val loss: 2.0537 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.1512 | Steps: 4 | Val loss: 3.8024 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=186833)[0m top1: 0.2630597014925373
[2m[36m(func pid=186833)[0m top5: 0.8311567164179104
[2m[36m(func pid=186833)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=186833)[0m f1_macro: 0.21987059382634494
[2m[36m(func pid=186833)[0m f1_weighted: 0.26345483694195015
[2m[36m(func pid=186833)[0m f1_per_class: [0.122, 0.52, 0.136, 0.388, 0.038, 0.233, 0.022, 0.38, 0.135, 0.224]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:15 (running for 00:23:08.16)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.825 |      0.372 |                   88 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.543 |      0.22  |                   65 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.463 |      0.268 |                   64 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.863 |      0.045 |                   60 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.39225746268656714
[2m[36m(func pid=181384)[0m top5: 0.8908582089552238
[2m[36m(func pid=181384)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=181384)[0m f1_macro: 0.37187591475265286
[2m[36m(func pid=181384)[0m f1_weighted: 0.3961130340415356
[2m[36m(func pid=181384)[0m f1_per_class: [0.29, 0.552, 0.571, 0.468, 0.094, 0.333, 0.277, 0.452, 0.269, 0.413]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.29617537313432835
[2m[36m(func pid=187437)[0m top5: 0.8488805970149254
[2m[36m(func pid=187437)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=187437)[0m f1_macro: 0.28372955112277876
[2m[36m(func pid=187437)[0m f1_weighted: 0.300481442021391
[2m[36m(func pid=187437)[0m f1_per_class: [0.117, 0.116, 0.643, 0.424, 0.217, 0.282, 0.295, 0.518, 0.08, 0.146]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.06436567164179105
[2m[36m(func pid=549)[0m top5: 0.240205223880597
[2m[36m(func pid=549)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=549)[0m f1_macro: 0.05811538140528368
[2m[36m(func pid=549)[0m f1_weighted: 0.012817003898850368
[2m[36m(func pid=549)[0m f1_per_class: [0.038, 0.0, 0.324, 0.0, 0.0, 0.0, 0.0, 0.114, 0.104, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.2266 | Steps: 4 | Val loss: 2.8354 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.8814 | Steps: 4 | Val loss: 1.9048 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6706 | Steps: 4 | Val loss: 2.0286 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.1802 | Steps: 4 | Val loss: 4.0007 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=186833)[0m top1: 0.3069029850746269
[2m[36m(func pid=186833)[0m top5: 0.8675373134328358
[2m[36m(func pid=186833)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=186833)[0m f1_macro: 0.2590352182273086
[2m[36m(func pid=186833)[0m f1_weighted: 0.28224089446216777
[2m[36m(func pid=186833)[0m f1_per_class: [0.092, 0.528, 0.182, 0.391, 0.059, 0.289, 0.03, 0.449, 0.191, 0.378]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:21 (running for 00:23:13.46)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.881 |      0.381 |                   89 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.227 |      0.259 |                   66 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.039 |      0.284 |                   65 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.151 |      0.058 |                   61 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.40345149253731344
[2m[36m(func pid=181384)[0m top5: 0.8903917910447762
[2m[36m(func pid=181384)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=181384)[0m f1_macro: 0.3807405941769736
[2m[36m(func pid=181384)[0m f1_weighted: 0.4175763481465359
[2m[36m(func pid=181384)[0m f1_per_class: [0.308, 0.548, 0.6, 0.447, 0.09, 0.386, 0.35, 0.483, 0.225, 0.371]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.2537313432835821
[2m[36m(func pid=187437)[0m top5: 0.8736007462686567
[2m[36m(func pid=187437)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=187437)[0m f1_macro: 0.2626837530888394
[2m[36m(func pid=187437)[0m f1_weighted: 0.21201681387808263
[2m[36m(func pid=187437)[0m f1_per_class: [0.115, 0.091, 0.593, 0.409, 0.314, 0.297, 0.025, 0.477, 0.07, 0.236]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.033582089552238806
[2m[36m(func pid=549)[0m top5: 0.13292910447761194
[2m[36m(func pid=549)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=549)[0m f1_macro: 0.03038595331413011
[2m[36m(func pid=549)[0m f1_weighted: 0.004046917423319254
[2m[36m(func pid=549)[0m f1_per_class: [0.037, 0.0, 0.205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8874 | Steps: 4 | Val loss: 2.6780 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.9254 | Steps: 4 | Val loss: 1.9510 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.9751 | Steps: 4 | Val loss: 2.0934 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 3.0117 | Steps: 4 | Val loss: 3.4895 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=186833)[0m top1: 0.29990671641791045
[2m[36m(func pid=186833)[0m top5: 0.8684701492537313
[2m[36m(func pid=186833)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=186833)[0m f1_macro: 0.28626008220406346
[2m[36m(func pid=186833)[0m f1_weighted: 0.3081653633931956
[2m[36m(func pid=186833)[0m f1_per_class: [0.107, 0.431, 0.429, 0.387, 0.056, 0.231, 0.205, 0.359, 0.233, 0.426]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:26 (running for 00:23:18.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.925 |      0.385 |                   90 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  0.887 |      0.286 |                   67 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.671 |      0.263 |                   66 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.18  |      0.03  |                   62 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=181384)[0m top1: 0.39132462686567165
[2m[36m(func pid=181384)[0m top5: 0.8885261194029851
[2m[36m(func pid=181384)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=181384)[0m f1_macro: 0.3845293310612817
[2m[36m(func pid=181384)[0m f1_weighted: 0.40657739961899914
[2m[36m(func pid=181384)[0m f1_per_class: [0.342, 0.532, 0.667, 0.43, 0.08, 0.377, 0.337, 0.483, 0.235, 0.364]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=187437)[0m top1: 0.23694029850746268
[2m[36m(func pid=187437)[0m top5: 0.8773320895522388
[2m[36m(func pid=187437)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=187437)[0m f1_macro: 0.26226618105518984
[2m[36m(func pid=187437)[0m f1_weighted: 0.19749628768166094
[2m[36m(func pid=187437)[0m f1_per_class: [0.105, 0.073, 0.64, 0.369, 0.231, 0.324, 0.015, 0.434, 0.119, 0.313]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.03404850746268657
[2m[36m(func pid=549)[0m top5: 0.13526119402985073
[2m[36m(func pid=549)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=549)[0m f1_macro: 0.03648235602781057
[2m[36m(func pid=549)[0m f1_weighted: 0.004417686753168436
[2m[36m(func pid=549)[0m f1_per_class: [0.036, 0.0, 0.267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8617 | Steps: 4 | Val loss: 2.5787 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.1706 | Steps: 4 | Val loss: 1.9077 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.9826 | Steps: 4 | Val loss: 2.0070 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.8506 | Steps: 4 | Val loss: 2.8827 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=186833)[0m top1: 0.32975746268656714
[2m[36m(func pid=186833)[0m top5: 0.8610074626865671
[2m[36m(func pid=186833)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=186833)[0m f1_macro: 0.2735509359267908
[2m[36m(func pid=186833)[0m f1_weighted: 0.3571582754364003
[2m[36m(func pid=186833)[0m f1_per_class: [0.219, 0.282, 0.364, 0.336, 0.05, 0.14, 0.558, 0.268, 0.197, 0.323]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:32 (running for 00:23:24.35)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.925 |      0.385 |                   90 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  0.862 |      0.274 |                   68 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.983 |      0.237 |                   68 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.012 |      0.036 |                   63 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.23507462686567165
[2m[36m(func pid=187437)[0m top5: 0.8871268656716418
[2m[36m(func pid=187437)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=187437)[0m f1_macro: 0.23718752356690062
[2m[36m(func pid=187437)[0m f1_weighted: 0.1915565945041501
[2m[36m(func pid=187437)[0m f1_per_class: [0.129, 0.058, 0.545, 0.382, 0.112, 0.363, 0.0, 0.343, 0.078, 0.361]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3726679104477612
[2m[36m(func pid=181384)[0m top5: 0.9011194029850746
[2m[36m(func pid=181384)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=181384)[0m f1_macro: 0.3774741912806846
[2m[36m(func pid=181384)[0m f1_weighted: 0.3785146924313332
[2m[36m(func pid=181384)[0m f1_per_class: [0.409, 0.528, 0.632, 0.418, 0.075, 0.398, 0.253, 0.44, 0.228, 0.394]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.03544776119402985
[2m[36m(func pid=549)[0m top5: 0.39925373134328357
[2m[36m(func pid=549)[0m f1_micro: 0.03544776119402985
[2m[36m(func pid=549)[0m f1_macro: 0.0422039745031871
[2m[36m(func pid=549)[0m f1_weighted: 0.005984391876388585
[2m[36m(func pid=549)[0m f1_per_class: [0.119, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.063, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3459 | Steps: 4 | Val loss: 2.9236 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.5346 | Steps: 4 | Val loss: 2.1318 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.8549 | Steps: 4 | Val loss: 1.8746 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.1386 | Steps: 4 | Val loss: 3.1356 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=186833)[0m top1: 0.2943097014925373
[2m[36m(func pid=186833)[0m top5: 0.8325559701492538
[2m[36m(func pid=186833)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=186833)[0m f1_macro: 0.22981648775761906
[2m[36m(func pid=186833)[0m f1_weighted: 0.32131475962337147
[2m[36m(func pid=186833)[0m f1_per_class: [0.226, 0.243, 0.375, 0.319, 0.047, 0.098, 0.543, 0.044, 0.133, 0.27]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:37 (running for 00:23:29.75)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.171 |      0.377 |                   91 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.346 |      0.23  |                   69 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.535 |      0.219 |                   69 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.851 |      0.042 |                   64 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2042910447761194
[2m[36m(func pid=187437)[0m top5: 0.8833955223880597
[2m[36m(func pid=187437)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=187437)[0m f1_macro: 0.2190643857993951
[2m[36m(func pid=187437)[0m f1_weighted: 0.16741222501134043
[2m[36m(func pid=187437)[0m f1_per_class: [0.121, 0.063, 0.593, 0.321, 0.129, 0.333, 0.0, 0.282, 0.074, 0.275]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.38386194029850745
[2m[36m(func pid=181384)[0m top5: 0.9053171641791045
[2m[36m(func pid=181384)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=181384)[0m f1_macro: 0.3816185741325319
[2m[36m(func pid=181384)[0m f1_weighted: 0.40345072038987845
[2m[36m(func pid=181384)[0m f1_per_class: [0.423, 0.526, 0.632, 0.426, 0.084, 0.374, 0.341, 0.443, 0.205, 0.364]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.26492537313432835
[2m[36m(func pid=549)[0m top5: 0.6357276119402985
[2m[36m(func pid=549)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=549)[0m f1_macro: 0.07059738634640439
[2m[36m(func pid=549)[0m f1_weighted: 0.12075719549003992
[2m[36m(func pid=549)[0m f1_per_class: [0.079, 0.0, 0.204, 0.423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.5777 | Steps: 4 | Val loss: 3.7600 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 6.1076 | Steps: 4 | Val loss: 2.1271 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.2096 | Steps: 4 | Val loss: 1.8639 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.0218 | Steps: 4 | Val loss: 2.9200 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=186833)[0m top1: 0.22527985074626866
[2m[36m(func pid=186833)[0m top5: 0.7840485074626866
[2m[36m(func pid=186833)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=186833)[0m f1_macro: 0.1913566989459533
[2m[36m(func pid=186833)[0m f1_weighted: 0.25874899903258486
[2m[36m(func pid=186833)[0m f1_per_class: [0.164, 0.178, 0.31, 0.351, 0.05, 0.082, 0.347, 0.08, 0.141, 0.211]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:42 (running for 00:23:34.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.855 |      0.382 |                   92 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.578 |      0.191 |                   70 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  6.108 |      0.228 |                   70 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.139 |      0.071 |                   65 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.20242537313432835
[2m[36m(func pid=187437)[0m top5: 0.8824626865671642
[2m[36m(func pid=187437)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=187437)[0m f1_macro: 0.22809581683397928
[2m[36m(func pid=187437)[0m f1_weighted: 0.17007245383400912
[2m[36m(func pid=187437)[0m f1_per_class: [0.11, 0.086, 0.647, 0.294, 0.103, 0.366, 0.0, 0.305, 0.12, 0.25]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.38992537313432835
[2m[36m(func pid=181384)[0m top5: 0.9029850746268657
[2m[36m(func pid=181384)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=181384)[0m f1_macro: 0.388202633235723
[2m[36m(func pid=181384)[0m f1_weighted: 0.40504540686017126
[2m[36m(func pid=181384)[0m f1_per_class: [0.5, 0.532, 0.632, 0.41, 0.102, 0.312, 0.377, 0.432, 0.207, 0.378]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.271455223880597
[2m[36m(func pid=549)[0m top5: 0.7663246268656716
[2m[36m(func pid=549)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=549)[0m f1_macro: 0.069251241381774
[2m[36m(func pid=549)[0m f1_weighted: 0.12289714136289935
[2m[36m(func pid=549)[0m f1_per_class: [0.083, 0.0, 0.179, 0.431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.0366 | Steps: 4 | Val loss: 5.3882 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 4.1272 | Steps: 4 | Val loss: 2.1061 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.9905 | Steps: 4 | Val loss: 3.0268 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9184 | Steps: 4 | Val loss: 1.8685 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=186833)[0m top1: 0.1515858208955224
[2m[36m(func pid=186833)[0m top5: 0.7056902985074627
[2m[36m(func pid=186833)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=186833)[0m f1_macro: 0.16195259365040116
[2m[36m(func pid=186833)[0m f1_weighted: 0.17680496101159546
[2m[36m(func pid=186833)[0m f1_per_class: [0.107, 0.131, 0.254, 0.231, 0.058, 0.127, 0.169, 0.271, 0.096, 0.177]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:48 (running for 00:23:40.32)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.21  |      0.388 |                   93 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.037 |      0.162 |                   71 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  4.127 |      0.224 |                   71 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.022 |      0.069 |                   66 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.21315298507462688
[2m[36m(func pid=187437)[0m top5: 0.8777985074626866
[2m[36m(func pid=187437)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=187437)[0m f1_macro: 0.2239852133837356
[2m[36m(func pid=187437)[0m f1_weighted: 0.18998115032986593
[2m[36m(func pid=187437)[0m f1_per_class: [0.119, 0.177, 0.533, 0.314, 0.074, 0.329, 0.003, 0.365, 0.099, 0.226]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3903917910447761
[2m[36m(func pid=181384)[0m top5: 0.9015858208955224
[2m[36m(func pid=181384)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=181384)[0m f1_macro: 0.39359535060163553
[2m[36m(func pid=181384)[0m f1_weighted: 0.4016827641813208
[2m[36m(func pid=181384)[0m f1_per_class: [0.366, 0.545, 0.833, 0.42, 0.116, 0.293, 0.365, 0.41, 0.209, 0.378]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.2658582089552239
[2m[36m(func pid=549)[0m top5: 0.7761194029850746
[2m[36m(func pid=549)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=549)[0m f1_macro: 0.06339482888254296
[2m[36m(func pid=549)[0m f1_weighted: 0.12042597992027591
[2m[36m(func pid=549)[0m f1_per_class: [0.051, 0.0, 0.159, 0.425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1361 | Steps: 4 | Val loss: 4.3482 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0097 | Steps: 4 | Val loss: 1.9927 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.9341 | Steps: 4 | Val loss: 1.8863 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.8796 | Steps: 4 | Val loss: 2.6591 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=186833)[0m top1: 0.2126865671641791
[2m[36m(func pid=186833)[0m top5: 0.7472014925373134
[2m[36m(func pid=186833)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=186833)[0m f1_macro: 0.23050888198246566
[2m[36m(func pid=186833)[0m f1_weighted: 0.22959568198144498
[2m[36m(func pid=186833)[0m f1_per_class: [0.119, 0.317, 0.333, 0.226, 0.066, 0.255, 0.15, 0.45, 0.125, 0.265]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:53 (running for 00:23:45.55)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.918 |      0.394 |                   94 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.136 |      0.231 |                   72 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.01  |      0.216 |                   72 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.991 |      0.063 |                   67 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.20942164179104478
[2m[36m(func pid=187437)[0m top5: 0.8656716417910447
[2m[36m(func pid=187437)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=187437)[0m f1_macro: 0.21603401309583123
[2m[36m(func pid=187437)[0m f1_weighted: 0.20440474627507155
[2m[36m(func pid=187437)[0m f1_per_class: [0.142, 0.271, 0.407, 0.357, 0.045, 0.195, 0.0, 0.43, 0.08, 0.233]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.376865671641791
[2m[36m(func pid=181384)[0m top5: 0.8987873134328358
[2m[36m(func pid=181384)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=181384)[0m f1_macro: 0.3728580306337517
[2m[36m(func pid=181384)[0m f1_weighted: 0.39007683954451555
[2m[36m(func pid=181384)[0m f1_per_class: [0.323, 0.54, 0.75, 0.432, 0.114, 0.281, 0.331, 0.401, 0.188, 0.368]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.26492537313432835
[2m[36m(func pid=549)[0m top5: 0.7765858208955224
[2m[36m(func pid=549)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=549)[0m f1_macro: 0.0681894880224287
[2m[36m(func pid=549)[0m f1_weighted: 0.1206796438270187
[2m[36m(func pid=549)[0m f1_per_class: [0.087, 0.0, 0.172, 0.423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.0341 | Steps: 4 | Val loss: 3.6541 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.8660 | Steps: 4 | Val loss: 1.9802 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.1633 | Steps: 4 | Val loss: 1.9351 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.8017 | Steps: 4 | Val loss: 2.3690 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=186833)[0m top1: 0.261660447761194
[2m[36m(func pid=186833)[0m top5: 0.769589552238806
[2m[36m(func pid=186833)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=186833)[0m f1_macro: 0.27434861245053
[2m[36m(func pid=186833)[0m f1_weighted: 0.2554567115184357
[2m[36m(func pid=186833)[0m f1_per_class: [0.105, 0.506, 0.44, 0.169, 0.075, 0.346, 0.135, 0.485, 0.112, 0.37]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:16:58 (running for 00:23:50.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.934 |      0.373 |                   95 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.034 |      0.274 |                   73 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  1.866 |      0.208 |                   73 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.88  |      0.068 |                   68 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.21735074626865672
[2m[36m(func pid=187437)[0m top5: 0.8782649253731343
[2m[36m(func pid=187437)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=187437)[0m f1_macro: 0.20800399486688015
[2m[36m(func pid=187437)[0m f1_weighted: 0.22195266807610756
[2m[36m(func pid=187437)[0m f1_per_class: [0.154, 0.382, 0.162, 0.381, 0.038, 0.084, 0.0, 0.509, 0.1, 0.271]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=181384)[0m top1: 0.3927238805970149
[2m[36m(func pid=181384)[0m top5: 0.8903917910447762
[2m[36m(func pid=181384)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=181384)[0m f1_macro: 0.36706580425470553
[2m[36m(func pid=181384)[0m f1_weighted: 0.4088030021573228
[2m[36m(func pid=181384)[0m f1_per_class: [0.328, 0.543, 0.558, 0.456, 0.105, 0.351, 0.336, 0.449, 0.215, 0.329]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.1707089552238806
[2m[36m(func pid=549)[0m top5: 0.7779850746268657
[2m[36m(func pid=549)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=549)[0m f1_macro: 0.053243574051407595
[2m[36m(func pid=549)[0m f1_weighted: 0.05276630921280989
[2m[36m(func pid=549)[0m f1_per_class: [0.053, 0.294, 0.186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5720 | Steps: 4 | Val loss: 3.8886 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.8540 | Steps: 4 | Val loss: 2.1516 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.8865 | Steps: 4 | Val loss: 1.9593 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.9038 | Steps: 4 | Val loss: 2.2526 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=186833)[0m top1: 0.23787313432835822
[2m[36m(func pid=186833)[0m top5: 0.7882462686567164
[2m[36m(func pid=186833)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=186833)[0m f1_macro: 0.231248091014796
[2m[36m(func pid=186833)[0m f1_weighted: 0.2168226810057119
[2m[36m(func pid=186833)[0m f1_per_class: [0.044, 0.543, 0.259, 0.109, 0.076, 0.355, 0.048, 0.481, 0.125, 0.273]
[2m[36m(func pid=186833)[0m 
== Status ==
Current time: 2024-01-07 13:17:03 (running for 00:23:56.12)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.319
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.163 |      0.367 |                   96 |
| train_52b21_00009 | RUNNING    | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.572 |      0.231 |                   74 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.854 |      0.194 |                   74 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.802 |      0.053 |                   69 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=187437)[0m top1: 0.2555970149253731
[2m[36m(func pid=187437)[0m top5: 0.8274253731343284
[2m[36m(func pid=187437)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=187437)[0m f1_macro: 0.19414695597980197
[2m[36m(func pid=187437)[0m f1_weighted: 0.28644220611589255
[2m[36m(func pid=187437)[0m f1_per_class: [0.043, 0.443, 0.063, 0.374, 0.075, 0.214, 0.19, 0.31, 0.109, 0.12]
[2m[36m(func pid=187437)[0m 
[2m[36m(func pid=549)[0m top1: 0.17490671641791045
[2m[36m(func pid=549)[0m top5: 0.6688432835820896
[2m[36m(func pid=549)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=549)[0m f1_macro: 0.07127831715210356
[2m[36m(func pid=549)[0m f1_weighted: 0.054006902898820736
[2m[36m(func pid=549)[0m f1_per_class: [0.036, 0.296, 0.381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.37546641791044777
[2m[36m(func pid=181384)[0m top5: 0.8843283582089553
[2m[36m(func pid=181384)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=181384)[0m f1_macro: 0.34753081265308106
[2m[36m(func pid=181384)[0m f1_weighted: 0.38370210013961803
[2m[36m(func pid=181384)[0m f1_per_class: [0.387, 0.534, 0.407, 0.425, 0.063, 0.368, 0.281, 0.428, 0.234, 0.349]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=186833)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2572 | Steps: 4 | Val loss: 4.3118 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=187437)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2593 | Steps: 4 | Val loss: 4.2954 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.0896 | Steps: 4 | Val loss: 1.9781 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.5944 | Steps: 4 | Val loss: 2.3899 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 13:17:08 (running for 00:24:01.27)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.31575
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 3 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.887 |      0.348 |                   97 |
| train_52b21_00010 | RUNNING    | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.854 |      0.194 |                   74 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.904 |      0.071 |                   70 |
| train_52b21_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING)


[2m[36m(func pid=186833)[0m top1: 0.20335820895522388
[2m[36m(func pid=186833)[0m top5: 0.800839552238806
[2m[36m(func pid=186833)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=186833)[0m f1_macro: 0.19882360554221656
[2m[36m(func pid=186833)[0m f1_weighted: 0.18621003993821178
[2m[36m(func pid=186833)[0m f1_per_class: [0.024, 0.521, 0.253, 0.105, 0.06, 0.216, 0.027, 0.433, 0.131, 0.219]
[2m[36m(func pid=187437)[0m top1: 0.17490671641791045
[2m[36m(func pid=187437)[0m top5: 0.5816231343283582
[2m[36m(func pid=187437)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=187437)[0m f1_macro: 0.10233135515961686
[2m[36m(func pid=187437)[0m f1_weighted: 0.18060211397084755
[2m[36m(func pid=187437)[0m f1_per_class: [0.0, 0.426, 0.0, 0.36, 0.136, 0.0, 0.012, 0.0, 0.051, 0.038]
[2m[36m(func pid=181384)[0m top1: 0.373134328358209
[2m[36m(func pid=181384)[0m top5: 0.8894589552238806
[2m[36m(func pid=181384)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=181384)[0m f1_macro: 0.36015777510438707
[2m[36m(func pid=181384)[0m f1_weighted: 0.3807103744564606
[2m[36m(func pid=181384)[0m f1_per_class: [0.464, 0.551, 0.453, 0.411, 0.06, 0.359, 0.272, 0.424, 0.23, 0.378]
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m top1: 0.17257462686567165
[2m[36m(func pid=549)[0m top5: 0.6669776119402985
[2m[36m(func pid=549)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=549)[0m f1_macro: 0.06250551695954229
[2m[36m(func pid=549)[0m f1_weighted: 0.053939066035779415
[2m[36m(func pid=549)[0m f1_per_class: [0.08, 0.295, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.9489 | Steps: 4 | Val loss: 2.6138 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.0279 | Steps: 4 | Val loss: 2.0225 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=549)[0m top1: 0.17024253731343283
[2m[36m(func pid=549)[0m top5: 0.6627798507462687
[2m[36m(func pid=549)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=549)[0m f1_macro: 0.0581023213409075
[2m[36m(func pid=549)[0m f1_weighted: 0.05348002387599139
[2m[36m(func pid=549)[0m f1_per_class: [0.083, 0.294, 0.204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m top1: 0.384794776119403
[2m[36m(func pid=181384)[0m top5: 0.8871268656716418
[2m[36m(func pid=181384)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=181384)[0m f1_macro: 0.3554829062629224
[2m[36m(func pid=181384)[0m f1_weighted: 0.4026116300627212
[2m[36m(func pid=181384)[0m f1_per_class: [0.428, 0.545, 0.329, 0.391, 0.075, 0.349, 0.366, 0.47, 0.248, 0.354]
[2m[36m(func pid=17787)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17787)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=17787)[0m Configuration completed!
[2m[36m(func pid=17787)[0m New optimizer parameters:
[2m[36m(func pid=17787)[0m SGD (
[2m[36m(func pid=17787)[0m Parameter Group 0
[2m[36m(func pid=17787)[0m     dampening: 0
[2m[36m(func pid=17787)[0m     differentiable: False
[2m[36m(func pid=17787)[0m     foreach: None
[2m[36m(func pid=17787)[0m     lr: 0.0001
[2m[36m(func pid=17787)[0m     maximize: False
[2m[36m(func pid=17787)[0m     momentum: 0.9
[2m[36m(func pid=17787)[0m     nesterov: False
[2m[36m(func pid=17787)[0m     weight_decay: 0.0001
[2m[36m(func pid=17787)[0m )
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:17:15 (running for 00:24:08.09)
Memory usage on this node: 20.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.3125
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.09  |      0.36  |                   98 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.949 |      0.058 |                   72 |
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=17891)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17891)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=17891)[0m Configuration completed!
[2m[36m(func pid=17891)[0m New optimizer parameters:
[2m[36m(func pid=17891)[0m SGD (
[2m[36m(func pid=17891)[0m Parameter Group 0
[2m[36m(func pid=17891)[0m     dampening: 0
[2m[36m(func pid=17891)[0m     differentiable: False
[2m[36m(func pid=17891)[0m     foreach: None
[2m[36m(func pid=17891)[0m     lr: 0.001
[2m[36m(func pid=17891)[0m     maximize: False
[2m[36m(func pid=17891)[0m     momentum: 0.9
[2m[36m(func pid=17891)[0m     nesterov: False
[2m[36m(func pid=17891)[0m     weight_decay: 0.0001
[2m[36m(func pid=17891)[0m )
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=181384)[0m 
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.8875 | Steps: 4 | Val loss: 2.5413 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:17:21 (running for 00:24:13.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.3125
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00008 | RUNNING    | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  1.028 |      0.355 |                   99 |
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  2.887 |      0.043 |                   73 |
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=549)[0m top1: 0.0625
[2m[36m(func pid=549)[0m top5: 0.6604477611940298
[2m[36m(func pid=549)[0m f1_micro: 0.0625
[2m[36m(func pid=549)[0m f1_macro: 0.043425286402309866
[2m[36m(func pid=549)[0m f1_weighted: 0.01200439245260756
[2m[36m(func pid=549)[0m f1_per_class: [0.124, 0.011, 0.187, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=181384)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.9820 | Steps: 4 | Val loss: 2.1559 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1644 | Steps: 4 | Val loss: 2.3242 | Batch size: 32 | lr: 0.0001 | Duration: 4.64s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9859 | Steps: 4 | Val loss: 2.3351 | Batch size: 32 | lr: 0.001 | Duration: 4.72s
[2m[36m(func pid=181384)[0m top1: 0.36007462686567165
[2m[36m(func pid=181384)[0m top5: 0.8777985074626866
[2m[36m(func pid=181384)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=181384)[0m f1_macro: 0.32466297847642134
[2m[36m(func pid=181384)[0m f1_weighted: 0.3827604532576718
[2m[36m(func pid=181384)[0m f1_per_class: [0.388, 0.552, 0.207, 0.357, 0.09, 0.301, 0.357, 0.467, 0.194, 0.333]
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.0339 | Steps: 4 | Val loss: 2.7737 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=17787)[0m top1: 0.1525186567164179
[2m[36m(func pid=17787)[0m top5: 0.5326492537313433
[2m[36m(func pid=17787)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=17787)[0m f1_macro: 0.04550045240016628
[2m[36m(func pid=17787)[0m f1_weighted: 0.08715598014987054
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.0, 0.0, 0.275, 0.0, 0.0, 0.0, 0.18, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m top1: 0.12686567164179105
[2m[36m(func pid=17891)[0m top5: 0.5041977611940298
[2m[36m(func pid=17891)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=17891)[0m f1_macro: 0.04502753460938971
[2m[36m(func pid=17891)[0m f1_weighted: 0.07598897612569441
[2m[36m(func pid=17891)[0m f1_per_class: [0.019, 0.0, 0.089, 0.253, 0.0, 0.0, 0.0, 0.073, 0.0, 0.016]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=549)[0m top1: 0.06296641791044776
[2m[36m(func pid=549)[0m top5: 0.6571828358208955
[2m[36m(func pid=549)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=549)[0m f1_macro: 0.047335990101263416
[2m[36m(func pid=549)[0m f1_weighted: 0.010804615762143237
[2m[36m(func pid=549)[0m f1_per_class: [0.144, 0.0, 0.216, 0.0, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
[2m[36m(func pid=549)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9963 | Steps: 4 | Val loss: 2.3101 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8947 | Steps: 4 | Val loss: 2.3156 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=549)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.0335 | Steps: 4 | Val loss: 2.6814 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=17787)[0m top1: 0.20662313432835822
[2m[36m(func pid=17787)[0m top5: 0.5886194029850746
[2m[36m(func pid=17787)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=17787)[0m f1_macro: 0.0435558963076547
[2m[36m(func pid=17787)[0m f1_weighted: 0.10507298833675881
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.0, 0.0, 0.361, 0.0, 0.0, 0.0, 0.074, 0.0, 0.0]
[2m[36m(func pid=17891)[0m top1: 0.03311567164179104
[2m[36m(func pid=17891)[0m top5: 0.5816231343283582
[2m[36m(func pid=17891)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=17891)[0m f1_macro: 0.024905116169241907
[2m[36m(func pid=17891)[0m f1_weighted: 0.03476701828316467
[2m[36m(func pid=17891)[0m f1_per_class: [0.074, 0.031, 0.016, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03]
[2m[36m(func pid=549)[0m top1: 0.06203358208955224
[2m[36m(func pid=549)[0m top5: 0.5004664179104478
[2m[36m(func pid=549)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=549)[0m f1_macro: 0.03971061000566866
[2m[36m(func pid=549)[0m f1_weighted: 0.009535083144915692
[2m[36m(func pid=549)[0m f1_per_class: [0.082, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.115, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:17:26 (running for 00:24:18.82)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3125
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.047 |                   74 |
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  3.164 |      0.046 |                    1 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.986 |      0.045 |                    1 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


== Status ==
Current time: 2024-01-07 13:17:32 (running for 00:24:24.64)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3125
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00011 | RUNNING    | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.047 |                   74 |
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.996 |      0.044 |                    2 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.986 |      0.045 |                    1 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=18906)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=18906)[0m Configuration completed!
[2m[36m(func pid=18906)[0m New optimizer parameters:
[2m[36m(func pid=18906)[0m SGD (
[2m[36m(func pid=18906)[0m Parameter Group 0
[2m[36m(func pid=18906)[0m     dampening: 0
[2m[36m(func pid=18906)[0m     differentiable: False
[2m[36m(func pid=18906)[0m     foreach: None
[2m[36m(func pid=18906)[0m     lr: 0.01
[2m[36m(func pid=18906)[0m     maximize: False
[2m[36m(func pid=18906)[0m     momentum: 0.9
[2m[36m(func pid=18906)[0m     nesterov: False
[2m[36m(func pid=18906)[0m     weight_decay: 0.0001
[2m[36m(func pid=18906)[0m )
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7734 | Steps: 4 | Val loss: 2.3190 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9773 | Steps: 4 | Val loss: 2.3112 | Batch size: 32 | lr: 0.0001 | Duration: 3.31s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9283 | Steps: 4 | Val loss: 3.9408 | Batch size: 32 | lr: 0.01 | Duration: 4.88s
[2m[36m(func pid=17891)[0m top1: 0.12686567164179105
[2m[36m(func pid=17891)[0m top5: 0.4230410447761194
[2m[36m(func pid=17891)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=17891)[0m f1_macro: 0.05230987469735621
[2m[36m(func pid=17891)[0m f1_weighted: 0.057511425470581544
[2m[36m(func pid=17891)[0m f1_per_class: [0.07, 0.314, 0.052, 0.0, 0.0, 0.0, 0.0, 0.016, 0.0, 0.071]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m top1: 0.22527985074626866
[2m[36m(func pid=17787)[0m top5: 0.5876865671641791
[2m[36m(func pid=17787)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=17787)[0m f1_macro: 0.05738670134241243
[2m[36m(func pid=17787)[0m f1_weighted: 0.11056462788697378
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.0, 0.125, 0.384, 0.0, 0.0, 0.0, 0.044, 0.0, 0.021]
[2m[36m(func pid=18906)[0m top1: 0.006063432835820896
[2m[36m(func pid=18906)[0m top5: 0.3400186567164179
[2m[36m(func pid=18906)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=18906)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=18906)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6071 | Steps: 4 | Val loss: 2.2768 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:17:37 (running for 00:24:30.18)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.996 |      0.044 |                    2 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.773 |      0.052 |                    3 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=19545)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=19545)[0m Configuration completed!
[2m[36m(func pid=19545)[0m New optimizer parameters:
[2m[36m(func pid=19545)[0m SGD (
[2m[36m(func pid=19545)[0m Parameter Group 0
[2m[36m(func pid=19545)[0m     dampening: 0
[2m[36m(func pid=19545)[0m     differentiable: False
[2m[36m(func pid=19545)[0m     foreach: None
[2m[36m(func pid=19545)[0m     lr: 0.1
[2m[36m(func pid=19545)[0m     maximize: False
[2m[36m(func pid=19545)[0m     momentum: 0.9
[2m[36m(func pid=19545)[0m     nesterov: False
[2m[36m(func pid=19545)[0m     weight_decay: 0.0001
[2m[36m(func pid=19545)[0m )
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:17:43 (running for 00:24:35.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.977 |      0.057 |                    3 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.607 |      0.088 |                    4 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.928 |      0.001 |                    1 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.1707089552238806
[2m[36m(func pid=17891)[0m top5: 0.5690298507462687
[2m[36m(func pid=17891)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=17891)[0m f1_macro: 0.08780048139933035
[2m[36m(func pid=17891)[0m f1_weighted: 0.09465939338165558
[2m[36m(func pid=17891)[0m f1_per_class: [0.09, 0.341, 0.242, 0.0, 0.0, 0.068, 0.077, 0.0, 0.061, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9395 | Steps: 4 | Val loss: 2.3127 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0223 | Steps: 4 | Val loss: 7.8130 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.5888 | Steps: 4 | Val loss: 2882.5420 | Batch size: 32 | lr: 0.1 | Duration: 5.01s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5206 | Steps: 4 | Val loss: 2.1972 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=17787)[0m top1: 0.23647388059701493
[2m[36m(func pid=17787)[0m top5: 0.5778917910447762
[2m[36m(func pid=17787)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=17787)[0m f1_macro: 0.08200810384087424
[2m[36m(func pid=17787)[0m f1_weighted: 0.11567469358908752
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.011, 0.381, 0.395, 0.0, 0.0, 0.0, 0.021, 0.0, 0.013]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m top1: 0.06063432835820896
[2m[36m(func pid=18906)[0m top5: 0.4766791044776119
[2m[36m(func pid=18906)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=18906)[0m f1_macro: 0.037771285475021205
[2m[36m(func pid=18906)[0m f1_weighted: 0.06411800559026608
[2m[36m(func pid=18906)[0m f1_per_class: [0.047, 0.243, 0.0, 0.0, 0.0, 0.0, 0.071, 0.0, 0.0, 0.017]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:17:49 (running for 00:24:41.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.94  |      0.082 |                    4 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.607 |      0.088 |                    4 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  3.022 |      0.038 |                    2 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  6.589 |      0.004 |                    1 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.25093283582089554
[2m[36m(func pid=17891)[0m top5: 0.6921641791044776
[2m[36m(func pid=17891)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=17891)[0m f1_macro: 0.12854423628951275
[2m[36m(func pid=17891)[0m f1_weighted: 0.18323329831751012
[2m[36m(func pid=17891)[0m f1_per_class: [0.158, 0.334, 0.353, 0.0, 0.0, 0.0, 0.399, 0.0, 0.041, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.020522388059701493
[2m[36m(func pid=19545)[0m top5: 0.5242537313432836
[2m[36m(func pid=19545)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=19545)[0m f1_macro: 0.0040219378427787935
[2m[36m(func pid=19545)[0m f1_weighted: 0.000825397691615051
[2m[36m(func pid=19545)[0m f1_per_class: [0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9335 | Steps: 4 | Val loss: 2.3077 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.3565 | Steps: 4 | Val loss: 5.9880 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5098 | Steps: 4 | Val loss: 2.0970 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 6.8522 | Steps: 4 | Val loss: 387822.9375 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=17787)[0m top1: 0.23740671641791045
[2m[36m(func pid=17787)[0m top5: 0.574160447761194
[2m[36m(func pid=17787)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=17787)[0m f1_macro: 0.07063835288286432
[2m[36m(func pid=17787)[0m f1_weighted: 0.113520009822824
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.005, 0.281, 0.394, 0.0, 0.0, 0.0, 0.014, 0.0, 0.012]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m top1: 0.11240671641791045
[2m[36m(func pid=18906)[0m top5: 0.7322761194029851
[2m[36m(func pid=18906)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=18906)[0m f1_macro: 0.0864886901366135
[2m[36m(func pid=18906)[0m f1_weighted: 0.14568960928253732
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.142, 0.017, 0.068, 0.157, 0.23, 0.251, 0.0, 0.0, 0.0]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:17:54 (running for 00:24:46.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.933 |      0.071 |                    5 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.521 |      0.129 |                    5 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  3.356 |      0.086 |                    3 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  6.852 |      0.001 |                    2 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3069029850746269
[2m[36m(func pid=17891)[0m top5: 0.8381529850746269
[2m[36m(func pid=17891)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=17891)[0m f1_macro: 0.16012480107016475
[2m[36m(func pid=17891)[0m f1_weighted: 0.2500283666539099
[2m[36m(func pid=17891)[0m f1_per_class: [0.0, 0.418, 0.421, 0.0, 0.087, 0.0, 0.576, 0.0, 0.099, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.006063432835820896
[2m[36m(func pid=19545)[0m top5: 0.5093283582089553
[2m[36m(func pid=19545)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=19545)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=19545)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7669 | Steps: 4 | Val loss: 2.2940 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.7330 | Steps: 4 | Val loss: 6.2996 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=17787)[0m top1: 0.25513059701492535
[2m[36m(func pid=17787)[0m top5: 0.5834888059701493
[2m[36m(func pid=17787)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=17787)[0m f1_macro: 0.08018253030566684
[2m[36m(func pid=17787)[0m f1_weighted: 0.12891891495941835
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.011, 0.173, 0.412, 0.0, 0.0, 0.0, 0.189, 0.0, 0.017]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4106 | Steps: 4 | Val loss: 2.0132 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 7.3988 | Steps: 4 | Val loss: 7013.8379 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=18906)[0m top1: 0.08722014925373134
[2m[36m(func pid=18906)[0m top5: 0.6581156716417911
[2m[36m(func pid=18906)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=18906)[0m f1_macro: 0.07833985102946966
[2m[36m(func pid=18906)[0m f1_weighted: 0.09656341921489038
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.436, 0.026, 0.01, 0.136, 0.151, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:17:59 (running for 00:24:52.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.767 |      0.08  |                    6 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.51  |      0.16  |                    6 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  3.733 |      0.078 |                    4 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  7.399 |      0.044 |                    3 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.2789179104477612
[2m[36m(func pid=19545)[0m top5: 0.7845149253731343
[2m[36m(func pid=19545)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=19545)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=19545)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.259794776119403
[2m[36m(func pid=17891)[0m top5: 0.9081156716417911
[2m[36m(func pid=17891)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=17891)[0m f1_macro: 0.19075255587899645
[2m[36m(func pid=17891)[0m f1_weighted: 0.2802444958475498
[2m[36m(func pid=17891)[0m f1_per_class: [0.0, 0.243, 0.319, 0.151, 0.042, 0.0, 0.543, 0.492, 0.119, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8055 | Steps: 4 | Val loss: 2.2860 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.3049 | Steps: 4 | Val loss: 4.3641 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=17787)[0m top1: 0.26492537313432835
[2m[36m(func pid=17787)[0m top5: 0.5806902985074627
[2m[36m(func pid=17787)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=17787)[0m f1_macro: 0.09196172565302234
[2m[36m(func pid=17787)[0m f1_weighted: 0.13382328069161972
[2m[36m(func pid=17787)[0m f1_per_class: [0.0, 0.0, 0.196, 0.417, 0.0, 0.0, 0.0, 0.275, 0.0, 0.032]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2126 | Steps: 4 | Val loss: 2.0365 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 6.5956 | Steps: 4 | Val loss: 152.5565 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=18906)[0m top1: 0.06296641791044776
[2m[36m(func pid=18906)[0m top5: 0.7947761194029851
[2m[36m(func pid=18906)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=18906)[0m f1_macro: 0.06648332992540429
[2m[36m(func pid=18906)[0m f1_weighted: 0.036907299674074696
[2m[36m(func pid=18906)[0m f1_per_class: [0.049, 0.116, 0.092, 0.0, 0.208, 0.04, 0.0, 0.16, 0.0, 0.0]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:05 (running for 00:24:57.51)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.806 |      0.092 |                    7 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.411 |      0.191 |                    7 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.305 |      0.066 |                    5 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  6.596 |      0.011 |                    4 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.18936567164179105
[2m[36m(func pid=17891)[0m top5: 0.9011194029850746
[2m[36m(func pid=17891)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=17891)[0m f1_macro: 0.10475353794138723
[2m[36m(func pid=17891)[0m f1_weighted: 0.16615831523537697
[2m[36m(func pid=17891)[0m f1_per_class: [0.0, 0.084, 0.104, 0.463, 0.035, 0.0, 0.003, 0.359, 0.0, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.05783582089552239
[2m[36m(func pid=19545)[0m top5: 0.6180037313432836
[2m[36m(func pid=19545)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=19545)[0m f1_macro: 0.010934744268077601
[2m[36m(func pid=19545)[0m f1_weighted: 0.006324199110268762
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8444 | Steps: 4 | Val loss: 2.2835 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7383 | Steps: 4 | Val loss: 2.5467 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=17787)[0m top1: 0.2555970149253731
[2m[36m(func pid=17787)[0m top5: 0.5797574626865671
[2m[36m(func pid=17787)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=17787)[0m f1_macro: 0.10746409564758061
[2m[36m(func pid=17787)[0m f1_weighted: 0.1443193525518048
[2m[36m(func pid=17787)[0m f1_per_class: [0.046, 0.03, 0.136, 0.404, 0.0, 0.0, 0.0, 0.42, 0.0, 0.038]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 4.2937 | Steps: 4 | Val loss: 710.0737 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2960 | Steps: 4 | Val loss: 2.0477 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=18906)[0m top1: 0.2294776119402985
[2m[36m(func pid=18906)[0m top5: 0.8540111940298507
[2m[36m(func pid=18906)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=18906)[0m f1_macro: 0.20424682659249793
[2m[36m(func pid=18906)[0m f1_weighted: 0.22486809830497942
[2m[36m(func pid=18906)[0m f1_per_class: [0.119, 0.133, 0.478, 0.209, 0.067, 0.354, 0.256, 0.35, 0.0, 0.077]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:10 (running for 00:25:02.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.844 |      0.107 |                    8 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.213 |      0.105 |                    8 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.738 |      0.204 |                    6 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  4.294 |      0.061 |                    5 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.166044776119403
[2m[36m(func pid=19545)[0m top5: 0.4724813432835821
[2m[36m(func pid=19545)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=19545)[0m f1_macro: 0.06075781777570867
[2m[36m(func pid=19545)[0m f1_weighted: 0.12189777408292131
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.045, 0.0, 0.0, 0.0, 0.371, 0.191, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.1884328358208955
[2m[36m(func pid=17891)[0m top5: 0.8404850746268657
[2m[36m(func pid=17891)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=17891)[0m f1_macro: 0.10479328841688448
[2m[36m(func pid=17891)[0m f1_weighted: 0.1698861756724802
[2m[36m(func pid=17891)[0m f1_per_class: [0.0, 0.115, 0.092, 0.464, 0.033, 0.0, 0.0, 0.344, 0.0, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7569 | Steps: 4 | Val loss: 2.2755 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.1642 | Steps: 4 | Val loss: 4.0966 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=17787)[0m top1: 0.2462686567164179
[2m[36m(func pid=17787)[0m top5: 0.5788246268656716
[2m[36m(func pid=17787)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=17787)[0m f1_macro: 0.10912109741746581
[2m[36m(func pid=17787)[0m f1_weighted: 0.14849786474549154
[2m[36m(func pid=17787)[0m f1_per_class: [0.019, 0.061, 0.099, 0.398, 0.0, 0.0, 0.0, 0.432, 0.0, 0.082]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.5864 | Steps: 4 | Val loss: 116.7593 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2054 | Steps: 4 | Val loss: 1.9892 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=18906)[0m top1: 0.15345149253731344
[2m[36m(func pid=18906)[0m top5: 0.8628731343283582
[2m[36m(func pid=18906)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=18906)[0m f1_macro: 0.12319360034634604
[2m[36m(func pid=18906)[0m f1_weighted: 0.14976089189400996
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.304, 0.0, 0.256, 0.2, 0.112, 0.0, 0.144, 0.039, 0.177]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:16 (running for 00:25:08.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.757 |      0.109 |                    9 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.296 |      0.105 |                    9 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.164 |      0.123 |                    7 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  5.586 |      0.084 |                    6 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.24486940298507462
[2m[36m(func pid=19545)[0m top5: 0.45755597014925375
[2m[36m(func pid=19545)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=19545)[0m f1_macro: 0.08375123178399133
[2m[36m(func pid=19545)[0m f1_weighted: 0.17315129657179287
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.066, 0.0, 0.163, 0.0, 0.569, 0.032, 0.008, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.2462686567164179
[2m[36m(func pid=17891)[0m top5: 0.8423507462686567
[2m[36m(func pid=17891)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=17891)[0m f1_macro: 0.17394808642202483
[2m[36m(func pid=17891)[0m f1_weighted: 0.21458707014203404
[2m[36m(func pid=17891)[0m f1_per_class: [0.22, 0.152, 0.3, 0.515, 0.041, 0.153, 0.0, 0.36, 0.0, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6984 | Steps: 4 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1999 | Steps: 4 | Val loss: 3.3608 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=17787)[0m top1: 0.2513992537313433
[2m[36m(func pid=17787)[0m top5: 0.6105410447761194
[2m[36m(func pid=17787)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=17787)[0m f1_macro: 0.11202647421046823
[2m[36m(func pid=17787)[0m f1_weighted: 0.1620735736645297
[2m[36m(func pid=17787)[0m f1_per_class: [0.041, 0.111, 0.091, 0.415, 0.0, 0.0, 0.0, 0.44, 0.0, 0.022]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.8063 | Steps: 4 | Val loss: 9.3192 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1605 | Steps: 4 | Val loss: 1.9291 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=18906)[0m top1: 0.16884328358208955
[2m[36m(func pid=18906)[0m top5: 0.7597947761194029
[2m[36m(func pid=18906)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=18906)[0m f1_macro: 0.12232015554216666
[2m[36m(func pid=18906)[0m f1_weighted: 0.20024700726371145
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.167, 0.0, 0.013, 0.123, 0.0, 0.483, 0.369, 0.035, 0.032]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:21 (running for 00:25:13.78)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.698 |      0.112 |                   10 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.205 |      0.174 |                   10 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.2   |      0.122 |                    8 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.806 |      0.031 |                    7 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.055970149253731345
[2m[36m(func pid=19545)[0m top5: 0.5685634328358209
[2m[36m(func pid=19545)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=19545)[0m f1_macro: 0.031295372075956375
[2m[36m(func pid=19545)[0m f1_weighted: 0.04095902786914303
[2m[36m(func pid=19545)[0m f1_per_class: [0.039, 0.0, 0.082, 0.0, 0.0, 0.0, 0.126, 0.0, 0.067, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.25279850746268656
[2m[36m(func pid=17891)[0m top5: 0.8297574626865671
[2m[36m(func pid=17891)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=17891)[0m f1_macro: 0.21309730844395053
[2m[36m(func pid=17891)[0m f1_weighted: 0.2355631666135398
[2m[36m(func pid=17891)[0m f1_per_class: [0.157, 0.43, 0.609, 0.434, 0.07, 0.144, 0.0, 0.287, 0.0, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6703 | Steps: 4 | Val loss: 2.2567 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8959 | Steps: 4 | Val loss: 4.2226 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=17787)[0m top1: 0.23880597014925373
[2m[36m(func pid=17787)[0m top5: 0.6263992537313433
[2m[36m(func pid=17787)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=17787)[0m f1_macro: 0.12416422284927842
[2m[36m(func pid=17787)[0m f1_weighted: 0.16450002505195754
[2m[36m(func pid=17787)[0m f1_per_class: [0.08, 0.126, 0.106, 0.405, 0.0, 0.008, 0.0, 0.433, 0.027, 0.057]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1588 | Steps: 4 | Val loss: 1.9264 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.1454 | Steps: 4 | Val loss: 3.3083 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=18906)[0m top1: 0.14738805970149255
[2m[36m(func pid=18906)[0m top5: 0.7705223880597015
[2m[36m(func pid=18906)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=18906)[0m f1_macro: 0.12131128702807166
[2m[36m(func pid=18906)[0m f1_weighted: 0.12278890573462253
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.154, 0.0, 0.141, 0.2, 0.333, 0.0, 0.246, 0.075, 0.064]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:27 (running for 00:25:19.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.67  |      0.124 |                   11 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.159 |      0.212 |                   12 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.896 |      0.121 |                    9 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.806 |      0.031 |                    7 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.279384328358209
[2m[36m(func pid=17891)[0m top5: 0.8372201492537313
[2m[36m(func pid=17891)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=17891)[0m f1_macro: 0.21207642200588944
[2m[36m(func pid=17891)[0m f1_weighted: 0.24198409361378137
[2m[36m(func pid=17891)[0m f1_per_class: [0.192, 0.513, 0.571, 0.447, 0.096, 0.063, 0.0, 0.238, 0.0, 0.0]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.1599813432835821
[2m[36m(func pid=19545)[0m top5: 0.6399253731343284
[2m[36m(func pid=19545)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=19545)[0m f1_macro: 0.11270211675822897
[2m[36m(func pid=19545)[0m f1_weighted: 0.156090067970299
[2m[36m(func pid=19545)[0m f1_per_class: [0.134, 0.0, 0.364, 0.0, 0.0, 0.0, 0.485, 0.069, 0.074, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6918 | Steps: 4 | Val loss: 2.2478 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.9510 | Steps: 4 | Val loss: 8.1868 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=17787)[0m top1: 0.22901119402985073
[2m[36m(func pid=17787)[0m top5: 0.6427238805970149
[2m[36m(func pid=17787)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=17787)[0m f1_macro: 0.1345867562318
[2m[36m(func pid=17787)[0m f1_weighted: 0.18368156267156205
[2m[36m(func pid=17787)[0m f1_per_class: [0.082, 0.24, 0.142, 0.405, 0.0, 0.016, 0.003, 0.395, 0.026, 0.036]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.2071 | Steps: 4 | Val loss: 1.8913 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5560 | Steps: 4 | Val loss: 3.5465 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=18906)[0m top1: 0.18516791044776118
[2m[36m(func pid=18906)[0m top5: 0.5601679104477612
[2m[36m(func pid=18906)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=18906)[0m f1_macro: 0.14789916475589968
[2m[36m(func pid=18906)[0m f1_weighted: 0.16831894519778876
[2m[36m(func pid=18906)[0m f1_per_class: [0.103, 0.385, 0.0, 0.139, 0.0, 0.332, 0.0, 0.324, 0.092, 0.102]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:32 (running for 00:25:24.60)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.692 |      0.135 |                   12 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.207 |      0.172 |                   13 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.951 |      0.148 |                   10 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.145 |      0.113 |                    8 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.2224813432835821
[2m[36m(func pid=17891)[0m top5: 0.8409514925373134
[2m[36m(func pid=17891)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=17891)[0m f1_macro: 0.17187906504442707
[2m[36m(func pid=17891)[0m f1_weighted: 0.14072103282506826
[2m[36m(func pid=17891)[0m f1_per_class: [0.243, 0.453, 0.373, 0.115, 0.106, 0.063, 0.0, 0.242, 0.0, 0.123]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.16791044776119404
[2m[36m(func pid=19545)[0m top5: 0.7201492537313433
[2m[36m(func pid=19545)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=19545)[0m f1_macro: 0.10800501316045927
[2m[36m(func pid=19545)[0m f1_weighted: 0.16599364288249258
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.163, 0.245, 0.049, 0.0, 0.261, 0.265, 0.097, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6445 | Steps: 4 | Val loss: 2.2371 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.9216 | Steps: 4 | Val loss: 9.4530 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=17787)[0m top1: 0.228544776119403
[2m[36m(func pid=17787)[0m top5: 0.6651119402985075
[2m[36m(func pid=17787)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=17787)[0m f1_macro: 0.1425399190582
[2m[36m(func pid=17787)[0m f1_weighted: 0.19253861472047096
[2m[36m(func pid=17787)[0m f1_per_class: [0.092, 0.283, 0.168, 0.401, 0.0, 0.039, 0.003, 0.39, 0.027, 0.023]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0404 | Steps: 4 | Val loss: 1.8672 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7093 | Steps: 4 | Val loss: 2.1053 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=18906)[0m top1: 0.17583955223880596
[2m[36m(func pid=18906)[0m top5: 0.6492537313432836
[2m[36m(func pid=18906)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=18906)[0m f1_macro: 0.12845791886201635
[2m[36m(func pid=18906)[0m f1_weighted: 0.11623450308009395
[2m[36m(func pid=18906)[0m f1_per_class: [0.068, 0.457, 0.047, 0.013, 0.15, 0.082, 0.0, 0.323, 0.06, 0.084]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:37 (running for 00:25:30.02)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.645 |      0.143 |                   13 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  2.04  |      0.194 |                   14 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  3.922 |      0.128 |                   11 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.556 |      0.108 |                    9 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.22667910447761194
[2m[36m(func pid=17891)[0m top5: 0.8460820895522388
[2m[36m(func pid=17891)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=17891)[0m f1_macro: 0.194090243286062
[2m[36m(func pid=17891)[0m f1_weighted: 0.14865463143597024
[2m[36m(func pid=17891)[0m f1_per_class: [0.267, 0.456, 0.513, 0.047, 0.113, 0.087, 0.062, 0.304, 0.026, 0.067]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7113 | Steps: 4 | Val loss: 2.2224 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=19545)[0m top1: 0.14972014925373134
[2m[36m(func pid=19545)[0m top5: 0.7583955223880597
[2m[36m(func pid=19545)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=19545)[0m f1_macro: 0.15688309256779287
[2m[36m(func pid=19545)[0m f1_weighted: 0.12564848804174947
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.75, 0.351, 0.031, 0.0, 0.0, 0.349, 0.088, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2249 | Steps: 4 | Val loss: 4.6617 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=17787)[0m top1: 0.23134328358208955
[2m[36m(func pid=17787)[0m top5: 0.7010261194029851
[2m[36m(func pid=17787)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=17787)[0m f1_macro: 0.14458178022330206
[2m[36m(func pid=17787)[0m f1_weighted: 0.19300805936077775
[2m[36m(func pid=17787)[0m f1_per_class: [0.084, 0.258, 0.185, 0.401, 0.0, 0.054, 0.012, 0.395, 0.028, 0.03]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7840 | Steps: 4 | Val loss: 1.8708 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.0271 | Steps: 4 | Val loss: 2.1047 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=18906)[0m top1: 0.283115671641791
[2m[36m(func pid=18906)[0m top5: 0.8101679104477612
[2m[36m(func pid=18906)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=18906)[0m f1_macro: 0.2709230168771612
[2m[36m(func pid=18906)[0m f1_weighted: 0.2573668614125464
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.38, 0.833, 0.0, 0.069, 0.0, 0.501, 0.504, 0.136, 0.286]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.32369402985074625
[2m[36m(func pid=17891)[0m top5: 0.8339552238805971
[2m[36m(func pid=17891)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=17891)[0m f1_macro: 0.2836020179059735
[2m[36m(func pid=17891)[0m f1_weighted: 0.30300646429798994
[2m[36m(func pid=17891)[0m f1_per_class: [0.207, 0.483, 0.727, 0.019, 0.1, 0.157, 0.517, 0.538, 0.036, 0.051]
== Status ==
Current time: 2024-01-07 13:18:43 (running for 00:25:35.50)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.711 |      0.145 |                   14 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.784 |      0.284 |                   15 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.225 |      0.271 |                   12 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.709 |      0.157 |                   10 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6109 | Steps: 4 | Val loss: 2.2006 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=19545)[0m top1: 0.14505597014925373
[2m[36m(func pid=19545)[0m top5: 0.7056902985074627
[2m[36m(func pid=19545)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=19545)[0m f1_macro: 0.14738108425602245
[2m[36m(func pid=19545)[0m f1_weighted: 0.13968398616990838
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.556, 0.389, 0.022, 0.0, 0.0, 0.44, 0.067, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0575 | Steps: 4 | Val loss: 6.8542 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=17787)[0m top1: 0.2635261194029851
[2m[36m(func pid=17787)[0m top5: 0.7392723880597015
[2m[36m(func pid=17787)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=17787)[0m f1_macro: 0.1733850784909094
[2m[36m(func pid=17787)[0m f1_weighted: 0.2248032811173233
[2m[36m(func pid=17787)[0m f1_per_class: [0.111, 0.329, 0.25, 0.434, 0.057, 0.052, 0.041, 0.405, 0.027, 0.026]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9879 | Steps: 4 | Val loss: 1.7267 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6674 | Steps: 4 | Val loss: 1.9780 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=18906)[0m top1: 0.17257462686567165
[2m[36m(func pid=18906)[0m top5: 0.6963619402985075
[2m[36m(func pid=18906)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=18906)[0m f1_macro: 0.1480070654041318
[2m[36m(func pid=18906)[0m f1_weighted: 0.13332643077031614
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.42, 0.0, 0.0, 0.05, 0.202, 0.006, 0.52, 0.108, 0.174]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:48 (running for 00:25:40.87)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.611 |      0.173 |                   15 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.988 |      0.27  |                   16 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.057 |      0.148 |                   13 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  4.027 |      0.147 |                   11 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.36800373134328357
[2m[36m(func pid=17891)[0m top5: 0.8423507462686567
[2m[36m(func pid=17891)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=17891)[0m f1_macro: 0.2701432736208556
[2m[36m(func pid=17891)[0m f1_weighted: 0.33582052875683327
[2m[36m(func pid=17891)[0m f1_per_class: [0.272, 0.475, 0.688, 0.141, 0.083, 0.212, 0.591, 0.0, 0.049, 0.19]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.2733208955223881
[2m[36m(func pid=19545)[0m top5: 0.7649253731343284
[2m[36m(func pid=19545)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=19545)[0m f1_macro: 0.1557810482272751
[2m[36m(func pid=19545)[0m f1_weighted: 0.30486711512799736
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.0, 0.456, 0.028, 0.0, 0.485, 0.544, 0.044, 0.0]
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5999 | Steps: 4 | Val loss: 2.1794 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8957 | Steps: 4 | Val loss: 4.8876 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.8292 | Steps: 4 | Val loss: 1.7916 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=17787)[0m top1: 0.29850746268656714
[2m[36m(func pid=17787)[0m top5: 0.7747201492537313
[2m[36m(func pid=17787)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=17787)[0m f1_macro: 0.19022449289735496
[2m[36m(func pid=17787)[0m f1_weighted: 0.24972825158167222
[2m[36m(func pid=17787)[0m f1_per_class: [0.126, 0.386, 0.273, 0.459, 0.074, 0.072, 0.058, 0.422, 0.0, 0.032]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7879 | Steps: 4 | Val loss: 1.8597 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=18906)[0m top1: 0.10401119402985075
[2m[36m(func pid=18906)[0m top5: 0.6571828358208955
[2m[36m(func pid=18906)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=18906)[0m f1_macro: 0.1293317345874249
[2m[36m(func pid=18906)[0m f1_weighted: 0.09837904440333585
[2m[36m(func pid=18906)[0m f1_per_class: [0.044, 0.319, 0.154, 0.0, 0.076, 0.106, 0.0, 0.433, 0.094, 0.068]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:53 (running for 00:25:46.01)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.6   |      0.19  |                   16 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.829 |      0.253 |                   17 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.896 |      0.129 |                   14 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.667 |      0.156 |                   12 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.384794776119403
[2m[36m(func pid=17891)[0m top5: 0.8414179104477612
[2m[36m(func pid=17891)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=17891)[0m f1_macro: 0.2529073346938524
[2m[36m(func pid=17891)[0m f1_weighted: 0.4165565425187144
[2m[36m(func pid=17891)[0m f1_per_class: [0.32, 0.498, 0.14, 0.456, 0.069, 0.244, 0.546, 0.0, 0.106, 0.15]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.2896455223880597
[2m[36m(func pid=19545)[0m top5: 0.7933768656716418
[2m[36m(func pid=19545)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=19545)[0m f1_macro: 0.16955112002627354
[2m[36m(func pid=19545)[0m f1_weighted: 0.2763442957020275
[2m[36m(func pid=19545)[0m f1_per_class: [0.044, 0.0, 0.667, 0.314, 0.0, 0.0, 0.613, 0.0, 0.027, 0.031]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6262 | Steps: 4 | Val loss: 2.1759 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4100 | Steps: 4 | Val loss: 3.3975 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8366 | Steps: 4 | Val loss: 1.8673 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=17787)[0m top1: 0.28078358208955223
[2m[36m(func pid=17787)[0m top5: 0.7826492537313433
[2m[36m(func pid=17787)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=17787)[0m f1_macro: 0.19550484644489033
[2m[36m(func pid=17787)[0m f1_weighted: 0.2414011508870808
[2m[36m(func pid=17787)[0m f1_per_class: [0.117, 0.385, 0.276, 0.391, 0.087, 0.107, 0.072, 0.469, 0.0, 0.051]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.9780 | Steps: 4 | Val loss: 1.9197 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=18906)[0m top1: 0.2537313432835821
[2m[36m(func pid=18906)[0m top5: 0.7691231343283582
[2m[36m(func pid=18906)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=18906)[0m f1_macro: 0.217254557177906
[2m[36m(func pid=18906)[0m f1_weighted: 0.25245467876112077
[2m[36m(func pid=18906)[0m f1_per_class: [0.086, 0.188, 0.583, 0.0, 0.096, 0.135, 0.598, 0.306, 0.026, 0.154]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:18:59 (running for 00:25:51.32)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.626 |      0.196 |                   17 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.837 |      0.264 |                   18 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.41  |      0.217 |                   15 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.788 |      0.17  |                   13 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3344216417910448
[2m[36m(func pid=17891)[0m top5: 0.808768656716418
[2m[36m(func pid=17891)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=17891)[0m f1_macro: 0.26436859017634917
[2m[36m(func pid=17891)[0m f1_weighted: 0.35304213750916386
[2m[36m(func pid=17891)[0m f1_per_class: [0.215, 0.534, 0.255, 0.399, 0.057, 0.171, 0.302, 0.527, 0.063, 0.12]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.19263059701492538
[2m[36m(func pid=19545)[0m top5: 0.7728544776119403
[2m[36m(func pid=19545)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=19545)[0m f1_macro: 0.16999948690440322
[2m[36m(func pid=19545)[0m f1_weighted: 0.21626879270167393
[2m[36m(func pid=19545)[0m f1_per_class: [0.038, 0.0, 0.529, 0.247, 0.0, 0.0, 0.389, 0.466, 0.0, 0.03]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5950 | Steps: 4 | Val loss: 2.1695 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.0726 | Steps: 4 | Val loss: 2.8392 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9599 | Steps: 4 | Val loss: 2.0316 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=17787)[0m top1: 0.2644589552238806
[2m[36m(func pid=17787)[0m top5: 0.7985074626865671
[2m[36m(func pid=17787)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=17787)[0m f1_macro: 0.15946766246022284
[2m[36m(func pid=17787)[0m f1_weighted: 0.2172817054810761
[2m[36m(func pid=17787)[0m f1_per_class: [0.093, 0.393, 0.163, 0.347, 0.0, 0.094, 0.042, 0.463, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.4595 | Steps: 4 | Val loss: 1.8534 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=18906)[0m top1: 0.251865671641791
[2m[36m(func pid=18906)[0m top5: 0.7873134328358209
[2m[36m(func pid=18906)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=18906)[0m f1_macro: 0.2563459137594254
[2m[36m(func pid=18906)[0m f1_weighted: 0.2928165326191404
[2m[36m(func pid=18906)[0m f1_per_class: [0.085, 0.513, 0.783, 0.286, 0.069, 0.111, 0.284, 0.302, 0.067, 0.063]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:04 (running for 00:25:56.62)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.595 |      0.159 |                   18 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.96  |      0.189 |                   19 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.073 |      0.256 |                   16 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.978 |      0.17  |                   14 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.24953358208955223
[2m[36m(func pid=17891)[0m top5: 0.7723880597014925
[2m[36m(func pid=17891)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=17891)[0m f1_macro: 0.18915237621805556
[2m[36m(func pid=17891)[0m f1_weighted: 0.19277506164926905
[2m[36m(func pid=17891)[0m f1_per_class: [0.113, 0.488, 0.197, 0.197, 0.062, 0.155, 0.0, 0.462, 0.133, 0.084]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.3125
[2m[36m(func pid=19545)[0m top5: 0.7719216417910447
[2m[36m(func pid=19545)[0m f1_micro: 0.3125
[2m[36m(func pid=19545)[0m f1_macro: 0.24014358449137246
[2m[36m(func pid=19545)[0m f1_weighted: 0.2983019825567101
[2m[36m(func pid=19545)[0m f1_per_class: [0.098, 0.0, 0.632, 0.455, 0.0, 0.354, 0.321, 0.496, 0.0, 0.045]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5349 | Steps: 4 | Val loss: 2.1605 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3308 | Steps: 4 | Val loss: 2.3012 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.7065 | Steps: 4 | Val loss: 2.1108 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.1009 | Steps: 4 | Val loss: 1.9933 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=17787)[0m top1: 0.27798507462686567
[2m[36m(func pid=17787)[0m top5: 0.8311567164179104
[2m[36m(func pid=17787)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=17787)[0m f1_macro: 0.18731595030757753
[2m[36m(func pid=17787)[0m f1_weighted: 0.24231012830273713
[2m[36m(func pid=17787)[0m f1_per_class: [0.097, 0.435, 0.111, 0.343, 0.178, 0.143, 0.091, 0.406, 0.0, 0.069]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m top1: 0.3558768656716418
[2m[36m(func pid=18906)[0m top5: 0.8488805970149254
[2m[36m(func pid=18906)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=18906)[0m f1_macro: 0.2619376050368205
[2m[36m(func pid=18906)[0m f1_weighted: 0.3736544801817688
[2m[36m(func pid=18906)[0m f1_per_class: [0.043, 0.533, 0.449, 0.163, 0.038, 0.286, 0.581, 0.409, 0.076, 0.041]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.23134328358208955
[2m[36m(func pid=17891)[0m top5: 0.7723880597014925
[2m[36m(func pid=17891)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=17891)[0m f1_macro: 0.21714409387090616
[2m[36m(func pid=17891)[0m f1_weighted: 0.15928491802709405
[2m[36m(func pid=17891)[0m f1_per_class: [0.278, 0.445, 0.44, 0.072, 0.105, 0.163, 0.009, 0.466, 0.126, 0.067]
[2m[36m(func pid=17891)[0m 
== Status ==
Current time: 2024-01-07 13:19:09 (running for 00:26:02.00)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.535 |      0.187 |                   19 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.707 |      0.217 |                   20 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.331 |      0.262 |                   17 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.459 |      0.24  |                   15 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.2896455223880597
[2m[36m(func pid=19545)[0m top5: 0.7322761194029851
[2m[36m(func pid=19545)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=19545)[0m f1_macro: 0.20479216823587656
[2m[36m(func pid=19545)[0m f1_weighted: 0.22127250110528798
[2m[36m(func pid=19545)[0m f1_per_class: [0.102, 0.031, 0.556, 0.539, 0.0, 0.305, 0.0, 0.402, 0.028, 0.085]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4865 | Steps: 4 | Val loss: 2.1514 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7712 | Steps: 4 | Val loss: 2.7542 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.5275 | Steps: 4 | Val loss: 1.8906 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=17787)[0m top1: 0.29244402985074625
[2m[36m(func pid=17787)[0m top5: 0.8404850746268657
[2m[36m(func pid=17787)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=17787)[0m f1_macro: 0.19466323050213116
[2m[36m(func pid=17787)[0m f1_weighted: 0.2616601390873087
[2m[36m(func pid=17787)[0m f1_per_class: [0.101, 0.459, 0.114, 0.333, 0.154, 0.279, 0.103, 0.404, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5791 | Steps: 4 | Val loss: 1.9505 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=18906)[0m top1: 0.2126865671641791
[2m[36m(func pid=18906)[0m top5: 0.8353544776119403
[2m[36m(func pid=18906)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=18906)[0m f1_macro: 0.16043913233093623
[2m[36m(func pid=18906)[0m f1_weighted: 0.22460893935030482
[2m[36m(func pid=18906)[0m f1_per_class: [0.23, 0.339, 0.0, 0.443, 0.07, 0.155, 0.034, 0.062, 0.142, 0.13]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:15 (running for 00:26:07.46)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.487 |      0.195 |                   20 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.528 |      0.315 |                   21 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.771 |      0.16  |                   18 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.101 |      0.205 |                   16 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3003731343283582
[2m[36m(func pid=17891)[0m top5: 0.8269589552238806
[2m[36m(func pid=17891)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=17891)[0m f1_macro: 0.3147646284828782
[2m[36m(func pid=17891)[0m f1_weighted: 0.28476088316623027
[2m[36m(func pid=17891)[0m f1_per_class: [0.299, 0.501, 0.667, 0.165, 0.087, 0.272, 0.24, 0.543, 0.162, 0.212]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.34375
[2m[36m(func pid=19545)[0m top5: 0.7817164179104478
[2m[36m(func pid=19545)[0m f1_micro: 0.34375
[2m[36m(func pid=19545)[0m f1_macro: 0.14686897692459366
[2m[36m(func pid=19545)[0m f1_weighted: 0.22550801949041505
[2m[36m(func pid=19545)[0m f1_per_class: [0.038, 0.0, 0.0, 0.562, 0.0, 0.324, 0.0, 0.517, 0.027, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6049 | Steps: 4 | Val loss: 2.1532 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.9780 | Steps: 4 | Val loss: 3.2369 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.4854 | Steps: 4 | Val loss: 1.8281 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=17787)[0m top1: 0.2798507462686567
[2m[36m(func pid=17787)[0m top5: 0.8498134328358209
[2m[36m(func pid=17787)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=17787)[0m f1_macro: 0.19371388694709876
[2m[36m(func pid=17787)[0m f1_weighted: 0.25385983955925745
[2m[36m(func pid=17787)[0m f1_per_class: [0.094, 0.462, 0.126, 0.28, 0.156, 0.291, 0.119, 0.411, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9791 | Steps: 4 | Val loss: 1.8566 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=18906)[0m top1: 0.19776119402985073
[2m[36m(func pid=18906)[0m top5: 0.8535447761194029
[2m[36m(func pid=18906)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=18906)[0m f1_macro: 0.2114855938185985
[2m[36m(func pid=18906)[0m f1_weighted: 0.21302538692200895
[2m[36m(func pid=18906)[0m f1_per_class: [0.185, 0.172, 0.571, 0.448, 0.092, 0.12, 0.057, 0.268, 0.1, 0.103]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.33488805970149255
[2m[36m(func pid=17891)[0m top5: 0.8465485074626866
[2m[36m(func pid=17891)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=17891)[0m f1_macro: 0.3011728741852666
[2m[36m(func pid=17891)[0m f1_weighted: 0.35075962565696595
[2m[36m(func pid=17891)[0m f1_per_class: [0.218, 0.506, 0.412, 0.184, 0.101, 0.312, 0.45, 0.488, 0.132, 0.21]
[2m[36m(func pid=17891)[0m 
== Status ==
Current time: 2024-01-07 13:19:20 (running for 00:26:12.89)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.194 |                   21 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.485 |      0.301 |                   22 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.978 |      0.211 |                   19 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.579 |      0.147 |                   17 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.32276119402985076
[2m[36m(func pid=19545)[0m top5: 0.8540111940298507
[2m[36m(func pid=19545)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=19545)[0m f1_macro: 0.21744482052587405
[2m[36m(func pid=19545)[0m f1_weighted: 0.23753300494574464
[2m[36m(func pid=19545)[0m f1_per_class: [0.083, 0.027, 0.667, 0.574, 0.0, 0.387, 0.0, 0.346, 0.091, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4905 | Steps: 4 | Val loss: 2.1440 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.8733 | Steps: 4 | Val loss: 2.4672 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.5168 | Steps: 4 | Val loss: 2.1671 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=17787)[0m top1: 0.28638059701492535
[2m[36m(func pid=17787)[0m top5: 0.8488805970149254
[2m[36m(func pid=17787)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=17787)[0m f1_macro: 0.20745193775631457
[2m[36m(func pid=17787)[0m f1_weighted: 0.2622623422319929
[2m[36m(func pid=17787)[0m f1_per_class: [0.109, 0.463, 0.18, 0.274, 0.186, 0.324, 0.139, 0.4, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4429 | Steps: 4 | Val loss: 1.8593 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=18906)[0m top1: 0.25093283582089554
[2m[36m(func pid=18906)[0m top5: 0.8796641791044776
[2m[36m(func pid=18906)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=18906)[0m f1_macro: 0.2563100439004749
[2m[36m(func pid=18906)[0m f1_weighted: 0.25516223308013447
[2m[36m(func pid=18906)[0m f1_per_class: [0.225, 0.424, 0.632, 0.306, 0.064, 0.0, 0.197, 0.369, 0.179, 0.167]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:25 (running for 00:26:18.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.491 |      0.207 |                   22 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.517 |      0.261 |                   23 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.873 |      0.256 |                   20 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.979 |      0.217 |                   18 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.2513992537313433
[2m[36m(func pid=17891)[0m top5: 0.8227611940298507
[2m[36m(func pid=17891)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=17891)[0m f1_macro: 0.2612260555928033
[2m[36m(func pid=17891)[0m f1_weighted: 0.29882868761736414
[2m[36m(func pid=17891)[0m f1_per_class: [0.158, 0.477, 0.571, 0.253, 0.106, 0.12, 0.314, 0.516, 0.031, 0.067]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.2453358208955224
[2m[36m(func pid=19545)[0m top5: 0.8297574626865671
[2m[36m(func pid=19545)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=19545)[0m f1_macro: 0.22828967404120473
[2m[36m(func pid=19545)[0m f1_weighted: 0.24996558362030816
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.289, 0.692, 0.281, 0.051, 0.266, 0.201, 0.417, 0.086, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4413 | Steps: 4 | Val loss: 2.1381 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2015 | Steps: 4 | Val loss: 2.8330 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4644 | Steps: 4 | Val loss: 1.9402 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=17787)[0m top1: 0.2896455223880597
[2m[36m(func pid=17787)[0m top5: 0.851679104477612
[2m[36m(func pid=17787)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=17787)[0m f1_macro: 0.20908334920480418
[2m[36m(func pid=17787)[0m f1_weighted: 0.27194797690001
[2m[36m(func pid=17787)[0m f1_per_class: [0.119, 0.491, 0.205, 0.349, 0.186, 0.284, 0.109, 0.347, 0.0, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2531 | Steps: 4 | Val loss: 1.8521 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=18906)[0m top1: 0.2248134328358209
[2m[36m(func pid=18906)[0m top5: 0.8498134328358209
[2m[36m(func pid=18906)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=18906)[0m f1_macro: 0.2528017668086058
[2m[36m(func pid=18906)[0m f1_weighted: 0.18178750428898308
[2m[36m(func pid=18906)[0m f1_per_class: [0.268, 0.473, 0.75, 0.09, 0.051, 0.0, 0.114, 0.399, 0.153, 0.23]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:31 (running for 00:26:23.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.441 |      0.209 |                   23 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.464 |      0.293 |                   24 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.202 |      0.253 |                   21 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.443 |      0.228 |                   19 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.31716417910447764
[2m[36m(func pid=17891)[0m top5: 0.8456156716417911
[2m[36m(func pid=17891)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=17891)[0m f1_macro: 0.2925183771071672
[2m[36m(func pid=17891)[0m f1_weighted: 0.35373957037412407
[2m[36m(func pid=17891)[0m f1_per_class: [0.226, 0.473, 0.533, 0.379, 0.098, 0.227, 0.334, 0.523, 0.035, 0.097]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.21595149253731344
[2m[36m(func pid=19545)[0m top5: 0.8292910447761194
[2m[36m(func pid=19545)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=19545)[0m f1_macro: 0.19523848963670845
[2m[36m(func pid=19545)[0m f1_weighted: 0.19546118158204415
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.299, 0.692, 0.081, 0.049, 0.205, 0.242, 0.338, 0.045, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5304 | Steps: 4 | Val loss: 2.1437 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8777 | Steps: 4 | Val loss: 2.4928 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.9420 | Steps: 4 | Val loss: 1.7748 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=17787)[0m top1: 0.27705223880597013
[2m[36m(func pid=17787)[0m top5: 0.8493470149253731
[2m[36m(func pid=17787)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=17787)[0m f1_macro: 0.21199159778286508
[2m[36m(func pid=17787)[0m f1_weighted: 0.2741994478649875
[2m[36m(func pid=17787)[0m f1_per_class: [0.119, 0.469, 0.16, 0.384, 0.133, 0.222, 0.114, 0.35, 0.022, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.8779 | Steps: 4 | Val loss: 1.9109 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=18906)[0m top1: 0.2933768656716418
[2m[36m(func pid=18906)[0m top5: 0.8722014925373134
[2m[36m(func pid=18906)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=18906)[0m f1_macro: 0.21480026127748345
[2m[36m(func pid=18906)[0m f1_weighted: 0.2567993482003091
[2m[36m(func pid=18906)[0m f1_per_class: [0.25, 0.539, 0.0, 0.345, 0.047, 0.306, 0.0, 0.373, 0.124, 0.165]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:36 (running for 00:26:28.83)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.53  |      0.212 |                   24 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.942 |      0.335 |                   25 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.878 |      0.215 |                   22 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.253 |      0.195 |                   20 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.39132462686567165
[2m[36m(func pid=17891)[0m top5: 0.8619402985074627
[2m[36m(func pid=17891)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=17891)[0m f1_macro: 0.33470588183305033
[2m[36m(func pid=17891)[0m f1_weighted: 0.42087374365641045
[2m[36m(func pid=17891)[0m f1_per_class: [0.219, 0.442, 0.453, 0.478, 0.074, 0.321, 0.433, 0.511, 0.164, 0.252]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.28078358208955223
[2m[36m(func pid=19545)[0m top5: 0.8101679104477612
[2m[36m(func pid=19545)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=19545)[0m f1_macro: 0.22209760860891437
[2m[36m(func pid=19545)[0m f1_weighted: 0.2810892773443245
[2m[36m(func pid=19545)[0m f1_per_class: [0.04, 0.281, 0.6, 0.487, 0.054, 0.176, 0.167, 0.349, 0.053, 0.013]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5110 | Steps: 4 | Val loss: 2.1354 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4160 | Steps: 4 | Val loss: 2.2185 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3998 | Steps: 4 | Val loss: 2.1132 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=17787)[0m top1: 0.26119402985074625
[2m[36m(func pid=17787)[0m top5: 0.847481343283582
[2m[36m(func pid=17787)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=17787)[0m f1_macro: 0.21318394952319575
[2m[36m(func pid=17787)[0m f1_weighted: 0.26222439131882597
[2m[36m(func pid=17787)[0m f1_per_class: [0.137, 0.411, 0.218, 0.361, 0.097, 0.281, 0.103, 0.347, 0.044, 0.133]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4616 | Steps: 4 | Val loss: 1.9156 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=18906)[0m top1: 0.3064365671641791
[2m[36m(func pid=18906)[0m top5: 0.8838619402985075
[2m[36m(func pid=18906)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=18906)[0m f1_macro: 0.29718587016456655
[2m[36m(func pid=18906)[0m f1_weighted: 0.272255415385896
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.55, 0.786, 0.386, 0.029, 0.28, 0.0, 0.444, 0.075, 0.423]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:41 (running for 00:26:34.23)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.511 |      0.213 |                   25 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.4   |      0.242 |                   26 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.416 |      0.297 |                   23 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  3.878 |      0.222 |                   21 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3003731343283582
[2m[36m(func pid=17891)[0m top5: 0.8269589552238806
[2m[36m(func pid=17891)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=17891)[0m f1_macro: 0.24204766233272657
[2m[36m(func pid=17891)[0m f1_weighted: 0.3406997315964127
[2m[36m(func pid=17891)[0m f1_per_class: [0.042, 0.462, 0.293, 0.428, 0.077, 0.209, 0.297, 0.357, 0.133, 0.125]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.28824626865671643
[2m[36m(func pid=19545)[0m top5: 0.804570895522388
[2m[36m(func pid=19545)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=19545)[0m f1_macro: 0.24628001154296145
[2m[36m(func pid=19545)[0m f1_weighted: 0.2597066479787357
[2m[36m(func pid=19545)[0m f1_per_class: [0.075, 0.31, 0.706, 0.507, 0.069, 0.278, 0.003, 0.392, 0.079, 0.044]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3831 | Steps: 4 | Val loss: 2.1299 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0466 | Steps: 4 | Val loss: 2.3006 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9318 | Steps: 4 | Val loss: 2.1248 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=17787)[0m top1: 0.2555970149253731
[2m[36m(func pid=17787)[0m top5: 0.8484141791044776
[2m[36m(func pid=17787)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=17787)[0m f1_macro: 0.2122604270004241
[2m[36m(func pid=17787)[0m f1_weighted: 0.24686312567140048
[2m[36m(func pid=17787)[0m f1_per_class: [0.133, 0.422, 0.239, 0.325, 0.085, 0.313, 0.062, 0.383, 0.024, 0.138]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1956 | Steps: 4 | Val loss: 1.9299 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:19:47 (running for 00:26:39.44)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.383 |      0.212 |                   26 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.4   |      0.242 |                   26 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.047 |      0.231 |                   24 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.462 |      0.246 |                   22 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.3260261194029851
[2m[36m(func pid=18906)[0m top5: 0.8698694029850746
[2m[36m(func pid=18906)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=18906)[0m f1_macro: 0.2305919712669294
[2m[36m(func pid=18906)[0m f1_weighted: 0.3378632321645411
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.347, 0.0, 0.113, 0.026, 0.277, 0.601, 0.461, 0.158, 0.323]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.269589552238806
[2m[36m(func pid=17891)[0m top5: 0.8339552238805971
[2m[36m(func pid=17891)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=17891)[0m f1_macro: 0.20177844851044108
[2m[36m(func pid=17891)[0m f1_weighted: 0.2897244616117829
[2m[36m(func pid=17891)[0m f1_per_class: [0.075, 0.476, 0.167, 0.371, 0.085, 0.274, 0.204, 0.032, 0.156, 0.178]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.27611940298507465
[2m[36m(func pid=19545)[0m top5: 0.7966417910447762
[2m[36m(func pid=19545)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=19545)[0m f1_macro: 0.20994637224806628
[2m[36m(func pid=19545)[0m f1_weighted: 0.22506771652207305
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.667, 0.573, 0.0, 0.178, 0.022, 0.54, 0.078, 0.041]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4003 | Steps: 4 | Val loss: 2.1251 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5898 | Steps: 4 | Val loss: 2.4879 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.5244 | Steps: 4 | Val loss: 1.9846 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=17787)[0m top1: 0.23507462686567165
[2m[36m(func pid=17787)[0m top5: 0.8507462686567164
[2m[36m(func pid=17787)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=17787)[0m f1_macro: 0.2023904363605758
[2m[36m(func pid=17787)[0m f1_weighted: 0.223679216176518
[2m[36m(func pid=17787)[0m f1_per_class: [0.141, 0.399, 0.306, 0.277, 0.066, 0.308, 0.045, 0.383, 0.025, 0.074]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3342 | Steps: 4 | Val loss: 2.0774 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=18906)[0m top1: 0.22154850746268656
[2m[36m(func pid=18906)[0m top5: 0.8656716417910447
[2m[36m(func pid=18906)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=18906)[0m f1_macro: 0.1681731953250873
[2m[36m(func pid=18906)[0m f1_weighted: 0.20974227683585725
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.177, 0.0, 0.0, 0.041, 0.016, 0.472, 0.471, 0.159, 0.346]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:19:52 (running for 00:26:44.97)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.4   |      0.202 |                   27 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.932 |      0.202 |                   27 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.59  |      0.168 |                   25 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.196 |      0.21  |                   23 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.32509328358208955
[2m[36m(func pid=17891)[0m top5: 0.8428171641791045
[2m[36m(func pid=17891)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=17891)[0m f1_macro: 0.2811496271409494
[2m[36m(func pid=17891)[0m f1_weighted: 0.3263177457494826
[2m[36m(func pid=17891)[0m f1_per_class: [0.292, 0.518, 0.316, 0.281, 0.074, 0.295, 0.303, 0.305, 0.179, 0.25]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.2789179104477612
[2m[36m(func pid=19545)[0m top5: 0.7877798507462687
[2m[36m(func pid=19545)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=19545)[0m f1_macro: 0.19548191264429696
[2m[36m(func pid=19545)[0m f1_weighted: 0.2627953782111765
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.692, 0.58, 0.0, 0.027, 0.246, 0.314, 0.054, 0.042]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4398 | Steps: 4 | Val loss: 2.1146 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5757 | Steps: 4 | Val loss: 2.3757 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2598 | Steps: 4 | Val loss: 1.8724 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=17787)[0m top1: 0.23087686567164178
[2m[36m(func pid=17787)[0m top5: 0.8484141791044776
[2m[36m(func pid=17787)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=17787)[0m f1_macro: 0.19927478099383786
[2m[36m(func pid=17787)[0m f1_weighted: 0.22730283919957667
[2m[36m(func pid=17787)[0m f1_per_class: [0.149, 0.321, 0.256, 0.348, 0.054, 0.294, 0.042, 0.38, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.9501 | Steps: 4 | Val loss: 1.8331 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 13:19:58 (running for 00:26:50.34)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.44  |      0.199 |                   28 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.524 |      0.281 |                   28 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.576 |      0.217 |                   26 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.334 |      0.195 |                   24 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.2966417910447761
[2m[36m(func pid=18906)[0m top5: 0.8428171641791045
[2m[36m(func pid=18906)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=18906)[0m f1_macro: 0.21665724546255266
[2m[36m(func pid=18906)[0m f1_weighted: 0.3285374758574296
[2m[36m(func pid=18906)[0m f1_per_class: [0.185, 0.258, 0.0, 0.427, 0.081, 0.346, 0.305, 0.471, 0.096, 0.0]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.38619402985074625
[2m[36m(func pid=17891)[0m top5: 0.8666044776119403
[2m[36m(func pid=17891)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=17891)[0m f1_macro: 0.34515822564818965
[2m[36m(func pid=17891)[0m f1_weighted: 0.3738533202833077
[2m[36m(func pid=17891)[0m f1_per_class: [0.288, 0.49, 0.588, 0.195, 0.081, 0.34, 0.491, 0.542, 0.165, 0.272]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.34654850746268656
[2m[36m(func pid=19545)[0m top5: 0.8344216417910447
[2m[36m(func pid=19545)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=19545)[0m f1_macro: 0.16953402112698143
[2m[36m(func pid=19545)[0m f1_weighted: 0.3235063678335847
[2m[36m(func pid=19545)[0m f1_per_class: [0.089, 0.036, 0.0, 0.552, 0.0, 0.1, 0.417, 0.413, 0.042, 0.047]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4796 | Steps: 4 | Val loss: 2.1162 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5309 | Steps: 4 | Val loss: 3.2651 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4899 | Steps: 4 | Val loss: 2.0409 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.9538 | Steps: 4 | Val loss: 2.1873 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=17787)[0m top1: 0.2140858208955224
[2m[36m(func pid=17787)[0m top5: 0.8488805970149254
[2m[36m(func pid=17787)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=17787)[0m f1_macro: 0.18490008527944773
[2m[36m(func pid=17787)[0m f1_weighted: 0.21609758687928046
[2m[36m(func pid=17787)[0m f1_per_class: [0.144, 0.262, 0.19, 0.364, 0.047, 0.249, 0.042, 0.363, 0.04, 0.148]
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:20:03 (running for 00:26:55.83)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.48  |      0.185 |                   29 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.26  |      0.345 |                   29 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.531 |      0.172 |                   27 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.95  |      0.17  |                   25 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.21735074626865672
[2m[36m(func pid=18906)[0m top5: 0.8418843283582089
[2m[36m(func pid=18906)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=18906)[0m f1_macro: 0.17230395885634336
[2m[36m(func pid=18906)[0m f1_weighted: 0.18946628231117515
[2m[36m(func pid=18906)[0m f1_per_class: [0.134, 0.175, 0.0, 0.42, 0.052, 0.045, 0.012, 0.41, 0.046, 0.429]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.32975746268656714
[2m[36m(func pid=17891)[0m top5: 0.8577425373134329
[2m[36m(func pid=17891)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=17891)[0m f1_macro: 0.26360524632409554
[2m[36m(func pid=17891)[0m f1_weighted: 0.32678815715750414
[2m[36m(func pid=17891)[0m f1_per_class: [0.284, 0.512, 0.27, 0.221, 0.152, 0.146, 0.401, 0.513, 0.055, 0.084]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.11567164179104478
[2m[36m(func pid=19545)[0m top5: 0.8470149253731343
[2m[36m(func pid=19545)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=19545)[0m f1_macro: 0.053097751079356756
[2m[36m(func pid=19545)[0m f1_weighted: 0.07679340460603373
[2m[36m(func pid=19545)[0m f1_per_class: [0.065, 0.0, 0.0, 0.203, 0.0, 0.0, 0.015, 0.248, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3272 | Steps: 4 | Val loss: 2.1084 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.3393 | Steps: 4 | Val loss: 3.5299 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2808 | Steps: 4 | Val loss: 2.3795 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=17787)[0m top1: 0.20569029850746268
[2m[36m(func pid=17787)[0m top5: 0.8563432835820896
[2m[36m(func pid=17787)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=17787)[0m f1_macro: 0.18195839245201773
[2m[36m(func pid=17787)[0m f1_weighted: 0.21576624490471227
[2m[36m(func pid=17787)[0m f1_per_class: [0.149, 0.258, 0.237, 0.364, 0.041, 0.138, 0.076, 0.399, 0.081, 0.077]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6800 | Steps: 4 | Val loss: 1.9243 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:20:09 (running for 00:27:01.34)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.327 |      0.182 |                   30 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.49  |      0.264 |                   30 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.339 |      0.219 |                   28 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.954 |      0.053 |                   26 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.2579291044776119
[2m[36m(func pid=18906)[0m top5: 0.8484141791044776
[2m[36m(func pid=18906)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=18906)[0m f1_macro: 0.21913158216061618
[2m[36m(func pid=18906)[0m f1_weighted: 0.3211269261053137
[2m[36m(func pid=18906)[0m f1_per_class: [0.157, 0.331, 0.044, 0.229, 0.048, 0.364, 0.46, 0.187, 0.114, 0.256]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.28031716417910446
[2m[36m(func pid=17891)[0m top5: 0.8460820895522388
[2m[36m(func pid=17891)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=17891)[0m f1_macro: 0.23071098480604807
[2m[36m(func pid=17891)[0m f1_weighted: 0.31152781704982646
[2m[36m(func pid=17891)[0m f1_per_class: [0.263, 0.533, 0.118, 0.359, 0.124, 0.062, 0.251, 0.5, 0.037, 0.061]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.34095149253731344
[2m[36m(func pid=19545)[0m top5: 0.8041044776119403
[2m[36m(func pid=19545)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=19545)[0m f1_macro: 0.19127834598018761
[2m[36m(func pid=19545)[0m f1_weighted: 0.3046376274331689
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.353, 0.353, 0.177, 0.125, 0.0, 0.584, 0.295, 0.0, 0.025]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3018 | Steps: 4 | Val loss: 2.1053 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5165 | Steps: 4 | Val loss: 4.8672 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4205 | Steps: 4 | Val loss: 1.7318 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=17787)[0m top1: 0.17723880597014927
[2m[36m(func pid=17787)[0m top5: 0.8642723880597015
[2m[36m(func pid=17787)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=17787)[0m f1_macro: 0.18626808354977822
[2m[36m(func pid=17787)[0m f1_weighted: 0.188289605120899
[2m[36m(func pid=17787)[0m f1_per_class: [0.179, 0.182, 0.407, 0.322, 0.032, 0.071, 0.08, 0.443, 0.07, 0.077]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4650 | Steps: 4 | Val loss: 2.6347 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=18906)[0m top1: 0.23507462686567165
[2m[36m(func pid=18906)[0m top5: 0.7621268656716418
[2m[36m(func pid=18906)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=18906)[0m f1_macro: 0.21911260458947512
[2m[36m(func pid=18906)[0m f1_weighted: 0.26985312789515453
[2m[36m(func pid=18906)[0m f1_per_class: [0.101, 0.095, 0.032, 0.107, 0.064, 0.324, 0.486, 0.538, 0.127, 0.317]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:20:14 (running for 00:27:06.68)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.302 |      0.186 |                   31 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.281 |      0.231 |                   31 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.517 |      0.219 |                   29 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.68  |      0.191 |                   27 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.40158582089552236
[2m[36m(func pid=17891)[0m top5: 0.8955223880597015
[2m[36m(func pid=17891)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=17891)[0m f1_macro: 0.3068703172273203
[2m[36m(func pid=17891)[0m f1_weighted: 0.4337940693743614
[2m[36m(func pid=17891)[0m f1_per_class: [0.229, 0.519, 0.195, 0.47, 0.088, 0.321, 0.456, 0.506, 0.079, 0.205]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.13386194029850745
[2m[36m(func pid=19545)[0m top5: 0.7066231343283582
[2m[36m(func pid=19545)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=19545)[0m f1_macro: 0.09071245282979705
[2m[36m(func pid=19545)[0m f1_weighted: 0.09661786089884176
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.249, 0.177, 0.106, 0.077, 0.091, 0.003, 0.175, 0.028, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5003 | Steps: 4 | Val loss: 2.0894 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9037 | Steps: 4 | Val loss: 3.1547 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.1724 | Steps: 4 | Val loss: 1.8837 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=17787)[0m top1: 0.1837686567164179
[2m[36m(func pid=17787)[0m top5: 0.8763992537313433
[2m[36m(func pid=17787)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=17787)[0m f1_macro: 0.1865550144538251
[2m[36m(func pid=17787)[0m f1_weighted: 0.2013730515124495
[2m[36m(func pid=17787)[0m f1_per_class: [0.183, 0.138, 0.431, 0.324, 0.031, 0.131, 0.13, 0.417, 0.079, 0.0]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3639 | Steps: 4 | Val loss: 2.7111 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 13:20:20 (running for 00:27:12.31)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.5   |      0.187 |                   32 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.42  |      0.307 |                   32 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.904 |      0.24  |                   30 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.465 |      0.091 |                   28 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.2644589552238806
[2m[36m(func pid=18906)[0m top5: 0.8390858208955224
[2m[36m(func pid=18906)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=18906)[0m f1_macro: 0.24042412308071678
[2m[36m(func pid=18906)[0m f1_weighted: 0.28763259245136713
[2m[36m(func pid=18906)[0m f1_per_class: [0.176, 0.397, 0.0, 0.26, 0.073, 0.363, 0.232, 0.413, 0.106, 0.384]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.38013059701492535
[2m[36m(func pid=17891)[0m top5: 0.8852611940298507
[2m[36m(func pid=17891)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=17891)[0m f1_macro: 0.3555493019753364
[2m[36m(func pid=17891)[0m f1_weighted: 0.4254620447903071
[2m[36m(func pid=17891)[0m f1_per_class: [0.294, 0.398, 0.783, 0.51, 0.087, 0.261, 0.463, 0.536, 0.09, 0.134]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.07695895522388059
[2m[36m(func pid=19545)[0m top5: 0.7290111940298507
[2m[36m(func pid=19545)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=19545)[0m f1_macro: 0.08343227598256171
[2m[36m(func pid=19545)[0m f1_weighted: 0.05667602600311444
[2m[36m(func pid=19545)[0m f1_per_class: [0.068, 0.06, 0.105, 0.022, 0.072, 0.202, 0.0, 0.214, 0.055, 0.036]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3142 | Steps: 4 | Val loss: 2.1048 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5473 | Steps: 4 | Val loss: 2.4680 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3616 | Steps: 4 | Val loss: 2.0221 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=17787)[0m top1: 0.17024253731343283
[2m[36m(func pid=17787)[0m top5: 0.8708022388059702
[2m[36m(func pid=17787)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=17787)[0m f1_macro: 0.17604443987809465
[2m[36m(func pid=17787)[0m f1_weighted: 0.18552010655443396
[2m[36m(func pid=17787)[0m f1_per_class: [0.161, 0.147, 0.324, 0.265, 0.031, 0.157, 0.118, 0.439, 0.042, 0.077]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1226 | Steps: 4 | Val loss: 2.6910 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:20:25 (running for 00:27:17.85)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.314 |      0.176 |                   33 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.172 |      0.356 |                   33 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.547 |      0.23  |                   31 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.364 |      0.083 |                   29 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.32649253731343286
[2m[36m(func pid=17891)[0m top5: 0.8768656716417911
[2m[36m(func pid=17891)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=17891)[0m f1_macro: 0.32136313533112826
[2m[36m(func pid=17891)[0m f1_weighted: 0.3666535829468271
[2m[36m(func pid=17891)[0m f1_per_class: [0.37, 0.201, 0.667, 0.476, 0.07, 0.254, 0.412, 0.516, 0.122, 0.127]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.23740671641791045
[2m[36m(func pid=18906)[0m top5: 0.8684701492537313
[2m[36m(func pid=18906)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=18906)[0m f1_macro: 0.230080534391553
[2m[36m(func pid=18906)[0m f1_weighted: 0.23878839053816175
[2m[36m(func pid=18906)[0m f1_per_class: [0.121, 0.437, 0.087, 0.271, 0.088, 0.408, 0.027, 0.357, 0.139, 0.366]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m top1: 0.14598880597014927
[2m[36m(func pid=19545)[0m top5: 0.7709888059701493
[2m[36m(func pid=19545)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=19545)[0m f1_macro: 0.11338992436831233
[2m[36m(func pid=19545)[0m f1_weighted: 0.08649498892819024
[2m[36m(func pid=19545)[0m f1_per_class: [0.064, 0.046, 0.047, 0.031, 0.053, 0.291, 0.006, 0.544, 0.028, 0.023]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2965 | Steps: 4 | Val loss: 2.1060 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4523 | Steps: 4 | Val loss: 2.4979 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3070 | Steps: 4 | Val loss: 2.3966 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=17787)[0m top1: 0.16791044776119404
[2m[36m(func pid=17787)[0m top5: 0.8698694029850746
[2m[36m(func pid=17787)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=17787)[0m f1_macro: 0.17525705806933445
[2m[36m(func pid=17787)[0m f1_weighted: 0.1726743265788769
[2m[36m(func pid=17787)[0m f1_per_class: [0.147, 0.134, 0.333, 0.243, 0.036, 0.227, 0.087, 0.386, 0.021, 0.138]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.1770 | Steps: 4 | Val loss: 2.3723 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:20:31 (running for 00:27:23.30)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.297 |      0.175 |                   34 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.362 |      0.321 |                   34 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.452 |      0.244 |                   32 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.123 |      0.113 |                   30 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.269589552238806
[2m[36m(func pid=18906)[0m top5: 0.8152985074626866
[2m[36m(func pid=18906)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=18906)[0m f1_macro: 0.2442478533003965
[2m[36m(func pid=18906)[0m f1_weighted: 0.2592237487364872
[2m[36m(func pid=18906)[0m f1_per_class: [0.171, 0.456, 0.0, 0.129, 0.084, 0.346, 0.225, 0.413, 0.142, 0.478]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.2798507462686567
[2m[36m(func pid=17891)[0m top5: 0.8278917910447762
[2m[36m(func pid=17891)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=17891)[0m f1_macro: 0.26088690335751646
[2m[36m(func pid=17891)[0m f1_weighted: 0.32779759841874956
[2m[36m(func pid=17891)[0m f1_per_class: [0.282, 0.442, 0.4, 0.473, 0.083, 0.062, 0.246, 0.46, 0.098, 0.063]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.20988805970149255
[2m[36m(func pid=19545)[0m top5: 0.7793843283582089
[2m[36m(func pid=19545)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=19545)[0m f1_macro: 0.15725465452670312
[2m[36m(func pid=19545)[0m f1_weighted: 0.24681462535057289
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.121, 0.059, 0.24, 0.047, 0.17, 0.363, 0.495, 0.055, 0.022]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2381 | Steps: 4 | Val loss: 2.1104 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.5750 | Steps: 4 | Val loss: 2.4063 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9694 | Steps: 4 | Val loss: 1.9303 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3776 | Steps: 4 | Val loss: 2.7329 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=17787)[0m top1: 0.17490671641791045
[2m[36m(func pid=17787)[0m top5: 0.8703358208955224
[2m[36m(func pid=17787)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=17787)[0m f1_macro: 0.17204581333295976
[2m[36m(func pid=17787)[0m f1_weighted: 0.18111519906540638
[2m[36m(func pid=17787)[0m f1_per_class: [0.126, 0.153, 0.256, 0.254, 0.038, 0.261, 0.082, 0.413, 0.0, 0.138]
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:20:36 (running for 00:27:28.86)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.238 |      0.172 |                   35 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.307 |      0.261 |                   35 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.575 |      0.303 |                   33 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.177 |      0.157 |                   31 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.35074626865671643
[2m[36m(func pid=18906)[0m top5: 0.8353544776119403
[2m[36m(func pid=18906)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=18906)[0m f1_macro: 0.30292983949560043
[2m[36m(func pid=18906)[0m f1_weighted: 0.3965823424774101
[2m[36m(func pid=18906)[0m f1_per_class: [0.169, 0.442, 0.267, 0.318, 0.058, 0.147, 0.582, 0.451, 0.141, 0.455]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.375
[2m[36m(func pid=17891)[0m top5: 0.8624067164179104
[2m[36m(func pid=17891)[0m f1_micro: 0.375
[2m[36m(func pid=17891)[0m f1_macro: 0.31713425742887646
[2m[36m(func pid=17891)[0m f1_weighted: 0.4010893050167706
[2m[36m(func pid=17891)[0m f1_per_class: [0.277, 0.583, 0.276, 0.391, 0.066, 0.285, 0.391, 0.421, 0.219, 0.263]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.19962686567164178
[2m[36m(func pid=19545)[0m top5: 0.7430037313432836
[2m[36m(func pid=19545)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=19545)[0m f1_macro: 0.16634047230786214
[2m[36m(func pid=19545)[0m f1_weighted: 0.2134127312669788
[2m[36m(func pid=19545)[0m f1_per_class: [0.073, 0.195, 0.253, 0.391, 0.036, 0.0, 0.115, 0.532, 0.067, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2808 | Steps: 4 | Val loss: 2.0982 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.9574 | Steps: 4 | Val loss: 2.6454 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.5680 | Steps: 4 | Val loss: 2.6507 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3267 | Steps: 4 | Val loss: 3.0225 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=17787)[0m top1: 0.18423507462686567
[2m[36m(func pid=17787)[0m top5: 0.8675373134328358
[2m[36m(func pid=17787)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=17787)[0m f1_macro: 0.17571032030649986
[2m[36m(func pid=17787)[0m f1_weighted: 0.19358084885668303
[2m[36m(func pid=17787)[0m f1_per_class: [0.145, 0.117, 0.227, 0.288, 0.037, 0.268, 0.108, 0.401, 0.024, 0.143]
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:20:42 (running for 00:27:34.48)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.281 |      0.176 |                   36 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.969 |      0.317 |                   36 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.957 |      0.278 |                   34 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.378 |      0.166 |                   32 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.3605410447761194
[2m[36m(func pid=18906)[0m top5: 0.8027052238805971
[2m[36m(func pid=18906)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=18906)[0m f1_macro: 0.27829064624558797
[2m[36m(func pid=18906)[0m f1_weighted: 0.38627692895850113
[2m[36m(func pid=18906)[0m f1_per_class: [0.179, 0.359, 0.19, 0.535, 0.152, 0.293, 0.356, 0.423, 0.088, 0.207]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m top1: 0.25046641791044777
[2m[36m(func pid=17891)[0m top5: 0.7873134328358209
[2m[36m(func pid=17891)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=17891)[0m f1_macro: 0.18158873220237665
[2m[36m(func pid=17891)[0m f1_weighted: 0.23378729953827884
[2m[36m(func pid=17891)[0m f1_per_class: [0.192, 0.545, 0.144, 0.232, 0.065, 0.112, 0.131, 0.245, 0.101, 0.05]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.16977611940298507
[2m[36m(func pid=19545)[0m top5: 0.5764925373134329
[2m[36m(func pid=19545)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=19545)[0m f1_macro: 0.16026064406838728
[2m[36m(func pid=19545)[0m f1_weighted: 0.18463750516073965
[2m[36m(func pid=19545)[0m f1_per_class: [0.076, 0.239, 0.314, 0.36, 0.035, 0.0, 0.03, 0.496, 0.053, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3753 | Steps: 4 | Val loss: 2.0861 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.2525 | Steps: 4 | Val loss: 2.0691 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0415 | Steps: 4 | Val loss: 2.9594 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.3100 | Steps: 4 | Val loss: 2.7675 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=17787)[0m top1: 0.20242537313432835
[2m[36m(func pid=17787)[0m top5: 0.8754664179104478
[2m[36m(func pid=17787)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=17787)[0m f1_macro: 0.17165836573983556
[2m[36m(func pid=17787)[0m f1_weighted: 0.20965890799559933
[2m[36m(func pid=17787)[0m f1_per_class: [0.146, 0.111, 0.17, 0.387, 0.039, 0.232, 0.092, 0.392, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m top1: 0.2677238805970149
[2m[36m(func pid=18906)[0m top5: 0.8334888059701493
[2m[36m(func pid=18906)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=18906)[0m f1_macro: 0.2797842293764604
[2m[36m(func pid=18906)[0m f1_weighted: 0.23804636311681268
[2m[36m(func pid=18906)[0m f1_per_class: [0.129, 0.505, 0.706, 0.323, 0.186, 0.226, 0.006, 0.323, 0.083, 0.311]
[2m[36m(func pid=18906)[0m 
== Status ==
Current time: 2024-01-07 13:20:47 (running for 00:27:40.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.375 |      0.172 |                   37 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.568 |      0.182 |                   37 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.041 |      0.28  |                   35 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.327 |      0.16  |                   33 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.345615671641791
[2m[36m(func pid=17891)[0m top5: 0.8708022388059702
[2m[36m(func pid=17891)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=17891)[0m f1_macro: 0.28273280552157076
[2m[36m(func pid=17891)[0m f1_weighted: 0.332779091710245
[2m[36m(func pid=17891)[0m f1_per_class: [0.229, 0.528, 0.303, 0.233, 0.061, 0.303, 0.34, 0.491, 0.073, 0.267]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.2560634328358209
[2m[36m(func pid=19545)[0m top5: 0.6944962686567164
[2m[36m(func pid=19545)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=19545)[0m f1_macro: 0.18910258751968018
[2m[36m(func pid=19545)[0m f1_weighted: 0.2770193808986346
[2m[36m(func pid=19545)[0m f1_per_class: [0.063, 0.12, 0.318, 0.51, 0.016, 0.0, 0.262, 0.528, 0.067, 0.008]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3095 | Steps: 4 | Val loss: 2.0656 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2611 | Steps: 4 | Val loss: 2.3649 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8178 | Steps: 4 | Val loss: 3.1506 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.4366 | Steps: 4 | Val loss: 1.9065 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=17787)[0m top1: 0.2224813432835821
[2m[36m(func pid=17787)[0m top5: 0.8843283582089553
[2m[36m(func pid=17787)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=17787)[0m f1_macro: 0.17497806916394695
[2m[36m(func pid=17787)[0m f1_weighted: 0.21837278502990612
[2m[36m(func pid=17787)[0m f1_per_class: [0.156, 0.1, 0.159, 0.434, 0.041, 0.273, 0.073, 0.366, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
== Status ==
Current time: 2024-01-07 13:20:53 (running for 00:27:45.57)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.309 |      0.175 |                   38 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.261 |      0.246 |                   39 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.041 |      0.28  |                   35 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.31  |      0.189 |                   34 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.2994402985074627
[2m[36m(func pid=17891)[0m top5: 0.8278917910447762
[2m[36m(func pid=17891)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=17891)[0m f1_macro: 0.2457435099209268
[2m[36m(func pid=17891)[0m f1_weighted: 0.2924480592304126
[2m[36m(func pid=17891)[0m f1_per_class: [0.291, 0.528, 0.296, 0.272, 0.101, 0.207, 0.23, 0.374, 0.086, 0.074]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.32509328358208955
[2m[36m(func pid=19545)[0m top5: 0.8097014925373134
[2m[36m(func pid=19545)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=19545)[0m f1_macro: 0.1786755606747174
[2m[36m(func pid=19545)[0m f1_weighted: 0.32656582382143357
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.0, 0.0, 0.536, 0.119, 0.0, 0.476, 0.53, 0.104, 0.022]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.38152985074626866
[2m[36m(func pid=18906)[0m top5: 0.8596082089552238
[2m[36m(func pid=18906)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=18906)[0m f1_macro: 0.33854746858236356
[2m[36m(func pid=18906)[0m f1_weighted: 0.346970103297299
[2m[36m(func pid=18906)[0m f1_per_class: [0.293, 0.467, 0.762, 0.129, 0.073, 0.237, 0.512, 0.543, 0.193, 0.176]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2581 | Steps: 4 | Val loss: 2.0548 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.1180 | Steps: 4 | Val loss: 1.9207 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.3828 | Steps: 4 | Val loss: 2.1358 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=17787)[0m top1: 0.24207089552238806
[2m[36m(func pid=17787)[0m top5: 0.8745335820895522
[2m[36m(func pid=17787)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=17787)[0m f1_macro: 0.18132455152474772
[2m[36m(func pid=17787)[0m f1_weighted: 0.2283703679251362
[2m[36m(func pid=17787)[0m f1_per_class: [0.168, 0.141, 0.148, 0.464, 0.048, 0.282, 0.05, 0.364, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4276 | Steps: 4 | Val loss: 2.4076 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:20:58 (running for 00:27:51.10)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.258 |      0.181 |                   39 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.118 |      0.308 |                   40 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.818 |      0.339 |                   36 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.437 |      0.179 |                   35 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.36240671641791045
[2m[36m(func pid=17891)[0m top5: 0.8847947761194029
[2m[36m(func pid=17891)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=17891)[0m f1_macro: 0.307935402069457
[2m[36m(func pid=17891)[0m f1_weighted: 0.3687349667489514
[2m[36m(func pid=17891)[0m f1_per_class: [0.292, 0.534, 0.222, 0.335, 0.079, 0.336, 0.333, 0.474, 0.224, 0.25]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.18516791044776118
[2m[36m(func pid=19545)[0m top5: 0.7131529850746269
[2m[36m(func pid=19545)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=19545)[0m f1_macro: 0.18606793438955402
[2m[36m(func pid=19545)[0m f1_weighted: 0.22474939008110106
[2m[36m(func pid=19545)[0m f1_per_class: [0.032, 0.0, 0.522, 0.322, 0.038, 0.0, 0.328, 0.52, 0.086, 0.013]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.4001865671641791
[2m[36m(func pid=18906)[0m top5: 0.8773320895522388
[2m[36m(func pid=18906)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=18906)[0m f1_macro: 0.3673391723260305
[2m[36m(func pid=18906)[0m f1_weighted: 0.43316938030168434
[2m[36m(func pid=18906)[0m f1_per_class: [0.344, 0.493, 0.923, 0.486, 0.06, 0.107, 0.515, 0.484, 0.136, 0.125]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2640 | Steps: 4 | Val loss: 2.0498 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0860 | Steps: 4 | Val loss: 1.8197 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=17787)[0m top1: 0.24580223880597016
[2m[36m(func pid=17787)[0m top5: 0.8731343283582089
[2m[36m(func pid=17787)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=17787)[0m f1_macro: 0.18954917426332088
[2m[36m(func pid=17787)[0m f1_weighted: 0.23154367431795017
[2m[36m(func pid=17787)[0m f1_per_class: [0.185, 0.16, 0.175, 0.459, 0.047, 0.294, 0.045, 0.383, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4365 | Steps: 4 | Val loss: 3.2881 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3663 | Steps: 4 | Val loss: 2.9741 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:21:04 (running for 00:27:56.47)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.264 |      0.19  |                   40 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.086 |      0.357 |                   41 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.428 |      0.367 |                   37 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.383 |      0.186 |                   36 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3871268656716418
[2m[36m(func pid=17891)[0m top5: 0.8894589552238806
[2m[36m(func pid=17891)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=17891)[0m f1_macro: 0.3568749834487769
[2m[36m(func pid=17891)[0m f1_weighted: 0.39417284728031776
[2m[36m(func pid=17891)[0m f1_per_class: [0.305, 0.561, 0.558, 0.456, 0.095, 0.339, 0.284, 0.45, 0.215, 0.306]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.1142723880597015
[2m[36m(func pid=19545)[0m top5: 0.7481343283582089
[2m[36m(func pid=19545)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=19545)[0m f1_macro: 0.11791426011855113
[2m[36m(func pid=19545)[0m f1_weighted: 0.08762053439665997
[2m[36m(func pid=19545)[0m f1_per_class: [0.094, 0.0, 0.313, 0.0, 0.054, 0.0, 0.187, 0.426, 0.082, 0.025]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.2294776119402985
[2m[36m(func pid=18906)[0m top5: 0.8101679104477612
[2m[36m(func pid=18906)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=18906)[0m f1_macro: 0.15067697058879131
[2m[36m(func pid=18906)[0m f1_weighted: 0.1970084434504227
[2m[36m(func pid=18906)[0m f1_per_class: [0.133, 0.486, 0.0, 0.257, 0.069, 0.064, 0.051, 0.168, 0.136, 0.143]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.2834 | Steps: 4 | Val loss: 2.0440 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2361 | Steps: 4 | Val loss: 1.9255 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=17787)[0m top1: 0.2583955223880597
[2m[36m(func pid=17787)[0m top5: 0.8656716417910447
[2m[36m(func pid=17787)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=17787)[0m f1_macro: 0.1976854393244124
[2m[36m(func pid=17787)[0m f1_weighted: 0.2396777407920932
[2m[36m(func pid=17787)[0m f1_per_class: [0.188, 0.196, 0.176, 0.474, 0.049, 0.314, 0.024, 0.407, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3030 | Steps: 4 | Val loss: 3.2431 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1778 | Steps: 4 | Val loss: 3.2484 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:21:09 (running for 00:28:01.88)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.283 |      0.198 |                   41 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.236 |      0.312 |                   42 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.366 |      0.151 |                   38 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.437 |      0.118 |                   37 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.376865671641791
[2m[36m(func pid=17891)[0m top5: 0.8824626865671642
[2m[36m(func pid=17891)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=17891)[0m f1_macro: 0.31197331085920676
[2m[36m(func pid=17891)[0m f1_weighted: 0.40684475389347124
[2m[36m(func pid=17891)[0m f1_per_class: [0.291, 0.554, 0.174, 0.439, 0.089, 0.295, 0.367, 0.523, 0.171, 0.218]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.1287313432835821
[2m[36m(func pid=19545)[0m top5: 0.8129664179104478
[2m[36m(func pid=19545)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=19545)[0m f1_macro: 0.1310779004414274
[2m[36m(func pid=19545)[0m f1_weighted: 0.1162348273736438
[2m[36m(func pid=19545)[0m f1_per_class: [0.088, 0.051, 0.192, 0.042, 0.051, 0.162, 0.149, 0.457, 0.082, 0.036]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3920 | Steps: 4 | Val loss: 2.0178 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=18906)[0m top1: 0.1571828358208955
[2m[36m(func pid=18906)[0m top5: 0.8083022388059702
[2m[36m(func pid=18906)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=18906)[0m f1_macro: 0.11873822842169932
[2m[36m(func pid=18906)[0m f1_weighted: 0.15934298124291302
[2m[36m(func pid=18906)[0m f1_per_class: [0.116, 0.157, 0.0, 0.342, 0.111, 0.125, 0.015, 0.215, 0.069, 0.039]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0689 | Steps: 4 | Val loss: 2.0750 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=17787)[0m top1: 0.2751865671641791
[2m[36m(func pid=17787)[0m top5: 0.8754664179104478
[2m[36m(func pid=17787)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=17787)[0m f1_macro: 0.21335743844289917
[2m[36m(func pid=17787)[0m f1_weighted: 0.2549396558195445
[2m[36m(func pid=17787)[0m f1_per_class: [0.219, 0.265, 0.238, 0.481, 0.056, 0.325, 0.028, 0.374, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2071 | Steps: 4 | Val loss: 2.6199 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0658 | Steps: 4 | Val loss: 2.2198 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 13:21:14 (running for 00:28:07.26)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.392 |      0.213 |                   42 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.069 |      0.301 |                   43 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.178 |      0.119 |                   39 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.303 |      0.131 |                   38 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.34468283582089554
[2m[36m(func pid=17891)[0m top5: 0.875
[2m[36m(func pid=17891)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=17891)[0m f1_macro: 0.30140674376241305
[2m[36m(func pid=17891)[0m f1_weighted: 0.38150245530247223
[2m[36m(func pid=17891)[0m f1_per_class: [0.258, 0.53, 0.145, 0.321, 0.073, 0.326, 0.394, 0.505, 0.216, 0.246]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.11473880597014925
[2m[36m(func pid=19545)[0m top5: 0.8498134328358209
[2m[36m(func pid=19545)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=19545)[0m f1_macro: 0.10721371367077884
[2m[36m(func pid=19545)[0m f1_weighted: 0.1055034380990228
[2m[36m(func pid=19545)[0m f1_per_class: [0.091, 0.138, 0.259, 0.165, 0.046, 0.201, 0.0, 0.148, 0.0, 0.024]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.1927 | Steps: 4 | Val loss: 2.0136 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=18906)[0m top1: 0.33675373134328357
[2m[36m(func pid=18906)[0m top5: 0.8763992537313433
[2m[36m(func pid=18906)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=18906)[0m f1_macro: 0.20581551263956893
[2m[36m(func pid=18906)[0m f1_weighted: 0.2986942747836218
[2m[36m(func pid=18906)[0m f1_per_class: [0.037, 0.427, 0.0, 0.557, 0.094, 0.262, 0.053, 0.293, 0.082, 0.253]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.28171641791044777
[2m[36m(func pid=17787)[0m top5: 0.8703358208955224
[2m[36m(func pid=17787)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=17787)[0m f1_macro: 0.22566923889312668
[2m[36m(func pid=17787)[0m f1_weighted: 0.27038006424899796
[2m[36m(func pid=17787)[0m f1_per_class: [0.206, 0.327, 0.282, 0.469, 0.054, 0.314, 0.054, 0.402, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.0538 | Steps: 4 | Val loss: 2.3927 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1126 | Steps: 4 | Val loss: 2.3789 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6135 | Steps: 4 | Val loss: 1.8913 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:21:20 (running for 00:28:12.83)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.193 |      0.226 |                   43 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.069 |      0.301 |                   43 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.066 |      0.206 |                   40 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.113 |      0.183 |                   40 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.23274253731343283
[2m[36m(func pid=19545)[0m top5: 0.8586753731343284
[2m[36m(func pid=19545)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=19545)[0m f1_macro: 0.18268068526336895
[2m[36m(func pid=19545)[0m f1_weighted: 0.2737256622131943
[2m[36m(func pid=19545)[0m f1_per_class: [0.086, 0.208, 0.359, 0.156, 0.05, 0.221, 0.507, 0.239, 0.0, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.2574626865671642
[2m[36m(func pid=17891)[0m top5: 0.8292910447761194
[2m[36m(func pid=17891)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=17891)[0m f1_macro: 0.23562503382166805
[2m[36m(func pid=17891)[0m f1_weighted: 0.2673883480915302
[2m[36m(func pid=17891)[0m f1_per_class: [0.201, 0.526, 0.202, 0.243, 0.072, 0.205, 0.177, 0.311, 0.199, 0.219]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2791 | Steps: 4 | Val loss: 2.0215 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m top1: 0.39598880597014924
[2m[36m(func pid=18906)[0m top5: 0.8917910447761194
[2m[36m(func pid=18906)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=18906)[0m f1_macro: 0.2880117685111735
[2m[36m(func pid=18906)[0m f1_weighted: 0.4213292141694256
[2m[36m(func pid=18906)[0m f1_per_class: [0.234, 0.355, 0.133, 0.536, 0.078, 0.287, 0.454, 0.523, 0.144, 0.136]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.279384328358209
[2m[36m(func pid=17787)[0m top5: 0.8624067164179104
[2m[36m(func pid=17787)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=17787)[0m f1_macro: 0.23214160042338783
[2m[36m(func pid=17787)[0m f1_weighted: 0.27042471358385095
[2m[36m(func pid=17787)[0m f1_per_class: [0.198, 0.359, 0.328, 0.458, 0.053, 0.3, 0.045, 0.432, 0.0, 0.148]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.1393 | Steps: 4 | Val loss: 2.0881 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1547 | Steps: 4 | Val loss: 2.2343 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3448 | Steps: 4 | Val loss: 1.9370 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 13:21:26 (running for 00:28:18.28)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.279 |      0.232 |                   44 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.054 |      0.236 |                   44 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.613 |      0.288 |                   41 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.139 |      0.179 |                   41 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.25466417910447764
[2m[36m(func pid=19545)[0m top5: 0.8684701492537313
[2m[36m(func pid=19545)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=19545)[0m f1_macro: 0.1791645575639812
[2m[36m(func pid=19545)[0m f1_weighted: 0.2585957693283306
[2m[36m(func pid=19545)[0m f1_per_class: [0.086, 0.179, 0.409, 0.093, 0.06, 0.032, 0.592, 0.274, 0.025, 0.042]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.3148320895522388
[2m[36m(func pid=17891)[0m top5: 0.8652052238805971
[2m[36m(func pid=17891)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=17891)[0m f1_macro: 0.3409534079685485
[2m[36m(func pid=17891)[0m f1_weighted: 0.3185404996022319
[2m[36m(func pid=17891)[0m f1_per_class: [0.425, 0.474, 0.609, 0.238, 0.066, 0.282, 0.288, 0.498, 0.214, 0.316]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1863 | Steps: 4 | Val loss: 2.0094 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=18906)[0m top1: 0.40531716417910446
[2m[36m(func pid=18906)[0m top5: 0.894589552238806
[2m[36m(func pid=18906)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=18906)[0m f1_macro: 0.3524263536449027
[2m[36m(func pid=18906)[0m f1_weighted: 0.4071626369388837
[2m[36m(func pid=18906)[0m f1_per_class: [0.28, 0.535, 0.815, 0.362, 0.076, 0.137, 0.51, 0.534, 0.053, 0.222]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2691231343283582
[2m[36m(func pid=17787)[0m top5: 0.8610074626865671
[2m[36m(func pid=17787)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=17787)[0m f1_macro: 0.2358581366765094
[2m[36m(func pid=17787)[0m f1_weighted: 0.26702839458122685
[2m[36m(func pid=17787)[0m f1_per_class: [0.203, 0.351, 0.386, 0.448, 0.047, 0.261, 0.062, 0.417, 0.0, 0.182]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3856 | Steps: 4 | Val loss: 2.1230 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1918 | Steps: 4 | Val loss: 2.1782 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0103 | Steps: 4 | Val loss: 2.3474 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 13:21:31 (running for 00:28:23.82)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.186 |      0.236 |                   45 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.155 |      0.341 |                   45 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.345 |      0.352 |                   42 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.386 |      0.182 |                   42 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.17817164179104478
[2m[36m(func pid=19545)[0m top5: 0.8376865671641791
[2m[36m(func pid=19545)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=19545)[0m f1_macro: 0.18229132985633828
[2m[36m(func pid=19545)[0m f1_weighted: 0.20908536631180855
[2m[36m(func pid=19545)[0m f1_per_class: [0.089, 0.232, 0.274, 0.033, 0.109, 0.038, 0.396, 0.512, 0.093, 0.047]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.30970149253731344
[2m[36m(func pid=17891)[0m top5: 0.8936567164179104
[2m[36m(func pid=17891)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=17891)[0m f1_macro: 0.32158639548442924
[2m[36m(func pid=17891)[0m f1_weighted: 0.33349396820855626
[2m[36m(func pid=17891)[0m f1_per_class: [0.351, 0.436, 0.478, 0.493, 0.052, 0.257, 0.145, 0.453, 0.253, 0.299]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3028 | Steps: 4 | Val loss: 1.9999 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=18906)[0m top1: 0.32322761194029853
[2m[36m(func pid=18906)[0m top5: 0.8684701492537313
[2m[36m(func pid=18906)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=18906)[0m f1_macro: 0.3050702034641942
[2m[36m(func pid=18906)[0m f1_weighted: 0.283249800172265
[2m[36m(func pid=18906)[0m f1_per_class: [0.295, 0.545, 0.833, 0.459, 0.063, 0.11, 0.033, 0.389, 0.05, 0.273]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2635261194029851
[2m[36m(func pid=17787)[0m top5: 0.8689365671641791
[2m[36m(func pid=17787)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=17787)[0m f1_macro: 0.2258999930516048
[2m[36m(func pid=17787)[0m f1_weighted: 0.2656993287354515
[2m[36m(func pid=17787)[0m f1_per_class: [0.22, 0.335, 0.319, 0.423, 0.048, 0.293, 0.087, 0.381, 0.0, 0.154]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.1309 | Steps: 4 | Val loss: 2.0935 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.0013 | Steps: 4 | Val loss: 2.0595 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.2496 | Steps: 4 | Val loss: 3.0177 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:21:36 (running for 00:28:29.25)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.303 |      0.226 |                   46 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.192 |      0.322 |                   46 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.01  |      0.305 |                   43 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.131 |      0.146 |                   43 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.13152985074626866
[2m[36m(func pid=19545)[0m top5: 0.804570895522388
[2m[36m(func pid=19545)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=19545)[0m f1_macro: 0.14634004269339485
[2m[36m(func pid=19545)[0m f1_weighted: 0.1033036518123381
[2m[36m(func pid=19545)[0m f1_per_class: [0.094, 0.247, 0.202, 0.023, 0.091, 0.167, 0.0, 0.477, 0.094, 0.069]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.30597014925373134
[2m[36m(func pid=17891)[0m top5: 0.886660447761194
[2m[36m(func pid=17891)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=17891)[0m f1_macro: 0.2753465124203728
[2m[36m(func pid=17891)[0m f1_weighted: 0.31922343059156344
[2m[36m(func pid=17891)[0m f1_per_class: [0.288, 0.268, 0.155, 0.5, 0.051, 0.286, 0.194, 0.439, 0.172, 0.4]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1814 | Steps: 4 | Val loss: 1.9963 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=18906)[0m top1: 0.26399253731343286
[2m[36m(func pid=18906)[0m top5: 0.8446828358208955
[2m[36m(func pid=18906)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=18906)[0m f1_macro: 0.22578800493811063
[2m[36m(func pid=18906)[0m f1_weighted: 0.21638552546826428
[2m[36m(func pid=18906)[0m f1_per_class: [0.031, 0.005, 0.556, 0.575, 0.047, 0.101, 0.003, 0.508, 0.174, 0.257]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.28404850746268656
[2m[36m(func pid=17787)[0m top5: 0.8675373134328358
[2m[36m(func pid=17787)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=17787)[0m f1_macro: 0.23859887051175246
[2m[36m(func pid=17787)[0m f1_weighted: 0.2950230682003537
[2m[36m(func pid=17787)[0m f1_per_class: [0.239, 0.413, 0.324, 0.425, 0.045, 0.252, 0.145, 0.433, 0.0, 0.111]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2264 | Steps: 4 | Val loss: 1.9516 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.4418 | Steps: 4 | Val loss: 2.0585 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3687 | Steps: 4 | Val loss: 2.3533 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=19545)[0m top1: 0.15671641791044777
[2m[36m(func pid=19545)[0m top5: 0.8073694029850746
[2m[36m(func pid=19545)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=19545)[0m f1_macro: 0.16919229873840397
[2m[36m(func pid=19545)[0m f1_weighted: 0.1384859089749005
[2m[36m(func pid=19545)[0m f1_per_class: [0.098, 0.108, 0.333, 0.134, 0.089, 0.231, 0.063, 0.489, 0.112, 0.033]
[2m[36m(func pid=19545)[0m 
== Status ==
Current time: 2024-01-07 13:21:42 (running for 00:28:34.62)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.181 |      0.239 |                   47 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.001 |      0.275 |                   47 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.25  |      0.226 |                   44 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.226 |      0.169 |                   44 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1907 | Steps: 4 | Val loss: 1.9866 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=17891)[0m top1: 0.27705223880597013
[2m[36m(func pid=17891)[0m top5: 0.8759328358208955
[2m[36m(func pid=17891)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=17891)[0m f1_macro: 0.21680149120244985
[2m[36m(func pid=17891)[0m f1_weighted: 0.2699600866500266
[2m[36m(func pid=17891)[0m f1_per_class: [0.241, 0.282, 0.104, 0.505, 0.081, 0.227, 0.084, 0.304, 0.09, 0.25]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.41511194029850745
[2m[36m(func pid=18906)[0m top5: 0.8717350746268657
[2m[36m(func pid=18906)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=18906)[0m f1_macro: 0.21307789054598972
[2m[36m(func pid=18906)[0m f1_weighted: 0.3933984602308396
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.135, 0.0, 0.575, 0.07, 0.201, 0.589, 0.0, 0.197, 0.364]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2849813432835821
[2m[36m(func pid=17787)[0m top5: 0.8745335820895522
[2m[36m(func pid=17787)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=17787)[0m f1_macro: 0.23700487697474903
[2m[36m(func pid=17787)[0m f1_weighted: 0.30191563454360365
[2m[36m(func pid=17787)[0m f1_per_class: [0.255, 0.382, 0.255, 0.413, 0.049, 0.297, 0.183, 0.414, 0.0, 0.121]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2357 | Steps: 4 | Val loss: 2.3063 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9353 | Steps: 4 | Val loss: 1.8659 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8412 | Steps: 4 | Val loss: 2.0695 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 13:21:47 (running for 00:28:40.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.191 |      0.237 |                   48 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.236 |      0.249 |                   49 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  2.369 |      0.213 |                   45 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.226 |      0.169 |                   44 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.30783582089552236
[2m[36m(func pid=17891)[0m top5: 0.8652052238805971
[2m[36m(func pid=17891)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=17891)[0m f1_macro: 0.2488144180949666
[2m[36m(func pid=17891)[0m f1_weighted: 0.3016917424324645
[2m[36m(func pid=17891)[0m f1_per_class: [0.258, 0.376, 0.3, 0.535, 0.111, 0.187, 0.101, 0.397, 0.133, 0.091]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.3493470149253731
[2m[36m(func pid=19545)[0m top5: 0.8246268656716418
[2m[36m(func pid=19545)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=19545)[0m f1_macro: 0.1953838221897231
[2m[36m(func pid=19545)[0m f1_weighted: 0.3558028817304318
[2m[36m(func pid=19545)[0m f1_per_class: [0.0, 0.326, 0.333, 0.562, 0.069, 0.124, 0.411, 0.0, 0.103, 0.025]
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1982 | Steps: 4 | Val loss: 1.9815 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.31902985074626866
[2m[36m(func pid=18906)[0m top5: 0.8759328358208955
[2m[36m(func pid=18906)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=18906)[0m f1_macro: 0.22727086559508725
[2m[36m(func pid=18906)[0m f1_weighted: 0.3302246309444402
[2m[36m(func pid=18906)[0m f1_per_class: [0.138, 0.525, 0.055, 0.481, 0.078, 0.008, 0.246, 0.346, 0.145, 0.25]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.28171641791044777
[2m[36m(func pid=17787)[0m top5: 0.8689365671641791
[2m[36m(func pid=17787)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=17787)[0m f1_macro: 0.22973319120271007
[2m[36m(func pid=17787)[0m f1_weighted: 0.3016546112456148
[2m[36m(func pid=17787)[0m f1_per_class: [0.215, 0.401, 0.209, 0.379, 0.051, 0.278, 0.213, 0.408, 0.023, 0.121]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4231 | Steps: 4 | Val loss: 2.0777 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.3022 | Steps: 4 | Val loss: 3.0853 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.8361 | Steps: 4 | Val loss: 2.6349 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:21:53 (running for 00:28:45.59)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.198 |      0.23  |                   49 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.236 |      0.249 |                   49 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.841 |      0.227 |                   46 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.423 |      0.162 |                   46 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.28591417910447764
[2m[36m(func pid=19545)[0m top5: 0.7826492537313433
[2m[36m(func pid=19545)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=19545)[0m f1_macro: 0.16153708708708087
[2m[36m(func pid=19545)[0m f1_weighted: 0.28280916719727955
[2m[36m(func pid=19545)[0m f1_per_class: [0.038, 0.384, 0.21, 0.53, 0.053, 0.125, 0.163, 0.0, 0.1, 0.012]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.0670 | Steps: 4 | Val loss: 1.9744 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=17891)[0m top1: 0.25513059701492535
[2m[36m(func pid=17891)[0m top5: 0.8372201492537313
[2m[36m(func pid=17891)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=17891)[0m f1_macro: 0.2664377398501268
[2m[36m(func pid=17891)[0m f1_weighted: 0.2907093330671886
[2m[36m(func pid=17891)[0m f1_per_class: [0.168, 0.457, 0.7, 0.452, 0.105, 0.066, 0.135, 0.466, 0.062, 0.053]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.18563432835820895
[2m[36m(func pid=18906)[0m top5: 0.7863805970149254
[2m[36m(func pid=18906)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=18906)[0m f1_macro: 0.16277414447173616
[2m[36m(func pid=18906)[0m f1_weighted: 0.1631828453381094
[2m[36m(func pid=18906)[0m f1_per_class: [0.2, 0.598, 0.025, 0.039, 0.05, 0.0, 0.048, 0.452, 0.08, 0.136]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.27238805970149255
[2m[36m(func pid=17787)[0m top5: 0.8703358208955224
[2m[36m(func pid=17787)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=17787)[0m f1_macro: 0.23370603747640645
[2m[36m(func pid=17787)[0m f1_weighted: 0.2810909242276178
[2m[36m(func pid=17787)[0m f1_per_class: [0.236, 0.422, 0.27, 0.326, 0.054, 0.253, 0.192, 0.375, 0.023, 0.188]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9917 | Steps: 4 | Val loss: 2.2903 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1108 | Steps: 4 | Val loss: 2.0753 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7201 | Steps: 4 | Val loss: 3.9616 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 13:21:58 (running for 00:28:50.99)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.067 |      0.234 |                   50 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.302 |      0.266 |                   50 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.836 |      0.163 |                   47 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.992 |      0.187 |                   47 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.23880597014925373
[2m[36m(func pid=19545)[0m top5: 0.7826492537313433
[2m[36m(func pid=19545)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=19545)[0m f1_macro: 0.1870423227201725
[2m[36m(func pid=19545)[0m f1_weighted: 0.22952453710357681
[2m[36m(func pid=19545)[0m f1_per_class: [0.034, 0.183, 0.214, 0.464, 0.059, 0.241, 0.015, 0.538, 0.071, 0.052]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1838 | Steps: 4 | Val loss: 1.9916 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=17891)[0m top1: 0.35494402985074625
[2m[36m(func pid=17891)[0m top5: 0.8763992537313433
[2m[36m(func pid=17891)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=17891)[0m f1_macro: 0.2870419980173954
[2m[36m(func pid=17891)[0m f1_weighted: 0.4068701971319371
[2m[36m(func pid=17891)[0m f1_per_class: [0.138, 0.459, 0.0, 0.385, 0.077, 0.349, 0.458, 0.542, 0.189, 0.274]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.18516791044776118
[2m[36m(func pid=18906)[0m top5: 0.761660447761194
[2m[36m(func pid=18906)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=18906)[0m f1_macro: 0.17578813524439763
[2m[36m(func pid=18906)[0m f1_weighted: 0.21487148157659725
[2m[36m(func pid=18906)[0m f1_per_class: [0.097, 0.175, 0.316, 0.066, 0.101, 0.072, 0.433, 0.38, 0.04, 0.077]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2621268656716418
[2m[36m(func pid=17787)[0m top5: 0.8596082089552238
[2m[36m(func pid=17787)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=17787)[0m f1_macro: 0.21740139652363188
[2m[36m(func pid=17787)[0m f1_weighted: 0.2649927152887476
[2m[36m(func pid=17787)[0m f1_per_class: [0.213, 0.441, 0.211, 0.281, 0.052, 0.276, 0.164, 0.384, 0.022, 0.13]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4566 | Steps: 4 | Val loss: 2.3862 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6329 | Steps: 4 | Val loss: 1.9423 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8665 | Steps: 4 | Val loss: 4.7025 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:22:04 (running for 00:28:56.55)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.184 |      0.217 |                   51 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.111 |      0.287 |                   51 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.72  |      0.176 |                   48 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.457 |      0.198 |                   48 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.22994402985074627
[2m[36m(func pid=19545)[0m top5: 0.7798507462686567
[2m[36m(func pid=19545)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=19545)[0m f1_macro: 0.19827235310763333
[2m[36m(func pid=19545)[0m f1_weighted: 0.20645412744564887
[2m[36m(func pid=19545)[0m f1_per_class: [0.067, 0.098, 0.294, 0.42, 0.08, 0.289, 0.0, 0.496, 0.194, 0.045]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.373134328358209
[2m[36m(func pid=17891)[0m top5: 0.8675373134328358
[2m[36m(func pid=17891)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=17891)[0m f1_macro: 0.2973762585752956
[2m[36m(func pid=17891)[0m f1_weighted: 0.42214563451909665
[2m[36m(func pid=17891)[0m f1_per_class: [0.175, 0.537, 0.0, 0.394, 0.06, 0.258, 0.488, 0.536, 0.173, 0.353]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1082 | Steps: 4 | Val loss: 2.0071 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=18906)[0m top1: 0.14925373134328357
[2m[36m(func pid=18906)[0m top5: 0.7639925373134329
[2m[36m(func pid=18906)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=18906)[0m f1_macro: 0.14840668099996088
[2m[36m(func pid=18906)[0m f1_weighted: 0.173650224535908
[2m[36m(func pid=18906)[0m f1_per_class: [0.178, 0.107, 0.314, 0.331, 0.062, 0.007, 0.128, 0.298, 0.0, 0.058]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.25046641791044777
[2m[36m(func pid=17787)[0m top5: 0.8381529850746269
[2m[36m(func pid=17787)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=17787)[0m f1_macro: 0.21244121202854188
[2m[36m(func pid=17787)[0m f1_weighted: 0.25974199891953
[2m[36m(func pid=17787)[0m f1_per_class: [0.205, 0.423, 0.192, 0.256, 0.052, 0.237, 0.189, 0.384, 0.095, 0.091]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2864 | Steps: 4 | Val loss: 2.1678 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0128 | Steps: 4 | Val loss: 1.9658 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.7280 | Steps: 4 | Val loss: 2.6660 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:22:09 (running for 00:29:01.98)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.108 |      0.212 |                   52 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.633 |      0.297 |                   52 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.866 |      0.148 |                   49 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.286 |      0.167 |                   49 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.19309701492537312
[2m[36m(func pid=19545)[0m top5: 0.7709888059701493
[2m[36m(func pid=19545)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=19545)[0m f1_macro: 0.16734755717439698
[2m[36m(func pid=19545)[0m f1_weighted: 0.1934704309314643
[2m[36m(func pid=19545)[0m f1_per_class: [0.067, 0.376, 0.141, 0.286, 0.053, 0.126, 0.0, 0.48, 0.116, 0.029]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.36427238805970147
[2m[36m(func pid=17891)[0m top5: 0.8722014925373134
[2m[36m(func pid=17891)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=17891)[0m f1_macro: 0.3235001215717995
[2m[36m(func pid=17891)[0m f1_weighted: 0.3899663758246206
[2m[36m(func pid=17891)[0m f1_per_class: [0.319, 0.569, 0.3, 0.407, 0.077, 0.215, 0.355, 0.514, 0.183, 0.296]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1346 | Steps: 4 | Val loss: 2.0058 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=18906)[0m top1: 0.28125
[2m[36m(func pid=18906)[0m top5: 0.8624067164179104
[2m[36m(func pid=18906)[0m f1_micro: 0.28125
[2m[36m(func pid=18906)[0m f1_macro: 0.2365723311038055
[2m[36m(func pid=18906)[0m f1_weighted: 0.3078559425156253
[2m[36m(func pid=18906)[0m f1_per_class: [0.289, 0.026, 0.344, 0.427, 0.051, 0.0, 0.487, 0.413, 0.139, 0.19]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2490671641791045
[2m[36m(func pid=17787)[0m top5: 0.8334888059701493
[2m[36m(func pid=17787)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=17787)[0m f1_macro: 0.22570294490508158
[2m[36m(func pid=17787)[0m f1_weighted: 0.2663671052130612
[2m[36m(func pid=17787)[0m f1_per_class: [0.236, 0.401, 0.289, 0.257, 0.046, 0.21, 0.221, 0.428, 0.098, 0.071]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2535 | Steps: 4 | Val loss: 2.4392 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0162 | Steps: 4 | Val loss: 2.2681 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.9783 | Steps: 4 | Val loss: 2.1505 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:22:15 (running for 00:29:07.54)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.135 |      0.226 |                   53 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.013 |      0.324 |                   53 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.728 |      0.237 |                   50 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.254 |      0.142 |                   50 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.18236940298507462
[2m[36m(func pid=19545)[0m top5: 0.6949626865671642
[2m[36m(func pid=19545)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=19545)[0m f1_macro: 0.14238051310481117
[2m[36m(func pid=19545)[0m f1_weighted: 0.16031156866905136
[2m[36m(func pid=19545)[0m f1_per_class: [0.067, 0.472, 0.075, 0.035, 0.016, 0.0, 0.107, 0.558, 0.093, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1180 | Steps: 4 | Val loss: 1.9973 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=17891)[0m top1: 0.292910447761194
[2m[36m(func pid=17891)[0m top5: 0.8675373134328358
[2m[36m(func pid=17891)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=17891)[0m f1_macro: 0.2461501373405255
[2m[36m(func pid=17891)[0m f1_weighted: 0.31046836306814635
[2m[36m(func pid=17891)[0m f1_per_class: [0.125, 0.567, 0.114, 0.373, 0.129, 0.105, 0.198, 0.464, 0.109, 0.278]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.33722014925373134
[2m[36m(func pid=18906)[0m top5: 0.8927238805970149
[2m[36m(func pid=18906)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=18906)[0m f1_macro: 0.2656767527573457
[2m[36m(func pid=18906)[0m f1_weighted: 0.32261108127059823
[2m[36m(func pid=18906)[0m f1_per_class: [0.337, 0.046, 0.364, 0.392, 0.076, 0.0, 0.53, 0.5, 0.176, 0.235]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.26026119402985076
[2m[36m(func pid=17787)[0m top5: 0.835820895522388
[2m[36m(func pid=17787)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=17787)[0m f1_macro: 0.24776151569126195
[2m[36m(func pid=17787)[0m f1_weighted: 0.292604988336202
[2m[36m(func pid=17787)[0m f1_per_class: [0.26, 0.383, 0.289, 0.236, 0.043, 0.253, 0.308, 0.471, 0.118, 0.115]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0925 | Steps: 4 | Val loss: 2.3713 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2308 | Steps: 4 | Val loss: 1.8693 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1952 | Steps: 4 | Val loss: 2.3170 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:22:20 (running for 00:29:12.73)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.118 |      0.248 |                   54 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.016 |      0.246 |                   54 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.978 |      0.266 |                   51 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.093 |      0.132 |                   51 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.14832089552238806
[2m[36m(func pid=19545)[0m top5: 0.7378731343283582
[2m[36m(func pid=19545)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=19545)[0m f1_macro: 0.1317718782570063
[2m[36m(func pid=19545)[0m f1_weighted: 0.1681128728030535
[2m[36m(func pid=19545)[0m f1_per_class: [0.072, 0.192, 0.036, 0.099, 0.041, 0.029, 0.232, 0.509, 0.107, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.34654850746268656
[2m[36m(func pid=17891)[0m top5: 0.8917910447761194
[2m[36m(func pid=17891)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=17891)[0m f1_macro: 0.2783961166117898
[2m[36m(func pid=17891)[0m f1_weighted: 0.3432622514689513
[2m[36m(func pid=17891)[0m f1_per_class: [0.0, 0.501, 0.233, 0.323, 0.131, 0.192, 0.358, 0.423, 0.194, 0.429]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.9922 | Steps: 4 | Val loss: 1.9887 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=18906)[0m top1: 0.3302238805970149
[2m[36m(func pid=18906)[0m top5: 0.8931902985074627
[2m[36m(func pid=18906)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=18906)[0m f1_macro: 0.2639556656678511
[2m[36m(func pid=18906)[0m f1_weighted: 0.36387643059630786
[2m[36m(func pid=18906)[0m f1_per_class: [0.148, 0.276, 0.4, 0.394, 0.113, 0.283, 0.476, 0.395, 0.061, 0.094]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.27658582089552236
[2m[36m(func pid=17787)[0m top5: 0.8381529850746269
[2m[36m(func pid=17787)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=17787)[0m f1_macro: 0.260093052271526
[2m[36m(func pid=17787)[0m f1_weighted: 0.31777587415765074
[2m[36m(func pid=17787)[0m f1_per_class: [0.238, 0.372, 0.333, 0.244, 0.044, 0.27, 0.384, 0.49, 0.109, 0.118]
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1683 | Steps: 4 | Val loss: 2.4397 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1358 | Steps: 4 | Val loss: 1.7899 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.2659 | Steps: 4 | Val loss: 3.2972 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 13:22:26 (running for 00:29:18.48)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.992 |      0.26  |                   55 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.231 |      0.278 |                   55 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.195 |      0.264 |                   52 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.168 |      0.155 |                   52 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.1669776119402985
[2m[36m(func pid=19545)[0m top5: 0.7448694029850746
[2m[36m(func pid=19545)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=19545)[0m f1_macro: 0.15547709632423945
[2m[36m(func pid=19545)[0m f1_weighted: 0.19294330467175516
[2m[36m(func pid=19545)[0m f1_per_class: [0.069, 0.117, 0.072, 0.179, 0.039, 0.198, 0.211, 0.529, 0.14, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.408115671641791
[2m[36m(func pid=17891)[0m top5: 0.8927238805970149
[2m[36m(func pid=17891)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=17891)[0m f1_macro: 0.3750799112431213
[2m[36m(func pid=17891)[0m f1_weighted: 0.42619027837825996
[2m[36m(func pid=17891)[0m f1_per_class: [0.456, 0.515, 0.533, 0.38, 0.135, 0.248, 0.507, 0.506, 0.17, 0.3]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0868 | Steps: 4 | Val loss: 1.9857 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=18906)[0m top1: 0.23507462686567165
[2m[36m(func pid=18906)[0m top5: 0.8544776119402985
[2m[36m(func pid=18906)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=18906)[0m f1_macro: 0.2455652615608408
[2m[36m(func pid=18906)[0m f1_weighted: 0.2635561177311798
[2m[36m(func pid=18906)[0m f1_per_class: [0.102, 0.413, 0.333, 0.212, 0.121, 0.268, 0.205, 0.515, 0.156, 0.129]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.0246 | Steps: 4 | Val loss: 2.7998 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=17787)[0m top1: 0.27005597014925375
[2m[36m(func pid=17787)[0m top5: 0.8381529850746269
[2m[36m(func pid=17787)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=17787)[0m f1_macro: 0.25352155613751526
[2m[36m(func pid=17787)[0m f1_weighted: 0.30076253446409656
[2m[36m(func pid=17787)[0m f1_per_class: [0.215, 0.399, 0.31, 0.214, 0.046, 0.255, 0.349, 0.464, 0.12, 0.164]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1419 | Steps: 4 | Val loss: 1.8871 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0151 | Steps: 4 | Val loss: 3.6366 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:22:31 (running for 00:29:23.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.087 |      0.254 |                   56 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.136 |      0.375 |                   56 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.266 |      0.246 |                   53 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.025 |      0.156 |                   53 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.19029850746268656
[2m[36m(func pid=19545)[0m top5: 0.7154850746268657
[2m[36m(func pid=19545)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=19545)[0m f1_macro: 0.1558125098520754
[2m[36m(func pid=19545)[0m f1_weighted: 0.2106675850203792
[2m[36m(func pid=19545)[0m f1_per_class: [0.05, 0.045, 0.188, 0.25, 0.037, 0.229, 0.257, 0.459, 0.044, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.38526119402985076
[2m[36m(func pid=17891)[0m top5: 0.8936567164179104
[2m[36m(func pid=17891)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=17891)[0m f1_macro: 0.3136423883792686
[2m[36m(func pid=17891)[0m f1_weighted: 0.41717730029663314
[2m[36m(func pid=17891)[0m f1_per_class: [0.425, 0.527, 0.0, 0.449, 0.101, 0.283, 0.407, 0.5, 0.177, 0.267]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1135 | Steps: 4 | Val loss: 1.9718 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=18906)[0m top1: 0.22061567164179105
[2m[36m(func pid=18906)[0m top5: 0.8134328358208955
[2m[36m(func pid=18906)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=18906)[0m f1_macro: 0.22685063350402984
[2m[36m(func pid=18906)[0m f1_weighted: 0.1939614438085443
[2m[36m(func pid=18906)[0m f1_per_class: [0.105, 0.462, 0.235, 0.128, 0.127, 0.381, 0.0, 0.338, 0.248, 0.243]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.9142 | Steps: 4 | Val loss: 2.2304 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=17787)[0m top1: 0.26492537313432835
[2m[36m(func pid=17787)[0m top5: 0.8381529850746269
[2m[36m(func pid=17787)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=17787)[0m f1_macro: 0.24372624352786382
[2m[36m(func pid=17787)[0m f1_weighted: 0.29229177880588464
[2m[36m(func pid=17787)[0m f1_per_class: [0.217, 0.364, 0.293, 0.199, 0.046, 0.248, 0.36, 0.456, 0.126, 0.129]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2895 | Steps: 4 | Val loss: 1.9003 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2300 | Steps: 4 | Val loss: 2.5721 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:22:37 (running for 00:29:29.36)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.113 |      0.244 |                   57 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.142 |      0.314 |                   57 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.015 |      0.227 |                   54 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.914 |      0.23  |                   54 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.3381529850746269
[2m[36m(func pid=19545)[0m top5: 0.7290111940298507
[2m[36m(func pid=19545)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=19545)[0m f1_macro: 0.22964943633953264
[2m[36m(func pid=19545)[0m f1_weighted: 0.3636899302377602
[2m[36m(func pid=19545)[0m f1_per_class: [0.071, 0.405, 0.202, 0.248, 0.026, 0.136, 0.579, 0.528, 0.102, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.363339552238806
[2m[36m(func pid=17891)[0m top5: 0.8899253731343284
[2m[36m(func pid=17891)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=17891)[0m f1_macro: 0.398167570126108
[2m[36m(func pid=17891)[0m f1_weighted: 0.37667656911705
[2m[36m(func pid=17891)[0m f1_per_class: [0.419, 0.53, 0.917, 0.465, 0.082, 0.3, 0.225, 0.494, 0.223, 0.327]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1054 | Steps: 4 | Val loss: 1.9699 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=18906)[0m top1: 0.29757462686567165
[2m[36m(func pid=18906)[0m top5: 0.8330223880597015
[2m[36m(func pid=18906)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=18906)[0m f1_macro: 0.25196758908149264
[2m[36m(func pid=18906)[0m f1_weighted: 0.2794136556278065
[2m[36m(func pid=18906)[0m f1_per_class: [0.256, 0.447, 0.231, 0.427, 0.099, 0.268, 0.037, 0.459, 0.205, 0.091]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6363 | Steps: 4 | Val loss: 2.0361 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=17787)[0m top1: 0.26865671641791045
[2m[36m(func pid=17787)[0m top5: 0.835820895522388
[2m[36m(func pid=17787)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=17787)[0m f1_macro: 0.24484487988609333
[2m[36m(func pid=17787)[0m f1_weighted: 0.29937021905306516
[2m[36m(func pid=17787)[0m f1_per_class: [0.237, 0.361, 0.229, 0.242, 0.048, 0.268, 0.339, 0.438, 0.138, 0.149]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0440 | Steps: 4 | Val loss: 1.9873 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2435 | Steps: 4 | Val loss: 2.3387 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:22:42 (running for 00:29:34.77)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.105 |      0.245 |                   58 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.289 |      0.398 |                   58 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.23  |      0.252 |                   55 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.636 |      0.221 |                   55 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.35261194029850745
[2m[36m(func pid=19545)[0m top5: 0.7607276119402985
[2m[36m(func pid=19545)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=19545)[0m f1_macro: 0.22094044954293607
[2m[36m(func pid=19545)[0m f1_weighted: 0.3620210550391558
[2m[36m(func pid=19545)[0m f1_per_class: [0.077, 0.43, 0.154, 0.245, 0.036, 0.03, 0.602, 0.526, 0.11, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.34421641791044777
[2m[36m(func pid=17891)[0m top5: 0.8833955223880597
[2m[36m(func pid=17891)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=17891)[0m f1_macro: 0.3417242339866441
[2m[36m(func pid=17891)[0m f1_weighted: 0.34643937935127944
[2m[36m(func pid=17891)[0m f1_per_class: [0.431, 0.498, 0.4, 0.507, 0.068, 0.28, 0.117, 0.494, 0.236, 0.387]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9295 | Steps: 4 | Val loss: 1.9752 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=18906)[0m top1: 0.38152985074626866
[2m[36m(func pid=18906)[0m top5: 0.863339552238806
[2m[36m(func pid=18906)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=18906)[0m f1_macro: 0.3525433973877766
[2m[36m(func pid=18906)[0m f1_weighted: 0.4080668976948143
[2m[36m(func pid=18906)[0m f1_per_class: [0.083, 0.43, 0.8, 0.475, 0.067, 0.327, 0.391, 0.505, 0.292, 0.155]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1406 | Steps: 4 | Val loss: 2.1102 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1240 | Steps: 4 | Val loss: 1.8657 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=17787)[0m top1: 0.28078358208955223
[2m[36m(func pid=17787)[0m top5: 0.8414179104477612
[2m[36m(func pid=17787)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=17787)[0m f1_macro: 0.2561738142227135
[2m[36m(func pid=17787)[0m f1_weighted: 0.3146421943163335
[2m[36m(func pid=17787)[0m f1_per_class: [0.217, 0.411, 0.203, 0.265, 0.05, 0.29, 0.326, 0.448, 0.153, 0.197]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5419 | Steps: 4 | Val loss: 2.7293 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 13:22:48 (running for 00:29:40.29)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.929 |      0.256 |                   59 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.044 |      0.342 |                   59 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.243 |      0.353 |                   56 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.141 |      0.164 |                   56 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.19542910447761194
[2m[36m(func pid=19545)[0m top5: 0.7765858208955224
[2m[36m(func pid=19545)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=19545)[0m f1_macro: 0.16405779927258618
[2m[36m(func pid=19545)[0m f1_weighted: 0.2434818382187731
[2m[36m(func pid=19545)[0m f1_per_class: [0.063, 0.404, 0.083, 0.176, 0.05, 0.0, 0.316, 0.441, 0.084, 0.024]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.3726679104477612
[2m[36m(func pid=17891)[0m top5: 0.886660447761194
[2m[36m(func pid=17891)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=17891)[0m f1_macro: 0.3247560302097291
[2m[36m(func pid=17891)[0m f1_weighted: 0.3545497451934809
[2m[36m(func pid=17891)[0m f1_per_class: [0.265, 0.547, 0.3, 0.541, 0.079, 0.323, 0.085, 0.45, 0.258, 0.4]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.9928 | Steps: 4 | Val loss: 1.9725 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=18906)[0m top1: 0.3666044776119403
[2m[36m(func pid=18906)[0m top5: 0.8418843283582089
[2m[36m(func pid=18906)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=18906)[0m f1_macro: 0.3071507334422271
[2m[36m(func pid=18906)[0m f1_weighted: 0.3385224219054823
[2m[36m(func pid=18906)[0m f1_per_class: [0.211, 0.537, 0.828, 0.568, 0.046, 0.158, 0.104, 0.412, 0.13, 0.078]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.9687 | Steps: 4 | Val loss: 2.2099 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=17787)[0m top1: 0.2751865671641791
[2m[36m(func pid=17787)[0m top5: 0.8521455223880597
[2m[36m(func pid=17787)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=17787)[0m f1_macro: 0.2465197251119445
[2m[36m(func pid=17787)[0m f1_weighted: 0.3101130220639361
[2m[36m(func pid=17787)[0m f1_per_class: [0.201, 0.413, 0.168, 0.3, 0.048, 0.25, 0.298, 0.433, 0.158, 0.197]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2857 | Steps: 4 | Val loss: 1.9211 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2915 | Steps: 4 | Val loss: 3.0377 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:22:53 (running for 00:29:45.78)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.993 |      0.247 |                   60 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.124 |      0.325 |                   60 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.542 |      0.307 |                   57 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.969 |      0.156 |                   57 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.15298507462686567
[2m[36m(func pid=19545)[0m top5: 0.7490671641791045
[2m[36m(func pid=19545)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=19545)[0m f1_macro: 0.15604046197064358
[2m[36m(func pid=19545)[0m f1_weighted: 0.16929784374287948
[2m[36m(func pid=19545)[0m f1_per_class: [0.077, 0.344, 0.212, 0.263, 0.027, 0.008, 0.0, 0.519, 0.067, 0.043]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.3414179104477612
[2m[36m(func pid=17891)[0m top5: 0.8941231343283582
[2m[36m(func pid=17891)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=17891)[0m f1_macro: 0.27691387012878727
[2m[36m(func pid=17891)[0m f1_weighted: 0.3367659928227164
[2m[36m(func pid=17891)[0m f1_per_class: [0.082, 0.54, 0.084, 0.476, 0.107, 0.389, 0.102, 0.382, 0.185, 0.423]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.0377 | Steps: 4 | Val loss: 1.9770 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=18906)[0m top1: 0.33955223880597013
[2m[36m(func pid=18906)[0m top5: 0.8064365671641791
[2m[36m(func pid=18906)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=18906)[0m f1_macro: 0.29750387766537
[2m[36m(func pid=18906)[0m f1_weighted: 0.32867736403454756
[2m[36m(func pid=18906)[0m f1_per_class: [0.283, 0.459, 0.381, 0.559, 0.048, 0.14, 0.105, 0.454, 0.246, 0.3]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.0012 | Steps: 4 | Val loss: 2.2444 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=17787)[0m top1: 0.2733208955223881
[2m[36m(func pid=17787)[0m top5: 0.8502798507462687
[2m[36m(func pid=17787)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=17787)[0m f1_macro: 0.23899916135216195
[2m[36m(func pid=17787)[0m f1_weighted: 0.30780917156349435
[2m[36m(func pid=17787)[0m f1_per_class: [0.164, 0.44, 0.136, 0.284, 0.047, 0.243, 0.291, 0.461, 0.161, 0.163]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1496 | Steps: 4 | Val loss: 2.0445 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.1065 | Steps: 4 | Val loss: 3.5372 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:22:58 (running for 00:29:51.21)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.038 |      0.239 |                   61 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.286 |      0.277 |                   61 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.292 |      0.298 |                   58 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.001 |      0.151 |                   58 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.16138059701492538
[2m[36m(func pid=19545)[0m top5: 0.753731343283582
[2m[36m(func pid=19545)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=19545)[0m f1_macro: 0.1513625987156832
[2m[36m(func pid=19545)[0m f1_weighted: 0.1805546166684857
[2m[36m(func pid=19545)[0m f1_per_class: [0.085, 0.37, 0.19, 0.292, 0.022, 0.007, 0.006, 0.478, 0.062, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.333955223880597
[2m[36m(func pid=17891)[0m top5: 0.8796641791044776
[2m[36m(func pid=17891)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=17891)[0m f1_macro: 0.2906144836398423
[2m[36m(func pid=17891)[0m f1_weighted: 0.3491154575969287
[2m[36m(func pid=17891)[0m f1_per_class: [0.274, 0.524, 0.078, 0.463, 0.092, 0.302, 0.163, 0.511, 0.204, 0.294]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1132 | Steps: 4 | Val loss: 1.9903 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=18906)[0m top1: 0.21222014925373134
[2m[36m(func pid=18906)[0m top5: 0.7737873134328358
[2m[36m(func pid=18906)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=18906)[0m f1_macro: 0.1795639105602101
[2m[36m(func pid=18906)[0m f1_weighted: 0.2014703115280845
[2m[36m(func pid=18906)[0m f1_per_class: [0.254, 0.036, 0.267, 0.47, 0.042, 0.212, 0.074, 0.015, 0.234, 0.193]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0674 | Steps: 4 | Val loss: 2.0841 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=17787)[0m top1: 0.2733208955223881
[2m[36m(func pid=17787)[0m top5: 0.8423507462686567
[2m[36m(func pid=17787)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=17787)[0m f1_macro: 0.24446501850926428
[2m[36m(func pid=17787)[0m f1_weighted: 0.31338026872374025
[2m[36m(func pid=17787)[0m f1_per_class: [0.166, 0.411, 0.129, 0.334, 0.047, 0.271, 0.264, 0.469, 0.183, 0.17]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.1354 | Steps: 4 | Val loss: 2.9864 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.6302 | Steps: 4 | Val loss: 3.7759 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:23:04 (running for 00:29:56.63)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.113 |      0.244 |                   62 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.15  |      0.291 |                   62 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.106 |      0.18  |                   59 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.067 |      0.188 |                   59 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.21828358208955223
[2m[36m(func pid=19545)[0m top5: 0.7882462686567164
[2m[36m(func pid=19545)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=19545)[0m f1_macro: 0.18775251132334694
[2m[36m(func pid=19545)[0m f1_weighted: 0.24547693492965533
[2m[36m(func pid=19545)[0m f1_per_class: [0.086, 0.426, 0.167, 0.371, 0.023, 0.084, 0.086, 0.448, 0.11, 0.077]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.21548507462686567
[2m[36m(func pid=17891)[0m top5: 0.8306902985074627
[2m[36m(func pid=17891)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=17891)[0m f1_macro: 0.20663764992932881
[2m[36m(func pid=17891)[0m f1_weighted: 0.24921060802456604
[2m[36m(func pid=17891)[0m f1_per_class: [0.333, 0.415, 0.06, 0.397, 0.136, 0.13, 0.065, 0.368, 0.066, 0.097]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1204 | Steps: 4 | Val loss: 1.9771 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=18906)[0m top1: 0.19263059701492538
[2m[36m(func pid=18906)[0m top5: 0.7336753731343284
[2m[36m(func pid=18906)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=18906)[0m f1_macro: 0.14257337542201287
[2m[36m(func pid=18906)[0m f1_weighted: 0.18049673924490228
[2m[36m(func pid=18906)[0m f1_per_class: [0.143, 0.096, 0.124, 0.448, 0.057, 0.202, 0.006, 0.058, 0.149, 0.142]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.7923 | Steps: 4 | Val loss: 1.9460 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=17787)[0m top1: 0.259794776119403
[2m[36m(func pid=17787)[0m top5: 0.8367537313432836
[2m[36m(func pid=17787)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=17787)[0m f1_macro: 0.2448974435914692
[2m[36m(func pid=17787)[0m f1_weighted: 0.28575113260902224
[2m[36m(func pid=17787)[0m f1_per_class: [0.175, 0.418, 0.164, 0.301, 0.045, 0.277, 0.196, 0.457, 0.165, 0.25]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6723 | Steps: 4 | Val loss: 3.3687 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.6115 | Steps: 4 | Val loss: 2.4068 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=19545)[0m top1: 0.292910447761194
[2m[36m(func pid=19545)[0m top5: 0.8143656716417911
[2m[36m(func pid=19545)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=19545)[0m f1_macro: 0.1761536443479647
[2m[36m(func pid=19545)[0m f1_weighted: 0.33522101773065244
[2m[36m(func pid=19545)[0m f1_per_class: [0.061, 0.314, 0.109, 0.434, 0.037, 0.179, 0.44, 0.031, 0.124, 0.032]
== Status ==
Current time: 2024-01-07 13:23:09 (running for 00:30:02.13)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.12  |      0.245 |                   63 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.135 |      0.207 |                   63 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.63  |      0.143 |                   60 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.792 |      0.176 |                   60 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.18703358208955223
[2m[36m(func pid=17891)[0m top5: 0.8013059701492538
[2m[36m(func pid=17891)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=17891)[0m f1_macro: 0.16376108192131186
[2m[36m(func pid=17891)[0m f1_weighted: 0.22369828147188997
[2m[36m(func pid=17891)[0m f1_per_class: [0.29, 0.403, 0.066, 0.354, 0.148, 0.078, 0.107, 0.089, 0.036, 0.065]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.9915 | Steps: 4 | Val loss: 1.9710 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=18906)[0m top1: 0.3726679104477612
[2m[36m(func pid=18906)[0m top5: 0.8442164179104478
[2m[36m(func pid=18906)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=18906)[0m f1_macro: 0.31159052043293267
[2m[36m(func pid=18906)[0m f1_weighted: 0.3890211998689204
[2m[36m(func pid=18906)[0m f1_per_class: [0.203, 0.534, 0.304, 0.408, 0.042, 0.324, 0.347, 0.429, 0.254, 0.27]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8989 | Steps: 4 | Val loss: 1.9915 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=17787)[0m top1: 0.2658582089552239
[2m[36m(func pid=17787)[0m top5: 0.8311567164179104
[2m[36m(func pid=17787)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=17787)[0m f1_macro: 0.2517322930975
[2m[36m(func pid=17787)[0m f1_weighted: 0.2837194520522169
[2m[36m(func pid=17787)[0m f1_per_class: [0.259, 0.447, 0.233, 0.282, 0.046, 0.27, 0.19, 0.467, 0.137, 0.186]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.9479 | Steps: 4 | Val loss: 1.9422 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1931 | Steps: 4 | Val loss: 2.2481 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:23:15 (running for 00:30:07.67)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.991 |      0.252 |                   64 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.672 |      0.164 |                   64 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.611 |      0.312 |                   61 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.899 |      0.158 |                   61 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.2513992537313433
[2m[36m(func pid=19545)[0m top5: 0.792910447761194
[2m[36m(func pid=19545)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=19545)[0m f1_macro: 0.15829115014105066
[2m[36m(func pid=19545)[0m f1_weighted: 0.2652871787651331
[2m[36m(func pid=19545)[0m f1_per_class: [0.083, 0.027, 0.146, 0.249, 0.188, 0.068, 0.569, 0.092, 0.122, 0.039]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.3843283582089552
[2m[36m(func pid=17891)[0m top5: 0.8722014925373134
[2m[36m(func pid=17891)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=17891)[0m f1_macro: 0.3195273046980661
[2m[36m(func pid=17891)[0m f1_weighted: 0.4151312450188609
[2m[36m(func pid=17891)[0m f1_per_class: [0.341, 0.521, 0.169, 0.347, 0.123, 0.346, 0.487, 0.443, 0.206, 0.212]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2055 | Steps: 4 | Val loss: 1.9547 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=18906)[0m top1: 0.35027985074626866
[2m[36m(func pid=18906)[0m top5: 0.8903917910447762
[2m[36m(func pid=18906)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=18906)[0m f1_macro: 0.3330079476273967
[2m[36m(func pid=18906)[0m f1_weighted: 0.34800065002624275
[2m[36m(func pid=18906)[0m f1_per_class: [0.29, 0.428, 0.667, 0.467, 0.075, 0.137, 0.277, 0.41, 0.227, 0.352]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.23087686567164178
[2m[36m(func pid=17787)[0m top5: 0.8269589552238806
[2m[36m(func pid=17787)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=17787)[0m f1_macro: 0.23523116923260887
[2m[36m(func pid=17787)[0m f1_weighted: 0.23258760155553723
[2m[36m(func pid=17787)[0m f1_per_class: [0.288, 0.385, 0.235, 0.23, 0.045, 0.281, 0.104, 0.413, 0.143, 0.229]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3143 | Steps: 4 | Val loss: 2.2106 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1982 | Steps: 4 | Val loss: 1.9574 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.2599 | Steps: 4 | Val loss: 2.1867 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=17891)[0m top1: 0.3726679104477612
[2m[36m(func pid=17891)[0m top5: 0.8777985074626866
[2m[36m(func pid=17891)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=17891)[0m f1_macro: 0.30991509546570006
[2m[36m(func pid=17891)[0m f1_weighted: 0.41797646546111944
[2m[36m(func pid=17891)[0m f1_per_class: [0.224, 0.481, 0.129, 0.377, 0.102, 0.348, 0.486, 0.502, 0.228, 0.222]
[2m[36m(func pid=17891)[0m 
== Status ==
Current time: 2024-01-07 13:23:20 (running for 00:30:13.21)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.205 |      0.235 |                   65 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.198 |      0.31  |                   66 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.193 |      0.333 |                   62 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.899 |      0.158 |                   61 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.19076492537313433
[2m[36m(func pid=19545)[0m top5: 0.7565298507462687
[2m[36m(func pid=19545)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=19545)[0m f1_macro: 0.17596911072258434
[2m[36m(func pid=19545)[0m f1_weighted: 0.22986751300430877
[2m[36m(func pid=19545)[0m f1_per_class: [0.08, 0.005, 0.169, 0.284, 0.235, 0.007, 0.383, 0.465, 0.093, 0.038]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.9951 | Steps: 4 | Val loss: 1.9654 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=18906)[0m top1: 0.33955223880597013
[2m[36m(func pid=18906)[0m top5: 0.9076492537313433
[2m[36m(func pid=18906)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=18906)[0m f1_macro: 0.32757040269226645
[2m[36m(func pid=18906)[0m f1_weighted: 0.3305637084059783
[2m[36m(func pid=18906)[0m f1_per_class: [0.241, 0.366, 0.647, 0.513, 0.074, 0.286, 0.153, 0.436, 0.24, 0.32]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0931 | Steps: 4 | Val loss: 2.3829 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9954 | Steps: 4 | Val loss: 2.6223 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=17787)[0m top1: 0.23880597014925373
[2m[36m(func pid=17787)[0m top5: 0.8236940298507462
[2m[36m(func pid=17787)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=17787)[0m f1_macro: 0.22676168025024687
[2m[36m(func pid=17787)[0m f1_weighted: 0.23617974623916457
[2m[36m(func pid=17787)[0m f1_per_class: [0.275, 0.422, 0.179, 0.206, 0.047, 0.282, 0.12, 0.437, 0.109, 0.19]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9453 | Steps: 4 | Val loss: 2.3864 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:23:26 (running for 00:30:18.81)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.995 |      0.227 |                   66 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.093 |      0.229 |                   67 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.26  |      0.328 |                   63 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.314 |      0.176 |                   62 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.279384328358209
[2m[36m(func pid=17891)[0m top5: 0.8544776119402985
[2m[36m(func pid=17891)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=17891)[0m f1_macro: 0.22943523025036766
[2m[36m(func pid=17891)[0m f1_weighted: 0.3342283253479567
[2m[36m(func pid=17891)[0m f1_per_class: [0.163, 0.416, 0.092, 0.358, 0.089, 0.257, 0.352, 0.318, 0.118, 0.131]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.22527985074626866
[2m[36m(func pid=19545)[0m top5: 0.6847014925373134
[2m[36m(func pid=19545)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=19545)[0m f1_macro: 0.17454349445767803
[2m[36m(func pid=19545)[0m f1_weighted: 0.22074182093042394
[2m[36m(func pid=19545)[0m f1_per_class: [0.098, 0.225, 0.211, 0.534, 0.119, 0.008, 0.0, 0.438, 0.062, 0.051]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.26259328358208955
[2m[36m(func pid=18906)[0m top5: 0.8833955223880597
[2m[36m(func pid=18906)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=18906)[0m f1_macro: 0.2861586153360333
[2m[36m(func pid=18906)[0m f1_weighted: 0.28939478137486874
[2m[36m(func pid=18906)[0m f1_per_class: [0.234, 0.311, 0.264, 0.453, 0.059, 0.167, 0.132, 0.52, 0.279, 0.444]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8788 | Steps: 4 | Val loss: 1.9763 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0461 | Steps: 4 | Val loss: 1.8885 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.0611 | Steps: 4 | Val loss: 2.4264 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=17787)[0m top1: 0.24253731343283583
[2m[36m(func pid=17787)[0m top5: 0.8264925373134329
[2m[36m(func pid=17787)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=17787)[0m f1_macro: 0.22268887733570217
[2m[36m(func pid=17787)[0m f1_weighted: 0.24022562894603278
[2m[36m(func pid=17787)[0m f1_per_class: [0.194, 0.44, 0.133, 0.201, 0.049, 0.269, 0.13, 0.454, 0.151, 0.207]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6940 | Steps: 4 | Val loss: 2.5294 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:23:31 (running for 00:30:24.26)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.879 |      0.223 |                   67 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.046 |      0.335 |                   68 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  0.945 |      0.286 |                   64 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.995 |      0.175 |                   63 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.3582089552238806
[2m[36m(func pid=17891)[0m top5: 0.9057835820895522
[2m[36m(func pid=17891)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=17891)[0m f1_macro: 0.33475331879071796
[2m[36m(func pid=17891)[0m f1_weighted: 0.39094300282596844
[2m[36m(func pid=17891)[0m f1_per_class: [0.406, 0.461, 0.273, 0.38, 0.063, 0.319, 0.405, 0.46, 0.21, 0.371]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.20848880597014927
[2m[36m(func pid=19545)[0m top5: 0.7215485074626866
[2m[36m(func pid=19545)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=19545)[0m f1_macro: 0.18857689054592544
[2m[36m(func pid=19545)[0m f1_weighted: 0.20628594228897132
[2m[36m(func pid=19545)[0m f1_per_class: [0.113, 0.159, 0.383, 0.493, 0.068, 0.025, 0.0, 0.527, 0.075, 0.042]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1520 | Steps: 4 | Val loss: 2.0141 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=18906)[0m top1: 0.29151119402985076
[2m[36m(func pid=18906)[0m top5: 0.8600746268656716
[2m[36m(func pid=18906)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=18906)[0m f1_macro: 0.21698795854840722
[2m[36m(func pid=18906)[0m f1_weighted: 0.3305244418242304
[2m[36m(func pid=18906)[0m f1_per_class: [0.24, 0.468, 0.189, 0.453, 0.047, 0.154, 0.303, 0.077, 0.098, 0.141]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8758 | Steps: 4 | Val loss: 1.8988 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0358 | Steps: 4 | Val loss: 2.5139 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=17787)[0m top1: 0.23740671641791045
[2m[36m(func pid=17787)[0m top5: 0.808768656716418
[2m[36m(func pid=17787)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=17787)[0m f1_macro: 0.20282206049491344
[2m[36m(func pid=17787)[0m f1_weighted: 0.23947998752729713
[2m[36m(func pid=17787)[0m f1_per_class: [0.155, 0.472, 0.1, 0.208, 0.048, 0.236, 0.126, 0.49, 0.049, 0.145]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.2573 | Steps: 4 | Val loss: 6.4203 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:23:37 (running for 00:30:29.84)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.152 |      0.203 |                   68 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.046 |      0.335 |                   68 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.694 |      0.217 |                   65 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.036 |      0.203 |                   65 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.197294776119403
[2m[36m(func pid=19545)[0m top5: 0.7854477611940298
[2m[36m(func pid=19545)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=19545)[0m f1_macro: 0.20330941156482676
[2m[36m(func pid=19545)[0m f1_weighted: 0.18924148483853778
[2m[36m(func pid=19545)[0m f1_per_class: [0.08, 0.211, 0.486, 0.31, 0.054, 0.259, 0.0, 0.469, 0.128, 0.035]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.375
[2m[36m(func pid=17891)[0m top5: 0.914179104477612
[2m[36m(func pid=17891)[0m f1_micro: 0.375
[2m[36m(func pid=17891)[0m f1_macro: 0.3942686298119132
[2m[36m(func pid=17891)[0m f1_weighted: 0.41648093778936857
[2m[36m(func pid=17891)[0m f1_per_class: [0.359, 0.358, 0.783, 0.477, 0.052, 0.348, 0.429, 0.536, 0.18, 0.423]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.23507462686567165
[2m[36m(func pid=18906)[0m top5: 0.6114738805970149
[2m[36m(func pid=18906)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=18906)[0m f1_macro: 0.14876147399734874
[2m[36m(func pid=18906)[0m f1_weighted: 0.17868147961610736
[2m[36m(func pid=18906)[0m f1_per_class: [0.251, 0.387, 0.387, 0.32, 0.03, 0.0, 0.048, 0.0, 0.0, 0.064]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.9395 | Steps: 4 | Val loss: 1.9966 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.8818 | Steps: 4 | Val loss: 2.7028 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4548 | Steps: 4 | Val loss: 2.5409 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=17787)[0m top1: 0.24486940298507462
[2m[36m(func pid=17787)[0m top5: 0.8180970149253731
[2m[36m(func pid=17787)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=17787)[0m f1_macro: 0.21158555335369242
[2m[36m(func pid=17787)[0m f1_weighted: 0.24610163077235464
[2m[36m(func pid=17787)[0m f1_per_class: [0.17, 0.461, 0.12, 0.2, 0.051, 0.269, 0.147, 0.478, 0.072, 0.147]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4230 | Steps: 4 | Val loss: 4.9622 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:23:42 (running for 00:30:35.11)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.939 |      0.212 |                   69 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.876 |      0.394 |                   69 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.257 |      0.149 |                   66 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.882 |      0.189 |                   66 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.17817164179104478
[2m[36m(func pid=19545)[0m top5: 0.7555970149253731
[2m[36m(func pid=19545)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=19545)[0m f1_macro: 0.18910533552119246
[2m[36m(func pid=19545)[0m f1_weighted: 0.17326311342017758
[2m[36m(func pid=19545)[0m f1_per_class: [0.093, 0.273, 0.282, 0.172, 0.05, 0.257, 0.031, 0.52, 0.154, 0.061]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.28031716417910446
[2m[36m(func pid=17891)[0m top5: 0.8661380597014925
[2m[36m(func pid=17891)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=17891)[0m f1_macro: 0.27135371414654014
[2m[36m(func pid=17891)[0m f1_weighted: 0.30554935991889043
[2m[36m(func pid=17891)[0m f1_per_class: [0.337, 0.297, 0.667, 0.519, 0.063, 0.167, 0.211, 0.186, 0.124, 0.145]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1430 | Steps: 4 | Val loss: 1.9781 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=18906)[0m top1: 0.1935634328358209
[2m[36m(func pid=18906)[0m top5: 0.6665111940298507
[2m[36m(func pid=18906)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=18906)[0m f1_macro: 0.1566431311953609
[2m[36m(func pid=18906)[0m f1_weighted: 0.18643698852706306
[2m[36m(func pid=18906)[0m f1_per_class: [0.322, 0.335, 0.0, 0.358, 0.113, 0.012, 0.0, 0.302, 0.057, 0.068]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0214 | Steps: 4 | Val loss: 2.4693 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.9811 | Steps: 4 | Val loss: 1.9435 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=17787)[0m top1: 0.2513992537313433
[2m[36m(func pid=17787)[0m top5: 0.8260261194029851
[2m[36m(func pid=17787)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=17787)[0m f1_macro: 0.21242458112828425
[2m[36m(func pid=17787)[0m f1_weighted: 0.25677853595115635
[2m[36m(func pid=17787)[0m f1_per_class: [0.181, 0.455, 0.107, 0.208, 0.055, 0.281, 0.184, 0.453, 0.025, 0.177]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.9386 | Steps: 4 | Val loss: 3.1236 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:23:48 (running for 00:30:40.59)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.143 |      0.212 |                   70 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.455 |      0.271 |                   70 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.423 |      0.157 |                   67 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.021 |      0.198 |                   67 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.21222014925373134
[2m[36m(func pid=19545)[0m top5: 0.7593283582089553
[2m[36m(func pid=19545)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=19545)[0m f1_macro: 0.19750421630471812
[2m[36m(func pid=19545)[0m f1_weighted: 0.22204683055432706
[2m[36m(func pid=19545)[0m f1_per_class: [0.081, 0.362, 0.361, 0.236, 0.062, 0.268, 0.106, 0.44, 0.06, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.3605410447761194
[2m[36m(func pid=17891)[0m top5: 0.8889925373134329
[2m[36m(func pid=17891)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=17891)[0m f1_macro: 0.35820836769119796
[2m[36m(func pid=17891)[0m f1_weighted: 0.39043825185790937
[2m[36m(func pid=17891)[0m f1_per_class: [0.385, 0.375, 0.545, 0.508, 0.072, 0.28, 0.345, 0.462, 0.176, 0.435]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.9843 | Steps: 4 | Val loss: 1.9613 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=18906)[0m top1: 0.20055970149253732
[2m[36m(func pid=18906)[0m top5: 0.8694029850746269
[2m[36m(func pid=18906)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=18906)[0m f1_macro: 0.18713446271962458
[2m[36m(func pid=18906)[0m f1_weighted: 0.17850235301759615
[2m[36m(func pid=18906)[0m f1_per_class: [0.26, 0.07, 0.0, 0.406, 0.058, 0.111, 0.003, 0.429, 0.123, 0.411]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2733208955223881
[2m[36m(func pid=17787)[0m top5: 0.8334888059701493
[2m[36m(func pid=17787)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=17787)[0m f1_macro: 0.228023343454486
[2m[36m(func pid=17787)[0m f1_weighted: 0.29088495687277427
[2m[36m(func pid=17787)[0m f1_per_class: [0.182, 0.454, 0.11, 0.207, 0.056, 0.296, 0.288, 0.473, 0.025, 0.189]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7279 | Steps: 4 | Val loss: 2.0327 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.3043 | Steps: 4 | Val loss: 2.3247 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3845 | Steps: 4 | Val loss: 3.4252 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:23:53 (running for 00:30:46.24)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.984 |      0.228 |                   71 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.728 |      0.358 |                   72 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.939 |      0.187 |                   68 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.021 |      0.198 |                   67 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=17891)[0m top1: 0.34654850746268656
[2m[36m(func pid=17891)[0m top5: 0.8736007462686567
[2m[36m(func pid=17891)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=17891)[0m f1_macro: 0.3581052329054394
[2m[36m(func pid=17891)[0m f1_weighted: 0.3534796092420645
[2m[36m(func pid=17891)[0m f1_per_class: [0.432, 0.476, 0.649, 0.452, 0.086, 0.345, 0.192, 0.439, 0.187, 0.324]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=19545)[0m top1: 0.302705223880597
[2m[36m(func pid=19545)[0m top5: 0.7458022388059702
[2m[36m(func pid=19545)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=19545)[0m f1_macro: 0.19573224246569315
[2m[36m(func pid=19545)[0m f1_weighted: 0.2877416844783101
[2m[36m(func pid=19545)[0m f1_per_class: [0.078, 0.35, 0.31, 0.531, 0.072, 0.199, 0.126, 0.225, 0.067, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0665 | Steps: 4 | Val loss: 1.9614 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=18906)[0m top1: 0.15811567164179105
[2m[36m(func pid=18906)[0m top5: 0.7518656716417911
[2m[36m(func pid=18906)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=18906)[0m f1_macro: 0.15920823766880254
[2m[36m(func pid=18906)[0m f1_weighted: 0.1594391079261515
[2m[36m(func pid=18906)[0m f1_per_class: [0.104, 0.079, 0.114, 0.284, 0.048, 0.007, 0.092, 0.467, 0.153, 0.242]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m top1: 0.2896455223880597
[2m[36m(func pid=17787)[0m top5: 0.8414179104477612
[2m[36m(func pid=17787)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=17787)[0m f1_macro: 0.238598098423338
[2m[36m(func pid=17787)[0m f1_weighted: 0.31941032839762684
[2m[36m(func pid=17787)[0m f1_per_class: [0.167, 0.476, 0.106, 0.34, 0.057, 0.297, 0.249, 0.464, 0.026, 0.205]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.5587 | Steps: 4 | Val loss: 1.8392 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8134 | Steps: 4 | Val loss: 2.4783 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4648 | Steps: 4 | Val loss: 2.9712 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:23:59 (running for 00:30:51.59)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  2.066 |      0.239 |                   72 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.728 |      0.358 |                   72 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.385 |      0.159 |                   69 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.559 |      0.258 |                   69 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.4146455223880597
[2m[36m(func pid=19545)[0m top5: 0.8395522388059702
[2m[36m(func pid=19545)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=19545)[0m f1_macro: 0.2584884309394872
[2m[36m(func pid=19545)[0m f1_weighted: 0.3806813274443079
[2m[36m(func pid=19545)[0m f1_per_class: [0.058, 0.117, 0.439, 0.565, 0.062, 0.375, 0.422, 0.493, 0.055, 0.0]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.2826492537313433
[2m[36m(func pid=17891)[0m top5: 0.8428171641791045
[2m[36m(func pid=17891)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=17891)[0m f1_macro: 0.2592621781068562
[2m[36m(func pid=17891)[0m f1_weighted: 0.295916509338599
[2m[36m(func pid=17891)[0m f1_per_class: [0.27, 0.455, 0.238, 0.418, 0.075, 0.228, 0.113, 0.449, 0.181, 0.166]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=18906)[0m top1: 0.24860074626865672
[2m[36m(func pid=18906)[0m top5: 0.8278917910447762
[2m[36m(func pid=18906)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=18906)[0m f1_macro: 0.21325457783509574
[2m[36m(func pid=18906)[0m f1_weighted: 0.27691926926641347
[2m[36m(func pid=18906)[0m f1_per_class: [0.249, 0.059, 0.104, 0.311, 0.054, 0.073, 0.439, 0.493, 0.101, 0.25]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.9888 | Steps: 4 | Val loss: 1.9264 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.8642 | Steps: 4 | Val loss: 2.2854 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.7443 | Steps: 4 | Val loss: 2.0060 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=17787)[0m top1: 0.2989738805970149
[2m[36m(func pid=17787)[0m top5: 0.8470149253731343
[2m[36m(func pid=17787)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=17787)[0m f1_macro: 0.24720512788017315
[2m[36m(func pid=17787)[0m f1_weighted: 0.32575375146578195
[2m[36m(func pid=17787)[0m f1_per_class: [0.187, 0.461, 0.143, 0.4, 0.056, 0.296, 0.225, 0.434, 0.026, 0.244]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2350 | Steps: 4 | Val loss: 2.0271 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 13:24:04 (running for 00:30:56.93)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.989 |      0.247 |                   73 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.813 |      0.259 |                   73 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.465 |      0.213 |                   70 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.744 |      0.248 |                   70 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.2957089552238806
[2m[36m(func pid=19545)[0m top5: 0.8255597014925373
[2m[36m(func pid=19545)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=19545)[0m f1_macro: 0.24834645708336942
[2m[36m(func pid=19545)[0m f1_weighted: 0.3098526190680048
[2m[36m(func pid=19545)[0m f1_per_class: [0.088, 0.154, 0.514, 0.538, 0.19, 0.15, 0.263, 0.509, 0.047, 0.03]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.28451492537313433
[2m[36m(func pid=17891)[0m top5: 0.8703358208955224
[2m[36m(func pid=17891)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=17891)[0m f1_macro: 0.28825768823917336
[2m[36m(func pid=17891)[0m f1_weighted: 0.325445019652525
[2m[36m(func pid=17891)[0m f1_per_class: [0.304, 0.347, 0.094, 0.376, 0.058, 0.287, 0.271, 0.481, 0.219, 0.447]
[2m[36m(func pid=17891)[0m 
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.8886 | Steps: 4 | Val loss: 1.9063 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=18906)[0m top1: 0.35074626865671643
[2m[36m(func pid=18906)[0m top5: 0.9011194029850746
[2m[36m(func pid=18906)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=18906)[0m f1_macro: 0.2754694827874124
[2m[36m(func pid=18906)[0m f1_weighted: 0.3476468055318282
[2m[36m(func pid=18906)[0m f1_per_class: [0.29, 0.207, 0.226, 0.494, 0.039, 0.132, 0.382, 0.542, 0.057, 0.385]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0725 | Steps: 4 | Val loss: 2.4243 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=17891)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2377 | Steps: 4 | Val loss: 2.2241 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=17787)[0m top1: 0.30223880597014924
[2m[36m(func pid=17787)[0m top5: 0.8512126865671642
[2m[36m(func pid=17787)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=17787)[0m f1_macro: 0.24527329636858358
[2m[36m(func pid=17787)[0m f1_weighted: 0.3252446897867494
[2m[36m(func pid=17787)[0m f1_per_class: [0.183, 0.45, 0.15, 0.438, 0.054, 0.297, 0.198, 0.422, 0.0, 0.26]
[2m[36m(func pid=17787)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3486 | Steps: 4 | Val loss: 2.4751 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:24:09 (running for 00:31:02.24)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.30925
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00012 | RUNNING    | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.889 |      0.245 |                   74 |
| train_52b21_00013 | RUNNING    | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  0.864 |      0.288 |                   74 |
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.235 |      0.275 |                   71 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.072 |      0.174 |                   71 |
| train_52b21_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=19545)[0m top1: 0.11007462686567164
[2m[36m(func pid=19545)[0m top5: 0.8390858208955224
[2m[36m(func pid=19545)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=19545)[0m f1_macro: 0.17397126468134777
[2m[36m(func pid=19545)[0m f1_weighted: 0.12314552661415211
[2m[36m(func pid=19545)[0m f1_per_class: [0.085, 0.032, 0.5, 0.041, 0.28, 0.0, 0.232, 0.491, 0.052, 0.029]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=17891)[0m top1: 0.314365671641791
[2m[36m(func pid=17891)[0m top5: 0.8922574626865671
[2m[36m(func pid=17891)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=17891)[0m f1_macro: 0.29512601873812544
[2m[36m(func pid=17891)[0m f1_weighted: 0.3717475266818986
[2m[36m(func pid=17891)[0m f1_per_class: [0.105, 0.358, 0.073, 0.419, 0.054, 0.29, 0.385, 0.498, 0.209, 0.56]
[2m[36m(func pid=17787)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9765 | Steps: 4 | Val loss: 1.9036 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=18906)[0m top1: 0.3498134328358209
[2m[36m(func pid=18906)[0m top5: 0.855410447761194
[2m[36m(func pid=18906)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=18906)[0m f1_macro: 0.257525005510996
[2m[36m(func pid=18906)[0m f1_weighted: 0.33889768371851603
[2m[36m(func pid=18906)[0m f1_per_class: [0.073, 0.442, 0.222, 0.314, 0.0, 0.287, 0.337, 0.577, 0.088, 0.235]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.7638 | Steps: 4 | Val loss: 2.0280 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=17787)[0m top1: 0.30223880597014924
[2m[36m(func pid=17787)[0m top5: 0.8521455223880597
[2m[36m(func pid=17787)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=17787)[0m f1_macro: 0.2507201690487123
[2m[36m(func pid=17787)[0m f1_weighted: 0.3171085933937722
[2m[36m(func pid=17787)[0m f1_per_class: [0.221, 0.449, 0.192, 0.448, 0.056, 0.309, 0.156, 0.399, 0.027, 0.25]
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2118 | Steps: 4 | Val loss: 3.4357 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=19545)[0m top1: 0.26119402985074625
[2m[36m(func pid=19545)[0m top5: 0.8451492537313433
[2m[36m(func pid=19545)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=19545)[0m f1_macro: 0.21058088641913825
[2m[36m(func pid=19545)[0m f1_weighted: 0.270296894262764
[2m[36m(func pid=19545)[0m f1_per_class: [0.091, 0.333, 0.255, 0.041, 0.157, 0.0, 0.545, 0.507, 0.148, 0.028]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=18906)[0m top1: 0.24953358208955223
[2m[36m(func pid=18906)[0m top5: 0.8278917910447762
[2m[36m(func pid=18906)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=18906)[0m f1_macro: 0.1953600937126125
[2m[36m(func pid=18906)[0m f1_weighted: 0.2441521136925853
[2m[36m(func pid=18906)[0m f1_per_class: [0.0, 0.345, 0.0, 0.177, 0.061, 0.316, 0.208, 0.519, 0.14, 0.188]
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2499 | Steps: 4 | Val loss: 2.5936 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=36467)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36467)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=36467)[0m Configuration completed!
[2m[36m(func pid=36467)[0m New optimizer parameters:
[2m[36m(func pid=36467)[0m SGD (
[2m[36m(func pid=36467)[0m Parameter Group 0
[2m[36m(func pid=36467)[0m     dampening: 0
[2m[36m(func pid=36467)[0m     differentiable: False
[2m[36m(func pid=36467)[0m     foreach: None
[2m[36m(func pid=36467)[0m     lr: 0.0001
[2m[36m(func pid=36467)[0m     maximize: False
[2m[36m(func pid=36467)[0m     momentum: 0.99
[2m[36m(func pid=36467)[0m     nesterov: False
[2m[36m(func pid=36467)[0m     weight_decay: 1e-05
[2m[36m(func pid=36467)[0m )
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=19545)[0m top1: 0.11986940298507463
[2m[36m(func pid=19545)[0m top5: 0.7933768656716418
[2m[36m(func pid=19545)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=19545)[0m f1_macro: 0.0903668845187364
[2m[36m(func pid=19545)[0m f1_weighted: 0.12337721168474336
[2m[36m(func pid=19545)[0m f1_per_class: [0.05, 0.375, 0.057, 0.041, 0.154, 0.0, 0.143, 0.0, 0.059, 0.025]
== Status ==
Current time: 2024-01-07 13:24:15 (running for 00:31:07.47)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.30325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.349 |      0.258 |                   72 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.764 |      0.211 |                   72 |
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36555)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=36555)[0m Configuration completed!
[2m[36m(func pid=36555)[0m New optimizer parameters:
[2m[36m(func pid=36555)[0m SGD (
[2m[36m(func pid=36555)[0m Parameter Group 0
[2m[36m(func pid=36555)[0m     dampening: 0
[2m[36m(func pid=36555)[0m     differentiable: False
[2m[36m(func pid=36555)[0m     foreach: None
[2m[36m(func pid=36555)[0m     lr: 0.001
[2m[36m(func pid=36555)[0m     maximize: False
[2m[36m(func pid=36555)[0m     momentum: 0.99
[2m[36m(func pid=36555)[0m     nesterov: False
[2m[36m(func pid=36555)[0m     weight_decay: 1e-05
[2m[36m(func pid=36555)[0m )
[2m[36m(func pid=36555)[0m 
== Status ==
Current time: 2024-01-07 13:24:22 (running for 00:31:14.44)
Memory usage on this node: 23.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.30325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.349 |      0.258 |                   72 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.25  |      0.09  |                   73 |
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1945 | Steps: 4 | Val loss: 2.3249 | Batch size: 32 | lr: 0.0001 | Duration: 4.70s
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.1861 | Steps: 4 | Val loss: 3.5953 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.6711 | Steps: 4 | Val loss: 1.9705 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=36467)[0m top1: 0.15671641791044777
[2m[36m(func pid=36467)[0m top5: 0.527518656716418
[2m[36m(func pid=36467)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=36467)[0m f1_macro: 0.04611048240280599
[2m[36m(func pid=36467)[0m f1_weighted: 0.0892756934118725
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.0, 0.0, 0.283, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0593 | Steps: 4 | Val loss: 2.3310 | Batch size: 32 | lr: 0.001 | Duration: 4.95s
== Status ==
Current time: 2024-01-07 13:24:27 (running for 00:31:19.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.30325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00014 | RUNNING    | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.186 |      0.179 |                   74 |
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  2.25  |      0.09  |                   73 |
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  3.194 |      0.046 |                    1 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.23367537313432835
[2m[36m(func pid=18906)[0m top5: 0.7854477611940298
[2m[36m(func pid=18906)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=18906)[0m f1_macro: 0.17871244707747258
[2m[36m(func pid=18906)[0m f1_weighted: 0.24698529020914176
[2m[36m(func pid=18906)[0m f1_per_class: [0.147, 0.348, 0.0, 0.411, 0.078, 0.092, 0.088, 0.489, 0.104, 0.03]
[2m[36m(func pid=18906)[0m 
[2m[36m(func pid=19545)[0m top1: 0.31669776119402987
[2m[36m(func pid=19545)[0m top5: 0.8166977611940298
[2m[36m(func pid=19545)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=19545)[0m f1_macro: 0.2085832028328945
[2m[36m(func pid=19545)[0m f1_weighted: 0.3458567222156361
[2m[36m(func pid=19545)[0m f1_per_class: [0.07, 0.137, 0.1, 0.383, 0.171, 0.082, 0.586, 0.433, 0.08, 0.046]
[2m[36m(func pid=19545)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1133 | Steps: 4 | Val loss: 2.3109 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36555)[0m top1: 0.09001865671641791
[2m[36m(func pid=36555)[0m top5: 0.5382462686567164
[2m[36m(func pid=36555)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=36555)[0m f1_macro: 0.0383035642380612
[2m[36m(func pid=36555)[0m f1_weighted: 0.06017751877810501
[2m[36m(func pid=36555)[0m f1_per_class: [0.07, 0.0, 0.039, 0.193, 0.0, 0.0, 0.0, 0.081, 0.0, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=18906)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8718 | Steps: 4 | Val loss: 2.8403 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=19545)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9147 | Steps: 4 | Val loss: 2.1676 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=36467)[0m top1: 0.20615671641791045
[2m[36m(func pid=36467)[0m top5: 0.5802238805970149
[2m[36m(func pid=36467)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=36467)[0m f1_macro: 0.04228423798749257
[2m[36m(func pid=36467)[0m f1_weighted: 0.10307478674706105
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.0, 0.0, 0.362, 0.0, 0.0, 0.0, 0.028, 0.0, 0.032]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9060 | Steps: 4 | Val loss: 2.3165 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:24:33 (running for 00:31:25.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3005
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00015 | RUNNING    | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.671 |      0.209 |                   74 |
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  3.113 |      0.042 |                    2 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  3.059 |      0.038 |                    1 |
| train_52b21_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=18906)[0m top1: 0.30177238805970147
[2m[36m(func pid=18906)[0m top5: 0.8647388059701493
[2m[36m(func pid=18906)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=18906)[0m f1_macro: 0.28207000853263703
[2m[36m(func pid=18906)[0m f1_weighted: 0.33425333141019303
[2m[36m(func pid=18906)[0m f1_per_class: [0.275, 0.24, 0.471, 0.477, 0.096, 0.26, 0.292, 0.5, 0.134, 0.078]
[2m[36m(func pid=19545)[0m top1: 0.23414179104477612
[2m[36m(func pid=19545)[0m top5: 0.7943097014925373
[2m[36m(func pid=19545)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=19545)[0m f1_macro: 0.17788082898359164
[2m[36m(func pid=19545)[0m f1_weighted: 0.2511440601650557
[2m[36m(func pid=19545)[0m f1_per_class: [0.067, 0.095, 0.139, 0.433, 0.095, 0.03, 0.249, 0.509, 0.094, 0.067]
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9914 | Steps: 4 | Val loss: 2.3114 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=36555)[0m top1: 0.0065298507462686565
[2m[36m(func pid=36555)[0m top5: 0.5928171641791045
[2m[36m(func pid=36555)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=36555)[0m f1_macro: 0.001561540334425045
[2m[36m(func pid=36555)[0m f1_weighted: 0.0010011762304849073
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.0, 0.012, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.22574626865671643
[2m[36m(func pid=36467)[0m top5: 0.5727611940298507
[2m[36m(func pid=36467)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=36467)[0m f1_macro: 0.06947288405820405
[2m[36m(func pid=36467)[0m f1_weighted: 0.11313964410831034
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.021, 0.259, 0.38, 0.0, 0.0, 0.0, 0.035, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8723 | Steps: 4 | Val loss: 2.3318 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9386 | Steps: 4 | Val loss: 2.3018 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36555)[0m top1: 0.029384328358208957
[2m[36m(func pid=36555)[0m top5: 0.5270522388059702
[2m[36m(func pid=36555)[0m f1_micro: 0.029384328358208953
[2m[36m(func pid=36555)[0m f1_macro: 0.019822062073045026
[2m[36m(func pid=36555)[0m f1_weighted: 0.020636780456345954
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.011, 0.028, 0.026, 0.0, 0.086, 0.0, 0.0, 0.048, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37814)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37814)[0m Configuration completed!
[2m[36m(func pid=37814)[0m New optimizer parameters:
[2m[36m(func pid=37814)[0m SGD (
[2m[36m(func pid=37814)[0m Parameter Group 0
[2m[36m(func pid=37814)[0m     dampening: 0
[2m[36m(func pid=37814)[0m     differentiable: False
[2m[36m(func pid=37814)[0m     foreach: None
[2m[36m(func pid=37814)[0m     lr: 0.01
[2m[36m(func pid=37814)[0m     maximize: False
[2m[36m(func pid=37814)[0m     momentum: 0.99
[2m[36m(func pid=37814)[0m     nesterov: False
[2m[36m(func pid=37814)[0m     weight_decay: 1e-05
[2m[36m(func pid=37814)[0m )
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.17117537313432835
[2m[36m(func pid=36467)[0m top5: 0.574160447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=36467)[0m f1_macro: 0.04536179701375379
[2m[36m(func pid=36467)[0m f1_weighted: 0.10314988160616
[2m[36m(func pid=36467)[0m f1_per_class: [0.015, 0.047, 0.041, 0.336, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:24:40 (running for 00:31:32.59)
Memory usage on this node: 20.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.991 |      0.069 |                    3 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.872 |      0.02  |                    3 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37884)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=37884)[0m Configuration completed!
[2m[36m(func pid=37884)[0m New optimizer parameters:
[2m[36m(func pid=37884)[0m SGD (
[2m[36m(func pid=37884)[0m Parameter Group 0
[2m[36m(func pid=37884)[0m     dampening: 0
[2m[36m(func pid=37884)[0m     differentiable: False
[2m[36m(func pid=37884)[0m     foreach: None
[2m[36m(func pid=37884)[0m     lr: 0.1
[2m[36m(func pid=37884)[0m     maximize: False
[2m[36m(func pid=37884)[0m     momentum: 0.99
[2m[36m(func pid=37884)[0m     nesterov: False
[2m[36m(func pid=37884)[0m     weight_decay: 1e-05
[2m[36m(func pid=37884)[0m )
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7536 | Steps: 4 | Val loss: 2.2749 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:24:45 (running for 00:31:38.16)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.939 |      0.045 |                    4 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.754 |      0.06  |                    4 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.061567164179104475
[2m[36m(func pid=36555)[0m top5: 0.78125
[2m[36m(func pid=36555)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=36555)[0m f1_macro: 0.05970575121003334
[2m[36m(func pid=36555)[0m f1_weighted: 0.04468952987933032
[2m[36m(func pid=36555)[0m f1_per_class: [0.075, 0.229, 0.13, 0.0, 0.095, 0.0, 0.0, 0.0, 0.067, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8075 | Steps: 4 | Val loss: 2.2872 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0671 | Steps: 4 | Val loss: 2.8901 | Batch size: 32 | lr: 0.01 | Duration: 5.03s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.8534 | Steps: 4 | Val loss: 9983.1533 | Batch size: 32 | lr: 0.1 | Duration: 4.91s
[2m[36m(func pid=36467)[0m top1: 0.08069029850746269
[2m[36m(func pid=36467)[0m top5: 0.6035447761194029
[2m[36m(func pid=36467)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=36467)[0m f1_macro: 0.03162800975123582
[2m[36m(func pid=36467)[0m f1_weighted: 0.0717602856840841
[2m[36m(func pid=36467)[0m f1_per_class: [0.026, 0.04, 0.02, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7834 | Steps: 4 | Val loss: 2.0909 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=37814)[0m top1: 0.021455223880597014
[2m[36m(func pid=37814)[0m top5: 0.6828358208955224
[2m[36m(func pid=37814)[0m f1_micro: 0.021455223880597014
[2m[36m(func pid=37814)[0m f1_macro: 0.030825456836799
[2m[36m(func pid=37814)[0m f1_weighted: 0.002470398480188848
[2m[36m(func pid=37814)[0m f1_per_class: [0.042, 0.0, 0.267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.17210820895522388
[2m[36m(func pid=37884)[0m top5: 0.5727611940298507
[2m[36m(func pid=37884)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=37884)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=37884)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:24:51 (running for 00:31:43.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.807 |      0.032 |                    5 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.783 |      0.089 |                    5 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.067 |      0.031 |                    1 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  5.853 |      0.029 |                    1 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.15298507462686567
[2m[36m(func pid=36555)[0m top5: 0.8302238805970149
[2m[36m(func pid=36555)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=36555)[0m f1_macro: 0.08871435204917891
[2m[36m(func pid=36555)[0m f1_weighted: 0.07819522410002011
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.427, 0.237, 0.0, 0.16, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7275 | Steps: 4 | Val loss: 2.2735 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0131 | Steps: 4 | Val loss: 5.5375 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 12.6545 | Steps: 4 | Val loss: 78061.7656 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=36467)[0m top1: 0.04291044776119403
[2m[36m(func pid=36467)[0m top5: 0.6459888059701493
[2m[36m(func pid=36467)[0m f1_micro: 0.04291044776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.03129750149649831
[2m[36m(func pid=36467)[0m f1_weighted: 0.05594809532541312
[2m[36m(func pid=36467)[0m f1_per_class: [0.02, 0.123, 0.017, 0.085, 0.0, 0.052, 0.015, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3960 | Steps: 4 | Val loss: 1.9203 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=37814)[0m top1: 0.011194029850746268
[2m[36m(func pid=37814)[0m top5: 0.7425373134328358
[2m[36m(func pid=37814)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=37814)[0m f1_macro: 0.006944283705901562
[2m[36m(func pid=37814)[0m f1_weighted: 0.008223482220758038
[2m[36m(func pid=37814)[0m f1_per_class: [0.003, 0.0, 0.018, 0.0, 0.0, 0.035, 0.014, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.006063432835820896
[2m[36m(func pid=37884)[0m top5: 0.5093283582089553
[2m[36m(func pid=37884)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=37884)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=37884)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:24:56 (running for 00:31:48.89)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.727 |      0.031 |                    6 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.396 |      0.137 |                    6 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.013 |      0.007 |                    2 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  | 12.655 |      0.001 |                    2 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.314365671641791
[2m[36m(func pid=36555)[0m top5: 0.8456156716417911
[2m[36m(func pid=36555)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=36555)[0m f1_macro: 0.13670276354603325
[2m[36m(func pid=36555)[0m f1_weighted: 0.22471677558901726
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.366, 0.4, 0.0, 0.068, 0.0, 0.533, 0.0, 0.0, 0.0]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5755 | Steps: 4 | Val loss: 2.2527 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.7005 | Steps: 4 | Val loss: 11.3712 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 15.6397 | Steps: 4 | Val loss: 60999.0781 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=36467)[0m top1: 0.10820895522388059
[2m[36m(func pid=36467)[0m top5: 0.7360074626865671
[2m[36m(func pid=36467)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=36467)[0m f1_macro: 0.07525283429156263
[2m[36m(func pid=36467)[0m f1_weighted: 0.13743497104499355
[2m[36m(func pid=36467)[0m f1_per_class: [0.032, 0.197, 0.023, 0.027, 0.0, 0.25, 0.223, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7284 | Steps: 4 | Val loss: 1.8449 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=37814)[0m top1: 0.021921641791044777
[2m[36m(func pid=37814)[0m top5: 0.7588619402985075
[2m[36m(func pid=37814)[0m f1_micro: 0.021921641791044777
[2m[36m(func pid=37814)[0m f1_macro: 0.01660732260318601
[2m[36m(func pid=37814)[0m f1_weighted: 0.02870466760294362
[2m[36m(func pid=37814)[0m f1_per_class: [0.003, 0.106, 0.02, 0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.2789179104477612
[2m[36m(func pid=37884)[0m top5: 0.7821828358208955
[2m[36m(func pid=37884)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=37884)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=37884)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m top1: 0.29244402985074625
[2m[36m(func pid=36555)[0m top5: 0.8535447761194029
[2m[36m(func pid=36555)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=36555)[0m f1_macro: 0.15719765716097717
[2m[36m(func pid=36555)[0m f1_weighted: 0.26308775511073057
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.445, 0.218, 0.469, 0.096, 0.0, 0.14, 0.204, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:25:01 (running for 00:31:54.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.576 |      0.075 |                    7 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.728 |      0.157 |                    7 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.7   |      0.017 |                    3 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  | 15.64  |      0.044 |                    3 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5109 | Steps: 4 | Val loss: 2.2562 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.3009 | Steps: 4 | Val loss: 9.0682 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 5.9662 | Steps: 4 | Val loss: 705280.4375 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2370 | Steps: 4 | Val loss: 2.0734 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=36467)[0m top1: 0.16044776119402984
[2m[36m(func pid=36467)[0m top5: 0.7028917910447762
[2m[36m(func pid=36467)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=36467)[0m f1_macro: 0.09795668887592694
[2m[36m(func pid=36467)[0m f1_weighted: 0.18812599463685598
[2m[36m(func pid=36467)[0m f1_per_class: [0.031, 0.202, 0.028, 0.007, 0.0, 0.334, 0.377, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.03917910447761194
[2m[36m(func pid=37814)[0m top5: 0.5583022388059702
[2m[36m(func pid=37814)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=37814)[0m f1_macro: 0.056778133869243866
[2m[36m(func pid=37814)[0m f1_weighted: 0.03100192363063419
[2m[36m(func pid=37814)[0m f1_per_class: [0.069, 0.148, 0.0, 0.0, 0.286, 0.0, 0.0, 0.026, 0.0, 0.039]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.006063432835820896
[2m[36m(func pid=37884)[0m top5: 0.5093283582089553
[2m[36m(func pid=37884)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=37884)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=37884)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:25:07 (running for 00:31:59.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.511 |      0.098 |                    8 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.237 |      0.127 |                    8 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.301 |      0.057 |                    4 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  5.966 |      0.001 |                    4 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.24253731343283583
[2m[36m(func pid=36555)[0m top5: 0.8339552238805971
[2m[36m(func pid=36555)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=36555)[0m f1_macro: 0.12709147138201995
[2m[36m(func pid=36555)[0m f1_weighted: 0.20389572613001386
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.28, 0.092, 0.502, 0.086, 0.0, 0.0, 0.234, 0.0, 0.077]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6336 | Steps: 4 | Val loss: 2.2714 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8499 | Steps: 4 | Val loss: 17.9220 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 4.7054 | Steps: 4 | Val loss: 219521.2188 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2281 | Steps: 4 | Val loss: 2.2399 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=36467)[0m top1: 0.2080223880597015
[2m[36m(func pid=36467)[0m top5: 0.6389925373134329
[2m[36m(func pid=36467)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=36467)[0m f1_macro: 0.10719609564990358
[2m[36m(func pid=36467)[0m f1_weighted: 0.1952174638972866
[2m[36m(func pid=36467)[0m f1_per_class: [0.028, 0.088, 0.041, 0.0, 0.167, 0.245, 0.503, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.014925373134328358
[2m[36m(func pid=37814)[0m top5: 0.2891791044776119
[2m[36m(func pid=37814)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=37814)[0m f1_macro: 0.02337693969635299
[2m[36m(func pid=37814)[0m f1_weighted: 0.003628809578167407
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.0, 0.021, 0.0, 0.154, 0.0, 0.0, 0.016, 0.043, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.006063432835820896
[2m[36m(func pid=37884)[0m top5: 0.5093283582089553
[2m[36m(func pid=37884)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=37884)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=37884)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:25:12 (running for 00:32:04.90)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.634 |      0.107 |                    9 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.228 |      0.127 |                    9 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.85  |      0.023 |                    5 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  4.705 |      0.001 |                    5 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2080223880597015
[2m[36m(func pid=36555)[0m top5: 0.8208955223880597
[2m[36m(func pid=36555)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=36555)[0m f1_macro: 0.12699955100159427
[2m[36m(func pid=36555)[0m f1_weighted: 0.1737563150253807
[2m[36m(func pid=36555)[0m f1_per_class: [0.115, 0.135, 0.093, 0.475, 0.105, 0.0, 0.0, 0.224, 0.0, 0.123]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8907 | Steps: 4 | Val loss: 2.2918 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.7504 | Steps: 4 | Val loss: 26.3929 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.0590 | Steps: 4 | Val loss: 229738.3906 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1350 | Steps: 4 | Val loss: 2.4625 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=36467)[0m top1: 0.21875
[2m[36m(func pid=36467)[0m top5: 0.6184701492537313
[2m[36m(func pid=36467)[0m f1_micro: 0.21875
[2m[36m(func pid=36467)[0m f1_macro: 0.0792632838471731
[2m[36m(func pid=36467)[0m f1_weighted: 0.15731067708980184
[2m[36m(func pid=36467)[0m f1_per_class: [0.026, 0.0, 0.078, 0.0, 0.148, 0.031, 0.509, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.014458955223880597
[2m[36m(func pid=37814)[0m top5: 0.25513059701492535
[2m[36m(func pid=37814)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=37814)[0m f1_macro: 0.033108365719406535
[2m[36m(func pid=37814)[0m f1_weighted: 0.005073900387493715
[2m[36m(func pid=37814)[0m f1_per_class: [0.009, 0.0, 0.019, 0.0, 0.222, 0.008, 0.0, 0.029, 0.0, 0.044]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.2980410447761194
[2m[36m(func pid=37884)[0m top5: 0.5149253731343284
[2m[36m(func pid=37884)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=37884)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=37884)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:25:17 (running for 00:32:10.16)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.891 |      0.079 |                   10 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.135 |      0.124 |                   10 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.75  |      0.033 |                    6 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  5.059 |      0.046 |                    6 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.13759328358208955
[2m[36m(func pid=36555)[0m top5: 0.7560634328358209
[2m[36m(func pid=36555)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=36555)[0m f1_macro: 0.1243976153756919
[2m[36m(func pid=36555)[0m f1_weighted: 0.11167612226873659
[2m[36m(func pid=36555)[0m f1_per_class: [0.125, 0.124, 0.235, 0.197, 0.121, 0.128, 0.0, 0.262, 0.0, 0.051]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4492 | Steps: 4 | Val loss: 2.2806 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3790 | Steps: 4 | Val loss: 13.2638 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.8654 | Steps: 4 | Val loss: 18244.7344 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1563 | Steps: 4 | Val loss: 2.2382 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=36467)[0m top1: 0.23647388059701493
[2m[36m(func pid=36467)[0m top5: 0.6352611940298507
[2m[36m(func pid=36467)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=36467)[0m f1_macro: 0.0820463045156223
[2m[36m(func pid=36467)[0m f1_weighted: 0.15312824777612588
[2m[36m(func pid=36467)[0m f1_per_class: [0.057, 0.0, 0.2, 0.0, 0.059, 0.0, 0.504, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.0960820895522388
[2m[36m(func pid=37814)[0m top5: 0.5918843283582089
[2m[36m(func pid=37814)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=37814)[0m f1_macro: 0.130035573441277
[2m[36m(func pid=37814)[0m f1_weighted: 0.055366211774047824
[2m[36m(func pid=37814)[0m f1_per_class: [0.081, 0.079, 0.64, 0.06, 0.152, 0.0, 0.015, 0.232, 0.0, 0.042]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.10587686567164178
[2m[36m(func pid=37884)[0m top5: 0.4146455223880597
[2m[36m(func pid=37884)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=37884)[0m f1_macro: 0.022264446183078122
[2m[36m(func pid=37884)[0m f1_weighted: 0.02234618096507515
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.029, 0.194, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:25:23 (running for 00:32:15.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.449 |      0.082 |                   11 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.156 |      0.201 |                   11 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.379 |      0.13  |                    7 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  4.865 |      0.022 |                    7 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2234141791044776
[2m[36m(func pid=36555)[0m top5: 0.7583955223880597
[2m[36m(func pid=36555)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=36555)[0m f1_macro: 0.20119246603516486
[2m[36m(func pid=36555)[0m f1_weighted: 0.2086888062664705
[2m[36m(func pid=36555)[0m f1_per_class: [0.138, 0.419, 0.344, 0.236, 0.083, 0.392, 0.003, 0.316, 0.018, 0.064]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6371 | Steps: 4 | Val loss: 2.2476 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.1059 | Steps: 4 | Val loss: 14.5549 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.8310 | Steps: 4 | Val loss: 6215.4214 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8242 | Steps: 4 | Val loss: 2.3079 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=36467)[0m top1: 0.25
[2m[36m(func pid=36467)[0m top5: 0.6553171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.25
[2m[36m(func pid=36467)[0m f1_macro: 0.1065269972141943
[2m[36m(func pid=36467)[0m f1_weighted: 0.1586935364043858
[2m[36m(func pid=36467)[0m f1_per_class: [0.066, 0.0, 0.361, 0.0, 0.121, 0.0, 0.518, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.09188432835820895
[2m[36m(func pid=37814)[0m top5: 0.6240671641791045
[2m[36m(func pid=37814)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=37814)[0m f1_macro: 0.07229268385415684
[2m[36m(func pid=37814)[0m f1_weighted: 0.053189675160977036
[2m[36m(func pid=37814)[0m f1_per_class: [0.066, 0.097, 0.0, 0.0, 0.143, 0.0, 0.055, 0.271, 0.035, 0.055]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:25:28 (running for 00:32:20.54)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.637 |      0.107 |                   12 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.156 |      0.201 |                   11 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.106 |      0.072 |                    8 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.831 |      0.009 |                    8 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.011194029850746268
[2m[36m(func pid=37884)[0m top5: 0.37173507462686567
[2m[36m(func pid=37884)[0m f1_micro: 0.01119402985074627
[2m[36m(func pid=37884)[0m f1_macro: 0.009257875723281624
[2m[36m(func pid=37884)[0m f1_weighted: 0.008328743148393571
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.069, 0.0, 0.0, 0.01, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m top1: 0.23180970149253732
[2m[36m(func pid=36555)[0m top5: 0.7635261194029851
[2m[36m(func pid=36555)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=36555)[0m f1_macro: 0.2246492205422841
[2m[36m(func pid=36555)[0m f1_weighted: 0.19453546040115866
[2m[36m(func pid=36555)[0m f1_per_class: [0.29, 0.506, 0.24, 0.153, 0.174, 0.302, 0.0, 0.275, 0.09, 0.215]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6005 | Steps: 4 | Val loss: 2.2318 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7314 | Steps: 4 | Val loss: 25.6666 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0928 | Steps: 4 | Val loss: 2.6914 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.3993 | Steps: 4 | Val loss: 2902.9771 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=36467)[0m top1: 0.25652985074626866
[2m[36m(func pid=36467)[0m top5: 0.6902985074626866
[2m[36m(func pid=36467)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=36467)[0m f1_macro: 0.13252573202434337
[2m[36m(func pid=36467)[0m f1_weighted: 0.1611324527516794
[2m[36m(func pid=36467)[0m f1_per_class: [0.087, 0.0, 0.571, 0.0, 0.148, 0.0, 0.519, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.013526119402985074
[2m[36m(func pid=37814)[0m top5: 0.480410447761194
[2m[36m(func pid=37814)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=37814)[0m f1_macro: 0.0061861532562096505
[2m[36m(func pid=37814)[0m f1_weighted: 0.005101451819414967
[2m[36m(func pid=37814)[0m f1_per_class: [0.013, 0.027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:25:33 (running for 00:32:26.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.6   |      0.133 |                   13 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.824 |      0.225 |                   12 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.731 |      0.006 |                    9 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.399 |      0.004 |                    9 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2248134328358209
[2m[36m(func pid=36555)[0m top5: 0.7835820895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=36555)[0m f1_macro: 0.1929639671381115
[2m[36m(func pid=36555)[0m f1_weighted: 0.18044335505257958
[2m[36m(func pid=36555)[0m f1_per_class: [0.211, 0.528, 0.195, 0.149, 0.1, 0.178, 0.0, 0.265, 0.118, 0.186]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.008395522388059701
[2m[36m(func pid=37884)[0m top5: 0.33908582089552236
[2m[36m(func pid=37884)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=37884)[0m f1_macro: 0.0038588812223779266
[2m[36m(func pid=37884)[0m f1_weighted: 0.0015688231636583974
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.005, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4029 | Steps: 4 | Val loss: 2.2168 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.4542 | Steps: 4 | Val loss: 49.6168 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.1685 | Steps: 4 | Val loss: 1061.5880 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2176 | Steps: 4 | Val loss: 2.7417 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=36467)[0m top1: 0.2583955223880597
[2m[36m(func pid=36467)[0m top5: 0.7322761194029851
[2m[36m(func pid=36467)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=36467)[0m f1_macro: 0.1292454668253466
[2m[36m(func pid=36467)[0m f1_weighted: 0.16847649219817365
[2m[36m(func pid=36467)[0m f1_per_class: [0.092, 0.0, 0.556, 0.0, 0.1, 0.0, 0.545, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.024253731343283583
[2m[36m(func pid=37814)[0m top5: 0.439365671641791
[2m[36m(func pid=37814)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=37814)[0m f1_macro: 0.018093339460123006
[2m[36m(func pid=37814)[0m f1_weighted: 0.00811586982438308
[2m[36m(func pid=37814)[0m f1_per_class: [0.03, 0.005, 0.041, 0.0, 0.0, 0.008, 0.003, 0.074, 0.0, 0.02]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:25:39 (running for 00:32:31.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.403 |      0.129 |                   14 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.218 |      0.201 |                   14 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.454 |      0.018 |                   10 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.399 |      0.004 |                    9 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.23367537313432835
[2m[36m(func pid=36555)[0m top5: 0.8022388059701493
[2m[36m(func pid=36555)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=36555)[0m f1_macro: 0.20071749680410092
[2m[36m(func pid=36555)[0m f1_weighted: 0.1680974249859231
[2m[36m(func pid=36555)[0m f1_per_class: [0.317, 0.54, 0.149, 0.117, 0.123, 0.089, 0.0, 0.288, 0.129, 0.255]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.016791044776119403
[2m[36m(func pid=37884)[0m top5: 0.32369402985074625
[2m[36m(func pid=37884)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=37884)[0m f1_macro: 0.006444108873776394
[2m[36m(func pid=37884)[0m f1_weighted: 0.008330505212901098
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.048, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6650 | Steps: 4 | Val loss: 2.2109 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4373 | Steps: 4 | Val loss: 50.6880 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7570 | Steps: 4 | Val loss: 2.4592 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.0139 | Steps: 4 | Val loss: 849.7737 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=36467)[0m top1: 0.25419776119402987
[2m[36m(func pid=36467)[0m top5: 0.7691231343283582
[2m[36m(func pid=36467)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=36467)[0m f1_macro: 0.13091118858996936
[2m[36m(func pid=36467)[0m f1_weighted: 0.17511272714976503
[2m[36m(func pid=36467)[0m f1_per_class: [0.095, 0.0, 0.556, 0.0, 0.091, 0.0, 0.567, 0.0, 0.0, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.26072761194029853
[2m[36m(func pid=37814)[0m top5: 0.5209888059701493
[2m[36m(func pid=37814)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=37814)[0m f1_macro: 0.0894458107972776
[2m[36m(func pid=37814)[0m f1_weighted: 0.19324294315829849
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.068, 0.034, 0.0, 0.125, 0.1, 0.567, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:25:44 (running for 00:32:36.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.665 |      0.131 |                   15 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.757 |      0.227 |                   15 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.437 |      0.089 |                   11 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.168 |      0.006 |                   10 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.251865671641791
[2m[36m(func pid=36555)[0m top5: 0.8138992537313433
[2m[36m(func pid=36555)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=36555)[0m f1_macro: 0.2274749090453136
[2m[36m(func pid=36555)[0m f1_weighted: 0.2070806786867657
[2m[36m(func pid=36555)[0m f1_per_class: [0.351, 0.542, 0.471, 0.277, 0.098, 0.0, 0.0, 0.371, 0.115, 0.051]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.022388059701492536
[2m[36m(func pid=37884)[0m top5: 0.31902985074626866
[2m[36m(func pid=37884)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=37884)[0m f1_macro: 0.008423035365833992
[2m[36m(func pid=37884)[0m f1_weighted: 0.01156984433891758
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.067, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4901 | Steps: 4 | Val loss: 2.1709 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.1628 | Steps: 4 | Val loss: 23.5345 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9214 | Steps: 4 | Val loss: 2.6977 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.1401 | Steps: 4 | Val loss: 772.8821 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=36467)[0m top1: 0.25
[2m[36m(func pid=36467)[0m top5: 0.7919776119402985
[2m[36m(func pid=36467)[0m f1_micro: 0.25
[2m[36m(func pid=36467)[0m f1_macro: 0.1382859169556545
[2m[36m(func pid=36467)[0m f1_weighted: 0.1797213099099244
[2m[36m(func pid=36467)[0m f1_per_class: [0.096, 0.005, 0.606, 0.0, 0.072, 0.0, 0.576, 0.0, 0.027, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:25:49 (running for 00:32:41.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.49  |      0.138 |                   16 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.757 |      0.227 |                   15 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.163 |      0.119 |                   12 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.014 |      0.008 |                   11 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.3362873134328358
[2m[36m(func pid=37814)[0m top5: 0.6534514925373134
[2m[36m(func pid=37814)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=37814)[0m f1_macro: 0.11884558205757019
[2m[36m(func pid=37814)[0m f1_weighted: 0.2440952471031174
[2m[36m(func pid=37814)[0m f1_per_class: [0.008, 0.297, 0.0, 0.0, 0.149, 0.148, 0.586, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.18563432835820895
[2m[36m(func pid=36555)[0m top5: 0.8157649253731343
[2m[36m(func pid=36555)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=36555)[0m f1_macro: 0.14394905606250893
[2m[36m(func pid=36555)[0m f1_weighted: 0.1791884618847305
[2m[36m(func pid=36555)[0m f1_per_class: [0.083, 0.456, 0.0, 0.207, 0.06, 0.0, 0.03, 0.514, 0.045, 0.044]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.036380597014925374
[2m[36m(func pid=37884)[0m top5: 0.4118470149253731
[2m[36m(func pid=37884)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=37884)[0m f1_macro: 0.012972383331874268
[2m[36m(func pid=37884)[0m f1_weighted: 0.019126266649023133
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.11, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4119 | Steps: 4 | Val loss: 2.1413 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9830 | Steps: 4 | Val loss: 13.7815 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.0209 | Steps: 4 | Val loss: 2.3025 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.0431 | Steps: 4 | Val loss: 536.4816 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=36467)[0m top1: 0.24067164179104478
[2m[36m(func pid=36467)[0m top5: 0.8166977611940298
[2m[36m(func pid=36467)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=36467)[0m f1_macro: 0.13601979605160766
[2m[36m(func pid=36467)[0m f1_weighted: 0.1854717498935304
[2m[36m(func pid=36467)[0m f1_per_class: [0.101, 0.027, 0.533, 0.0, 0.061, 0.0, 0.581, 0.0, 0.058, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:25:54 (running for 00:32:47.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.412 |      0.136 |                   17 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.921 |      0.144 |                   16 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.983 |      0.1   |                   13 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.14  |      0.013 |                   12 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.2513992537313433
[2m[36m(func pid=37814)[0m top5: 0.5904850746268657
[2m[36m(func pid=37814)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=37814)[0m f1_macro: 0.10011077988086521
[2m[36m(func pid=37814)[0m f1_weighted: 0.19908914129196642
[2m[36m(func pid=37814)[0m f1_per_class: [0.071, 0.0, 0.0, 0.007, 0.132, 0.15, 0.594, 0.0, 0.0, 0.048]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.2453358208955224
[2m[36m(func pid=36555)[0m top5: 0.8456156716417911
[2m[36m(func pid=36555)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=36555)[0m f1_macro: 0.17723867303970264
[2m[36m(func pid=36555)[0m f1_weighted: 0.2583993173528039
[2m[36m(func pid=36555)[0m f1_per_class: [0.087, 0.407, 0.0, 0.239, 0.063, 0.162, 0.246, 0.426, 0.061, 0.082]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.06949626865671642
[2m[36m(func pid=37884)[0m top5: 0.5186567164179104
[2m[36m(func pid=37884)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=37884)[0m f1_macro: 0.02760360863039608
[2m[36m(func pid=37884)[0m f1_weighted: 0.059586706345690306
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.105, 0.022, 0.148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5410 | Steps: 4 | Val loss: 2.1362 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.9774 | Steps: 4 | Val loss: 2.1505 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7610 | Steps: 4 | Val loss: 17.1131 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9668 | Steps: 4 | Val loss: 537.1475 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=36467)[0m top1: 0.22994402985074627
[2m[36m(func pid=36467)[0m top5: 0.8264925373134329
[2m[36m(func pid=36467)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=36467)[0m f1_macro: 0.14164596961197323
[2m[36m(func pid=36467)[0m f1_weighted: 0.1925659923589437
[2m[36m(func pid=36467)[0m f1_per_class: [0.15, 0.052, 0.468, 0.0, 0.053, 0.032, 0.573, 0.0, 0.088, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:26:00 (running for 00:32:52.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.541 |      0.142 |                   18 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.977 |      0.316 |                   18 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.983 |      0.1   |                   13 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.043 |      0.028 |                   13 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.3111007462686567
[2m[36m(func pid=36555)[0m top5: 0.8596082089552238
[2m[36m(func pid=36555)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=36555)[0m f1_macro: 0.31601867284339774
[2m[36m(func pid=36555)[0m f1_weighted: 0.3318760694616755
[2m[36m(func pid=36555)[0m f1_per_class: [0.381, 0.433, 0.818, 0.428, 0.092, 0.243, 0.238, 0.414, 0.027, 0.087]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.22294776119402984
[2m[36m(func pid=37814)[0m top5: 0.5638992537313433
[2m[36m(func pid=37814)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=37814)[0m f1_macro: 0.1073576589114504
[2m[36m(func pid=37814)[0m f1_weighted: 0.211800847683071
[2m[36m(func pid=37814)[0m f1_per_class: [0.07, 0.0, 0.0, 0.0, 0.0, 0.379, 0.552, 0.038, 0.0, 0.034]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.07695895522388059
[2m[36m(func pid=37884)[0m top5: 0.5032649253731343
[2m[36m(func pid=37884)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=37884)[0m f1_macro: 0.020552538098625412
[2m[36m(func pid=37884)[0m f1_weighted: 0.05073769363604723
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.024, 0.181, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3048 | Steps: 4 | Val loss: 2.0736 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5992 | Steps: 4 | Val loss: 4.2740 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4265 | Steps: 4 | Val loss: 7.4498 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.5061 | Steps: 4 | Val loss: 335.8791 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=36467)[0m top1: 0.22527985074626866
[2m[36m(func pid=36467)[0m top5: 0.8521455223880597
[2m[36m(func pid=36467)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=36467)[0m f1_macro: 0.1640755867270297
[2m[36m(func pid=36467)[0m f1_weighted: 0.20805005371747173
[2m[36m(func pid=36467)[0m f1_per_class: [0.304, 0.089, 0.44, 0.0, 0.05, 0.1, 0.567, 0.0, 0.091, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:26:05 (running for 00:32:57.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.305 |      0.164 |                   19 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.599 |      0.196 |                   19 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.761 |      0.107 |                   14 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.967 |      0.021 |                   14 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.23460820895522388
[2m[36m(func pid=36555)[0m top5: 0.8157649253731343
[2m[36m(func pid=36555)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=36555)[0m f1_macro: 0.19557776181850955
[2m[36m(func pid=36555)[0m f1_weighted: 0.25744630740285196
[2m[36m(func pid=36555)[0m f1_per_class: [0.079, 0.38, 0.039, 0.147, 0.149, 0.282, 0.297, 0.443, 0.0, 0.139]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.13199626865671643
[2m[36m(func pid=37814)[0m top5: 0.65625
[2m[36m(func pid=37814)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=37814)[0m f1_macro: 0.08780267694811218
[2m[36m(func pid=37814)[0m f1_weighted: 0.12094981396189926
[2m[36m(func pid=37814)[0m f1_per_class: [0.081, 0.0, 0.0, 0.0, 0.0, 0.324, 0.238, 0.184, 0.0, 0.051]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.09375
[2m[36m(func pid=37884)[0m top5: 0.5186567164179104
[2m[36m(func pid=37884)[0m f1_micro: 0.09375
[2m[36m(func pid=37884)[0m f1_macro: 0.023863926019992238
[2m[36m(func pid=37884)[0m f1_weighted: 0.05934386002355395
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.026, 0.212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.1678 | Steps: 4 | Val loss: 2.0352 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9483 | Steps: 4 | Val loss: 5.4594 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8437 | Steps: 4 | Val loss: 7.5688 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.2171 | Steps: 4 | Val loss: 148.6089 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=36467)[0m top1: 0.21548507462686567
[2m[36m(func pid=36467)[0m top5: 0.8652052238805971
[2m[36m(func pid=36467)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=36467)[0m f1_macro: 0.1735683583176542
[2m[36m(func pid=36467)[0m f1_weighted: 0.21809732072234903
[2m[36m(func pid=36467)[0m f1_per_class: [0.085, 0.145, 0.611, 0.0, 0.049, 0.203, 0.539, 0.0, 0.103, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:26:10 (running for 00:33:03.23)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.168 |      0.174 |                   20 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.948 |      0.212 |                   20 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.427 |      0.088 |                   15 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.506 |      0.024 |                   15 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.25093283582089554
[2m[36m(func pid=36555)[0m top5: 0.820429104477612
[2m[36m(func pid=36555)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=36555)[0m f1_macro: 0.21160482980625966
[2m[36m(func pid=36555)[0m f1_weighted: 0.26623325614675697
[2m[36m(func pid=36555)[0m f1_per_class: [0.023, 0.371, 0.037, 0.089, 0.182, 0.343, 0.358, 0.448, 0.024, 0.242]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.11287313432835822
[2m[36m(func pid=37814)[0m top5: 0.7014925373134329
[2m[36m(func pid=37814)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=37814)[0m f1_macro: 0.09112782271832631
[2m[36m(func pid=37814)[0m f1_weighted: 0.08176050492094994
[2m[36m(func pid=37814)[0m f1_per_class: [0.045, 0.161, 0.063, 0.0, 0.0, 0.302, 0.006, 0.252, 0.044, 0.038]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m top1: 0.12826492537313433
[2m[36m(func pid=37884)[0m top5: 0.5536380597014925
[2m[36m(func pid=37884)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=37884)[0m f1_macro: 0.02958782683060554
[2m[36m(func pid=37884)[0m f1_weighted: 0.07398792896513917
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.031, 0.265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1816 | Steps: 4 | Val loss: 1.9832 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6100 | Steps: 4 | Val loss: 2.3480 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.1964 | Steps: 4 | Val loss: 75.9787 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2744 | Steps: 4 | Val loss: 7.2024 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=36467)[0m top1: 0.21175373134328357
[2m[36m(func pid=36467)[0m top5: 0.8759328358208955
[2m[36m(func pid=36467)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=36467)[0m f1_macro: 0.19563306194835786
[2m[36m(func pid=36467)[0m f1_weighted: 0.2348645948586182
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.217, 0.643, 0.0, 0.043, 0.277, 0.497, 0.173, 0.107, 0.0]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:26:16 (running for 00:33:08.38)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.182 |      0.196 |                   21 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.948 |      0.212 |                   20 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.844 |      0.091 |                   16 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.217 |      0.03  |                   16 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2826492537313433
[2m[36m(func pid=36555)[0m top5: 0.9057835820895522
[2m[36m(func pid=36555)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=36555)[0m f1_macro: 0.221906316425114
[2m[36m(func pid=36555)[0m f1_weighted: 0.2760856434251237
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.497, 0.075, 0.128, 0.116, 0.229, 0.333, 0.375, 0.048, 0.418]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.16557835820895522
[2m[36m(func pid=37884)[0m top5: 0.585820895522388
[2m[36m(func pid=37884)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=37884)[0m f1_macro: 0.03779454791647085
[2m[36m(func pid=37884)[0m f1_weighted: 0.09419016926309078
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.043, 0.312, 0.0, 0.0, 0.023, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.1767723880597015
[2m[36m(func pid=37814)[0m top5: 0.7210820895522388
[2m[36m(func pid=37814)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=37814)[0m f1_macro: 0.10638769822141533
[2m[36m(func pid=37814)[0m f1_weighted: 0.12491425212055081
[2m[36m(func pid=37814)[0m f1_per_class: [0.02, 0.481, 0.007, 0.01, 0.067, 0.205, 0.0, 0.239, 0.036, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2368 | Steps: 4 | Val loss: 1.9979 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2252 | Steps: 4 | Val loss: 2.2302 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.1328 | Steps: 4 | Val loss: 52.1606 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.3044 | Steps: 4 | Val loss: 27.9715 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:26:21 (running for 00:33:13.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.182 |      0.196 |                   21 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.225 |      0.255 |                   22 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.274 |      0.106 |                   17 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.196 |      0.038 |                   17 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.29617537313432835
[2m[36m(func pid=36555)[0m top5: 0.8950559701492538
[2m[36m(func pid=36555)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=36555)[0m f1_macro: 0.25538805269391035
[2m[36m(func pid=36555)[0m f1_weighted: 0.27173780622467514
[2m[36m(func pid=36555)[0m f1_per_class: [0.041, 0.454, 0.369, 0.13, 0.082, 0.158, 0.347, 0.388, 0.124, 0.459]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.18470149253731344
[2m[36m(func pid=36467)[0m top5: 0.8745335820895522
[2m[36m(func pid=36467)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=36467)[0m f1_macro: 0.21323425425828607
[2m[36m(func pid=36467)[0m f1_weighted: 0.21114850453800227
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.302, 0.629, 0.007, 0.041, 0.242, 0.315, 0.485, 0.112, 0.0]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m top1: 0.2658582089552239
[2m[36m(func pid=37884)[0m top5: 0.6114738805970149
[2m[36m(func pid=37884)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=37884)[0m f1_macro: 0.0631478829355752
[2m[36m(func pid=37884)[0m f1_weighted: 0.14983174261086254
[2m[36m(func pid=37884)[0m f1_per_class: [0.087, 0.0, 0.049, 0.0, 0.0, 0.0, 0.496, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.15298507462686567
[2m[36m(func pid=37814)[0m top5: 0.539179104477612
[2m[36m(func pid=37814)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=37814)[0m f1_macro: 0.05497394404697141
[2m[36m(func pid=37814)[0m f1_weighted: 0.09019868541264521
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.267, 0.0, 0.09, 0.104, 0.0, 0.056, 0.021, 0.012, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0868 | Steps: 4 | Val loss: 1.9594 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.5249 | Steps: 4 | Val loss: 2.2220 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.0005 | Steps: 4 | Val loss: 50.5602 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.9470 | Steps: 4 | Val loss: 47.8605 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 13:26:27 (running for 00:33:19.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.237 |      0.213 |                   22 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.525 |      0.274 |                   23 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.304 |      0.055 |                   18 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.133 |      0.063 |                   18 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.20382462686567165
[2m[36m(func pid=36467)[0m top5: 0.8694029850746269
[2m[36m(func pid=36467)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=36467)[0m f1_macro: 0.20754278583585567
[2m[36m(func pid=36467)[0m f1_weighted: 0.21348177168801102
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.388, 0.431, 0.092, 0.041, 0.245, 0.192, 0.485, 0.125, 0.077]
[2m[36m(func pid=36555)[0m top1: 0.26072761194029853
[2m[36m(func pid=36555)[0m top5: 0.8717350746268657
[2m[36m(func pid=36555)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=36555)[0m f1_macro: 0.2742958526291704
[2m[36m(func pid=36555)[0m f1_weighted: 0.26119818702309444
[2m[36m(func pid=36555)[0m f1_per_class: [0.161, 0.363, 0.714, 0.185, 0.064, 0.093, 0.329, 0.387, 0.128, 0.319]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m top1: 0.2658582089552239
[2m[36m(func pid=37884)[0m top5: 0.425839552238806
[2m[36m(func pid=37884)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=37884)[0m f1_macro: 0.06193862554535899
[2m[36m(func pid=37884)[0m f1_weighted: 0.14961459688627368
[2m[36m(func pid=37884)[0m f1_per_class: [0.068, 0.0, 0.055, 0.0, 0.0, 0.0, 0.496, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.11520522388059702
[2m[36m(func pid=37814)[0m top5: 0.46361940298507465
[2m[36m(func pid=37814)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=37814)[0m f1_macro: 0.048650972907619605
[2m[36m(func pid=37814)[0m f1_weighted: 0.08238522980943543
[2m[36m(func pid=37814)[0m f1_per_class: [0.015, 0.184, 0.0, 0.171, 0.08, 0.0, 0.003, 0.0, 0.033, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1497 | Steps: 4 | Val loss: 1.8903 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3740 | Steps: 4 | Val loss: 2.2407 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9553 | Steps: 4 | Val loss: 49.8957 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 5.1323 | Steps: 4 | Val loss: 41.7431 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:26:32 (running for 00:33:24.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.15  |      0.205 |                   24 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.525 |      0.274 |                   23 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.947 |      0.049 |                   19 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3     |      0.062 |                   19 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.22201492537313433
[2m[36m(func pid=36467)[0m top5: 0.875
[2m[36m(func pid=36467)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=36467)[0m f1_macro: 0.20495825053330244
[2m[36m(func pid=36467)[0m f1_weighted: 0.21985695255675652
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.393, 0.381, 0.187, 0.048, 0.24, 0.141, 0.378, 0.135, 0.148]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m top1: 0.2719216417910448
[2m[36m(func pid=36555)[0m top5: 0.8819962686567164
[2m[36m(func pid=36555)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=36555)[0m f1_macro: 0.27239260638766083
[2m[36m(func pid=36555)[0m f1_weighted: 0.30102208720564083
[2m[36m(func pid=36555)[0m f1_per_class: [0.22, 0.157, 0.7, 0.428, 0.061, 0.034, 0.362, 0.491, 0.102, 0.17]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.2593283582089552
[2m[36m(func pid=37884)[0m top5: 0.4155783582089552
[2m[36m(func pid=37884)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=37884)[0m f1_macro: 0.05468966863092126
[2m[36m(func pid=37884)[0m f1_weighted: 0.1512834022506683
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.005, 0.038, 0.0, 0.0, 0.0, 0.504, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.06203358208955224
[2m[36m(func pid=37814)[0m top5: 0.46175373134328357
[2m[36m(func pid=37814)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=37814)[0m f1_macro: 0.057780863166912666
[2m[36m(func pid=37814)[0m f1_weighted: 0.07788514172826136
[2m[36m(func pid=37814)[0m f1_per_class: [0.022, 0.0, 0.016, 0.174, 0.126, 0.237, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.8912 | Steps: 4 | Val loss: 2.8021 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1901 | Steps: 4 | Val loss: 1.8717 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8860 | Steps: 4 | Val loss: 31.7525 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.7798 | Steps: 4 | Val loss: 18.5407 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:26:37 (running for 00:33:30.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.15  |      0.205 |                   24 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.891 |      0.189 |                   25 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  5.132 |      0.058 |                   20 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.955 |      0.055 |                   20 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2719216417910448
[2m[36m(func pid=36555)[0m top5: 0.8306902985074627
[2m[36m(func pid=36555)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=36555)[0m f1_macro: 0.18931511298129886
[2m[36m(func pid=36555)[0m f1_weighted: 0.2754315861089501
[2m[36m(func pid=36555)[0m f1_per_class: [0.166, 0.112, 0.0, 0.396, 0.099, 0.0, 0.381, 0.399, 0.048, 0.293]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.24253731343283583
[2m[36m(func pid=36467)[0m top5: 0.8638059701492538
[2m[36m(func pid=36467)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=36467)[0m f1_macro: 0.1941459118547737
[2m[36m(func pid=36467)[0m f1_weighted: 0.23291315573369356
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.433, 0.24, 0.272, 0.047, 0.184, 0.107, 0.381, 0.13, 0.148]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m top1: 0.012126865671641791
[2m[36m(func pid=37884)[0m top5: 0.48274253731343286
[2m[36m(func pid=37884)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=37884)[0m f1_macro: 0.0059729559528391
[2m[36m(func pid=37884)[0m f1_weighted: 0.00045914415389379233
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.0914179104477612
[2m[36m(func pid=37814)[0m top5: 0.5475746268656716
[2m[36m(func pid=37814)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=37814)[0m f1_macro: 0.08687808744305531
[2m[36m(func pid=37814)[0m f1_weighted: 0.09068768809790359
[2m[36m(func pid=37814)[0m f1_per_class: [0.043, 0.0, 0.0, 0.034, 0.135, 0.354, 0.112, 0.035, 0.072, 0.084]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6518 | Steps: 4 | Val loss: 3.3980 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2469 | Steps: 4 | Val loss: 1.8590 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7780 | Steps: 4 | Val loss: 22.8720 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.1134 | Steps: 4 | Val loss: 11.9181 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:26:43 (running for 00:33:35.78)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.19  |      0.194 |                   25 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.652 |      0.187 |                   26 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  4.78  |      0.087 |                   21 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.886 |      0.006 |                   21 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2555970149253731
[2m[36m(func pid=36555)[0m top5: 0.7854477611940298
[2m[36m(func pid=36555)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=36555)[0m f1_macro: 0.18667221617060017
[2m[36m(func pid=36555)[0m f1_weighted: 0.2675884341499806
[2m[36m(func pid=36555)[0m f1_per_class: [0.116, 0.184, 0.0, 0.345, 0.124, 0.0, 0.369, 0.365, 0.052, 0.312]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2537313432835821
[2m[36m(func pid=36467)[0m top5: 0.8568097014925373
[2m[36m(func pid=36467)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=36467)[0m f1_macro: 0.18095182349495445
[2m[36m(func pid=36467)[0m f1_weighted: 0.22875406548287902
[2m[36m(func pid=36467)[0m f1_per_class: [0.0, 0.462, 0.168, 0.303, 0.052, 0.17, 0.063, 0.371, 0.054, 0.167]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m top1: 0.02845149253731343
[2m[36m(func pid=37884)[0m top5: 0.46828358208955223
[2m[36m(func pid=37884)[0m f1_micro: 0.02845149253731343
[2m[36m(func pid=37884)[0m f1_macro: 0.010696386946386947
[2m[36m(func pid=37884)[0m f1_weighted: 0.002154886080958842
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.056, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.08348880597014925
[2m[36m(func pid=37814)[0m top5: 0.6268656716417911
[2m[36m(func pid=37814)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=37814)[0m f1_macro: 0.07375657389960547
[2m[36m(func pid=37814)[0m f1_weighted: 0.0434616743790035
[2m[36m(func pid=37814)[0m f1_per_class: [0.052, 0.0, 0.0, 0.0, 0.132, 0.234, 0.0, 0.207, 0.065, 0.047]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.5574 | Steps: 4 | Val loss: 4.9099 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9999 | Steps: 4 | Val loss: 1.8460 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.3511 | Steps: 4 | Val loss: 24.4167 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5718 | Steps: 4 | Val loss: 7.1078 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 13:26:48 (running for 00:33:40.99)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.247 |      0.181 |                   26 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.557 |      0.173 |                   27 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  4.113 |      0.074 |                   22 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.778 |      0.011 |                   22 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.1982276119402985
[2m[36m(func pid=36555)[0m top5: 0.7042910447761194
[2m[36m(func pid=36555)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=36555)[0m f1_macro: 0.17322571151269814
[2m[36m(func pid=36555)[0m f1_weighted: 0.1994189181806039
[2m[36m(func pid=36555)[0m f1_per_class: [0.09, 0.312, 0.0, 0.091, 0.239, 0.0, 0.305, 0.356, 0.066, 0.275]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2579291044776119
[2m[36m(func pid=36467)[0m top5: 0.8456156716417911
[2m[36m(func pid=36467)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=36467)[0m f1_macro: 0.17518885279591478
[2m[36m(func pid=36467)[0m f1_weighted: 0.222892026100659
[2m[36m(func pid=36467)[0m f1_per_class: [0.041, 0.468, 0.203, 0.326, 0.05, 0.172, 0.025, 0.36, 0.0, 0.107]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m top1: 0.028917910447761194
[2m[36m(func pid=37884)[0m top5: 0.3302238805970149
[2m[36m(func pid=37884)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=37884)[0m f1_macro: 0.010779506582757423
[2m[36m(func pid=37884)[0m f1_weighted: 0.002194168438145937
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.10401119402985075
[2m[36m(func pid=37814)[0m top5: 0.6469216417910447
[2m[36m(func pid=37814)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=37814)[0m f1_macro: 0.0753350398249922
[2m[36m(func pid=37814)[0m f1_weighted: 0.051095507180946693
[2m[36m(func pid=37814)[0m f1_per_class: [0.074, 0.0, 0.0, 0.0, 0.112, 0.297, 0.0, 0.237, 0.033, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4084 | Steps: 4 | Val loss: 7.0934 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.0588 | Steps: 4 | Val loss: 27.4251 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9018 | Steps: 4 | Val loss: 1.8419 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.7625 | Steps: 4 | Val loss: 4.4766 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:26:54 (running for 00:33:46.39)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2     |      0.175 |                   27 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.408 |      0.162 |                   28 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.572 |      0.075 |                   23 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.351 |      0.011 |                   23 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.1865671641791045
[2m[36m(func pid=36555)[0m top5: 0.7644589552238806
[2m[36m(func pid=36555)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=36555)[0m f1_macro: 0.16188425126470185
[2m[36m(func pid=36555)[0m f1_weighted: 0.18462906570779444
[2m[36m(func pid=36555)[0m f1_per_class: [0.085, 0.398, 0.0, 0.099, 0.156, 0.063, 0.185, 0.314, 0.066, 0.254]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.03031716417910448
[2m[36m(func pid=37884)[0m top5: 0.34328358208955223
[2m[36m(func pid=37884)[0m f1_micro: 0.03031716417910448
[2m[36m(func pid=37884)[0m f1_macro: 0.01760872513657752
[2m[36m(func pid=37884)[0m f1_weighted: 0.004384073008947115
[2m[36m(func pid=37884)[0m f1_per_class: [0.061, 0.005, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2667910447761194
[2m[36m(func pid=36467)[0m top5: 0.8442164179104478
[2m[36m(func pid=36467)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=36467)[0m f1_macro: 0.19933856576135542
[2m[36m(func pid=36467)[0m f1_weighted: 0.2361423460827031
[2m[36m(func pid=36467)[0m f1_per_class: [0.115, 0.456, 0.273, 0.371, 0.056, 0.216, 0.009, 0.354, 0.0, 0.143]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.2224813432835821
[2m[36m(func pid=37814)[0m top5: 0.75
[2m[36m(func pid=37814)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=37814)[0m f1_macro: 0.14255481449089802
[2m[36m(func pid=37814)[0m f1_weighted: 0.20725297715312918
[2m[36m(func pid=37814)[0m f1_per_class: [0.081, 0.183, 0.0, 0.0, 0.104, 0.311, 0.394, 0.353, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3741 | Steps: 4 | Val loss: 5.9983 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.2578 | Steps: 4 | Val loss: 15.1357 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0551 | Steps: 4 | Val loss: 1.8370 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0978 | Steps: 4 | Val loss: 4.0015 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 13:26:59 (running for 00:33:51.72)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.902 |      0.199 |                   28 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.374 |      0.172 |                   29 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.762 |      0.143 |                   24 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.059 |      0.018 |                   24 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.19169776119402984
[2m[36m(func pid=36555)[0m top5: 0.7910447761194029
[2m[36m(func pid=36555)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=36555)[0m f1_macro: 0.17166532880573662
[2m[36m(func pid=36555)[0m f1_weighted: 0.1760481108159858
[2m[36m(func pid=36555)[0m f1_per_class: [0.101, 0.457, 0.0, 0.087, 0.143, 0.153, 0.107, 0.26, 0.027, 0.381]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.045242537313432835
[2m[36m(func pid=37884)[0m top5: 0.3493470149253731
[2m[36m(func pid=37884)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=37884)[0m f1_macro: 0.02959515671392359
[2m[36m(func pid=37884)[0m f1_weighted: 0.023416336938913887
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.121, 0.118, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.28171641791044777
[2m[36m(func pid=36467)[0m top5: 0.8428171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=36467)[0m f1_macro: 0.22529664099426525
[2m[36m(func pid=36467)[0m f1_weighted: 0.2533563584927653
[2m[36m(func pid=36467)[0m f1_per_class: [0.262, 0.47, 0.329, 0.413, 0.062, 0.237, 0.003, 0.345, 0.0, 0.132]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.27238805970149255
[2m[36m(func pid=37814)[0m top5: 0.7789179104477612
[2m[36m(func pid=37814)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=37814)[0m f1_macro: 0.15005628997598516
[2m[36m(func pid=37814)[0m f1_weighted: 0.24596792286768884
[2m[36m(func pid=37814)[0m f1_per_class: [0.113, 0.151, 0.0, 0.0, 0.0, 0.206, 0.568, 0.421, 0.0, 0.042]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1914 | Steps: 4 | Val loss: 3.6745 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.1447 | Steps: 4 | Val loss: 14.9396 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9402 | Steps: 4 | Val loss: 1.8417 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3722 | Steps: 4 | Val loss: 3.5439 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 13:27:04 (running for 00:33:57.05)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.055 |      0.225 |                   29 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.191 |      0.177 |                   30 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.098 |      0.15  |                   25 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.258 |      0.03  |                   25 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.24067164179104478
[2m[36m(func pid=36555)[0m top5: 0.835820895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=36555)[0m f1_macro: 0.17737981002687286
[2m[36m(func pid=36555)[0m f1_weighted: 0.1986015523445784
[2m[36m(func pid=36555)[0m f1_per_class: [0.214, 0.506, 0.0, 0.062, 0.066, 0.361, 0.099, 0.286, 0.0, 0.181]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.14505597014925373
[2m[36m(func pid=37884)[0m top5: 0.34841417910447764
[2m[36m(func pid=37884)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=37884)[0m f1_macro: 0.03657117295018305
[2m[36m(func pid=37884)[0m f1_weighted: 0.04910789174487484
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.28, 0.069, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2933768656716418
[2m[36m(func pid=36467)[0m top5: 0.8390858208955224
[2m[36m(func pid=36467)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=36467)[0m f1_macro: 0.23756971592216844
[2m[36m(func pid=36467)[0m f1_weighted: 0.26804842283617797
[2m[36m(func pid=36467)[0m f1_per_class: [0.256, 0.483, 0.364, 0.438, 0.067, 0.279, 0.003, 0.352, 0.0, 0.133]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.1525186567164179
[2m[36m(func pid=37814)[0m top5: 0.7728544776119403
[2m[36m(func pid=37814)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=37814)[0m f1_macro: 0.09860241544732887
[2m[36m(func pid=37814)[0m f1_weighted: 0.13850784845385297
[2m[36m(func pid=37814)[0m f1_per_class: [0.066, 0.085, 0.0, 0.0, 0.143, 0.047, 0.328, 0.318, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.7108 | Steps: 4 | Val loss: 3.7864 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 3.3142 | Steps: 4 | Val loss: 21.1856 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9819 | Steps: 4 | Val loss: 1.8421 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5246 | Steps: 4 | Val loss: 3.7038 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:27:09 (running for 00:34:02.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.94  |      0.238 |                   30 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.711 |      0.176 |                   31 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.372 |      0.099 |                   26 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.145 |      0.037 |                   26 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.25326492537313433
[2m[36m(func pid=36555)[0m top5: 0.8652052238805971
[2m[36m(func pid=36555)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=36555)[0m f1_macro: 0.1761480622984475
[2m[36m(func pid=36555)[0m f1_weighted: 0.1738990246074867
[2m[36m(func pid=36555)[0m f1_per_class: [0.353, 0.478, 0.0, 0.013, 0.107, 0.297, 0.088, 0.322, 0.0, 0.104]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.13992537313432835
[2m[36m(func pid=37884)[0m top5: 0.3941231343283582
[2m[36m(func pid=37884)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=37884)[0m f1_macro: 0.03691377971901183
[2m[36m(func pid=37884)[0m f1_weighted: 0.048277754119495156
[2m[36m(func pid=37884)[0m f1_per_class: [0.038, 0.274, 0.057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.29617537313432835
[2m[36m(func pid=36467)[0m top5: 0.8423507462686567
[2m[36m(func pid=36467)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=36467)[0m f1_macro: 0.25712692369869905
[2m[36m(func pid=36467)[0m f1_weighted: 0.2703359029536504
[2m[36m(func pid=36467)[0m f1_per_class: [0.362, 0.494, 0.444, 0.424, 0.066, 0.294, 0.0, 0.375, 0.0, 0.112]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.09281716417910447
[2m[36m(func pid=37814)[0m top5: 0.7555970149253731
[2m[36m(func pid=37814)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=37814)[0m f1_macro: 0.07091003078688632
[2m[36m(func pid=37814)[0m f1_weighted: 0.07404340110199262
[2m[36m(func pid=37814)[0m f1_per_class: [0.067, 0.074, 0.002, 0.076, 0.0, 0.092, 0.039, 0.263, 0.0, 0.095]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.7026 | Steps: 4 | Val loss: 3.9034 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.0302 | Steps: 4 | Val loss: 16.4535 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3219 | Steps: 4 | Val loss: 1.8540 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.9320 | Steps: 4 | Val loss: 4.5802 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=36555)[0m top1: 0.240205223880597
[2m[36m(func pid=36555)[0m top5: 0.8694029850746269
[2m[36m(func pid=36555)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=36555)[0m f1_macro: 0.14873768957892167
[2m[36m(func pid=36555)[0m f1_weighted: 0.17235998711834913
[2m[36m(func pid=36555)[0m f1_per_class: [0.2, 0.48, 0.0, 0.007, 0.091, 0.146, 0.16, 0.307, 0.0, 0.097]
== Status ==
Current time: 2024-01-07 13:27:15 (running for 00:34:07.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.982 |      0.257 |                   31 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.703 |      0.149 |                   32 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.525 |      0.071 |                   27 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.314 |      0.037 |                   27 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.0625
[2m[36m(func pid=37884)[0m top5: 0.40904850746268656
[2m[36m(func pid=37884)[0m f1_micro: 0.0625
[2m[36m(func pid=37884)[0m f1_macro: 0.029875145978515722
[2m[36m(func pid=37884)[0m f1_weighted: 0.009531639230969216
[2m[36m(func pid=37884)[0m f1_per_class: [0.07, 0.0, 0.073, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.03]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3138992537313433
[2m[36m(func pid=36467)[0m top5: 0.8465485074626866
[2m[36m(func pid=36467)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=36467)[0m f1_macro: 0.2745447528273909
[2m[36m(func pid=36467)[0m f1_weighted: 0.2857619680604135
[2m[36m(func pid=36467)[0m f1_per_class: [0.301, 0.521, 0.579, 0.441, 0.062, 0.308, 0.006, 0.43, 0.0, 0.098]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.13852611940298507
[2m[36m(func pid=37814)[0m top5: 0.7887126865671642
[2m[36m(func pid=37814)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=37814)[0m f1_macro: 0.09272501726218052
[2m[36m(func pid=37814)[0m f1_weighted: 0.11577304662840196
[2m[36m(func pid=37814)[0m f1_per_class: [0.094, 0.123, 0.0, 0.267, 0.163, 0.016, 0.0, 0.264, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.7536 | Steps: 4 | Val loss: 2.9234 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.0173 | Steps: 4 | Val loss: 19.9237 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8589 | Steps: 4 | Val loss: 1.8096 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7304 | Steps: 4 | Val loss: 4.6939 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=36555)[0m top1: 0.2555970149253731
[2m[36m(func pid=36555)[0m top5: 0.8927238805970149
[2m[36m(func pid=36555)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=36555)[0m f1_macro: 0.18137319365280535
[2m[36m(func pid=36555)[0m f1_weighted: 0.23714901473116515
[2m[36m(func pid=36555)[0m f1_per_class: [0.25, 0.43, 0.0, 0.044, 0.087, 0.114, 0.356, 0.403, 0.049, 0.081]
[2m[36m(func pid=36555)[0m 
== Status ==
Current time: 2024-01-07 13:27:20 (running for 00:34:12.81)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.322 |      0.275 |                   32 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.754 |      0.181 |                   33 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  4.932 |      0.093 |                   28 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.03  |      0.03  |                   28 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.0625
[2m[36m(func pid=37884)[0m top5: 0.5886194029850746
[2m[36m(func pid=37884)[0m f1_micro: 0.0625
[2m[36m(func pid=37884)[0m f1_macro: 0.02672199830636724
[2m[36m(func pid=37884)[0m f1_weighted: 0.009090443630687367
[2m[36m(func pid=37884)[0m f1_per_class: [0.065, 0.0, 0.076, 0.0, 0.0, 0.0, 0.0, 0.126, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3246268656716418
[2m[36m(func pid=36467)[0m top5: 0.8582089552238806
[2m[36m(func pid=36467)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=36467)[0m f1_macro: 0.2816837282364218
[2m[36m(func pid=36467)[0m f1_weighted: 0.2930844050699991
[2m[36m(func pid=36467)[0m f1_per_class: [0.292, 0.521, 0.593, 0.461, 0.07, 0.324, 0.006, 0.423, 0.0, 0.127]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.21735074626865672
[2m[36m(func pid=37814)[0m top5: 0.7947761194029851
[2m[36m(func pid=37814)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=37814)[0m f1_macro: 0.09404392920077853
[2m[36m(func pid=37814)[0m f1_weighted: 0.15944875001562112
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.032, 0.0, 0.459, 0.077, 0.069, 0.0, 0.304, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8497 | Steps: 4 | Val loss: 3.5116 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.9614 | Steps: 4 | Val loss: 17.5145 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.0492 | Steps: 4 | Val loss: 1.7709 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1724 | Steps: 4 | Val loss: 4.0829 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:27:25 (running for 00:34:18.19)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.859 |      0.282 |                   33 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.85  |      0.181 |                   34 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.73  |      0.094 |                   29 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.017 |      0.027 |                   29 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.24673507462686567
[2m[36m(func pid=36555)[0m top5: 0.8376865671641791
[2m[36m(func pid=36555)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=36555)[0m f1_macro: 0.18112189926836944
[2m[36m(func pid=36555)[0m f1_weighted: 0.3063817466552291
[2m[36m(func pid=36555)[0m f1_per_class: [0.188, 0.221, 0.0, 0.358, 0.072, 0.079, 0.449, 0.318, 0.056, 0.07]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.06296641791044776
[2m[36m(func pid=37884)[0m top5: 0.4864738805970149
[2m[36m(func pid=37884)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=37884)[0m f1_macro: 0.03226316375576466
[2m[36m(func pid=37884)[0m f1_weighted: 0.009106205572139035
[2m[36m(func pid=37884)[0m f1_per_class: [0.06, 0.0, 0.141, 0.0, 0.0, 0.0, 0.0, 0.121, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m top1: 0.33348880597014924
[2m[36m(func pid=36467)[0m top5: 0.8624067164179104
[2m[36m(func pid=36467)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=36467)[0m f1_macro: 0.2850267935135064
[2m[36m(func pid=36467)[0m f1_weighted: 0.30359413473153257
[2m[36m(func pid=36467)[0m f1_per_class: [0.271, 0.508, 0.56, 0.478, 0.082, 0.355, 0.024, 0.407, 0.0, 0.166]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m top1: 0.17630597014925373
[2m[36m(func pid=37814)[0m top5: 0.8003731343283582
[2m[36m(func pid=37814)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=37814)[0m f1_macro: 0.0933994866531915
[2m[36m(func pid=37814)[0m f1_weighted: 0.1496322638220644
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.047, 0.0, 0.407, 0.065, 0.088, 0.0, 0.266, 0.06, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7041 | Steps: 4 | Val loss: 2.7356 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.2545 | Steps: 4 | Val loss: 10.5864 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.9426 | Steps: 4 | Val loss: 3.6162 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.0206 | Steps: 4 | Val loss: 1.7811 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:27:31 (running for 00:34:23.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.049 |      0.285 |                   34 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.704 |      0.216 |                   35 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.172 |      0.093 |                   30 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.961 |      0.032 |                   30 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.29197761194029853
[2m[36m(func pid=36555)[0m top5: 0.8768656716417911
[2m[36m(func pid=36555)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=36555)[0m f1_macro: 0.21586957494160158
[2m[36m(func pid=36555)[0m f1_weighted: 0.3250867614721294
[2m[36m(func pid=36555)[0m f1_per_class: [0.191, 0.145, 0.0, 0.486, 0.078, 0.151, 0.37, 0.453, 0.142, 0.143]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.06669776119402986
[2m[36m(func pid=37884)[0m top5: 0.6534514925373134
[2m[36m(func pid=37884)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=37884)[0m f1_macro: 0.04068618808413642
[2m[36m(func pid=37884)[0m f1_weighted: 0.016100678292614794
[2m[36m(func pid=37884)[0m f1_per_class: [0.049, 0.0, 0.197, 0.023, 0.0, 0.0, 0.0, 0.118, 0.02, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.11054104477611941
[2m[36m(func pid=37814)[0m top5: 0.7761194029850746
[2m[36m(func pid=37814)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=37814)[0m f1_macro: 0.0880890034453552
[2m[36m(func pid=37814)[0m f1_weighted: 0.08358940973848333
[2m[36m(func pid=37814)[0m f1_per_class: [0.134, 0.057, 0.0, 0.126, 0.07, 0.109, 0.012, 0.279, 0.094, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.32882462686567165
[2m[36m(func pid=36467)[0m top5: 0.8591417910447762
[2m[36m(func pid=36467)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=36467)[0m f1_macro: 0.29417450885566326
[2m[36m(func pid=36467)[0m f1_weighted: 0.30111764833248444
[2m[36m(func pid=36467)[0m f1_per_class: [0.276, 0.514, 0.609, 0.438, 0.093, 0.335, 0.045, 0.462, 0.0, 0.169]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7915 | Steps: 4 | Val loss: 2.5565 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.2697 | Steps: 4 | Val loss: 12.5684 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.1390 | Steps: 4 | Val loss: 4.0016 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7319 | Steps: 4 | Val loss: 1.8070 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:27:36 (running for 00:34:28.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.021 |      0.294 |                   35 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.791 |      0.202 |                   36 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.943 |      0.088 |                   31 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.255 |      0.041 |                   31 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.30223880597014924
[2m[36m(func pid=36555)[0m top5: 0.8843283582089553
[2m[36m(func pid=36555)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=36555)[0m f1_macro: 0.20158148962091849
[2m[36m(func pid=36555)[0m f1_weighted: 0.2943343894238299
[2m[36m(func pid=36555)[0m f1_per_class: [0.182, 0.103, 0.0, 0.535, 0.068, 0.158, 0.251, 0.411, 0.152, 0.157]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.23180970149253732
[2m[36m(func pid=37884)[0m top5: 0.6511194029850746
[2m[36m(func pid=37884)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=37884)[0m f1_macro: 0.06789222505415271
[2m[36m(func pid=37884)[0m f1_weighted: 0.11141247565331275
[2m[36m(func pid=37884)[0m f1_per_class: [0.085, 0.0, 0.156, 0.383, 0.0, 0.0, 0.0, 0.0, 0.055, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.09235074626865672
[2m[36m(func pid=37814)[0m top5: 0.753731343283582
[2m[36m(func pid=37814)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=37814)[0m f1_macro: 0.07598037489178093
[2m[36m(func pid=37814)[0m f1_weighted: 0.04964369251895442
[2m[36m(func pid=37814)[0m f1_per_class: [0.138, 0.042, 0.0, 0.013, 0.093, 0.0, 0.042, 0.34, 0.091, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.32649253731343286
[2m[36m(func pid=36467)[0m top5: 0.8540111940298507
[2m[36m(func pid=36467)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=36467)[0m f1_macro: 0.2892175059091052
[2m[36m(func pid=36467)[0m f1_weighted: 0.30440739423228635
[2m[36m(func pid=36467)[0m f1_per_class: [0.303, 0.518, 0.538, 0.433, 0.095, 0.303, 0.068, 0.478, 0.0, 0.156]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4735 | Steps: 4 | Val loss: 2.7550 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.0488 | Steps: 4 | Val loss: 20.7004 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.7583 | Steps: 4 | Val loss: 4.3601 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:27:42 (running for 00:34:34.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.732 |      0.289 |                   36 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.474 |      0.179 |                   37 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.139 |      0.076 |                   32 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.27  |      0.068 |                   32 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6644 | Steps: 4 | Val loss: 1.7990 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=36555)[0m top1: 0.2733208955223881
[2m[36m(func pid=36555)[0m top5: 0.8763992537313433
[2m[36m(func pid=36555)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=36555)[0m f1_macro: 0.17944436825229731
[2m[36m(func pid=36555)[0m f1_weighted: 0.2321195098257466
[2m[36m(func pid=36555)[0m f1_per_class: [0.36, 0.093, 0.0, 0.546, 0.035, 0.022, 0.095, 0.321, 0.143, 0.18]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.22388059701492538
[2m[36m(func pid=37884)[0m top5: 0.6534514925373134
[2m[36m(func pid=37884)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=37884)[0m f1_macro: 0.050450252508759105
[2m[36m(func pid=37884)[0m f1_weighted: 0.10654647109578563
[2m[36m(func pid=37884)[0m f1_per_class: [0.024, 0.0, 0.102, 0.378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.1394589552238806
[2m[36m(func pid=37814)[0m top5: 0.7000932835820896
[2m[36m(func pid=37814)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=37814)[0m f1_macro: 0.11078377393917896
[2m[36m(func pid=37814)[0m f1_weighted: 0.12291839849983537
[2m[36m(func pid=37814)[0m f1_per_class: [0.157, 0.021, 0.003, 0.003, 0.129, 0.0, 0.293, 0.414, 0.087, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.333955223880597
[2m[36m(func pid=36467)[0m top5: 0.8540111940298507
[2m[36m(func pid=36467)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=36467)[0m f1_macro: 0.2906442163381543
[2m[36m(func pid=36467)[0m f1_weighted: 0.32608039066613015
[2m[36m(func pid=36467)[0m f1_per_class: [0.283, 0.509, 0.516, 0.454, 0.102, 0.3, 0.13, 0.472, 0.0, 0.14]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6130 | Steps: 4 | Val loss: 3.1471 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.9897 | Steps: 4 | Val loss: 14.2069 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4729 | Steps: 4 | Val loss: 3.0698 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.9636 | Steps: 4 | Val loss: 1.8496 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 13:27:47 (running for 00:34:39.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.664 |      0.291 |                   37 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.613 |      0.186 |                   38 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.758 |      0.111 |                   33 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.049 |      0.05  |                   33 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.26399253731343286
[2m[36m(func pid=36555)[0m top5: 0.8292910447761194
[2m[36m(func pid=36555)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=36555)[0m f1_macro: 0.1863325279406542
[2m[36m(func pid=36555)[0m f1_weighted: 0.24570111585632753
[2m[36m(func pid=36555)[0m f1_per_class: [0.303, 0.208, 0.0, 0.516, 0.07, 0.05, 0.097, 0.319, 0.125, 0.174]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.23647388059701493
[2m[36m(func pid=37884)[0m top5: 0.6152052238805971
[2m[36m(func pid=37884)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=37884)[0m f1_macro: 0.060473806435621946
[2m[36m(func pid=37884)[0m f1_weighted: 0.11182765626897978
[2m[36m(func pid=37884)[0m f1_per_class: [0.098, 0.0, 0.116, 0.391, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.1791044776119403
[2m[36m(func pid=37814)[0m top5: 0.699160447761194
[2m[36m(func pid=37814)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=37814)[0m f1_macro: 0.13249141464793768
[2m[36m(func pid=37814)[0m f1_weighted: 0.15546186552332508
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.021, 0.03, 0.0, 0.15, 0.008, 0.404, 0.422, 0.067, 0.222]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.322294776119403
[2m[36m(func pid=36467)[0m top5: 0.8428171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.2763616961384516
[2m[36m(func pid=36467)[0m f1_weighted: 0.3235261940470313
[2m[36m(func pid=36467)[0m f1_per_class: [0.292, 0.522, 0.462, 0.425, 0.105, 0.267, 0.168, 0.406, 0.0, 0.116]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.3416 | Steps: 4 | Val loss: 4.3400 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.1049 | Steps: 4 | Val loss: 11.9770 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.4660 | Steps: 4 | Val loss: 2.8202 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7917 | Steps: 4 | Val loss: 1.8496 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 13:27:53 (running for 00:34:45.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.964 |      0.276 |                   38 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.342 |      0.205 |                   39 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.473 |      0.132 |                   34 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.99  |      0.06  |                   34 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.24720149253731344
[2m[36m(func pid=36555)[0m top5: 0.7569962686567164
[2m[36m(func pid=36555)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=36555)[0m f1_macro: 0.2049246612876595
[2m[36m(func pid=36555)[0m f1_weighted: 0.26058605542459573
[2m[36m(func pid=36555)[0m f1_per_class: [0.335, 0.393, 0.0, 0.442, 0.148, 0.038, 0.102, 0.396, 0.079, 0.115]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m top1: 0.30597014925373134
[2m[36m(func pid=37884)[0m top5: 0.6119402985074627
[2m[36m(func pid=37884)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=37884)[0m f1_macro: 0.07875149777498361
[2m[36m(func pid=37884)[0m f1_weighted: 0.1752929889690777
[2m[36m(func pid=37884)[0m f1_per_class: [0.072, 0.0, 0.129, 0.093, 0.0, 0.0, 0.494, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.1828358208955224
[2m[36m(func pid=37814)[0m top5: 0.7355410447761194
[2m[36m(func pid=37814)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=37814)[0m f1_macro: 0.13213490718447427
[2m[36m(func pid=37814)[0m f1_weighted: 0.16873130621935684
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.027, 0.036, 0.0, 0.136, 0.0, 0.442, 0.47, 0.077, 0.133]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3362873134328358
[2m[36m(func pid=36467)[0m top5: 0.8381529850746269
[2m[36m(func pid=36467)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=36467)[0m f1_macro: 0.27099144815965465
[2m[36m(func pid=36467)[0m f1_weighted: 0.34438369279647213
[2m[36m(func pid=36467)[0m f1_per_class: [0.304, 0.548, 0.358, 0.434, 0.109, 0.259, 0.228, 0.366, 0.0, 0.104]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.2929 | Steps: 4 | Val loss: 8.5947 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.2754 | Steps: 4 | Val loss: 11.0078 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1219 | Steps: 4 | Val loss: 3.0695 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:27:58 (running for 00:34:50.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.792 |      0.271 |                   39 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.293 |      0.125 |                   40 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.466 |      0.132 |                   35 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.105 |      0.079 |                   35 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.18796641791044777
[2m[36m(func pid=36555)[0m top5: 0.6585820895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=36555)[0m f1_macro: 0.1254779796372881
[2m[36m(func pid=36555)[0m f1_weighted: 0.15939422926351424
[2m[36m(func pid=36555)[0m f1_per_class: [0.272, 0.499, 0.0, 0.161, 0.125, 0.0, 0.062, 0.0, 0.095, 0.041]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7170 | Steps: 4 | Val loss: 1.7469 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=37884)[0m top1: 0.28404850746268656
[2m[36m(func pid=37884)[0m top5: 0.7140858208955224
[2m[36m(func pid=37884)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=37884)[0m f1_macro: 0.07521703267272647
[2m[36m(func pid=37884)[0m f1_weighted: 0.15061443276724565
[2m[36m(func pid=37884)[0m f1_per_class: [0.085, 0.0, 0.16, 0.003, 0.0, 0.0, 0.493, 0.0, 0.0, 0.011]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.17723880597014927
[2m[36m(func pid=37814)[0m top5: 0.7397388059701493
[2m[36m(func pid=37814)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=37814)[0m f1_macro: 0.14533548376893707
[2m[36m(func pid=37814)[0m f1_weighted: 0.17943128270146702
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.319, 0.035, 0.003, 0.0, 0.276, 0.212, 0.421, 0.102, 0.086]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.375
[2m[36m(func pid=36467)[0m top5: 0.8442164179104478
[2m[36m(func pid=36467)[0m f1_micro: 0.375
[2m[36m(func pid=36467)[0m f1_macro: 0.28454725866189057
[2m[36m(func pid=36467)[0m f1_weighted: 0.38569016030655834
[2m[36m(func pid=36467)[0m f1_per_class: [0.324, 0.549, 0.358, 0.433, 0.122, 0.307, 0.364, 0.274, 0.0, 0.114]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.4167 | Steps: 4 | Val loss: 5.3404 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.8967 | Steps: 4 | Val loss: 9.4538 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2214 | Steps: 4 | Val loss: 3.1427 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=36555)[0m top1: 0.18889925373134328
[2m[36m(func pid=36555)[0m top5: 0.7364738805970149
[2m[36m(func pid=36555)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=36555)[0m f1_macro: 0.13397570763266764
[2m[36m(func pid=36555)[0m f1_weighted: 0.1473332074648595
[2m[36m(func pid=36555)[0m f1_per_class: [0.237, 0.487, 0.0, 0.006, 0.254, 0.054, 0.144, 0.068, 0.041, 0.048]
== Status ==
Current time: 2024-01-07 13:28:03 (running for 00:34:56.08)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.717 |      0.285 |                   40 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.417 |      0.134 |                   41 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.122 |      0.145 |                   36 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.275 |      0.075 |                   36 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.6000 | Steps: 4 | Val loss: 1.6670 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37884)[0m top1: 0.01912313432835821
[2m[36m(func pid=37884)[0m top5: 0.47574626865671643
[2m[36m(func pid=37884)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=37884)[0m f1_macro: 0.034276313291226386
[2m[36m(func pid=37884)[0m f1_weighted: 0.00360070605419035
[2m[36m(func pid=37884)[0m f1_per_class: [0.097, 0.0, 0.224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.18190298507462688
[2m[36m(func pid=37814)[0m top5: 0.738339552238806
[2m[36m(func pid=37814)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=37814)[0m f1_macro: 0.16535507046256098
[2m[36m(func pid=37814)[0m f1_weighted: 0.14005515852135222
[2m[36m(func pid=37814)[0m f1_per_class: [0.029, 0.364, 0.043, 0.007, 0.0, 0.365, 0.0, 0.433, 0.147, 0.267]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.41044776119402987
[2m[36m(func pid=36467)[0m top5: 0.8460820895522388
[2m[36m(func pid=36467)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=36467)[0m f1_macro: 0.2844255686593731
[2m[36m(func pid=36467)[0m f1_weighted: 0.4178856524122764
[2m[36m(func pid=36467)[0m f1_per_class: [0.352, 0.546, 0.3, 0.408, 0.127, 0.324, 0.513, 0.135, 0.024, 0.114]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2333 | Steps: 4 | Val loss: 4.3144 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.9225 | Steps: 4 | Val loss: 6.0714 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:28:09 (running for 00:35:01.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.6   |      0.284 |                   41 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.233 |      0.233 |                   42 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.221 |      0.165 |                   37 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.897 |      0.034 |                   37 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.29384328358208955
[2m[36m(func pid=36555)[0m top5: 0.7765858208955224
[2m[36m(func pid=36555)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=36555)[0m f1_macro: 0.23304276101431612
[2m[36m(func pid=36555)[0m f1_weighted: 0.2603968540196742
[2m[36m(func pid=36555)[0m f1_per_class: [0.159, 0.531, 0.375, 0.003, 0.116, 0.14, 0.403, 0.346, 0.117, 0.139]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6103 | Steps: 4 | Val loss: 2.9496 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8409 | Steps: 4 | Val loss: 1.6708 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37884)[0m top1: 0.11847014925373134
[2m[36m(func pid=37884)[0m top5: 0.46222014925373134
[2m[36m(func pid=37884)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=37884)[0m f1_macro: 0.0702876568237559
[2m[36m(func pid=37884)[0m f1_weighted: 0.030764481816871986
[2m[36m(func pid=37884)[0m f1_per_class: [0.102, 0.0, 0.361, 0.0, 0.0, 0.231, 0.0, 0.0, 0.0, 0.01]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.19309701492537312
[2m[36m(func pid=37814)[0m top5: 0.7192164179104478
[2m[36m(func pid=37814)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=37814)[0m f1_macro: 0.16501906469279234
[2m[36m(func pid=37814)[0m f1_weighted: 0.14991110641294014
[2m[36m(func pid=37814)[0m f1_per_class: [0.06, 0.38, 0.118, 0.045, 0.0, 0.343, 0.0, 0.453, 0.078, 0.174]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4440 | Steps: 4 | Val loss: 7.2502 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=36467)[0m top1: 0.396455223880597
[2m[36m(func pid=36467)[0m top5: 0.8390858208955224
[2m[36m(func pid=36467)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=36467)[0m f1_macro: 0.2645511389858529
[2m[36m(func pid=36467)[0m f1_weighted: 0.3978734020664778
[2m[36m(func pid=36467)[0m f1_per_class: [0.312, 0.538, 0.27, 0.365, 0.133, 0.282, 0.527, 0.016, 0.084, 0.119]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.8294 | Steps: 4 | Val loss: 4.5183 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:28:14 (running for 00:35:06.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.841 |      0.265 |                   42 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.444 |      0.191 |                   43 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.61  |      0.165 |                   38 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.922 |      0.07  |                   38 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.25
[2m[36m(func pid=36555)[0m top5: 0.6707089552238806
[2m[36m(func pid=36555)[0m f1_micro: 0.25
[2m[36m(func pid=36555)[0m f1_macro: 0.19078529266429162
[2m[36m(func pid=36555)[0m f1_weighted: 0.23819560027058256
[2m[36m(func pid=36555)[0m f1_per_class: [0.125, 0.522, 0.016, 0.0, 0.12, 0.146, 0.341, 0.367, 0.102, 0.169]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1871 | Steps: 4 | Val loss: 2.8209 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8348 | Steps: 4 | Val loss: 1.6834 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=37884)[0m top1: 0.12126865671641791
[2m[36m(func pid=37884)[0m top5: 0.47341417910447764
[2m[36m(func pid=37884)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=37884)[0m f1_macro: 0.07435112452274606
[2m[36m(func pid=37884)[0m f1_weighted: 0.030751799677192364
[2m[36m(func pid=37884)[0m f1_per_class: [0.116, 0.0, 0.386, 0.0, 0.0, 0.226, 0.0, 0.0, 0.0, 0.016]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.2080223880597015
[2m[36m(func pid=37814)[0m top5: 0.6884328358208955
[2m[36m(func pid=37814)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=37814)[0m f1_macro: 0.1897629690852028
[2m[36m(func pid=37814)[0m f1_weighted: 0.19133729476194106
[2m[36m(func pid=37814)[0m f1_per_class: [0.066, 0.372, 0.296, 0.201, 0.0, 0.31, 0.0, 0.518, 0.065, 0.071]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.3278 | Steps: 4 | Val loss: 8.6652 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=36467)[0m top1: 0.4006529850746269
[2m[36m(func pid=36467)[0m top5: 0.8428171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=36467)[0m f1_macro: 0.27818947743165556
[2m[36m(func pid=36467)[0m f1_weighted: 0.4016205753859922
[2m[36m(func pid=36467)[0m f1_per_class: [0.317, 0.541, 0.308, 0.334, 0.132, 0.264, 0.56, 0.031, 0.171, 0.123]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.7061 | Steps: 4 | Val loss: 5.0250 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:28:19 (running for 00:35:12.18)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.835 |      0.278 |                   43 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.328 |      0.177 |                   44 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.187 |      0.19  |                   39 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.829 |      0.074 |                   39 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.18423507462686567
[2m[36m(func pid=36555)[0m top5: 0.6217350746268657
[2m[36m(func pid=36555)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=36555)[0m f1_macro: 0.17688591993229816
[2m[36m(func pid=36555)[0m f1_weighted: 0.1661855635949669
[2m[36m(func pid=36555)[0m f1_per_class: [0.193, 0.396, 0.058, 0.01, 0.13, 0.271, 0.111, 0.339, 0.141, 0.118]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5907 | Steps: 4 | Val loss: 3.0052 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0900 | Steps: 4 | Val loss: 1.7282 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37884)[0m top1: 0.11893656716417911
[2m[36m(func pid=37884)[0m top5: 0.33302238805970147
[2m[36m(func pid=37884)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=37884)[0m f1_macro: 0.05383817609626549
[2m[36m(func pid=37884)[0m f1_weighted: 0.031119303423449315
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.216, 0.0, 0.0, 0.236, 0.0, 0.0, 0.086, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.21128731343283583
[2m[36m(func pid=37814)[0m top5: 0.6693097014925373
[2m[36m(func pid=37814)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=37814)[0m f1_macro: 0.16356427853298502
[2m[36m(func pid=37814)[0m f1_weighted: 0.21497746215044367
[2m[36m(func pid=37814)[0m f1_per_class: [0.077, 0.37, 0.0, 0.32, 0.0, 0.254, 0.0, 0.493, 0.073, 0.049]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.0269 | Steps: 4 | Val loss: 7.5661 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=36467)[0m top1: 0.38199626865671643
[2m[36m(func pid=36467)[0m top5: 0.847481343283582
[2m[36m(func pid=36467)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=36467)[0m f1_macro: 0.29747573338255473
[2m[36m(func pid=36467)[0m f1_weighted: 0.37772546118491074
[2m[36m(func pid=36467)[0m f1_per_class: [0.196, 0.492, 0.522, 0.23, 0.169, 0.213, 0.583, 0.258, 0.173, 0.138]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6637 | Steps: 4 | Val loss: 3.3079 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:28:25 (running for 00:35:17.57)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  2.09  |      0.297 |                   44 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.027 |      0.15  |                   45 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.591 |      0.164 |                   40 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.706 |      0.054 |                   40 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.15811567164179105
[2m[36m(func pid=36555)[0m top5: 0.6926305970149254
[2m[36m(func pid=36555)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=36555)[0m f1_macro: 0.1496853288710687
[2m[36m(func pid=36555)[0m f1_weighted: 0.13896197879988895
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.338, 0.04, 0.063, 0.111, 0.331, 0.0, 0.285, 0.188, 0.142]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7092 | Steps: 4 | Val loss: 2.8091 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5256 | Steps: 4 | Val loss: 1.7150 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37884)[0m top1: 0.03684701492537314
[2m[36m(func pid=37884)[0m top5: 0.3381529850746269
[2m[36m(func pid=37884)[0m f1_micro: 0.03684701492537314
[2m[36m(func pid=37884)[0m f1_macro: 0.03739849179179723
[2m[36m(func pid=37884)[0m f1_weighted: 0.003952895099931882
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7552 | Steps: 4 | Val loss: 7.0487 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=37814)[0m top1: 0.1837686567164179
[2m[36m(func pid=37814)[0m top5: 0.7019589552238806
[2m[36m(func pid=37814)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=37814)[0m f1_macro: 0.18283651746412968
[2m[36m(func pid=37814)[0m f1_weighted: 0.20685419621329498
[2m[36m(func pid=37814)[0m f1_per_class: [0.067, 0.294, 0.222, 0.369, 0.118, 0.165, 0.0, 0.481, 0.078, 0.034]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3894589552238806
[2m[36m(func pid=36467)[0m top5: 0.8493470149253731
[2m[36m(func pid=36467)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=36467)[0m f1_macro: 0.33257980217041394
[2m[36m(func pid=36467)[0m f1_weighted: 0.40354603684946855
[2m[36m(func pid=36467)[0m f1_per_class: [0.2, 0.521, 0.522, 0.258, 0.135, 0.246, 0.556, 0.556, 0.18, 0.153]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.9307 | Steps: 4 | Val loss: 3.0793 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:28:30 (running for 00:35:22.80)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.526 |      0.333 |                   45 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.755 |      0.11  |                   46 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.709 |      0.183 |                   41 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.664 |      0.037 |                   41 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.12406716417910447
[2m[36m(func pid=36555)[0m top5: 0.6520522388059702
[2m[36m(func pid=36555)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=36555)[0m f1_macro: 0.10995513483778292
[2m[36m(func pid=36555)[0m f1_weighted: 0.1482886543352966
[2m[36m(func pid=36555)[0m f1_per_class: [0.0, 0.332, 0.041, 0.234, 0.082, 0.13, 0.0, 0.069, 0.165, 0.046]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.8094 | Steps: 4 | Val loss: 2.5080 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7499 | Steps: 4 | Val loss: 1.7563 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=37884)[0m top1: 0.14972014925373134
[2m[36m(func pid=37884)[0m top5: 0.6268656716417911
[2m[36m(func pid=37884)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=37884)[0m f1_macro: 0.05901795659582463
[2m[36m(func pid=37884)[0m f1_weighted: 0.05188265532956809
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.278, 0.233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.079, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7715 | Steps: 4 | Val loss: 12.7447 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=37814)[0m top1: 0.16930970149253732
[2m[36m(func pid=37814)[0m top5: 0.742070895522388
[2m[36m(func pid=37814)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=37814)[0m f1_macro: 0.18412394868448134
[2m[36m(func pid=37814)[0m f1_weighted: 0.19169407294266885
[2m[36m(func pid=37814)[0m f1_per_class: [0.07, 0.095, 0.421, 0.392, 0.092, 0.106, 0.06, 0.489, 0.085, 0.032]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.384794776119403
[2m[36m(func pid=36467)[0m top5: 0.8535447761194029
[2m[36m(func pid=36467)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.3476598259558899
[2m[36m(func pid=36467)[0m f1_weighted: 0.41234958392964116
[2m[36m(func pid=36467)[0m f1_per_class: [0.355, 0.54, 0.48, 0.294, 0.115, 0.284, 0.513, 0.582, 0.174, 0.14]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m top1: 0.09701492537313433
[2m[36m(func pid=36555)[0m top5: 0.4748134328358209
[2m[36m(func pid=36555)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=36555)[0m f1_macro: 0.0813878716045233
[2m[36m(func pid=36555)[0m f1_weighted: 0.11243327987931666
[2m[36m(func pid=36555)[0m f1_per_class: [0.01, 0.25, 0.036, 0.22, 0.053, 0.024, 0.0, 0.0, 0.089, 0.132]
[2m[36m(func pid=36555)[0m 
== Status ==
Current time: 2024-01-07 13:28:35 (running for 00:35:28.05)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.75  |      0.348 |                   46 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.772 |      0.081 |                   47 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.809 |      0.184 |                   42 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.931 |      0.059 |                   42 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.1727 | Steps: 4 | Val loss: 3.3134 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.8211 | Steps: 4 | Val loss: 2.2865 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.6184 | Steps: 4 | Val loss: 1.9105 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=37884)[0m top1: 0.15111940298507462
[2m[36m(func pid=37884)[0m top5: 0.6254664179104478
[2m[36m(func pid=37884)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=37884)[0m f1_macro: 0.05812215312825982
[2m[36m(func pid=37884)[0m f1_weighted: 0.052561618083839655
[2m[36m(func pid=37884)[0m f1_per_class: [0.039, 0.279, 0.183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7915 | Steps: 4 | Val loss: 8.1553 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=37814)[0m top1: 0.2630597014925373
[2m[36m(func pid=37814)[0m top5: 0.8101679104477612
[2m[36m(func pid=37814)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=37814)[0m f1_macro: 0.20534946963155698
[2m[36m(func pid=37814)[0m f1_weighted: 0.302436637167795
[2m[36m(func pid=37814)[0m f1_per_class: [0.075, 0.051, 0.462, 0.411, 0.084, 0.008, 0.502, 0.366, 0.045, 0.048]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.33955223880597013
[2m[36m(func pid=36467)[0m top5: 0.8376865671641791
[2m[36m(func pid=36467)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=36467)[0m f1_macro: 0.309007469206602
[2m[36m(func pid=36467)[0m f1_weighted: 0.3697114643732689
[2m[36m(func pid=36467)[0m f1_per_class: [0.415, 0.562, 0.293, 0.281, 0.088, 0.248, 0.394, 0.542, 0.147, 0.119]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:28:41 (running for 00:35:33.55)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.618 |      0.309 |                   47 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.791 |      0.091 |                   48 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.821 |      0.205 |                   43 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.173 |      0.058 |                   43 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.11707089552238806
[2m[36m(func pid=36555)[0m top5: 0.5522388059701493
[2m[36m(func pid=36555)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=36555)[0m f1_macro: 0.09123565911051129
[2m[36m(func pid=36555)[0m f1_weighted: 0.13026795394155644
[2m[36m(func pid=36555)[0m f1_per_class: [0.035, 0.279, 0.165, 0.272, 0.048, 0.024, 0.0, 0.0, 0.021, 0.068]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.8527 | Steps: 4 | Val loss: 2.6137 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.1112 | Steps: 4 | Val loss: 2.4830 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5957 | Steps: 4 | Val loss: 1.8850 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37884)[0m top1: 0.16371268656716417
[2m[36m(func pid=37884)[0m top5: 0.5942164179104478
[2m[36m(func pid=37884)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=37884)[0m f1_macro: 0.06607611353235321
[2m[36m(func pid=37884)[0m f1_weighted: 0.05490563255497861
[2m[36m(func pid=37884)[0m f1_per_class: [0.041, 0.287, 0.235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4469 | Steps: 4 | Val loss: 4.0072 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=37814)[0m top1: 0.32742537313432835
[2m[36m(func pid=37814)[0m top5: 0.8339552238805971
[2m[36m(func pid=37814)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=37814)[0m f1_macro: 0.16331258570811624
[2m[36m(func pid=37814)[0m f1_weighted: 0.34002638332979596
[2m[36m(func pid=37814)[0m f1_per_class: [0.082, 0.208, 0.178, 0.479, 0.078, 0.0, 0.558, 0.0, 0.025, 0.025]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3362873134328358
[2m[36m(func pid=36467)[0m top5: 0.8428171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=36467)[0m f1_macro: 0.3272985461065315
[2m[36m(func pid=36467)[0m f1_weighted: 0.35373784315176
[2m[36m(func pid=36467)[0m f1_per_class: [0.404, 0.549, 0.48, 0.254, 0.097, 0.285, 0.357, 0.524, 0.163, 0.16]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:28:46 (running for 00:35:38.78)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.596 |      0.327 |                   48 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.447 |      0.221 |                   49 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.111 |      0.163 |                   44 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.853 |      0.066 |                   44 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.24580223880597016
[2m[36m(func pid=36555)[0m top5: 0.7504664179104478
[2m[36m(func pid=36555)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=36555)[0m f1_macro: 0.22068692753380623
[2m[36m(func pid=36555)[0m f1_weighted: 0.2886101959805211
[2m[36m(func pid=36555)[0m f1_per_class: [0.089, 0.323, 0.48, 0.352, 0.074, 0.166, 0.31, 0.261, 0.06, 0.092]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6356 | Steps: 4 | Val loss: 2.4107 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6578 | Steps: 4 | Val loss: 2.6041 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6704 | Steps: 4 | Val loss: 1.8620 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=37884)[0m top1: 0.16557835820895522
[2m[36m(func pid=37884)[0m top5: 0.659981343283582
[2m[36m(func pid=37884)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=37884)[0m f1_macro: 0.057826231135468385
[2m[36m(func pid=37884)[0m f1_weighted: 0.053725991459734994
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.287, 0.197, 0.0, 0.0, 0.0, 0.0, 0.0, 0.094, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8840 | Steps: 4 | Val loss: 2.4142 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=37814)[0m top1: 0.39365671641791045
[2m[36m(func pid=37814)[0m top5: 0.8619402985074627
[2m[36m(func pid=37814)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=37814)[0m f1_macro: 0.1662648437976605
[2m[36m(func pid=37814)[0m f1_weighted: 0.37353120268071255
[2m[36m(func pid=37814)[0m f1_per_class: [0.067, 0.32, 0.0, 0.487, 0.09, 0.0, 0.6, 0.0, 0.025, 0.073]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.30783582089552236
[2m[36m(func pid=36467)[0m top5: 0.855410447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=36467)[0m f1_macro: 0.33122126339052266
[2m[36m(func pid=36467)[0m f1_weighted: 0.3128355051386666
[2m[36m(func pid=36467)[0m f1_per_class: [0.389, 0.515, 0.625, 0.213, 0.074, 0.307, 0.273, 0.475, 0.151, 0.289]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:28:51 (running for 00:35:44.04)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.67  |      0.331 |                   49 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.884 |      0.274 |                   50 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.658 |      0.166 |                   45 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.636 |      0.058 |                   45 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.38572761194029853
[2m[36m(func pid=36555)[0m top5: 0.878731343283582
[2m[36m(func pid=36555)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=36555)[0m f1_macro: 0.27368337294675316
[2m[36m(func pid=36555)[0m f1_weighted: 0.40391122197101714
[2m[36m(func pid=36555)[0m f1_per_class: [0.137, 0.397, 0.267, 0.425, 0.165, 0.236, 0.523, 0.442, 0.06, 0.086]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.9203 | Steps: 4 | Val loss: 2.3350 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7412 | Steps: 4 | Val loss: 2.4574 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5306 | Steps: 4 | Val loss: 1.8968 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=37884)[0m top1: 0.16930970149253732
[2m[36m(func pid=37884)[0m top5: 0.65625
[2m[36m(func pid=37884)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=37884)[0m f1_macro: 0.05184928682887885
[2m[36m(func pid=37884)[0m f1_weighted: 0.06151788934112331
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.289, 0.18, 0.035, 0.0, 0.0, 0.0, 0.013, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0136 | Steps: 4 | Val loss: 2.0933 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=37814)[0m top1: 0.41044776119402987
[2m[36m(func pid=37814)[0m top5: 0.8694029850746269
[2m[36m(func pid=37814)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=37814)[0m f1_macro: 0.17063997046520937
[2m[36m(func pid=37814)[0m f1_weighted: 0.38003506685409194
[2m[36m(func pid=37814)[0m f1_per_class: [0.029, 0.394, 0.0, 0.463, 0.102, 0.008, 0.6, 0.0, 0.029, 0.082]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2966417910447761
[2m[36m(func pid=36467)[0m top5: 0.8563432835820896
[2m[36m(func pid=36467)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=36467)[0m f1_macro: 0.3261192081267681
[2m[36m(func pid=36467)[0m f1_weighted: 0.304241117780846
[2m[36m(func pid=36467)[0m f1_per_class: [0.357, 0.52, 0.606, 0.29, 0.061, 0.261, 0.187, 0.477, 0.159, 0.344]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:28:57 (running for 00:35:49.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.531 |      0.326 |                   50 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  2.014 |      0.291 |                   51 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.741 |      0.171 |                   46 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.92  |      0.052 |                   46 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.39365671641791045
[2m[36m(func pid=36555)[0m top5: 0.8801305970149254
[2m[36m(func pid=36555)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=36555)[0m f1_macro: 0.29144418210820583
[2m[36m(func pid=36555)[0m f1_weighted: 0.4148493218960363
[2m[36m(func pid=36555)[0m f1_per_class: [0.127, 0.374, 0.375, 0.427, 0.08, 0.366, 0.521, 0.379, 0.169, 0.095]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.9712 | Steps: 4 | Val loss: 2.2848 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5666 | Steps: 4 | Val loss: 2.1484 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6788 | Steps: 4 | Val loss: 2.0844 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37884)[0m top1: 0.06436567164179105
[2m[36m(func pid=37884)[0m top5: 0.6375932835820896
[2m[36m(func pid=37884)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=37884)[0m f1_macro: 0.03781514860863148
[2m[36m(func pid=37884)[0m f1_weighted: 0.010837544995014936
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.255, 0.01, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6468 | Steps: 4 | Val loss: 2.1945 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=37814)[0m top1: 0.37220149253731344
[2m[36m(func pid=37814)[0m top5: 0.8726679104477612
[2m[36m(func pid=37814)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=37814)[0m f1_macro: 0.18474453279775246
[2m[36m(func pid=37814)[0m f1_weighted: 0.3688936347140455
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.416, 0.222, 0.398, 0.066, 0.03, 0.594, 0.0, 0.121, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2653917910447761
[2m[36m(func pid=36467)[0m top5: 0.8418843283582089
[2m[36m(func pid=36467)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=36467)[0m f1_macro: 0.2952579600435773
[2m[36m(func pid=36467)[0m f1_weighted: 0.27281946530346635
[2m[36m(func pid=36467)[0m f1_per_class: [0.299, 0.536, 0.585, 0.327, 0.049, 0.11, 0.102, 0.472, 0.152, 0.319]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:29:02 (running for 00:35:54.53)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.679 |      0.295 |                   51 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.647 |      0.301 |                   52 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.567 |      0.185 |                   47 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.971 |      0.038 |                   47 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.34095149253731344
[2m[36m(func pid=36555)[0m top5: 0.8610074626865671
[2m[36m(func pid=36555)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=36555)[0m f1_macro: 0.300517374465829
[2m[36m(func pid=36555)[0m f1_weighted: 0.3664505993288753
[2m[36m(func pid=36555)[0m f1_per_class: [0.09, 0.403, 0.762, 0.384, 0.0, 0.34, 0.39, 0.369, 0.192, 0.076]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8536 | Steps: 4 | Val loss: 2.2975 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.8197 | Steps: 4 | Val loss: 2.9767 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.2317 | Steps: 4 | Val loss: 2.1256 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.3848 | Steps: 4 | Val loss: 2.7842 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=37884)[0m top1: 0.06669776119402986
[2m[36m(func pid=37884)[0m top5: 0.6399253731343284
[2m[36m(func pid=37884)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=37884)[0m f1_macro: 0.053760731599693204
[2m[36m(func pid=37884)[0m f1_weighted: 0.016163929126102917
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.0, 0.4, 0.026, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.21875
[2m[36m(func pid=37814)[0m top5: 0.7966417910447762
[2m[36m(func pid=37814)[0m f1_micro: 0.21875
[2m[36m(func pid=37814)[0m f1_macro: 0.14675107252474567
[2m[36m(func pid=37814)[0m f1_weighted: 0.2552836549006234
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.423, 0.27, 0.26, 0.031, 0.008, 0.343, 0.016, 0.116, 0.0]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:29:07 (running for 00:35:59.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.232 |      0.285 |                   52 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.647 |      0.301 |                   52 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.82  |      0.147 |                   48 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.854 |      0.054 |                   48 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.259794776119403
[2m[36m(func pid=36467)[0m top5: 0.8386194029850746
[2m[36m(func pid=36467)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.2849508862079426
[2m[36m(func pid=36467)[0m f1_weighted: 0.2615529346389799
[2m[36m(func pid=36467)[0m f1_per_class: [0.265, 0.528, 0.585, 0.369, 0.05, 0.113, 0.034, 0.471, 0.143, 0.291]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m top1: 0.24580223880597016
[2m[36m(func pid=36555)[0m top5: 0.7915111940298507
[2m[36m(func pid=36555)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=36555)[0m f1_macro: 0.24370135656471786
[2m[36m(func pid=36555)[0m f1_weighted: 0.2467240741158199
[2m[36m(func pid=36555)[0m f1_per_class: [0.087, 0.428, 0.727, 0.29, 0.0, 0.218, 0.12, 0.319, 0.188, 0.06]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7055 | Steps: 4 | Val loss: 2.3370 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.8770 | Steps: 4 | Val loss: 3.3828 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5074 | Steps: 4 | Val loss: 2.2847 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.3909 | Steps: 4 | Val loss: 3.4214 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=37884)[0m top1: 0.06436567164179105
[2m[36m(func pid=37884)[0m top5: 0.6431902985074627
[2m[36m(func pid=37884)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=37884)[0m f1_macro: 0.06538907396617352
[2m[36m(func pid=37884)[0m f1_weighted: 0.0139120425389982
[2m[36m(func pid=37884)[0m f1_per_class: [0.041, 0.005, 0.486, 0.01, 0.0, 0.0, 0.0, 0.111, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m top1: 0.11707089552238806
[2m[36m(func pid=37814)[0m top5: 0.6627798507462687
[2m[36m(func pid=37814)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=37814)[0m f1_macro: 0.11966895066846499
[2m[36m(func pid=37814)[0m f1_weighted: 0.13271884001847475
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.331, 0.273, 0.054, 0.034, 0.007, 0.131, 0.279, 0.089, 0.0]
[2m[36m(func pid=37814)[0m 
== Status ==
Current time: 2024-01-07 13:29:12 (running for 00:36:05.13)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.232 |      0.285 |                   52 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.391 |      0.149 |                   54 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.877 |      0.12  |                   49 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.706 |      0.065 |                   49 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.19029850746268656
[2m[36m(func pid=36555)[0m top5: 0.746268656716418
[2m[36m(func pid=36555)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=36555)[0m f1_macro: 0.14893857631085197
[2m[36m(func pid=36555)[0m f1_weighted: 0.17791905338424574
[2m[36m(func pid=36555)[0m f1_per_class: [0.158, 0.443, 0.0, 0.211, 0.033, 0.097, 0.015, 0.298, 0.165, 0.068]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2462686567164179
[2m[36m(func pid=36467)[0m top5: 0.816231343283582
[2m[36m(func pid=36467)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=36467)[0m f1_macro: 0.24575041710483628
[2m[36m(func pid=36467)[0m f1_weighted: 0.2531589246996542
[2m[36m(func pid=36467)[0m f1_per_class: [0.216, 0.515, 0.421, 0.413, 0.052, 0.083, 0.0, 0.449, 0.139, 0.17]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.2130 | Steps: 4 | Val loss: 2.3011 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7074 | Steps: 4 | Val loss: 2.6045 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2570 | Steps: 4 | Val loss: 4.1205 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=37884)[0m top1: 0.06436567164179105
[2m[36m(func pid=37884)[0m top5: 0.8283582089552238
[2m[36m(func pid=37884)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=37884)[0m f1_macro: 0.07741073933626832
[2m[36m(func pid=37884)[0m f1_weighted: 0.014228995438306226
[2m[36m(func pid=37884)[0m f1_per_class: [0.043, 0.011, 0.533, 0.003, 0.0, 0.0, 0.0, 0.113, 0.0, 0.071]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4721 | Steps: 4 | Val loss: 2.3441 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 13:29:18 (running for 00:36:10.36)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.507 |      0.246 |                   53 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.391 |      0.149 |                   54 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.707 |      0.147 |                   50 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.213 |      0.077 |                   50 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.10914179104477612
[2m[36m(func pid=37814)[0m top5: 0.7103544776119403
[2m[36m(func pid=37814)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=37814)[0m f1_macro: 0.1474035598358489
[2m[36m(func pid=37814)[0m f1_weighted: 0.09859668659150779
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.278, 0.471, 0.033, 0.039, 0.035, 0.003, 0.534, 0.083, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.1478544776119403
[2m[36m(func pid=36555)[0m top5: 0.7523320895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=36555)[0m f1_macro: 0.12301782915344076
[2m[36m(func pid=36555)[0m f1_weighted: 0.13333177302816648
[2m[36m(func pid=36555)[0m f1_per_class: [0.152, 0.278, 0.0, 0.163, 0.072, 0.073, 0.025, 0.272, 0.123, 0.071]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.24673507462686567
[2m[36m(func pid=36467)[0m top5: 0.8129664179104478
[2m[36m(func pid=36467)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=36467)[0m f1_macro: 0.23923954850098567
[2m[36m(func pid=36467)[0m f1_weighted: 0.2532494449683491
[2m[36m(func pid=36467)[0m f1_per_class: [0.22, 0.484, 0.369, 0.428, 0.053, 0.096, 0.0, 0.459, 0.126, 0.158]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 3.0130 | Steps: 4 | Val loss: 2.4811 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4979 | Steps: 4 | Val loss: 2.3503 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.9356 | Steps: 4 | Val loss: 4.1873 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=37884)[0m top1: 0.016791044776119403
[2m[36m(func pid=37884)[0m top5: 0.6627798507462687
[2m[36m(func pid=37884)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=37884)[0m f1_macro: 0.056421361244544624
[2m[36m(func pid=37884)[0m f1_weighted: 0.004142187740306803
[2m[36m(func pid=37884)[0m f1_per_class: [0.041, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3491 | Steps: 4 | Val loss: 2.2266 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:29:23 (running for 00:36:15.80)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.472 |      0.239 |                   54 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.257 |      0.123 |                   55 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.498 |      0.155 |                   51 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.013 |      0.056 |                   51 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.13852611940298507
[2m[36m(func pid=37814)[0m top5: 0.7555970149253731
[2m[36m(func pid=37814)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=37814)[0m f1_macro: 0.15495728899352051
[2m[36m(func pid=37814)[0m f1_weighted: 0.10872179421209956
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.274, 0.444, 0.017, 0.049, 0.216, 0.0, 0.448, 0.102, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.1478544776119403
[2m[36m(func pid=36555)[0m top5: 0.7672574626865671
[2m[36m(func pid=36555)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=36555)[0m f1_macro: 0.1627881141938548
[2m[36m(func pid=36555)[0m f1_weighted: 0.12796203019016059
[2m[36m(func pid=36555)[0m f1_per_class: [0.211, 0.293, 0.455, 0.124, 0.06, 0.02, 0.048, 0.274, 0.065, 0.08]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2574626865671642
[2m[36m(func pid=36467)[0m top5: 0.832089552238806
[2m[36m(func pid=36467)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=36467)[0m f1_macro: 0.24053245269136242
[2m[36m(func pid=36467)[0m f1_weighted: 0.2564485388550306
[2m[36m(func pid=36467)[0m f1_per_class: [0.237, 0.442, 0.329, 0.446, 0.057, 0.146, 0.0, 0.434, 0.145, 0.169]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.9206 | Steps: 4 | Val loss: 2.3591 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6956 | Steps: 4 | Val loss: 2.3731 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6259 | Steps: 4 | Val loss: 3.2395 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=37884)[0m top1: 0.016791044776119403
[2m[36m(func pid=37884)[0m top5: 0.6194029850746269
[2m[36m(func pid=37884)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=37884)[0m f1_macro: 0.06960410176531671
[2m[36m(func pid=37884)[0m f1_weighted: 0.0059524890636381955
[2m[36m(func pid=37884)[0m f1_per_class: [0.111, 0.0, 0.562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4640 | Steps: 4 | Val loss: 2.2042 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 13:29:28 (running for 00:36:21.26)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.349 |      0.241 |                   55 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.936 |      0.163 |                   56 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.696 |      0.129 |                   52 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.921 |      0.07  |                   52 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.1501865671641791
[2m[36m(func pid=37814)[0m top5: 0.7700559701492538
[2m[36m(func pid=37814)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=37814)[0m f1_macro: 0.12938873423779468
[2m[36m(func pid=37814)[0m f1_weighted: 0.11376678685788516
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.29, 0.22, 0.055, 0.061, 0.193, 0.0, 0.361, 0.113, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.20848880597014927
[2m[36m(func pid=36555)[0m top5: 0.8260261194029851
[2m[36m(func pid=36555)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=36555)[0m f1_macro: 0.20243831034921073
[2m[36m(func pid=36555)[0m f1_weighted: 0.2071226381341855
[2m[36m(func pid=36555)[0m f1_per_class: [0.22, 0.28, 0.444, 0.14, 0.059, 0.16, 0.246, 0.314, 0.023, 0.137]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.2579291044776119
[2m[36m(func pid=36467)[0m top5: 0.8428171641791045
[2m[36m(func pid=36467)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=36467)[0m f1_macro: 0.25200300278499804
[2m[36m(func pid=36467)[0m f1_weighted: 0.2530429378088841
[2m[36m(func pid=36467)[0m f1_per_class: [0.247, 0.371, 0.512, 0.462, 0.061, 0.198, 0.0, 0.424, 0.073, 0.171]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7825 | Steps: 4 | Val loss: 2.6089 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.8525 | Steps: 4 | Val loss: 2.4521 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.4054 | Steps: 4 | Val loss: 2.5639 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.1413 | Steps: 4 | Val loss: 2.1428 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=37884)[0m top1: 0.017723880597014924
[2m[36m(func pid=37884)[0m top5: 0.6175373134328358
[2m[36m(func pid=37884)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=37884)[0m f1_macro: 0.06457077575889457
[2m[36m(func pid=37884)[0m f1_weighted: 0.00582143445810876
[2m[36m(func pid=37884)[0m f1_per_class: [0.123, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:29:34 (running for 00:36:26.66)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.464 |      0.252 |                   56 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.626 |      0.202 |                   57 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.853 |      0.137 |                   53 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.782 |      0.065 |                   53 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.17350746268656717
[2m[36m(func pid=37814)[0m top5: 0.7873134328358209
[2m[36m(func pid=37814)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=37814)[0m f1_macro: 0.13718928891947035
[2m[36m(func pid=37814)[0m f1_weighted: 0.15086806847817721
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.283, 0.157, 0.182, 0.077, 0.248, 0.0, 0.306, 0.119, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.3558768656716418
[2m[36m(func pid=36555)[0m top5: 0.8460820895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=36555)[0m f1_macro: 0.27494718643573324
[2m[36m(func pid=36555)[0m f1_weighted: 0.3603311883299387
[2m[36m(func pid=36555)[0m f1_per_class: [0.205, 0.419, 0.338, 0.185, 0.069, 0.33, 0.554, 0.41, 0.025, 0.213]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.28125
[2m[36m(func pid=36467)[0m top5: 0.8479477611940298
[2m[36m(func pid=36467)[0m f1_micro: 0.28125
[2m[36m(func pid=36467)[0m f1_macro: 0.27078411054819773
[2m[36m(func pid=36467)[0m f1_weighted: 0.2706953324549502
[2m[36m(func pid=36467)[0m f1_per_class: [0.278, 0.38, 0.571, 0.5, 0.067, 0.235, 0.0, 0.421, 0.096, 0.16]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.0884 | Steps: 4 | Val loss: 2.5120 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4672 | Steps: 4 | Val loss: 2.3762 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6177 | Steps: 4 | Val loss: 2.4582 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=37884)[0m top1: 0.302705223880597
[2m[36m(func pid=37884)[0m top5: 0.6189365671641791
[2m[36m(func pid=37884)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=37884)[0m f1_macro: 0.11777711439623703
[2m[36m(func pid=37884)[0m f1_weighted: 0.14517270708623176
[2m[36m(func pid=37884)[0m f1_per_class: [0.177, 0.0, 0.537, 0.0, 0.0, 0.0, 0.464, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2291 | Steps: 4 | Val loss: 2.1150 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37814)[0m top1: 0.21548507462686567
[2m[36m(func pid=37814)[0m top5: 0.7989738805970149
[2m[36m(func pid=37814)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=37814)[0m f1_macro: 0.16090652376953707
[2m[36m(func pid=37814)[0m f1_weighted: 0.20249064631784716
[2m[36m(func pid=37814)[0m f1_per_class: [0.068, 0.336, 0.164, 0.346, 0.074, 0.225, 0.0, 0.261, 0.137, 0.0]
== Status ==
Current time: 2024-01-07 13:29:39 (running for 00:36:32.07)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.141 |      0.271 |                   57 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.405 |      0.275 |                   58 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.467 |      0.161 |                   54 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.088 |      0.118 |                   54 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.373134328358209
[2m[36m(func pid=36555)[0m top5: 0.8815298507462687
[2m[36m(func pid=36555)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=36555)[0m f1_macro: 0.2922674886647759
[2m[36m(func pid=36555)[0m f1_weighted: 0.3630778798911324
[2m[36m(func pid=36555)[0m f1_per_class: [0.197, 0.462, 0.69, 0.252, 0.085, 0.361, 0.527, 0.0, 0.098, 0.252]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.29990671641791045
[2m[36m(func pid=36467)[0m top5: 0.8586753731343284
[2m[36m(func pid=36467)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=36467)[0m f1_macro: 0.27954509989013043
[2m[36m(func pid=36467)[0m f1_weighted: 0.28627345969025025
[2m[36m(func pid=36467)[0m f1_per_class: [0.304, 0.38, 0.564, 0.551, 0.068, 0.231, 0.0, 0.457, 0.07, 0.171]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.0402 | Steps: 4 | Val loss: 2.5244 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3259 | Steps: 4 | Val loss: 2.2825 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3968 | Steps: 4 | Val loss: 2.2443 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=37884)[0m top1: 0.302705223880597
[2m[36m(func pid=37884)[0m top5: 0.46361940298507465
[2m[36m(func pid=37884)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=37884)[0m f1_macro: 0.11224906428804826
[2m[36m(func pid=37884)[0m f1_weighted: 0.14484084682874812
[2m[36m(func pid=37884)[0m f1_per_class: [0.146, 0.0, 0.511, 0.0, 0.0, 0.0, 0.466, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9459 | Steps: 4 | Val loss: 1.9866 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:29:45 (running for 00:36:37.50)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.229 |      0.28  |                   58 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.618 |      0.292 |                   59 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.326 |      0.193 |                   55 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.04  |      0.112 |                   55 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37814)[0m top1: 0.2667910447761194
[2m[36m(func pid=37814)[0m top5: 0.8180970149253731
[2m[36m(func pid=37814)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=37814)[0m f1_macro: 0.1931955532551634
[2m[36m(func pid=37814)[0m f1_weighted: 0.2364291929782489
[2m[36m(func pid=37814)[0m f1_per_class: [0.056, 0.451, 0.286, 0.386, 0.091, 0.234, 0.0, 0.265, 0.163, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36555)[0m top1: 0.38899253731343286
[2m[36m(func pid=36555)[0m top5: 0.9076492537313433
[2m[36m(func pid=36555)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=36555)[0m f1_macro: 0.31954133776062704
[2m[36m(func pid=36555)[0m f1_weighted: 0.38683639178508583
[2m[36m(func pid=36555)[0m f1_per_class: [0.254, 0.465, 0.762, 0.325, 0.098, 0.392, 0.515, 0.0, 0.14, 0.246]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3260261194029851
[2m[36m(func pid=36467)[0m top5: 0.8661380597014925
[2m[36m(func pid=36467)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=36467)[0m f1_macro: 0.3021495727925086
[2m[36m(func pid=36467)[0m f1_weighted: 0.31225146114596264
[2m[36m(func pid=36467)[0m f1_per_class: [0.302, 0.362, 0.649, 0.559, 0.085, 0.3, 0.063, 0.431, 0.097, 0.173]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.2642 | Steps: 4 | Val loss: 2.5116 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3739 | Steps: 4 | Val loss: 2.3961 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.2204 | Steps: 4 | Val loss: 2.3194 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=37884)[0m top1: 0.3050373134328358
[2m[36m(func pid=37884)[0m top5: 0.6226679104477612
[2m[36m(func pid=37884)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=37884)[0m f1_macro: 0.12316281137155935
[2m[36m(func pid=37884)[0m f1_weighted: 0.1475985314196696
[2m[36m(func pid=37884)[0m f1_per_class: [0.19, 0.0, 0.571, 0.0, 0.0, 0.0, 0.471, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5318 | Steps: 4 | Val loss: 1.9731 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:29:50 (running for 00:36:42.91)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.946 |      0.302 |                   59 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.374 |      0.304 |                   61 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.326 |      0.193 |                   55 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.264 |      0.123 |                   56 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.3572761194029851
[2m[36m(func pid=36555)[0m top5: 0.9071828358208955
[2m[36m(func pid=36555)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=36555)[0m f1_macro: 0.30444434669613074
[2m[36m(func pid=36555)[0m f1_weighted: 0.3646101261151777
[2m[36m(func pid=36555)[0m f1_per_class: [0.273, 0.459, 0.818, 0.416, 0.106, 0.345, 0.382, 0.0, 0.097, 0.148]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.26725746268656714
[2m[36m(func pid=37814)[0m top5: 0.8180970149253731
[2m[36m(func pid=37814)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=37814)[0m f1_macro: 0.18706041020135444
[2m[36m(func pid=37814)[0m f1_weighted: 0.21460177363870808
[2m[36m(func pid=37814)[0m f1_per_class: [0.036, 0.488, 0.364, 0.309, 0.084, 0.185, 0.0, 0.259, 0.146, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.34328358208955223
[2m[36m(func pid=36467)[0m top5: 0.8642723880597015
[2m[36m(func pid=36467)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=36467)[0m f1_macro: 0.33730829289584785
[2m[36m(func pid=36467)[0m f1_weighted: 0.3420938743981492
[2m[36m(func pid=36467)[0m f1_per_class: [0.37, 0.439, 0.774, 0.543, 0.087, 0.277, 0.128, 0.455, 0.121, 0.18]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7195 | Steps: 4 | Val loss: 2.3153 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1479 | Steps: 4 | Val loss: 2.3267 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1475 | Steps: 4 | Val loss: 2.3061 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=37884)[0m top1: 0.302705223880597
[2m[36m(func pid=37884)[0m top5: 0.6180037313432836
[2m[36m(func pid=37884)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=37884)[0m f1_macro: 0.12415721088807578
[2m[36m(func pid=37884)[0m f1_weighted: 0.14719496107373953
[2m[36m(func pid=37884)[0m f1_per_class: [0.178, 0.0, 0.571, 0.0, 0.0, 0.0, 0.467, 0.0, 0.025, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2817 | Steps: 4 | Val loss: 2.0409 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 13:29:55 (running for 00:36:48.16)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.532 |      0.337 |                   60 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.148 |      0.306 |                   62 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.22  |      0.187 |                   56 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.72  |      0.124 |                   57 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.3498134328358209
[2m[36m(func pid=36555)[0m top5: 0.909981343283582
[2m[36m(func pid=36555)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=36555)[0m f1_macro: 0.30600477955189503
[2m[36m(func pid=36555)[0m f1_weighted: 0.35925123911683915
[2m[36m(func pid=36555)[0m f1_per_class: [0.222, 0.464, 0.471, 0.407, 0.102, 0.345, 0.308, 0.318, 0.155, 0.269]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.25652985074626866
[2m[36m(func pid=37814)[0m top5: 0.8264925373134329
[2m[36m(func pid=37814)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=37814)[0m f1_macro: 0.16123513136155546
[2m[36m(func pid=37814)[0m f1_weighted: 0.18692653146232519
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.493, 0.375, 0.256, 0.044, 0.088, 0.0, 0.257, 0.1, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3400186567164179
[2m[36m(func pid=36467)[0m top5: 0.8605410447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=36467)[0m f1_macro: 0.3325405326739893
[2m[36m(func pid=36467)[0m f1_weighted: 0.34985823500622637
[2m[36m(func pid=36467)[0m f1_per_class: [0.4, 0.399, 0.733, 0.553, 0.093, 0.256, 0.179, 0.438, 0.125, 0.15]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.1880 | Steps: 4 | Val loss: 2.3993 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7489 | Steps: 4 | Val loss: 2.5408 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.9944 | Steps: 4 | Val loss: 2.2837 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37884)[0m top1: 0.12220149253731344
[2m[36m(func pid=37884)[0m top5: 0.6371268656716418
[2m[36m(func pid=37884)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=37884)[0m f1_macro: 0.09720961122032025
[2m[36m(func pid=37884)[0m f1_weighted: 0.03640357728290463
[2m[36m(func pid=37884)[0m f1_per_class: [0.135, 0.026, 0.552, 0.0, 0.0, 0.213, 0.0, 0.0, 0.047, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.4955 | Steps: 4 | Val loss: 1.9110 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 13:30:01 (running for 00:36:53.44)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.282 |      0.333 |                   61 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.749 |      0.267 |                   63 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.147 |      0.161 |                   57 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.188 |      0.097 |                   58 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.30736940298507465
[2m[36m(func pid=36555)[0m top5: 0.8931902985074627
[2m[36m(func pid=36555)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=36555)[0m f1_macro: 0.26701760552814696
[2m[36m(func pid=36555)[0m f1_weighted: 0.29099031475971476
[2m[36m(func pid=36555)[0m f1_per_class: [0.222, 0.465, 0.143, 0.353, 0.071, 0.32, 0.104, 0.525, 0.144, 0.323]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.2653917910447761
[2m[36m(func pid=37814)[0m top5: 0.8069029850746269
[2m[36m(func pid=37814)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=37814)[0m f1_macro: 0.14209668241100876
[2m[36m(func pid=37814)[0m f1_weighted: 0.18993684007819941
[2m[36m(func pid=37814)[0m f1_per_class: [0.032, 0.501, 0.143, 0.282, 0.028, 0.024, 0.0, 0.279, 0.131, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.9578 | Steps: 4 | Val loss: 2.3475 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=36467)[0m top1: 0.3694029850746269
[2m[36m(func pid=36467)[0m top5: 0.8759328358208955
[2m[36m(func pid=36467)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=36467)[0m f1_macro: 0.356536151545872
[2m[36m(func pid=36467)[0m f1_weighted: 0.3899793711091166
[2m[36m(func pid=36467)[0m f1_per_class: [0.438, 0.431, 0.71, 0.554, 0.09, 0.29, 0.279, 0.412, 0.142, 0.221]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2315 | Steps: 4 | Val loss: 2.7404 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.9963 | Steps: 4 | Val loss: 2.1412 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=37884)[0m top1: 0.17583955223880596
[2m[36m(func pid=37884)[0m top5: 0.6399253731343284
[2m[36m(func pid=37884)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=37884)[0m f1_macro: 0.10626235922188751
[2m[36m(func pid=37884)[0m f1_weighted: 0.05856584148940654
[2m[36m(func pid=37884)[0m f1_per_class: [0.105, 0.297, 0.611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3341 | Steps: 4 | Val loss: 1.9489 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:30:06 (running for 00:36:58.91)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.496 |      0.357 |                   62 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.232 |      0.236 |                   64 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.994 |      0.142 |                   58 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.958 |      0.106 |                   59 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.271455223880597
[2m[36m(func pid=36555)[0m top5: 0.8894589552238806
[2m[36m(func pid=36555)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=36555)[0m f1_macro: 0.23606845743300192
[2m[36m(func pid=36555)[0m f1_weighted: 0.24555827451078296
[2m[36m(func pid=36555)[0m f1_per_class: [0.194, 0.443, 0.0, 0.331, 0.071, 0.269, 0.022, 0.411, 0.198, 0.421]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.29850746268656714
[2m[36m(func pid=37814)[0m top5: 0.8017723880597015
[2m[36m(func pid=37814)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=37814)[0m f1_macro: 0.1626805342774823
[2m[36m(func pid=37814)[0m f1_weighted: 0.24285662138597
[2m[36m(func pid=37814)[0m f1_per_class: [0.0, 0.519, 0.143, 0.439, 0.019, 0.069, 0.0, 0.312, 0.126, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.8812 | Steps: 4 | Val loss: 2.3300 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=36467)[0m top1: 0.3666044776119403
[2m[36m(func pid=36467)[0m top5: 0.8759328358208955
[2m[36m(func pid=36467)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.3570637360219853
[2m[36m(func pid=36467)[0m f1_weighted: 0.395534982490533
[2m[36m(func pid=36467)[0m f1_per_class: [0.44, 0.455, 0.733, 0.542, 0.095, 0.277, 0.303, 0.409, 0.117, 0.2]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.1498 | Steps: 4 | Val loss: 2.9517 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.4276 | Steps: 4 | Val loss: 2.0709 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=37884)[0m top1: 0.17583955223880596
[2m[36m(func pid=37884)[0m top5: 0.6431902985074627
[2m[36m(func pid=37884)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=37884)[0m f1_macro: 0.10630204067704067
[2m[36m(func pid=37884)[0m f1_weighted: 0.05779744614865883
[2m[36m(func pid=37884)[0m f1_per_class: [0.156, 0.296, 0.611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.2249 | Steps: 4 | Val loss: 1.8204 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:30:11 (running for 00:37:04.19)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.334 |      0.357 |                   63 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.15  |      0.208 |                   65 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.996 |      0.163 |                   59 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.881 |      0.106 |                   60 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2579291044776119
[2m[36m(func pid=36555)[0m top5: 0.8563432835820896
[2m[36m(func pid=36555)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=36555)[0m f1_macro: 0.20764795466849817
[2m[36m(func pid=36555)[0m f1_weighted: 0.2248936385734805
[2m[36m(func pid=36555)[0m f1_per_class: [0.214, 0.47, 0.095, 0.281, 0.057, 0.19, 0.031, 0.384, 0.147, 0.208]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.31763059701492535
[2m[36m(func pid=37814)[0m top5: 0.8050373134328358
[2m[36m(func pid=37814)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=37814)[0m f1_macro: 0.21941651382332478
[2m[36m(func pid=37814)[0m f1_weighted: 0.2884614697525521
[2m[36m(func pid=37814)[0m f1_per_class: [0.147, 0.495, 0.267, 0.535, 0.031, 0.203, 0.0, 0.377, 0.126, 0.015]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.1979 | Steps: 4 | Val loss: 2.5237 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36467)[0m top1: 0.40111940298507465
[2m[36m(func pid=36467)[0m top5: 0.8833955223880597
[2m[36m(func pid=36467)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=36467)[0m f1_macro: 0.3812309386896878
[2m[36m(func pid=36467)[0m f1_weighted: 0.43436641355149685
[2m[36m(func pid=36467)[0m f1_per_class: [0.434, 0.483, 0.727, 0.517, 0.106, 0.313, 0.415, 0.457, 0.13, 0.23]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7801 | Steps: 4 | Val loss: 2.9717 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.0873 | Steps: 4 | Val loss: 2.0642 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=37884)[0m top1: 0.1730410447761194
[2m[36m(func pid=37884)[0m top5: 0.6389925373134329
[2m[36m(func pid=37884)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=37884)[0m f1_macro: 0.10316226334357279
[2m[36m(func pid=37884)[0m f1_weighted: 0.05891990535001557
[2m[36m(func pid=37884)[0m f1_per_class: [0.169, 0.293, 0.512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.1085 | Steps: 4 | Val loss: 1.7097 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 13:30:17 (running for 00:37:09.56)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.225 |      0.381 |                   64 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.78  |      0.268 |                   66 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  3.428 |      0.219 |                   60 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.198 |      0.103 |                   61 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.27098880597014924
[2m[36m(func pid=36555)[0m top5: 0.8493470149253731
[2m[36m(func pid=36555)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=36555)[0m f1_macro: 0.2678668613001313
[2m[36m(func pid=36555)[0m f1_weighted: 0.23991592852956
[2m[36m(func pid=36555)[0m f1_per_class: [0.234, 0.485, 0.571, 0.246, 0.059, 0.266, 0.065, 0.363, 0.175, 0.213]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.29990671641791045
[2m[36m(func pid=37814)[0m top5: 0.8218283582089553
[2m[36m(func pid=37814)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=37814)[0m f1_macro: 0.23934724080362607
[2m[36m(func pid=37814)[0m f1_weighted: 0.2976388733659801
[2m[36m(func pid=37814)[0m f1_per_class: [0.134, 0.349, 0.444, 0.523, 0.06, 0.227, 0.109, 0.413, 0.088, 0.046]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.2177 | Steps: 4 | Val loss: 2.6095 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=36467)[0m top1: 0.4230410447761194
[2m[36m(func pid=36467)[0m top5: 0.8885261194029851
[2m[36m(func pid=36467)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=36467)[0m f1_macro: 0.40448336207341284
[2m[36m(func pid=36467)[0m f1_weighted: 0.4505548843816551
[2m[36m(func pid=36467)[0m f1_per_class: [0.442, 0.516, 0.667, 0.485, 0.101, 0.351, 0.449, 0.482, 0.202, 0.35]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.1959 | Steps: 4 | Val loss: 2.9861 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3321 | Steps: 4 | Val loss: 2.0650 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=37884)[0m top1: 0.039645522388059705
[2m[36m(func pid=37884)[0m top5: 0.5802238805970149
[2m[36m(func pid=37884)[0m f1_micro: 0.039645522388059705
[2m[36m(func pid=37884)[0m f1_macro: 0.07932396524522348
[2m[36m(func pid=37884)[0m f1_weighted: 0.009304134377143879
[2m[36m(func pid=37884)[0m f1_per_class: [0.194, 0.0, 0.537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6109 | Steps: 4 | Val loss: 1.7562 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:30:22 (running for 00:37:15.05)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.108 |      0.404 |                   65 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.196 |      0.289 |                   67 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.087 |      0.239 |                   61 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.218 |      0.079 |                   62 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2971082089552239
[2m[36m(func pid=36555)[0m top5: 0.8409514925373134
[2m[36m(func pid=36555)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=36555)[0m f1_macro: 0.2886356917918234
[2m[36m(func pid=36555)[0m f1_weighted: 0.28335845021574096
[2m[36m(func pid=36555)[0m f1_per_class: [0.237, 0.482, 0.407, 0.271, 0.076, 0.33, 0.153, 0.424, 0.145, 0.361]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.28125
[2m[36m(func pid=37814)[0m top5: 0.8390858208955224
[2m[36m(func pid=37814)[0m f1_micro: 0.28125
[2m[36m(func pid=37814)[0m f1_macro: 0.2254595432037579
[2m[36m(func pid=37814)[0m f1_weighted: 0.30784064204575384
[2m[36m(func pid=37814)[0m f1_per_class: [0.162, 0.152, 0.261, 0.455, 0.069, 0.24, 0.308, 0.462, 0.079, 0.066]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.4099813432835821
[2m[36m(func pid=36467)[0m top5: 0.886660447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=36467)[0m f1_macro: 0.37872128911387926
[2m[36m(func pid=36467)[0m f1_weighted: 0.4316965931786874
[2m[36m(func pid=36467)[0m f1_per_class: [0.405, 0.55, 0.533, 0.429, 0.083, 0.33, 0.433, 0.477, 0.219, 0.33]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.7781 | Steps: 4 | Val loss: 2.5117 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4803 | Steps: 4 | Val loss: 3.3054 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0560 | Steps: 4 | Val loss: 2.1258 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=37884)[0m top1: 0.06576492537313433
[2m[36m(func pid=37884)[0m top5: 0.2966417910447761
[2m[36m(func pid=37884)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=37884)[0m f1_macro: 0.0985750745454997
[2m[36m(func pid=37884)[0m f1_weighted: 0.014927310964310578
[2m[36m(func pid=37884)[0m f1_per_class: [0.077, 0.0, 0.643, 0.0, 0.0, 0.0, 0.0, 0.114, 0.051, 0.101]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.9696 | Steps: 4 | Val loss: 1.8011 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:30:28 (running for 00:37:20.58)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.611 |      0.379 |                   66 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.48  |      0.312 |                   68 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.332 |      0.225 |                   62 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.778 |      0.099 |                   63 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.3162313432835821
[2m[36m(func pid=36555)[0m top5: 0.8348880597014925
[2m[36m(func pid=36555)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=36555)[0m f1_macro: 0.3118101521861577
[2m[36m(func pid=36555)[0m f1_weighted: 0.33482431092761394
[2m[36m(func pid=36555)[0m f1_per_class: [0.222, 0.453, 0.349, 0.294, 0.072, 0.335, 0.303, 0.503, 0.142, 0.444]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.28451492537313433
[2m[36m(func pid=37814)[0m top5: 0.8152985074626866
[2m[36m(func pid=37814)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=37814)[0m f1_macro: 0.22185056349370819
[2m[36m(func pid=37814)[0m f1_weighted: 0.3263581031610931
[2m[36m(func pid=37814)[0m f1_per_class: [0.141, 0.051, 0.205, 0.347, 0.06, 0.283, 0.51, 0.505, 0.054, 0.062]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.4048507462686567
[2m[36m(func pid=36467)[0m top5: 0.8852611940298507
[2m[36m(func pid=36467)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=36467)[0m f1_macro: 0.36385022446170456
[2m[36m(func pid=36467)[0m f1_weighted: 0.42801082722886247
[2m[36m(func pid=36467)[0m f1_per_class: [0.379, 0.561, 0.407, 0.425, 0.091, 0.325, 0.422, 0.494, 0.203, 0.333]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.7707 | Steps: 4 | Val loss: 2.6101 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.2152 | Steps: 4 | Val loss: 3.8548 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2068 | Steps: 4 | Val loss: 2.1594 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=37884)[0m top1: 0.06763059701492537
[2m[36m(func pid=37884)[0m top5: 0.42723880597014924
[2m[36m(func pid=37884)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=37884)[0m f1_macro: 0.11300084747703795
[2m[36m(func pid=37884)[0m f1_weighted: 0.018033243047457688
[2m[36m(func pid=37884)[0m f1_per_class: [0.214, 0.0, 0.643, 0.0, 0.0, 0.0, 0.0, 0.114, 0.06, 0.099]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.2694 | Steps: 4 | Val loss: 1.8591 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:30:33 (running for 00:37:26.02)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.97  |      0.364 |                   67 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.215 |      0.224 |                   69 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.056 |      0.222 |                   63 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.771 |      0.113 |                   64 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2635261194029851
[2m[36m(func pid=36555)[0m top5: 0.8003731343283582
[2m[36m(func pid=36555)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=36555)[0m f1_macro: 0.22441663074853654
[2m[36m(func pid=36555)[0m f1_weighted: 0.2543852851570131
[2m[36m(func pid=36555)[0m f1_per_class: [0.227, 0.408, 0.319, 0.354, 0.077, 0.311, 0.115, 0.016, 0.136, 0.283]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.25419776119402987
[2m[36m(func pid=37814)[0m top5: 0.7714552238805971
[2m[36m(func pid=37814)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=37814)[0m f1_macro: 0.15639625818611447
[2m[36m(func pid=37814)[0m f1_weighted: 0.2639616194831002
[2m[36m(func pid=37814)[0m f1_per_class: [0.108, 0.021, 0.137, 0.188, 0.084, 0.244, 0.564, 0.092, 0.062, 0.063]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3931902985074627
[2m[36m(func pid=36467)[0m top5: 0.8843283582089553
[2m[36m(func pid=36467)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=36467)[0m f1_macro: 0.3373875531210931
[2m[36m(func pid=36467)[0m f1_weighted: 0.41735549313088327
[2m[36m(func pid=36467)[0m f1_per_class: [0.342, 0.563, 0.273, 0.452, 0.088, 0.31, 0.377, 0.473, 0.19, 0.306]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.7146 | Steps: 4 | Val loss: 2.7259 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.9410 | Steps: 4 | Val loss: 4.2819 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.8109 | Steps: 4 | Val loss: 2.1674 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=37884)[0m top1: 0.01585820895522388
[2m[36m(func pid=37884)[0m top5: 0.43236940298507465
[2m[36m(func pid=37884)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=37884)[0m f1_macro: 0.07902653201652367
[2m[36m(func pid=37884)[0m f1_weighted: 0.008269655187390075
[2m[36m(func pid=37884)[0m f1_per_class: [0.12, 0.0, 0.562, 0.0, 0.016, 0.0, 0.0, 0.0, 0.056, 0.035]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1272 | Steps: 4 | Val loss: 1.9297 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:30:39 (running for 00:37:31.50)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.269 |      0.337 |                   68 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.941 |      0.197 |                   70 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.207 |      0.156 |                   64 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.715 |      0.079 |                   65 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.23227611940298507
[2m[36m(func pid=36555)[0m top5: 0.7630597014925373
[2m[36m(func pid=36555)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=36555)[0m f1_macro: 0.19728412705372386
[2m[36m(func pid=36555)[0m f1_weighted: 0.23446924107661507
[2m[36m(func pid=36555)[0m f1_per_class: [0.235, 0.35, 0.31, 0.352, 0.093, 0.252, 0.114, 0.0, 0.123, 0.143]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=37814)[0m top1: 0.24720149253731344
[2m[36m(func pid=37814)[0m top5: 0.7593283582089553
[2m[36m(func pid=37814)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=37814)[0m f1_macro: 0.1474523357081
[2m[36m(func pid=37814)[0m f1_weighted: 0.2382269695586461
[2m[36m(func pid=37814)[0m f1_per_class: [0.106, 0.026, 0.239, 0.107, 0.065, 0.229, 0.574, 0.0, 0.053, 0.076]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.37826492537313433
[2m[36m(func pid=36467)[0m top5: 0.8796641791044776
[2m[36m(func pid=36467)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=36467)[0m f1_macro: 0.32726865464249866
[2m[36m(func pid=36467)[0m f1_weighted: 0.4084746447516212
[2m[36m(func pid=36467)[0m f1_per_class: [0.336, 0.553, 0.186, 0.461, 0.071, 0.293, 0.351, 0.459, 0.234, 0.33]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.0858 | Steps: 4 | Val loss: 2.4781 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.8975 | Steps: 4 | Val loss: 4.4474 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6830 | Steps: 4 | Val loss: 2.1911 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=37884)[0m top1: 0.017257462686567165
[2m[36m(func pid=37884)[0m top5: 0.659981343283582
[2m[36m(func pid=37884)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=37884)[0m f1_macro: 0.09240043290043289
[2m[36m(func pid=37884)[0m f1_weighted: 0.011032893567874909
[2m[36m(func pid=37884)[0m f1_per_class: [0.234, 0.0, 0.6, 0.0, 0.015, 0.0, 0.0, 0.0, 0.075, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:30:44 (running for 00:37:36.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.127 |      0.327 |                   69 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.897 |      0.193 |                   71 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.811 |      0.147 |                   65 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.086 |      0.092 |                   66 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.2234141791044776
[2m[36m(func pid=36555)[0m top5: 0.7574626865671642
[2m[36m(func pid=36555)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=36555)[0m f1_macro: 0.19261078784754943
[2m[36m(func pid=36555)[0m f1_weighted: 0.2382932290255936
[2m[36m(func pid=36555)[0m f1_per_class: [0.254, 0.284, 0.306, 0.372, 0.093, 0.227, 0.156, 0.0, 0.133, 0.102]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.3930 | Steps: 4 | Val loss: 2.1613 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=37814)[0m top1: 0.2490671641791045
[2m[36m(func pid=37814)[0m top5: 0.7602611940298507
[2m[36m(func pid=37814)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=37814)[0m f1_macro: 0.14936511056943563
[2m[36m(func pid=37814)[0m f1_weighted: 0.23415033400839558
[2m[36m(func pid=37814)[0m f1_per_class: [0.096, 0.026, 0.291, 0.083, 0.063, 0.231, 0.585, 0.0, 0.0, 0.118]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 3.0124 | Steps: 4 | Val loss: 2.5755 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=36467)[0m top1: 0.33255597014925375
[2m[36m(func pid=36467)[0m top5: 0.8638059701492538
[2m[36m(func pid=36467)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=36467)[0m f1_macro: 0.27979192283384746
[2m[36m(func pid=36467)[0m f1_weighted: 0.3611729452250361
[2m[36m(func pid=36467)[0m f1_per_class: [0.308, 0.526, 0.151, 0.473, 0.075, 0.234, 0.25, 0.354, 0.195, 0.232]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1395 | Steps: 4 | Val loss: 4.3240 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6395 | Steps: 4 | Val loss: 2.1940 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=37884)[0m top1: 0.01632462686567164
[2m[36m(func pid=37884)[0m top5: 0.6646455223880597
[2m[36m(func pid=37884)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=37884)[0m f1_macro: 0.0594582950794344
[2m[36m(func pid=37884)[0m f1_weighted: 0.005803482598151157
[2m[36m(func pid=37884)[0m f1_per_class: [0.151, 0.0, 0.429, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:30:49 (running for 00:37:41.94)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.393 |      0.28  |                   70 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.14  |      0.201 |                   72 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.683 |      0.149 |                   66 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.012 |      0.059 |                   67 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36555)[0m top1: 0.228544776119403
[2m[36m(func pid=36555)[0m top5: 0.7681902985074627
[2m[36m(func pid=36555)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=36555)[0m f1_macro: 0.20140891862681057
[2m[36m(func pid=36555)[0m f1_weighted: 0.24966059145185676
[2m[36m(func pid=36555)[0m f1_per_class: [0.262, 0.25, 0.333, 0.359, 0.105, 0.216, 0.224, 0.0, 0.168, 0.097]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.1368 | Steps: 4 | Val loss: 2.1043 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=37814)[0m top1: 0.2439365671641791
[2m[36m(func pid=37814)[0m top5: 0.761660447761194
[2m[36m(func pid=37814)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=37814)[0m f1_macro: 0.18985664922696693
[2m[36m(func pid=37814)[0m f1_weighted: 0.2550301786963145
[2m[36m(func pid=37814)[0m f1_per_class: [0.093, 0.056, 0.552, 0.142, 0.052, 0.255, 0.565, 0.0, 0.028, 0.157]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 4.9935 | Steps: 4 | Val loss: 2.5293 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=36467)[0m top1: 0.3474813432835821
[2m[36m(func pid=36467)[0m top5: 0.8731343283582089
[2m[36m(func pid=36467)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=36467)[0m f1_macro: 0.29853274761600707
[2m[36m(func pid=36467)[0m f1_weighted: 0.38120621428442053
[2m[36m(func pid=36467)[0m f1_per_class: [0.304, 0.531, 0.141, 0.458, 0.061, 0.257, 0.301, 0.431, 0.21, 0.291]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4372 | Steps: 4 | Val loss: 3.2031 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0641 | Steps: 4 | Val loss: 2.1654 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 13:30:54 (running for 00:37:47.08)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.137 |      0.299 |                   71 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.14  |      0.201 |                   72 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.639 |      0.19  |                   67 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  4.993 |      0.073 |                   68 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.01912313432835821
[2m[36m(func pid=37884)[0m top5: 0.7667910447761194
[2m[36m(func pid=37884)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=37884)[0m f1_macro: 0.07263245723821443
[2m[36m(func pid=37884)[0m f1_weighted: 0.0073177603845603015
[2m[36m(func pid=37884)[0m f1_per_class: [0.2, 0.0, 0.511, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m top1: 0.3003731343283582
[2m[36m(func pid=36555)[0m top5: 0.8367537313432836
[2m[36m(func pid=36555)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=36555)[0m f1_macro: 0.28020030355194475
[2m[36m(func pid=36555)[0m f1_weighted: 0.33319876926841197
[2m[36m(func pid=36555)[0m f1_per_class: [0.253, 0.283, 0.537, 0.391, 0.082, 0.301, 0.376, 0.194, 0.18, 0.204]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.1365 | Steps: 4 | Val loss: 2.0800 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37814)[0m top1: 0.23180970149253732
[2m[36m(func pid=37814)[0m top5: 0.7607276119402985
[2m[36m(func pid=37814)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=37814)[0m f1_macro: 0.17898079544428258
[2m[36m(func pid=37814)[0m f1_weighted: 0.24866995198496553
[2m[36m(func pid=37814)[0m f1_per_class: [0.1, 0.063, 0.632, 0.142, 0.051, 0.257, 0.545, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.9641 | Steps: 4 | Val loss: 2.2332 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0380 | Steps: 4 | Val loss: 2.9053 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=36467)[0m top1: 0.341884328358209
[2m[36m(func pid=36467)[0m top5: 0.8763992537313433
[2m[36m(func pid=36467)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=36467)[0m f1_macro: 0.29932825373988325
[2m[36m(func pid=36467)[0m f1_weighted: 0.37884016558507183
[2m[36m(func pid=36467)[0m f1_per_class: [0.268, 0.506, 0.151, 0.415, 0.066, 0.29, 0.333, 0.457, 0.204, 0.303]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1723 | Steps: 4 | Val loss: 2.0971 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:31:00 (running for 00:37:52.46)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.29774999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.137 |      0.299 |                   72 |
| train_52b21_00017 | RUNNING    | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  1.437 |      0.28  |                   73 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.064 |      0.179 |                   68 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.964 |      0.084 |                   69 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.017257462686567165
[2m[36m(func pid=37884)[0m top5: 0.7677238805970149
[2m[36m(func pid=37884)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=37884)[0m f1_macro: 0.0844014789917041
[2m[36m(func pid=37884)[0m f1_weighted: 0.008560574442437822
[2m[36m(func pid=37884)[0m f1_per_class: [0.186, 0.0, 0.615, 0.0, 0.016, 0.0, 0.0, 0.0, 0.027, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36555)[0m top1: 0.3162313432835821
[2m[36m(func pid=36555)[0m top5: 0.8460820895522388
[2m[36m(func pid=36555)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=36555)[0m f1_macro: 0.3311458047079748
[2m[36m(func pid=36555)[0m f1_weighted: 0.3368469986214787
[2m[36m(func pid=36555)[0m f1_per_class: [0.178, 0.379, 0.8, 0.425, 0.074, 0.302, 0.26, 0.383, 0.183, 0.328]
[2m[36m(func pid=36555)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9066 | Steps: 4 | Val loss: 1.9660 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=37814)[0m top1: 0.2490671641791045
[2m[36m(func pid=37814)[0m top5: 0.7709888059701493
[2m[36m(func pid=37814)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=37814)[0m f1_macro: 0.21687054076185358
[2m[36m(func pid=37814)[0m f1_weighted: 0.2856071763087218
[2m[36m(func pid=37814)[0m f1_per_class: [0.103, 0.119, 0.632, 0.18, 0.054, 0.266, 0.545, 0.27, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.0000 | Steps: 4 | Val loss: 2.1280 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=36467)[0m top1: 0.3656716417910448
[2m[36m(func pid=36467)[0m top5: 0.8857276119402985
[2m[36m(func pid=36467)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=36467)[0m f1_macro: 0.32039725161526716
[2m[36m(func pid=36467)[0m f1_weighted: 0.40496747526652754
[2m[36m(func pid=36467)[0m f1_per_class: [0.29, 0.476, 0.224, 0.449, 0.072, 0.326, 0.389, 0.447, 0.211, 0.319]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=36555)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5615 | Steps: 4 | Val loss: 3.2003 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.4194 | Steps: 4 | Val loss: 1.9949 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=36555)[0m top1: 0.2653917910447761
[2m[36m(func pid=36555)[0m top5: 0.8386194029850746
[2m[36m(func pid=36555)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=36555)[0m f1_macro: 0.26735496446849344
[2m[36m(func pid=36555)[0m f1_weighted: 0.24660083675988276
[2m[36m(func pid=36555)[0m f1_per_class: [0.101, 0.452, 0.783, 0.405, 0.046, 0.053, 0.05, 0.318, 0.174, 0.291]
== Status ==
Current time: 2024-01-07 13:31:05 (running for 00:37:57.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.295
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.907 |      0.32  |                   73 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.172 |      0.217 |                   69 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.964 |      0.084 |                   69 |
| train_52b21_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.30550373134328357
[2m[36m(func pid=37884)[0m top5: 0.7667910447761194
[2m[36m(func pid=37884)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=37884)[0m f1_macro: 0.13502489101737583
[2m[36m(func pid=37884)[0m f1_weighted: 0.15411807970645944
[2m[36m(func pid=37884)[0m f1_per_class: [0.192, 0.0, 0.667, 0.02, 0.0, 0.0, 0.472, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.4363 | Steps: 4 | Val loss: 1.9434 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=37814)[0m top1: 0.28125
[2m[36m(func pid=37814)[0m top5: 0.7854477611940298
[2m[36m(func pid=37814)[0m f1_micro: 0.28125
[2m[36m(func pid=37814)[0m f1_macro: 0.26264509207230213
[2m[36m(func pid=37814)[0m f1_weighted: 0.3337577351181657
[2m[36m(func pid=37814)[0m f1_per_class: [0.109, 0.258, 0.632, 0.256, 0.046, 0.294, 0.493, 0.54, 0.0, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.37966417910447764
[2m[36m(func pid=36467)[0m top5: 0.8913246268656716
[2m[36m(func pid=36467)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=36467)[0m f1_macro: 0.3359899371612681
[2m[36m(func pid=36467)[0m f1_weighted: 0.4173843659377862
[2m[36m(func pid=36467)[0m f1_per_class: [0.281, 0.498, 0.3, 0.469, 0.07, 0.341, 0.39, 0.464, 0.207, 0.34]
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.8982 | Steps: 4 | Val loss: 2.0783 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1888 | Steps: 4 | Val loss: 1.9477 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=37884)[0m top1: 0.12546641791044777
[2m[36m(func pid=37884)[0m top5: 0.9263059701492538
[2m[36m(func pid=37884)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=37884)[0m f1_macro: 0.10580921508793788
[2m[36m(func pid=37884)[0m f1_weighted: 0.039407791270032265
[2m[36m(func pid=37884)[0m f1_per_class: [0.222, 0.0, 0.6, 0.026, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3068 | Steps: 4 | Val loss: 2.1940 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=37814)[0m top1: 0.26725746268656714
[2m[36m(func pid=37814)[0m top5: 0.7905783582089553
[2m[36m(func pid=37814)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=37814)[0m f1_macro: 0.25297150141926134
[2m[36m(func pid=37814)[0m f1_weighted: 0.3000037018792538
[2m[36m(func pid=37814)[0m f1_per_class: [0.111, 0.426, 0.632, 0.223, 0.04, 0.301, 0.322, 0.474, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:31:11 (running for 00:38:03.54)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.295
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.436 |      0.336 |                   74 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.419 |      0.263 |                   70 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.898 |      0.106 |                   71 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.3451492537313433
[2m[36m(func pid=36467)[0m top5: 0.8605410447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=36467)[0m f1_macro: 0.29761409772891184
[2m[36m(func pid=36467)[0m f1_weighted: 0.37787499622125387
[2m[36m(func pid=36467)[0m f1_per_class: [0.281, 0.502, 0.226, 0.459, 0.09, 0.291, 0.299, 0.432, 0.171, 0.225]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.0334 | Steps: 4 | Val loss: 2.1387 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=54942)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=54942)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=54942)[0m Configuration completed!
[2m[36m(func pid=54942)[0m New optimizer parameters:
[2m[36m(func pid=54942)[0m SGD (
[2m[36m(func pid=54942)[0m Parameter Group 0
[2m[36m(func pid=54942)[0m     dampening: 0
[2m[36m(func pid=54942)[0m     differentiable: False
[2m[36m(func pid=54942)[0m     foreach: None
[2m[36m(func pid=54942)[0m     lr: 0.0001
[2m[36m(func pid=54942)[0m     maximize: False
[2m[36m(func pid=54942)[0m     momentum: 0.9
[2m[36m(func pid=54942)[0m     nesterov: False
[2m[36m(func pid=54942)[0m     weight_decay: 1e-05
[2m[36m(func pid=54942)[0m )
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=37884)[0m top1: 0.1226679104477612
[2m[36m(func pid=37884)[0m top5: 0.9286380597014925
[2m[36m(func pid=37884)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=37884)[0m f1_macro: 0.07640289040676893
[2m[36m(func pid=37884)[0m f1_weighted: 0.033530274414704055
[2m[36m(func pid=37884)[0m f1_per_class: [0.111, 0.0, 0.426, 0.016, 0.0, 0.211, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
== Status ==
Current time: 2024-01-07 13:31:16 (running for 00:38:09.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.29725
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.307 |      0.298 |                   75 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.189 |      0.253 |                   71 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.033 |      0.076 |                   72 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.1636 | Steps: 4 | Val loss: 2.1019 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2158 | Steps: 4 | Val loss: 1.9008 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0265 | Steps: 4 | Val loss: 2.3274 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.1117 | Steps: 4 | Val loss: 2.1863 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=37814)[0m top1: 0.27798507462686567
[2m[36m(func pid=37814)[0m top5: 0.8003731343283582
[2m[36m(func pid=37814)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=37814)[0m f1_macro: 0.22759135682132245
[2m[36m(func pid=37814)[0m f1_weighted: 0.27804964071468025
[2m[36m(func pid=37814)[0m f1_per_class: [0.111, 0.516, 0.444, 0.274, 0.022, 0.307, 0.16, 0.417, 0.025, 0.0]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3614738805970149
[2m[36m(func pid=36467)[0m top5: 0.8819962686567164
[2m[36m(func pid=36467)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=36467)[0m f1_macro: 0.3192580667701362
[2m[36m(func pid=36467)[0m f1_weighted: 0.3969482186580482
[2m[36m(func pid=36467)[0m f1_per_class: [0.316, 0.508, 0.324, 0.468, 0.081, 0.316, 0.338, 0.426, 0.164, 0.25]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=54942)[0m top1: 0.14319029850746268
[2m[36m(func pid=54942)[0m top5: 0.5345149253731343
[2m[36m(func pid=54942)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=54942)[0m f1_macro: 0.043083645195188085
[2m[36m(func pid=54942)[0m f1_weighted: 0.0817632740693365
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.0, 0.0, 0.257, 0.0, 0.0, 0.0, 0.174, 0.0, 0.0]
[2m[36m(func pid=54942)[0m 
== Status ==
Current time: 2024-01-07 13:31:22 (running for 00:38:14.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.29725
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.164 |      0.319 |                   76 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.216 |      0.228 |                   72 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.112 |      0.066 |                   73 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  3.026 |      0.043 |                    1 |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.12033582089552239
[2m[36m(func pid=37884)[0m top5: 0.929570895522388
[2m[36m(func pid=37884)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=37884)[0m f1_macro: 0.06558659217877096
[2m[36m(func pid=37884)[0m f1_weighted: 0.02778175089314921
[2m[36m(func pid=37884)[0m f1_per_class: [0.077, 0.0, 0.369, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3862 | Steps: 4 | Val loss: 2.0408 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.2747 | Steps: 4 | Val loss: 1.8590 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1466 | Steps: 4 | Val loss: 2.3139 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=37814)[0m top1: 0.2947761194029851
[2m[36m(func pid=37814)[0m top5: 0.8115671641791045
[2m[36m(func pid=37814)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=37814)[0m f1_macro: 0.2153967748906122
[2m[36m(func pid=37814)[0m f1_weighted: 0.2620987518197911
[2m[36m(func pid=37814)[0m f1_per_class: [0.131, 0.514, 0.222, 0.343, 0.0, 0.313, 0.034, 0.416, 0.097, 0.085]
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.4425 | Steps: 4 | Val loss: 2.2441 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=36467)[0m top1: 0.3908582089552239
[2m[36m(func pid=36467)[0m top5: 0.8964552238805971
[2m[36m(func pid=36467)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=36467)[0m f1_macro: 0.34735670816892933
[2m[36m(func pid=36467)[0m f1_weighted: 0.417272553914482
[2m[36m(func pid=36467)[0m f1_per_class: [0.319, 0.541, 0.282, 0.464, 0.078, 0.233, 0.402, 0.443, 0.254, 0.457]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=54942)[0m top1: 0.197294776119403
[2m[36m(func pid=54942)[0m top5: 0.5736940298507462
[2m[36m(func pid=54942)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=54942)[0m f1_macro: 0.04182619640532141
[2m[36m(func pid=54942)[0m f1_weighted: 0.10186027841912325
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.0, 0.0, 0.351, 0.0, 0.0, 0.0, 0.067, 0.0, 0.0]
[2m[36m(func pid=54942)[0m 
== Status ==
Current time: 2024-01-07 13:31:27 (running for 00:38:19.99)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.29725
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.275 |      0.347 |                   77 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.386 |      0.215 |                   73 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.443 |      0.059 |                   74 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  3.147 |      0.042 |                    2 |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=37884)[0m top1: 0.11940298507462686
[2m[36m(func pid=37884)[0m top5: 0.6100746268656716
[2m[36m(func pid=37884)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=37884)[0m f1_macro: 0.058936136152075894
[2m[36m(func pid=37884)[0m f1_weighted: 0.02798082013049368
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.01, 0.369, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37884)[0m 
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3979 | Steps: 4 | Val loss: 2.0774 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.9453 | Steps: 4 | Val loss: 1.8902 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9703 | Steps: 4 | Val loss: 2.3191 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=37814)[0m top1: 0.2966417910447761
[2m[36m(func pid=37814)[0m top5: 0.8232276119402985
[2m[36m(func pid=37814)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=37814)[0m f1_macro: 0.2259033574423735
[2m[36m(func pid=37814)[0m f1_weighted: 0.25762171716770454
[2m[36m(func pid=37814)[0m f1_per_class: [0.117, 0.498, 0.316, 0.363, 0.0, 0.315, 0.006, 0.385, 0.148, 0.111]
[2m[36m(func pid=37884)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.8666 | Steps: 4 | Val loss: 2.2592 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=37814)[0m 
[2m[36m(func pid=36467)[0m top1: 0.36847014925373134
[2m[36m(func pid=36467)[0m top5: 0.8894589552238806
[2m[36m(func pid=36467)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=36467)[0m f1_macro: 0.3294094998747056
[2m[36m(func pid=36467)[0m f1_weighted: 0.38387325264529654
[2m[36m(func pid=36467)[0m f1_per_class: [0.292, 0.539, 0.253, 0.43, 0.076, 0.187, 0.35, 0.423, 0.19, 0.554]
[2m[36m(func pid=36467)[0m 
== Status ==
Current time: 2024-01-07 13:31:32 (running for 00:38:25.00)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.29725
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.945 |      0.329 |                   78 |
| train_52b21_00018 | RUNNING    | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  2.398 |      0.226 |                   74 |
| train_52b21_00019 | RUNNING    | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  3.443 |      0.059 |                   74 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.97  |      0.053 |                    3 |
| train_52b21_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.21548507462686567
[2m[36m(func pid=54942)[0m top5: 0.5620335820895522
[2m[36m(func pid=54942)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=54942)[0m f1_macro: 0.05256202340516085
[2m[36m(func pid=54942)[0m f1_weighted: 0.10664160847580849
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.0, 0.125, 0.374, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=37884)[0m top1: 0.1767723880597015
[2m[36m(func pid=37884)[0m top5: 0.6138059701492538
[2m[36m(func pid=37884)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=37884)[0m f1_macro: 0.08184246599416542
[2m[36m(func pid=37884)[0m f1_weighted: 0.054225545281570774
[2m[36m(func pid=37884)[0m f1_per_class: [0.0, 0.297, 0.522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=37814)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9119 | Steps: 4 | Val loss: 2.1481 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.1104 | Steps: 4 | Val loss: 1.8973 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8988 | Steps: 4 | Val loss: 2.3195 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=37814)[0m top1: 0.302705223880597
[2m[36m(func pid=37814)[0m top5: 0.8316231343283582
[2m[36m(func pid=37814)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=37814)[0m f1_macro: 0.23453128610913163
[2m[36m(func pid=37814)[0m f1_weighted: 0.2665944798336136
[2m[36m(func pid=37814)[0m f1_per_class: [0.126, 0.489, 0.381, 0.4, 0.0, 0.342, 0.0, 0.355, 0.158, 0.093]
[2m[36m(func pid=36467)[0m top1: 0.3694029850746269
[2m[36m(func pid=36467)[0m top5: 0.8889925373134329
[2m[36m(func pid=36467)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=36467)[0m f1_macro: 0.3208805042432912
[2m[36m(func pid=36467)[0m f1_weighted: 0.38352103333501
[2m[36m(func pid=36467)[0m f1_per_class: [0.28, 0.514, 0.22, 0.428, 0.075, 0.125, 0.385, 0.45, 0.218, 0.514]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=54942)[0m top1: 0.2234141791044776
[2m[36m(func pid=54942)[0m top5: 0.5578358208955224
[2m[36m(func pid=54942)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=54942)[0m f1_macro: 0.06509438984734135
[2m[36m(func pid=54942)[0m f1_weighted: 0.11612011216893595
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.042, 0.182, 0.379, 0.0, 0.0, 0.0, 0.032, 0.0, 0.016]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.4110 | Steps: 4 | Val loss: 1.8806 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=56305)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56305)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=56305)[0m Configuration completed!
[2m[36m(func pid=56305)[0m New optimizer parameters:
[2m[36m(func pid=56305)[0m SGD (
[2m[36m(func pid=56305)[0m Parameter Group 0
[2m[36m(func pid=56305)[0m     dampening: 0
[2m[36m(func pid=56305)[0m     differentiable: False
[2m[36m(func pid=56305)[0m     foreach: None
[2m[36m(func pid=56305)[0m     lr: 0.001
[2m[36m(func pid=56305)[0m     maximize: False
[2m[36m(func pid=56305)[0m     momentum: 0.9
[2m[36m(func pid=56305)[0m     nesterov: False
[2m[36m(func pid=56305)[0m     weight_decay: 1e-05
[2m[36m(func pid=56305)[0m )
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8473 | Steps: 4 | Val loss: 2.3121 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=36467)[0m top1: 0.38199626865671643
[2m[36m(func pid=36467)[0m top5: 0.8871268656716418
[2m[36m(func pid=36467)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=36467)[0m f1_macro: 0.3245903720957692
[2m[36m(func pid=36467)[0m f1_weighted: 0.4069524883357914
[2m[36m(func pid=36467)[0m f1_per_class: [0.258, 0.509, 0.185, 0.409, 0.079, 0.229, 0.444, 0.468, 0.239, 0.427]
[2m[36m(func pid=54942)[0m top1: 0.23460820895522388
[2m[36m(func pid=54942)[0m top5: 0.5573694029850746
[2m[36m(func pid=54942)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=54942)[0m f1_macro: 0.07694833297496044
[2m[36m(func pid=54942)[0m f1_weighted: 0.12726302502082384
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.077, 0.222, 0.387, 0.0, 0.0, 0.0, 0.084, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 13:31:38 (running for 00:38:30.44)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.11  |      0.321 |                   79 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.899 |      0.065 |                    4 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 13:31:45 (running for 00:38:37.68)
Memory usage on this node: 23.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.411 |      0.325 |                   80 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.899 |      0.065 |                    4 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56437)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=56437)[0m Configuration completed!
[2m[36m(func pid=56437)[0m New optimizer parameters:
[2m[36m(func pid=56437)[0m SGD (
[2m[36m(func pid=56437)[0m Parameter Group 0
[2m[36m(func pid=56437)[0m     dampening: 0
[2m[36m(func pid=56437)[0m     differentiable: False
[2m[36m(func pid=56437)[0m     foreach: None
[2m[36m(func pid=56437)[0m     lr: 0.01
[2m[36m(func pid=56437)[0m     maximize: False
[2m[36m(func pid=56437)[0m     momentum: 0.9
[2m[36m(func pid=56437)[0m     nesterov: False
[2m[36m(func pid=56437)[0m     weight_decay: 1e-05
[2m[36m(func pid=56437)[0m )
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0835 | Steps: 4 | Val loss: 2.3233 | Batch size: 32 | lr: 0.001 | Duration: 4.50s
[2m[36m(func pid=56305)[0m top1: 0.10820895522388059
[2m[36m(func pid=56305)[0m top5: 0.5083955223880597
[2m[36m(func pid=56305)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=56305)[0m f1_macro: 0.0430992299051801
[2m[36m(func pid=56305)[0m f1_weighted: 0.07593559745792433
[2m[36m(func pid=56305)[0m f1_per_class: [0.038, 0.011, 0.033, 0.239, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.3448 | Steps: 4 | Val loss: 1.9632 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8771 | Steps: 4 | Val loss: 2.3142 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0838 | Steps: 4 | Val loss: 2.7719 | Batch size: 32 | lr: 0.01 | Duration: 4.78s
== Status ==
Current time: 2024-01-07 13:31:50 (running for 00:38:43.14)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.411 |      0.325 |                   80 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.877 |      0.085 |                    6 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  3.084 |      0.043 |                    1 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.22621268656716417
[2m[36m(func pid=54942)[0m top5: 0.539179104477612
[2m[36m(func pid=54942)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=54942)[0m f1_macro: 0.08499265970364642
[2m[36m(func pid=54942)[0m f1_weighted: 0.13635969741300863
[2m[36m(func pid=54942)[0m f1_per_class: [0.039, 0.104, 0.096, 0.37, 0.0, 0.0, 0.0, 0.241, 0.0, 0.0]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3894589552238806
[2m[36m(func pid=36467)[0m top5: 0.8768656716417911
[2m[36m(func pid=36467)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=36467)[0m f1_macro: 0.3297838298132619
[2m[36m(func pid=36467)[0m f1_weighted: 0.4188141581932753
[2m[36m(func pid=36467)[0m f1_per_class: [0.28, 0.536, 0.194, 0.378, 0.07, 0.311, 0.468, 0.472, 0.204, 0.384]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6961 | Steps: 4 | Val loss: 2.3284 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=56437)[0m top1: 0.012126865671641791
[2m[36m(func pid=56437)[0m top5: 0.40904850746268656
[2m[36m(func pid=56437)[0m f1_micro: 0.012126865671641791
[2m[36m(func pid=56437)[0m f1_macro: 0.01296083474375353
[2m[36m(func pid=56437)[0m f1_weighted: 0.003446165105906786
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.108, 0.0, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.006063432835820896
[2m[36m(func pid=56305)[0m top5: 0.5559701492537313
[2m[36m(func pid=56305)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=56305)[0m f1_macro: 0.0012115563839701772
[2m[36m(func pid=56305)[0m f1_weighted: 7.346190761013201e-05
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7384 | Steps: 4 | Val loss: 2.3003 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.8783 | Steps: 4 | Val loss: 1.8836 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9874 | Steps: 4 | Val loss: 4.5253 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 13:31:56 (running for 00:38:48.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.345 |      0.33  |                   81 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.738 |      0.097 |                    7 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.696 |      0.001 |                    2 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  3.084 |      0.013 |                    1 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.22014925373134328
[2m[36m(func pid=54942)[0m top5: 0.5559701492537313
[2m[36m(func pid=54942)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=54942)[0m f1_macro: 0.0968179084500544
[2m[36m(func pid=54942)[0m f1_weighted: 0.14623065533047078
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.169, 0.084, 0.35, 0.0, 0.0, 0.0, 0.32, 0.0, 0.045]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3880597014925373
[2m[36m(func pid=36467)[0m top5: 0.8847947761194029
[2m[36m(func pid=36467)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=36467)[0m f1_macro: 0.32232816140629844
[2m[36m(func pid=36467)[0m f1_weighted: 0.4036380674319166
[2m[36m(func pid=36467)[0m f1_per_class: [0.292, 0.533, 0.226, 0.36, 0.08, 0.179, 0.488, 0.45, 0.202, 0.413]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9502 | Steps: 4 | Val loss: 2.3103 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=56437)[0m top1: 0.02332089552238806
[2m[36m(func pid=56437)[0m top5: 0.45009328358208955
[2m[36m(func pid=56437)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=56437)[0m f1_macro: 0.009343261656915635
[2m[36m(func pid=56437)[0m f1_weighted: 0.0014851706440066215
[2m[36m(func pid=56437)[0m f1_per_class: [0.045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7362 | Steps: 4 | Val loss: 2.2969 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56305)[0m top1: 0.006063432835820896
[2m[36m(func pid=56305)[0m top5: 0.7210820895522388
[2m[36m(func pid=56305)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=56305)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=56305)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.3273 | Steps: 4 | Val loss: 1.8875 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4485 | Steps: 4 | Val loss: 8.6475 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 13:32:01 (running for 00:38:53.94)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.878 |      0.322 |                   82 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.736 |      0.111 |                    8 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.95  |      0.001 |                    3 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.987 |      0.009 |                    2 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19542910447761194
[2m[36m(func pid=54942)[0m top5: 0.558768656716418
[2m[36m(func pid=54942)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=54942)[0m f1_macro: 0.11067493132112845
[2m[36m(func pid=54942)[0m f1_weighted: 0.1490428838259612
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.268, 0.068, 0.283, 0.0, 0.0, 0.0, 0.388, 0.0, 0.1]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3908582089552239
[2m[36m(func pid=36467)[0m top5: 0.8815298507462687
[2m[36m(func pid=36467)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=36467)[0m f1_macro: 0.3273063638319767
[2m[36m(func pid=36467)[0m f1_weighted: 0.3893244037846565
[2m[36m(func pid=36467)[0m f1_per_class: [0.261, 0.536, 0.308, 0.332, 0.064, 0.106, 0.487, 0.458, 0.205, 0.516]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7933 | Steps: 4 | Val loss: 2.2097 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=56437)[0m top1: 0.06716417910447761
[2m[36m(func pid=56437)[0m top5: 0.5652985074626866
[2m[36m(func pid=56437)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=56437)[0m f1_macro: 0.049961388606909116
[2m[36m(func pid=56437)[0m f1_weighted: 0.07101848397297714
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.392, 0.019, 0.0, 0.0, 0.0, 0.0, 0.041, 0.024, 0.024]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.1553171641791045
[2m[36m(func pid=56305)[0m top5: 0.8297574626865671
[2m[36m(func pid=56305)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=56305)[0m f1_macro: 0.11414972988107022
[2m[36m(func pid=56305)[0m f1_weighted: 0.1674302819080249
[2m[36m(func pid=56305)[0m f1_per_class: [0.071, 0.026, 0.314, 0.098, 0.133, 0.008, 0.431, 0.0, 0.06, 0.0]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6711 | Steps: 4 | Val loss: 2.2970 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.9836 | Steps: 4 | Val loss: 1.8837 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1637 | Steps: 4 | Val loss: 3.9471 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:32:07 (running for 00:38:59.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.327 |      0.327 |                   83 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.671 |      0.116 |                    9 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.793 |      0.114 |                    4 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.448 |      0.05  |                    3 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19076492537313433
[2m[36m(func pid=54942)[0m top5: 0.5573694029850746
[2m[36m(func pid=54942)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=54942)[0m f1_macro: 0.11569996994590848
[2m[36m(func pid=54942)[0m f1_weighted: 0.14188670281477334
[2m[36m(func pid=54942)[0m f1_per_class: [0.0, 0.335, 0.07, 0.208, 0.0, 0.0, 0.0, 0.425, 0.0, 0.12]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.3829291044776119
[2m[36m(func pid=36467)[0m top5: 0.8908582089552238
[2m[36m(func pid=36467)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=36467)[0m f1_macro: 0.3569777061026312
[2m[36m(func pid=36467)[0m f1_weighted: 0.37859119577410555
[2m[36m(func pid=36467)[0m f1_per_class: [0.349, 0.533, 0.55, 0.333, 0.087, 0.122, 0.436, 0.456, 0.204, 0.5]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7031 | Steps: 4 | Val loss: 2.1148 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=56437)[0m top1: 0.240205223880597
[2m[36m(func pid=56437)[0m top5: 0.8134328358208955
[2m[36m(func pid=56437)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=56437)[0m f1_macro: 0.1225628810014903
[2m[36m(func pid=56437)[0m f1_weighted: 0.27096143029488146
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.378, 0.073, 0.262, 0.0, 0.0, 0.436, 0.0, 0.076, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.20335820895522388
[2m[36m(func pid=56305)[0m top5: 0.8964552238805971
[2m[36m(func pid=56305)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=56305)[0m f1_macro: 0.1258727770472059
[2m[36m(func pid=56305)[0m f1_weighted: 0.2096119923905329
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.348, 0.143, 0.023, 0.12, 0.008, 0.46, 0.0, 0.08, 0.077]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7378 | Steps: 4 | Val loss: 2.2895 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.9771 | Steps: 4 | Val loss: 1.8326 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9274 | Steps: 4 | Val loss: 26.9499 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 13:32:12 (running for 00:39:04.98)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.984 |      0.357 |                   84 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.738 |      0.117 |                   10 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.703 |      0.126 |                    5 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  3.164 |      0.123 |                    4 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19076492537313433
[2m[36m(func pid=54942)[0m top5: 0.5699626865671642
[2m[36m(func pid=54942)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=54942)[0m f1_macro: 0.117040883530791
[2m[36m(func pid=54942)[0m f1_weighted: 0.12791409408663149
[2m[36m(func pid=54942)[0m f1_per_class: [0.023, 0.356, 0.063, 0.132, 0.0, 0.0, 0.0, 0.477, 0.0, 0.12]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.394589552238806
[2m[36m(func pid=36467)[0m top5: 0.8913246268656716
[2m[36m(func pid=36467)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=36467)[0m f1_macro: 0.3738912594188471
[2m[36m(func pid=36467)[0m f1_weighted: 0.40045878491124026
[2m[36m(func pid=36467)[0m f1_per_class: [0.414, 0.556, 0.524, 0.373, 0.101, 0.205, 0.424, 0.453, 0.208, 0.481]
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5094 | Steps: 4 | Val loss: 1.9839 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.10167910447761194
[2m[36m(func pid=56437)[0m top5: 0.5657649253731343
[2m[36m(func pid=56437)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=56437)[0m f1_macro: 0.07901339906781789
[2m[36m(func pid=56437)[0m f1_weighted: 0.0684898018518667
[2m[36m(func pid=56437)[0m f1_per_class: [0.014, 0.276, 0.036, 0.0, 0.3, 0.0, 0.047, 0.048, 0.032, 0.037]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.4001865671641791
[2m[36m(func pid=56305)[0m top5: 0.9118470149253731
[2m[36m(func pid=56305)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=56305)[0m f1_macro: 0.20720808764593782
[2m[36m(func pid=56305)[0m f1_weighted: 0.3618221351048467
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.481, 0.313, 0.361, 0.152, 0.0, 0.575, 0.0, 0.067, 0.122]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6867 | Steps: 4 | Val loss: 2.2806 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.0861 | Steps: 4 | Val loss: 1.8098 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0104 | Steps: 4 | Val loss: 86.8992 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:32:18 (running for 00:39:10.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.977 |      0.374 |                   85 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.687 |      0.113 |                   11 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.509 |      0.207 |                    6 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.927 |      0.079 |                    5 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19962686567164178
[2m[36m(func pid=54942)[0m top5: 0.5788246268656716
[2m[36m(func pid=54942)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=54942)[0m f1_macro: 0.11272591893209201
[2m[36m(func pid=54942)[0m f1_weighted: 0.11606710333686511
[2m[36m(func pid=54942)[0m f1_per_class: [0.052, 0.372, 0.066, 0.079, 0.0, 0.0, 0.0, 0.475, 0.0, 0.083]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3323 | Steps: 4 | Val loss: 2.0076 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=36467)[0m top1: 0.39925373134328357
[2m[36m(func pid=36467)[0m top5: 0.8917910447761194
[2m[36m(func pid=36467)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=36467)[0m f1_macro: 0.38866545290431853
[2m[36m(func pid=36467)[0m f1_weighted: 0.4192672724539898
[2m[36m(func pid=36467)[0m f1_per_class: [0.41, 0.562, 0.6, 0.453, 0.09, 0.22, 0.406, 0.43, 0.186, 0.528]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.08255597014925373
[2m[36m(func pid=56437)[0m top5: 0.4314365671641791
[2m[36m(func pid=56437)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=56437)[0m f1_macro: 0.0957570216230559
[2m[36m(func pid=56437)[0m f1_weighted: 0.055605462173221175
[2m[36m(func pid=56437)[0m f1_per_class: [0.031, 0.076, 0.022, 0.0, 0.28, 0.206, 0.0, 0.252, 0.028, 0.063]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.23180970149253732
[2m[36m(func pid=56305)[0m top5: 0.8680037313432836
[2m[36m(func pid=56305)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=56305)[0m f1_macro: 0.16085020676344794
[2m[36m(func pid=56305)[0m f1_weighted: 0.20310089679348295
[2m[36m(func pid=56305)[0m f1_per_class: [0.075, 0.474, 0.163, 0.323, 0.143, 0.0, 0.018, 0.38, 0.0, 0.032]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7239 | Steps: 4 | Val loss: 2.2820 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.9701 | Steps: 4 | Val loss: 1.7774 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.2395 | Steps: 4 | Val loss: 19.6654 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:32:23 (running for 00:39:15.79)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.086 |      0.389 |                   86 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.724 |      0.11  |                   12 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.332 |      0.161 |                    7 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  3.01  |      0.096 |                    6 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.1958955223880597
[2m[36m(func pid=54942)[0m top5: 0.5680970149253731
[2m[36m(func pid=54942)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=54942)[0m f1_macro: 0.11007883377969055
[2m[36m(func pid=54942)[0m f1_weighted: 0.10128785274894077
[2m[36m(func pid=54942)[0m f1_per_class: [0.064, 0.377, 0.075, 0.022, 0.0, 0.0, 0.0, 0.476, 0.0, 0.087]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3488 | Steps: 4 | Val loss: 2.0267 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=36467)[0m top1: 0.4230410447761194
[2m[36m(func pid=36467)[0m top5: 0.8899253731343284
[2m[36m(func pid=36467)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=36467)[0m f1_macro: 0.39597373701182226
[2m[36m(func pid=36467)[0m f1_weighted: 0.4374696084809294
[2m[36m(func pid=36467)[0m f1_per_class: [0.421, 0.591, 0.545, 0.507, 0.081, 0.214, 0.399, 0.443, 0.193, 0.565]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.12779850746268656
[2m[36m(func pid=56437)[0m top5: 0.5690298507462687
[2m[36m(func pid=56437)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=56437)[0m f1_macro: 0.1086886555038233
[2m[36m(func pid=56437)[0m f1_weighted: 0.08419402646896088
[2m[36m(func pid=56437)[0m f1_per_class: [0.076, 0.134, 0.0, 0.0, 0.116, 0.331, 0.0, 0.344, 0.0, 0.086]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.20382462686567165
[2m[36m(func pid=56305)[0m top5: 0.8190298507462687
[2m[36m(func pid=56305)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=56305)[0m f1_macro: 0.12403754380293863
[2m[36m(func pid=56305)[0m f1_weighted: 0.146295431422831
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.409, 0.12, 0.188, 0.097, 0.0, 0.0, 0.368, 0.0, 0.058]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6218 | Steps: 4 | Val loss: 2.2678 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.9938 | Steps: 4 | Val loss: 1.7608 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2073 | Steps: 4 | Val loss: 14.8079 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 13:32:29 (running for 00:39:21.43)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.97  |      0.396 |                   87 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.622 |      0.113 |                   13 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.349 |      0.124 |                    8 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.239 |      0.109 |                    7 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19636194029850745
[2m[36m(func pid=54942)[0m top5: 0.5881529850746269
[2m[36m(func pid=54942)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=54942)[0m f1_macro: 0.11346170356662974
[2m[36m(func pid=54942)[0m f1_weighted: 0.10442024541702165
[2m[36m(func pid=54942)[0m f1_per_class: [0.063, 0.366, 0.099, 0.031, 0.0, 0.0, 0.0, 0.524, 0.0, 0.053]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3090 | Steps: 4 | Val loss: 2.0998 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=36467)[0m top1: 0.43236940298507465
[2m[36m(func pid=36467)[0m top5: 0.894589552238806
[2m[36m(func pid=36467)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=36467)[0m f1_macro: 0.3865346253188629
[2m[36m(func pid=36467)[0m f1_weighted: 0.4431440048851887
[2m[36m(func pid=36467)[0m f1_per_class: [0.436, 0.584, 0.436, 0.536, 0.08, 0.217, 0.391, 0.464, 0.212, 0.51]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.1357276119402985
[2m[36m(func pid=56437)[0m top5: 0.6473880597014925
[2m[36m(func pid=56437)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=56437)[0m f1_macro: 0.12543828737025814
[2m[36m(func pid=56437)[0m f1_weighted: 0.09170670086178499
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.098, 0.174, 0.039, 0.129, 0.332, 0.0, 0.377, 0.053, 0.053]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.20242537313432835
[2m[36m(func pid=56305)[0m top5: 0.75
[2m[36m(func pid=56305)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=56305)[0m f1_macro: 0.14730257836862254
[2m[36m(func pid=56305)[0m f1_weighted: 0.10709793570867596
[2m[36m(func pid=56305)[0m f1_per_class: [0.167, 0.419, 0.18, 0.003, 0.118, 0.031, 0.0, 0.337, 0.143, 0.075]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5718 | Steps: 4 | Val loss: 2.2520 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.9848 | Steps: 4 | Val loss: 1.7836 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.0177 | Steps: 4 | Val loss: 20.3515 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:32:34 (running for 00:39:27.08)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.994 |      0.387 |                   88 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.572 |      0.115 |                   14 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.309 |      0.147 |                    9 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.207 |      0.125 |                    8 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.19682835820895522
[2m[36m(func pid=54942)[0m top5: 0.6217350746268657
[2m[36m(func pid=54942)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=54942)[0m f1_macro: 0.11531527997783857
[2m[36m(func pid=54942)[0m f1_weighted: 0.10622496800012124
[2m[36m(func pid=54942)[0m f1_per_class: [0.054, 0.361, 0.101, 0.038, 0.0, 0.0, 0.0, 0.537, 0.0, 0.062]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0912 | Steps: 4 | Val loss: 2.0163 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=36467)[0m top1: 0.42490671641791045
[2m[36m(func pid=36467)[0m top5: 0.8955223880597015
[2m[36m(func pid=36467)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=36467)[0m f1_macro: 0.3643355006324177
[2m[36m(func pid=36467)[0m f1_weighted: 0.42732298257726276
[2m[36m(func pid=36467)[0m f1_per_class: [0.406, 0.584, 0.375, 0.553, 0.082, 0.214, 0.336, 0.428, 0.204, 0.462]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.19309701492537312
[2m[36m(func pid=56437)[0m top5: 0.6254664179104478
[2m[36m(func pid=56437)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=56437)[0m f1_macro: 0.12554078070234292
[2m[36m(func pid=56437)[0m f1_weighted: 0.2120289376283428
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.0, 0.0, 0.048, 0.072, 0.099, 0.535, 0.466, 0.0, 0.035]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2248134328358209
[2m[36m(func pid=56305)[0m top5: 0.7779850746268657
[2m[36m(func pid=56305)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=56305)[0m f1_macro: 0.23506828637375793
[2m[36m(func pid=56305)[0m f1_weighted: 0.1534871296873572
[2m[36m(func pid=56305)[0m f1_per_class: [0.19, 0.456, 0.571, 0.0, 0.115, 0.348, 0.0, 0.362, 0.118, 0.19]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5853 | Steps: 4 | Val loss: 2.2360 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.7350 | Steps: 4 | Val loss: 1.8042 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.0662 | Steps: 4 | Val loss: 7.2753 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:32:40 (running for 00:39:32.62)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.985 |      0.364 |                   89 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.585 |      0.112 |                   15 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.091 |      0.235 |                   10 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  3.018 |      0.126 |                    9 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.20475746268656717
[2m[36m(func pid=54942)[0m top5: 0.6707089552238806
[2m[36m(func pid=54942)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=54942)[0m f1_macro: 0.11206819708019514
[2m[36m(func pid=54942)[0m f1_weighted: 0.10181811308790688
[2m[36m(func pid=54942)[0m f1_per_class: [0.077, 0.378, 0.115, 0.022, 0.0, 0.016, 0.0, 0.442, 0.0, 0.069]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0437 | Steps: 4 | Val loss: 2.0770 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=36467)[0m top1: 0.3978544776119403
[2m[36m(func pid=36467)[0m top5: 0.9039179104477612
[2m[36m(func pid=36467)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=36467)[0m f1_macro: 0.33912276553881066
[2m[36m(func pid=36467)[0m f1_weighted: 0.40334027243682846
[2m[36m(func pid=36467)[0m f1_per_class: [0.388, 0.552, 0.235, 0.537, 0.09, 0.184, 0.309, 0.393, 0.202, 0.5]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.03871268656716418
[2m[36m(func pid=56437)[0m top5: 0.5452425373134329
[2m[36m(func pid=56437)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=56437)[0m f1_macro: 0.035990211154599036
[2m[36m(func pid=56437)[0m f1_weighted: 0.014695609192290127
[2m[36m(func pid=56437)[0m f1_per_class: [0.063, 0.0, 0.0, 0.019, 0.162, 0.0, 0.015, 0.0, 0.05, 0.051]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.24813432835820895
[2m[36m(func pid=56305)[0m top5: 0.7887126865671642
[2m[36m(func pid=56305)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=56305)[0m f1_macro: 0.24543919334817127
[2m[36m(func pid=56305)[0m f1_weighted: 0.16167109515746164
[2m[36m(func pid=56305)[0m f1_per_class: [0.273, 0.468, 0.458, 0.0, 0.19, 0.355, 0.0, 0.444, 0.088, 0.176]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5690 | Steps: 4 | Val loss: 2.2337 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.0288 | Steps: 4 | Val loss: 1.8233 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.3107 | Steps: 4 | Val loss: 3.9986 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:32:46 (running for 00:39:38.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.735 |      0.339 |                   90 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.569 |      0.115 |                   16 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.044 |      0.245 |                   11 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  3.066 |      0.036 |                   10 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.1982276119402985
[2m[36m(func pid=54942)[0m top5: 0.6893656716417911
[2m[36m(func pid=54942)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=54942)[0m f1_macro: 0.11482742796076177
[2m[36m(func pid=54942)[0m f1_weighted: 0.09562490608093147
[2m[36m(func pid=54942)[0m f1_per_class: [0.076, 0.378, 0.173, 0.003, 0.0, 0.008, 0.0, 0.436, 0.0, 0.074]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1964 | Steps: 4 | Val loss: 1.9279 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=36467)[0m top1: 0.41138059701492535
[2m[36m(func pid=36467)[0m top5: 0.8973880597014925
[2m[36m(func pid=36467)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=36467)[0m f1_macro: 0.34190122348803753
[2m[36m(func pid=36467)[0m f1_weighted: 0.4199968445306359
[2m[36m(func pid=36467)[0m f1_per_class: [0.42, 0.559, 0.231, 0.547, 0.096, 0.29, 0.31, 0.419, 0.202, 0.345]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.12220149253731344
[2m[36m(func pid=56437)[0m top5: 0.6674440298507462
[2m[36m(func pid=56437)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=56437)[0m f1_macro: 0.1595553377659084
[2m[36m(func pid=56437)[0m f1_weighted: 0.11917894601960102
[2m[36m(func pid=56437)[0m f1_per_class: [0.084, 0.357, 0.375, 0.026, 0.081, 0.19, 0.0, 0.394, 0.021, 0.068]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2574626865671642
[2m[36m(func pid=56305)[0m top5: 0.8232276119402985
[2m[36m(func pid=56305)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=56305)[0m f1_macro: 0.22939253326954573
[2m[36m(func pid=56305)[0m f1_weighted: 0.15203609664745552
[2m[36m(func pid=56305)[0m f1_per_class: [0.21, 0.434, 0.512, 0.0, 0.131, 0.342, 0.0, 0.469, 0.025, 0.171]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5475 | Steps: 4 | Val loss: 2.2188 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.2037 | Steps: 4 | Val loss: 2.0313 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2320 | Steps: 4 | Val loss: 3.9746 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:32:51 (running for 00:39:43.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.029 |      0.342 |                   91 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.547 |      0.13  |                   17 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.196 |      0.229 |                   12 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.311 |      0.16  |                   11 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.20475746268656717
[2m[36m(func pid=54942)[0m top5: 0.7215485074626866
[2m[36m(func pid=54942)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=54942)[0m f1_macro: 0.13020391538953952
[2m[36m(func pid=54942)[0m f1_weighted: 0.09910531068982334
[2m[36m(func pid=54942)[0m f1_per_class: [0.091, 0.384, 0.256, 0.01, 0.062, 0.016, 0.0, 0.408, 0.0, 0.074]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0399 | Steps: 4 | Val loss: 1.8581 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=36467)[0m top1: 0.4244402985074627
[2m[36m(func pid=36467)[0m top5: 0.8871268656716418
[2m[36m(func pid=36467)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=36467)[0m f1_macro: 0.35312953256007773
[2m[36m(func pid=36467)[0m f1_weighted: 0.44206638164426687
[2m[36m(func pid=36467)[0m f1_per_class: [0.465, 0.575, 0.276, 0.535, 0.084, 0.342, 0.357, 0.463, 0.217, 0.218]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.24207089552238806
[2m[36m(func pid=56437)[0m top5: 0.7709888059701493
[2m[36m(func pid=56437)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=56437)[0m f1_macro: 0.19495679322180465
[2m[36m(func pid=56437)[0m f1_weighted: 0.26165515355317515
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.329, 0.0, 0.056, 0.104, 0.364, 0.384, 0.485, 0.098, 0.131]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2555970149253731
[2m[36m(func pid=56305)[0m top5: 0.8498134328358209
[2m[36m(func pid=56305)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=56305)[0m f1_macro: 0.20124093183537042
[2m[36m(func pid=56305)[0m f1_weighted: 0.18920724454255233
[2m[36m(func pid=56305)[0m f1_per_class: [0.21, 0.441, 0.134, 0.088, 0.094, 0.342, 0.051, 0.462, 0.0, 0.193]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5646 | Steps: 4 | Val loss: 2.2241 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.1209 | Steps: 4 | Val loss: 2.6719 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.2697 | Steps: 4 | Val loss: 5.5535 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 13:32:57 (running for 00:39:49.30)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.204 |      0.353 |                   92 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.565 |      0.124 |                   18 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.04  |      0.201 |                   13 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.232 |      0.195 |                   12 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.20149253731343283
[2m[36m(func pid=54942)[0m top5: 0.7192164179104478
[2m[36m(func pid=54942)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=54942)[0m f1_macro: 0.12436089510872347
[2m[36m(func pid=54942)[0m f1_weighted: 0.10097135483955381
[2m[36m(func pid=54942)[0m f1_per_class: [0.088, 0.386, 0.196, 0.007, 0.051, 0.039, 0.003, 0.399, 0.0, 0.074]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.8247 | Steps: 4 | Val loss: 1.9396 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=36467)[0m top1: 0.37220149253731344
[2m[36m(func pid=36467)[0m top5: 0.8582089552238806
[2m[36m(func pid=36467)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=36467)[0m f1_macro: 0.31683243711249853
[2m[36m(func pid=36467)[0m f1_weighted: 0.3858204019429624
[2m[36m(func pid=36467)[0m f1_per_class: [0.425, 0.576, 0.282, 0.528, 0.135, 0.234, 0.228, 0.45, 0.182, 0.13]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.11287313432835822
[2m[36m(func pid=56437)[0m top5: 0.6417910447761194
[2m[36m(func pid=56437)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=56437)[0m f1_macro: 0.09266915153448227
[2m[36m(func pid=56437)[0m f1_weighted: 0.07679906237776078
[2m[36m(func pid=56437)[0m f1_per_class: [0.074, 0.178, 0.0, 0.007, 0.162, 0.0, 0.084, 0.228, 0.055, 0.139]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.22527985074626866
[2m[36m(func pid=56305)[0m top5: 0.8311567164179104
[2m[36m(func pid=56305)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=56305)[0m f1_macro: 0.21173581583383677
[2m[36m(func pid=56305)[0m f1_weighted: 0.24954837475349334
[2m[36m(func pid=56305)[0m f1_per_class: [0.317, 0.387, 0.189, 0.283, 0.062, 0.189, 0.156, 0.471, 0.0, 0.064]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5732 | Steps: 4 | Val loss: 2.2310 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9346 | Steps: 4 | Val loss: 3.1167 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5917 | Steps: 4 | Val loss: 6.2743 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0287 | Steps: 4 | Val loss: 1.9373 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=54942)[0m top1: 0.20755597014925373
[2m[36m(func pid=54942)[0m top5: 0.7126865671641791
[2m[36m(func pid=54942)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=54942)[0m f1_macro: 0.14150120850253453
[2m[36m(func pid=54942)[0m f1_weighted: 0.12329530078573767
[2m[36m(func pid=54942)[0m f1_per_class: [0.075, 0.408, 0.174, 0.007, 0.104, 0.112, 0.034, 0.406, 0.023, 0.074]
[2m[36m(func pid=54942)[0m 
== Status ==
Current time: 2024-01-07 13:33:02 (running for 00:39:54.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.121 |      0.317 |                   93 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.573 |      0.142 |                   19 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.825 |      0.212 |                   14 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.27  |      0.093 |                   13 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.32882462686567165
[2m[36m(func pid=36467)[0m top5: 0.8246268656716418
[2m[36m(func pid=36467)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=36467)[0m f1_macro: 0.2802201109909622
[2m[36m(func pid=36467)[0m f1_weighted: 0.3472568781115146
[2m[36m(func pid=36467)[0m f1_per_class: [0.385, 0.567, 0.25, 0.506, 0.134, 0.186, 0.167, 0.364, 0.14, 0.102]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.12173507462686567
[2m[36m(func pid=56437)[0m top5: 0.659981343283582
[2m[36m(func pid=56437)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=56437)[0m f1_macro: 0.0826549082841254
[2m[36m(func pid=56437)[0m f1_weighted: 0.0931576541976071
[2m[36m(func pid=56437)[0m f1_per_class: [0.081, 0.225, 0.0, 0.047, 0.097, 0.0, 0.077, 0.237, 0.062, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2019589552238806
[2m[36m(func pid=56305)[0m top5: 0.8344216417910447
[2m[36m(func pid=56305)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=56305)[0m f1_macro: 0.20131398308707057
[2m[36m(func pid=56305)[0m f1_weighted: 0.2153225699002013
[2m[36m(func pid=56305)[0m f1_per_class: [0.263, 0.356, 0.279, 0.204, 0.065, 0.188, 0.137, 0.459, 0.0, 0.062]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4935 | Steps: 4 | Val loss: 2.2244 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.8190 | Steps: 4 | Val loss: 1.9765 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.2650 | Steps: 4 | Val loss: 3.0874 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7366 | Steps: 4 | Val loss: 1.9724 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:33:08 (running for 00:40:00.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.935 |      0.28  |                   94 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.494 |      0.151 |                   20 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  2.029 |      0.201 |                   15 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.592 |      0.083 |                   14 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.21548507462686567
[2m[36m(func pid=54942)[0m top5: 0.7145522388059702
[2m[36m(func pid=54942)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=54942)[0m f1_macro: 0.15106156525252215
[2m[36m(func pid=54942)[0m f1_weighted: 0.13282404969950803
[2m[36m(func pid=54942)[0m f1_per_class: [0.081, 0.434, 0.183, 0.007, 0.053, 0.197, 0.015, 0.391, 0.078, 0.071]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.40205223880597013
[2m[36m(func pid=36467)[0m top5: 0.8936567164179104
[2m[36m(func pid=36467)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=36467)[0m f1_macro: 0.3531049858558334
[2m[36m(func pid=36467)[0m f1_weighted: 0.42873567797765366
[2m[36m(func pid=36467)[0m f1_per_class: [0.355, 0.57, 0.348, 0.471, 0.128, 0.301, 0.401, 0.443, 0.167, 0.349]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.1875
[2m[36m(func pid=56437)[0m top5: 0.804570895522388
[2m[36m(func pid=56437)[0m f1_micro: 0.1875
[2m[36m(func pid=56437)[0m f1_macro: 0.12115412545520077
[2m[36m(func pid=56437)[0m f1_weighted: 0.18319844946890657
[2m[36m(func pid=56437)[0m f1_per_class: [0.097, 0.305, 0.0, 0.282, 0.04, 0.0, 0.096, 0.33, 0.062, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2080223880597015
[2m[36m(func pid=56305)[0m top5: 0.8260261194029851
[2m[36m(func pid=56305)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=56305)[0m f1_macro: 0.20997270096462511
[2m[36m(func pid=56305)[0m f1_weighted: 0.1923557589624381
[2m[36m(func pid=56305)[0m f1_per_class: [0.365, 0.417, 0.264, 0.122, 0.06, 0.22, 0.074, 0.498, 0.0, 0.079]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4689 | Steps: 4 | Val loss: 2.2098 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.0108 | Steps: 4 | Val loss: 1.8831 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5637 | Steps: 4 | Val loss: 3.0888 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5629 | Steps: 4 | Val loss: 1.9544 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:33:13 (running for 00:40:05.84)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.819 |      0.353 |                   95 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.469 |      0.171 |                   21 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.737 |      0.21  |                   16 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.265 |      0.121 |                   15 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.22061567164179105
[2m[36m(func pid=54942)[0m top5: 0.7294776119402985
[2m[36m(func pid=54942)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=54942)[0m f1_macro: 0.17123449555955794
[2m[36m(func pid=54942)[0m f1_weighted: 0.14383860314275174
[2m[36m(func pid=54942)[0m f1_per_class: [0.096, 0.471, 0.253, 0.02, 0.111, 0.231, 0.012, 0.339, 0.058, 0.121]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.42863805970149255
[2m[36m(func pid=36467)[0m top5: 0.8861940298507462
[2m[36m(func pid=36467)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=36467)[0m f1_macro: 0.35927802210407334
[2m[36m(func pid=36467)[0m f1_weighted: 0.4335686865979857
[2m[36m(func pid=36467)[0m f1_per_class: [0.337, 0.555, 0.329, 0.414, 0.143, 0.141, 0.523, 0.5, 0.194, 0.457]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.2248134328358209
[2m[36m(func pid=56437)[0m top5: 0.7346082089552238
[2m[36m(func pid=56437)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=56437)[0m f1_macro: 0.15715794157227966
[2m[36m(func pid=56437)[0m f1_weighted: 0.2092758464706958
[2m[36m(func pid=56437)[0m f1_per_class: [0.127, 0.445, 0.0, 0.026, 0.045, 0.196, 0.251, 0.377, 0.104, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2196828358208955
[2m[36m(func pid=56305)[0m top5: 0.8274253731343284
[2m[36m(func pid=56305)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=56305)[0m f1_macro: 0.24283080305685947
[2m[36m(func pid=56305)[0m f1_weighted: 0.2115421734842215
[2m[36m(func pid=56305)[0m f1_per_class: [0.193, 0.433, 0.636, 0.165, 0.052, 0.246, 0.085, 0.479, 0.0, 0.138]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5388 | Steps: 4 | Val loss: 2.2052 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.9580 | Steps: 4 | Val loss: 1.9430 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6533 | Steps: 4 | Val loss: 3.7610 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.8741 | Steps: 4 | Val loss: 1.8914 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:33:19 (running for 00:40:11.32)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.011 |      0.359 |                   96 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.539 |      0.166 |                   22 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.563 |      0.243 |                   17 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.564 |      0.157 |                   16 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.22014925373134328
[2m[36m(func pid=54942)[0m top5: 0.7444029850746269
[2m[36m(func pid=54942)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=54942)[0m f1_macro: 0.16586173155695053
[2m[36m(func pid=54942)[0m f1_weighted: 0.14263362660030954
[2m[36m(func pid=54942)[0m f1_per_class: [0.081, 0.469, 0.169, 0.016, 0.121, 0.247, 0.006, 0.355, 0.041, 0.154]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m top1: 0.26865671641791045
[2m[36m(func pid=56437)[0m top5: 0.6301305970149254
[2m[36m(func pid=56437)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=56437)[0m f1_macro: 0.1694855051178993
[2m[36m(func pid=56437)[0m f1_weighted: 0.2661337000569518
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.35, 0.192, 0.0, 0.049, 0.142, 0.555, 0.386, 0.0, 0.02]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=36467)[0m top1: 0.4314365671641791
[2m[36m(func pid=36467)[0m top5: 0.882929104477612
[2m[36m(func pid=36467)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=36467)[0m f1_macro: 0.35596285693294194
[2m[36m(func pid=36467)[0m f1_weighted: 0.4317979643687987
[2m[36m(func pid=36467)[0m f1_per_class: [0.317, 0.542, 0.324, 0.382, 0.123, 0.09, 0.568, 0.554, 0.152, 0.507]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56305)[0m top1: 0.2537313432835821
[2m[36m(func pid=56305)[0m top5: 0.8372201492537313
[2m[36m(func pid=56305)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=56305)[0m f1_macro: 0.23300209227115376
[2m[36m(func pid=56305)[0m f1_weighted: 0.23698837709803147
[2m[36m(func pid=56305)[0m f1_per_class: [0.217, 0.433, 0.385, 0.288, 0.073, 0.336, 0.039, 0.371, 0.048, 0.14]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4293 | Steps: 4 | Val loss: 2.1718 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.5023 | Steps: 4 | Val loss: 1.9652 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4275 | Steps: 4 | Val loss: 4.1814 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5173 | Steps: 4 | Val loss: 1.9656 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=54942)[0m top1: 0.2439365671641791
[2m[36m(func pid=54942)[0m top5: 0.777518656716418
[2m[36m(func pid=54942)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=54942)[0m f1_macro: 0.1739523005500211
[2m[36m(func pid=54942)[0m f1_weighted: 0.16646693507135615
[2m[36m(func pid=54942)[0m f1_per_class: [0.106, 0.464, 0.163, 0.059, 0.107, 0.274, 0.028, 0.41, 0.069, 0.061]
[2m[36m(func pid=54942)[0m 
== Status ==
Current time: 2024-01-07 13:33:24 (running for 00:40:16.85)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.958 |      0.356 |                   97 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.429 |      0.174 |                   23 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.874 |      0.233 |                   18 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.653 |      0.169 |                   17 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=36467)[0m top1: 0.42677238805970147
[2m[36m(func pid=36467)[0m top5: 0.8861940298507462
[2m[36m(func pid=36467)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=36467)[0m f1_macro: 0.3541481240696908
[2m[36m(func pid=36467)[0m f1_weighted: 0.43259008542308197
[2m[36m(func pid=36467)[0m f1_per_class: [0.32, 0.514, 0.369, 0.402, 0.094, 0.081, 0.571, 0.543, 0.185, 0.462]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.15111940298507462
[2m[36m(func pid=56437)[0m top5: 0.6576492537313433
[2m[36m(func pid=56437)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=56437)[0m f1_macro: 0.11730427413422581
[2m[36m(func pid=56437)[0m f1_weighted: 0.1283572415237703
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.314, 0.0, 0.0, 0.044, 0.032, 0.132, 0.46, 0.106, 0.085]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.23367537313432835
[2m[36m(func pid=56305)[0m top5: 0.820429104477612
[2m[36m(func pid=56305)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=56305)[0m f1_macro: 0.22936490182642202
[2m[36m(func pid=56305)[0m f1_weighted: 0.1724523086506673
[2m[36m(func pid=56305)[0m f1_per_class: [0.385, 0.445, 0.381, 0.075, 0.131, 0.312, 0.012, 0.333, 0.116, 0.103]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4688 | Steps: 4 | Val loss: 2.1750 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.0621 | Steps: 4 | Val loss: 1.8721 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.3802 | Steps: 4 | Val loss: 3.0080 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6061 | Steps: 4 | Val loss: 2.2476 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:33:30 (running for 00:40:22.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.502 |      0.354 |                   98 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.469 |      0.168 |                   24 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.517 |      0.229 |                   19 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.427 |      0.117 |                   18 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.23740671641791045
[2m[36m(func pid=54942)[0m top5: 0.7835820895522388
[2m[36m(func pid=54942)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=54942)[0m f1_macro: 0.16801536845499707
[2m[36m(func pid=54942)[0m f1_weighted: 0.16703782431528966
[2m[36m(func pid=54942)[0m f1_per_class: [0.091, 0.472, 0.152, 0.1, 0.098, 0.23, 0.006, 0.415, 0.048, 0.069]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.43330223880597013
[2m[36m(func pid=36467)[0m top5: 0.8913246268656716
[2m[36m(func pid=36467)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36467)[0m f1_macro: 0.3606070746835718
[2m[36m(func pid=36467)[0m f1_weighted: 0.4484201067968326
[2m[36m(func pid=36467)[0m f1_per_class: [0.356, 0.504, 0.316, 0.453, 0.09, 0.127, 0.567, 0.527, 0.181, 0.486]
[2m[36m(func pid=36467)[0m 
[2m[36m(func pid=56437)[0m top1: 0.23134328358208955
[2m[36m(func pid=56437)[0m top5: 0.7532649253731343
[2m[36m(func pid=56437)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=56437)[0m f1_macro: 0.16487550055910322
[2m[36m(func pid=56437)[0m f1_weighted: 0.1853582144599822
[2m[36m(func pid=56437)[0m f1_per_class: [0.118, 0.482, 0.095, 0.016, 0.025, 0.284, 0.119, 0.392, 0.116, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.20662313432835822
[2m[36m(func pid=56305)[0m top5: 0.7947761194029851
[2m[36m(func pid=56305)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=56305)[0m f1_macro: 0.19753988876937922
[2m[36m(func pid=56305)[0m f1_weighted: 0.12582120923069326
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.452, 0.615, 0.003, 0.169, 0.102, 0.0, 0.433, 0.144, 0.057]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4652 | Steps: 4 | Val loss: 2.1673 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=36467)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8758 | Steps: 4 | Val loss: 1.8988 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.1528 | Steps: 4 | Val loss: 3.0569 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6211 | Steps: 4 | Val loss: 1.9111 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:33:35 (running for 00:40:27.68)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00016 | RUNNING    | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  1.062 |      0.361 |                   99 |
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.465 |      0.186 |                   25 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.606 |      0.198 |                   20 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.38  |      0.165 |                   19 |
| train_52b21_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2416044776119403
[2m[36m(func pid=54942)[0m top5: 0.7807835820895522
[2m[36m(func pid=54942)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=54942)[0m f1_macro: 0.1861793032029165
[2m[36m(func pid=54942)[0m f1_weighted: 0.17809483691464756
[2m[36m(func pid=54942)[0m f1_per_class: [0.103, 0.483, 0.152, 0.121, 0.096, 0.234, 0.003, 0.419, 0.122, 0.129]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=36467)[0m top1: 0.4253731343283582
[2m[36m(func pid=36467)[0m top5: 0.8936567164179104
[2m[36m(func pid=36467)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=36467)[0m f1_macro: 0.37575573049637734
[2m[36m(func pid=36467)[0m f1_weighted: 0.45805668910262887
[2m[36m(func pid=36467)[0m f1_per_class: [0.347, 0.48, 0.4, 0.442, 0.083, 0.339, 0.547, 0.476, 0.239, 0.404]
[2m[36m(func pid=56437)[0m top1: 0.2630597014925373
[2m[36m(func pid=56437)[0m top5: 0.847481343283582
[2m[36m(func pid=56437)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=56437)[0m f1_macro: 0.1881667601129273
[2m[36m(func pid=56437)[0m f1_weighted: 0.27711744826206086
[2m[36m(func pid=56437)[0m f1_per_class: [0.204, 0.094, 0.151, 0.431, 0.051, 0.243, 0.277, 0.431, 0.0, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.25513059701492535
[2m[36m(func pid=56305)[0m top5: 0.8395522388059702
[2m[36m(func pid=56305)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=56305)[0m f1_macro: 0.20663095862570763
[2m[36m(func pid=56305)[0m f1_weighted: 0.1693276023489361
[2m[36m(func pid=56305)[0m f1_per_class: [0.0, 0.468, 0.324, 0.01, 0.133, 0.347, 0.051, 0.348, 0.168, 0.217]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4912 | Steps: 4 | Val loss: 2.1654 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0037 | Steps: 4 | Val loss: 2.4574 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.5636 | Steps: 4 | Val loss: 1.8188 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=54942)[0m top1: 0.2392723880597015
[2m[36m(func pid=54942)[0m top5: 0.7742537313432836
[2m[36m(func pid=54942)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=54942)[0m f1_macro: 0.1937189834023755
[2m[36m(func pid=54942)[0m f1_weighted: 0.1756023586431117
[2m[36m(func pid=54942)[0m f1_per_class: [0.097, 0.496, 0.143, 0.104, 0.093, 0.202, 0.012, 0.405, 0.144, 0.242]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m top1: 0.2957089552238806
[2m[36m(func pid=56437)[0m top5: 0.824160447761194
[2m[36m(func pid=56437)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=56437)[0m f1_macro: 0.243625610307984
[2m[36m(func pid=56437)[0m f1_weighted: 0.2674411256914078
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.441, 0.667, 0.413, 0.079, 0.265, 0.045, 0.428, 0.098, 0.0]
[2m[36m(func pid=56305)[0m top1: 0.3138992537313433
[2m[36m(func pid=56305)[0m top5: 0.8484141791044776
[2m[36m(func pid=56305)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=56305)[0m f1_macro: 0.27689939088598803
[2m[36m(func pid=56305)[0m f1_weighted: 0.29855654518368774
[2m[36m(func pid=56305)[0m f1_per_class: [0.278, 0.491, 0.369, 0.152, 0.098, 0.286, 0.324, 0.472, 0.135, 0.163]
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4578 | Steps: 4 | Val loss: 2.1545 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:33:40 (running for 00:40:33.06)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.491 |      0.194 |                   26 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.621 |      0.207 |                   21 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.153 |      0.188 |                   20 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=61908)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=61908)[0m Configuration completed!
[2m[36m(func pid=61908)[0m New optimizer parameters:
[2m[36m(func pid=61908)[0m SGD (
[2m[36m(func pid=61908)[0m Parameter Group 0
[2m[36m(func pid=61908)[0m     dampening: 0
[2m[36m(func pid=61908)[0m     differentiable: False
[2m[36m(func pid=61908)[0m     foreach: None
[2m[36m(func pid=61908)[0m     lr: 0.1
[2m[36m(func pid=61908)[0m     maximize: False
[2m[36m(func pid=61908)[0m     momentum: 0.9
[2m[36m(func pid=61908)[0m     nesterov: False
[2m[36m(func pid=61908)[0m     weight_decay: 1e-05
[2m[36m(func pid=61908)[0m )
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.23740671641791045
[2m[36m(func pid=54942)[0m top5: 0.7770522388059702
[2m[36m(func pid=54942)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=54942)[0m f1_macro: 0.18363384470555694
[2m[36m(func pid=54942)[0m f1_weighted: 0.16199731718243698
[2m[36m(func pid=54942)[0m f1_per_class: [0.106, 0.481, 0.156, 0.073, 0.107, 0.216, 0.009, 0.364, 0.124, 0.2]
[2m[36m(func pid=54942)[0m 
== Status ==
Current time: 2024-01-07 13:33:46 (running for 00:40:38.73)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.458 |      0.184 |                   27 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.564 |      0.277 |                   22 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.004 |      0.244 |                   21 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.6206 | Steps: 4 | Val loss: 1.8886 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8049 | Steps: 4 | Val loss: 2.9741 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4112 | Steps: 4 | Val loss: 2.1433 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.9750 | Steps: 4 | Val loss: 4692.9580 | Batch size: 32 | lr: 0.1 | Duration: 5.04s
[2m[36m(func pid=56305)[0m top1: 0.355410447761194
[2m[36m(func pid=56305)[0m top5: 0.855410447761194
[2m[36m(func pid=56305)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=56305)[0m f1_macro: 0.3157020374322864
[2m[36m(func pid=56305)[0m f1_weighted: 0.4045185669706295
[2m[36m(func pid=56305)[0m f1_per_class: [0.22, 0.469, 0.606, 0.409, 0.081, 0.286, 0.462, 0.507, 0.0, 0.118]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m top1: 0.3041044776119403
[2m[36m(func pid=56437)[0m top5: 0.804570895522388
[2m[36m(func pid=56437)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=56437)[0m f1_macro: 0.18258930726132705
[2m[36m(func pid=56437)[0m f1_weighted: 0.27941548064237787
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.46, 0.0, 0.182, 0.071, 0.389, 0.274, 0.316, 0.134, 0.0]
[2m[36m(func pid=56437)[0m 
== Status ==
Current time: 2024-01-07 13:33:51 (running for 00:40:44.13)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.411 |      0.188 |                   28 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.621 |      0.316 |                   23 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.805 |      0.183 |                   22 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2490671641791045
[2m[36m(func pid=54942)[0m top5: 0.7793843283582089
[2m[36m(func pid=54942)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=54942)[0m f1_macro: 0.1878233211872867
[2m[36m(func pid=54942)[0m f1_weighted: 0.18037540710333622
[2m[36m(func pid=54942)[0m f1_per_class: [0.111, 0.491, 0.151, 0.142, 0.146, 0.224, 0.003, 0.333, 0.133, 0.143]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.17210820895522388
[2m[36m(func pid=61908)[0m top5: 0.5727611940298507
[2m[36m(func pid=61908)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=61908)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=61908)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7463 | Steps: 4 | Val loss: 3.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3846 | Steps: 4 | Val loss: 1.7515 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3338 | Steps: 4 | Val loss: 2.1320 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 12.3183 | Steps: 4 | Val loss: 787125.3125 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=56437)[0m top1: 0.20615671641791045
[2m[36m(func pid=56437)[0m top5: 0.7933768656716418
[2m[36m(func pid=56437)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=56437)[0m f1_macro: 0.16873855717997746
[2m[36m(func pid=56437)[0m f1_weighted: 0.1606962743776041
[2m[36m(func pid=56437)[0m f1_per_class: [0.189, 0.34, 0.435, 0.104, 0.092, 0.314, 0.092, 0.0, 0.065, 0.057]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3978544776119403
[2m[36m(func pid=56305)[0m top5: 0.8819962686567164
[2m[36m(func pid=56305)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=56305)[0m f1_macro: 0.325812834546651
[2m[36m(func pid=56305)[0m f1_weighted: 0.42401376225054227
[2m[36m(func pid=56305)[0m f1_per_class: [0.242, 0.319, 0.571, 0.548, 0.071, 0.321, 0.462, 0.532, 0.0, 0.191]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:33:57 (running for 00:40:49.65)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.334 |      0.193 |                   29 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.385 |      0.326 |                   24 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.746 |      0.169 |                   23 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  4.975 |      0.029 |                    1 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.25886194029850745
[2m[36m(func pid=54942)[0m top5: 0.7868470149253731
[2m[36m(func pid=54942)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=54942)[0m f1_macro: 0.19300320752243685
[2m[36m(func pid=54942)[0m f1_weighted: 0.1963412658648296
[2m[36m(func pid=54942)[0m f1_per_class: [0.106, 0.497, 0.178, 0.177, 0.151, 0.289, 0.003, 0.335, 0.077, 0.118]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.006063432835820896
[2m[36m(func pid=61908)[0m top5: 0.5093283582089553
[2m[36m(func pid=61908)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=61908)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=61908)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7500 | Steps: 4 | Val loss: 3.9633 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.4163 | Steps: 4 | Val loss: 1.7226 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4369 | Steps: 4 | Val loss: 2.1279 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=56437)[0m top1: 0.1501865671641791
[2m[36m(func pid=56437)[0m top5: 0.6581156716417911
[2m[36m(func pid=56437)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=56437)[0m f1_macro: 0.18887937423385526
[2m[36m(func pid=56437)[0m f1_weighted: 0.10669058010073429
[2m[36m(func pid=56437)[0m f1_per_class: [0.109, 0.208, 0.69, 0.0, 0.063, 0.288, 0.009, 0.447, 0.074, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 5.8412 | Steps: 4 | Val loss: 82837.9141 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=56305)[0m top1: 0.42117537313432835
[2m[36m(func pid=56305)[0m top5: 0.8824626865671642
[2m[36m(func pid=56305)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=56305)[0m f1_macro: 0.3386385596127766
[2m[36m(func pid=56305)[0m f1_weighted: 0.45271205520299956
[2m[36m(func pid=56305)[0m f1_per_class: [0.262, 0.412, 0.55, 0.542, 0.08, 0.316, 0.51, 0.542, 0.0, 0.173]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:34:02 (running for 00:40:55.02)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.437 |      0.212 |                   30 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.416 |      0.339 |                   25 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.75  |      0.189 |                   24 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  | 12.318 |      0.001 |                    2 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2653917910447761
[2m[36m(func pid=54942)[0m top5: 0.7863805970149254
[2m[36m(func pid=54942)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=54942)[0m f1_macro: 0.21228445662014264
[2m[36m(func pid=54942)[0m f1_weighted: 0.1995724348831591
[2m[36m(func pid=54942)[0m f1_per_class: [0.151, 0.483, 0.258, 0.173, 0.157, 0.283, 0.015, 0.378, 0.047, 0.178]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.03311567164179104
[2m[36m(func pid=61908)[0m top5: 0.5149253731343284
[2m[36m(func pid=61908)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=61908)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=61908)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5714 | Steps: 4 | Val loss: 3.2813 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.5512 | Steps: 4 | Val loss: 1.7873 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3098 | Steps: 4 | Val loss: 2.1020 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56437)[0m top1: 0.2224813432835821
[2m[36m(func pid=56437)[0m top5: 0.6632462686567164
[2m[36m(func pid=56437)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=56437)[0m f1_macro: 0.19023965928025455
[2m[36m(func pid=56437)[0m f1_weighted: 0.15742581855078536
[2m[36m(func pid=56437)[0m f1_per_class: [0.121, 0.513, 0.333, 0.0, 0.065, 0.328, 0.0, 0.385, 0.12, 0.037]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 8.9920 | Steps: 4 | Val loss: 54402.8164 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=56305)[0m top1: 0.384794776119403
[2m[36m(func pid=56305)[0m top5: 0.8642723880597015
[2m[36m(func pid=56305)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=56305)[0m f1_macro: 0.30012727936341077
[2m[36m(func pid=56305)[0m f1_weighted: 0.40859393306495284
[2m[36m(func pid=56305)[0m f1_per_class: [0.314, 0.531, 0.279, 0.351, 0.074, 0.323, 0.48, 0.5, 0.0, 0.149]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:34:08 (running for 00:41:00.59)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.31  |      0.223 |                   31 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.551 |      0.3   |                   26 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.571 |      0.19  |                   25 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  5.841 |      0.006 |                    3 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.27425373134328357
[2m[36m(func pid=54942)[0m top5: 0.8064365671641791
[2m[36m(func pid=54942)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=54942)[0m f1_macro: 0.22342286844403833
[2m[36m(func pid=54942)[0m f1_weighted: 0.19828836801714272
[2m[36m(func pid=54942)[0m f1_per_class: [0.188, 0.47, 0.319, 0.127, 0.14, 0.304, 0.042, 0.406, 0.072, 0.167]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.03311567164179104
[2m[36m(func pid=61908)[0m top5: 0.5149253731343284
[2m[36m(func pid=61908)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=61908)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=61908)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1645 | Steps: 4 | Val loss: 2.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3292 | Steps: 4 | Val loss: 2.2737 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4317 | Steps: 4 | Val loss: 2.1005 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=56437)[0m top1: 0.34468283582089554
[2m[36m(func pid=56437)[0m top5: 0.832089552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=56437)[0m f1_macro: 0.2284302928256726
[2m[36m(func pid=56437)[0m f1_weighted: 0.3725055232189272
[2m[36m(func pid=56437)[0m f1_per_class: [0.171, 0.535, 0.0, 0.517, 0.049, 0.179, 0.281, 0.432, 0.082, 0.039]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.24720149253731344
[2m[36m(func pid=56305)[0m top5: 0.8423507462686567
[2m[36m(func pid=56305)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=56305)[0m f1_macro: 0.20655672668542424
[2m[36m(func pid=56305)[0m f1_weighted: 0.2242346980248695
[2m[36m(func pid=56305)[0m f1_per_class: [0.044, 0.483, 0.5, 0.075, 0.111, 0.257, 0.228, 0.297, 0.0, 0.07]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 6.2666 | Steps: 4 | Val loss: 2234.9553 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:34:13 (running for 00:41:05.97)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.432 |      0.219 |                   32 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.329 |      0.207 |                   27 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.164 |      0.228 |                   26 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  8.992 |      0.006 |                    4 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2677238805970149
[2m[36m(func pid=54942)[0m top5: 0.808768656716418
[2m[36m(func pid=54942)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=54942)[0m f1_macro: 0.21868773483072784
[2m[36m(func pid=54942)[0m f1_weighted: 0.19134160344889237
[2m[36m(func pid=54942)[0m f1_per_class: [0.169, 0.462, 0.344, 0.099, 0.138, 0.281, 0.06, 0.414, 0.053, 0.167]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.036380597014925374
[2m[36m(func pid=61908)[0m top5: 0.5209888059701493
[2m[36m(func pid=61908)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=61908)[0m f1_macro: 0.009358824680627069
[2m[36m(func pid=61908)[0m f1_weighted: 0.010155761252852627
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.0, 0.029, 0.0, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8412 | Steps: 4 | Val loss: 3.0202 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4100 | Steps: 4 | Val loss: 1.9727 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2981 | Steps: 4 | Val loss: 2.0790 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=56437)[0m top1: 0.292910447761194
[2m[36m(func pid=56437)[0m top5: 0.8264925373134329
[2m[36m(func pid=56437)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=56437)[0m f1_macro: 0.12643362429581542
[2m[36m(func pid=56437)[0m f1_weighted: 0.31639913847184886
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.113, 0.0, 0.454, 0.036, 0.015, 0.555, 0.0, 0.092, 0.0]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.31576492537313433
[2m[36m(func pid=56305)[0m top5: 0.8498134328358209
[2m[36m(func pid=56305)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=56305)[0m f1_macro: 0.25933842081164804
[2m[36m(func pid=56305)[0m f1_weighted: 0.28562229208988504
[2m[36m(func pid=56305)[0m f1_per_class: [0.044, 0.472, 0.558, 0.04, 0.103, 0.342, 0.392, 0.532, 0.0, 0.108]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.5285 | Steps: 4 | Val loss: 1078.7291 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:34:19 (running for 00:41:11.39)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.298 |      0.228 |                   33 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.41  |      0.259 |                   28 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.841 |      0.126 |                   27 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  6.267 |      0.009 |                    5 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.28078358208955223
[2m[36m(func pid=54942)[0m top5: 0.8185634328358209
[2m[36m(func pid=54942)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=54942)[0m f1_macro: 0.22817045453123774
[2m[36m(func pid=54942)[0m f1_weighted: 0.21413601177805597
[2m[36m(func pid=54942)[0m f1_per_class: [0.173, 0.477, 0.383, 0.168, 0.129, 0.274, 0.068, 0.416, 0.027, 0.167]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6712 | Steps: 4 | Val loss: 2.5159 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=61908)[0m top1: 0.008395522388059701
[2m[36m(func pid=61908)[0m top5: 0.5209888059701493
[2m[36m(func pid=61908)[0m f1_micro: 0.008395522388059701
[2m[36m(func pid=61908)[0m f1_macro: 0.023573573573573574
[2m[36m(func pid=61908)[0m f1_weighted: 0.0017403130742682984
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.014, 0.0, 0.222, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.2468 | Steps: 4 | Val loss: 1.9573 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4611 | Steps: 4 | Val loss: 2.0820 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56437)[0m top1: 0.22201492537313433
[2m[36m(func pid=56437)[0m top5: 0.8138992537313433
[2m[36m(func pid=56437)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=56437)[0m f1_macro: 0.1676036904931683
[2m[36m(func pid=56437)[0m f1_weighted: 0.2721011426746257
[2m[36m(func pid=56437)[0m f1_per_class: [0.0, 0.228, 0.0, 0.247, 0.045, 0.205, 0.383, 0.354, 0.139, 0.075]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3069029850746269
[2m[36m(func pid=56305)[0m top5: 0.8414179104477612
[2m[36m(func pid=56305)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=56305)[0m f1_macro: 0.2998061394194612
[2m[36m(func pid=56305)[0m f1_weighted: 0.2732672390234327
[2m[36m(func pid=56305)[0m f1_per_class: [0.39, 0.473, 0.462, 0.087, 0.111, 0.334, 0.272, 0.511, 0.143, 0.215]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.7319 | Steps: 4 | Val loss: 101.7210 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 13:34:24 (running for 00:41:16.89)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.461 |      0.213 |                   34 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.247 |      0.3   |                   29 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.671 |      0.168 |                   28 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.528 |      0.024 |                    6 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.28451492537313433
[2m[36m(func pid=54942)[0m top5: 0.8134328358208955
[2m[36m(func pid=54942)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=54942)[0m f1_macro: 0.21253912691755367
[2m[36m(func pid=54942)[0m f1_weighted: 0.22726232469903285
[2m[36m(func pid=54942)[0m f1_per_class: [0.17, 0.472, 0.235, 0.194, 0.12, 0.257, 0.099, 0.433, 0.027, 0.118]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5743 | Steps: 4 | Val loss: 2.5971 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=61908)[0m top1: 0.012593283582089552
[2m[36m(func pid=61908)[0m top5: 0.394589552238806
[2m[36m(func pid=61908)[0m f1_micro: 0.012593283582089552
[2m[36m(func pid=61908)[0m f1_macro: 0.009544315585227567
[2m[36m(func pid=61908)[0m f1_weighted: 0.00966532605125686
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.026, 0.019, 0.0, 0.012, 0.023, 0.006, 0.009, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.3746 | Steps: 4 | Val loss: 2.3255 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3707 | Steps: 4 | Val loss: 2.0796 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=56437)[0m top1: 0.1310634328358209
[2m[36m(func pid=56437)[0m top5: 0.8027052238805971
[2m[36m(func pid=56437)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=56437)[0m f1_macro: 0.1620393396253536
[2m[36m(func pid=56437)[0m f1_weighted: 0.1308651288471956
[2m[36m(func pid=56437)[0m f1_per_class: [0.184, 0.261, 0.296, 0.062, 0.0, 0.133, 0.051, 0.504, 0.092, 0.037]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.25466417910447764
[2m[36m(func pid=56305)[0m top5: 0.8297574626865671
[2m[36m(func pid=56305)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=56305)[0m f1_macro: 0.24783535251450123
[2m[36m(func pid=56305)[0m f1_weighted: 0.24592598718860026
[2m[36m(func pid=56305)[0m f1_per_class: [0.378, 0.505, 0.304, 0.185, 0.093, 0.193, 0.154, 0.41, 0.116, 0.14]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.0232 | Steps: 4 | Val loss: 139.0947 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 13:34:30 (running for 00:41:22.41)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.371 |      0.201 |                   35 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.375 |      0.248 |                   30 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.574 |      0.162 |                   29 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.732 |      0.01  |                    7 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.28451492537313433
[2m[36m(func pid=54942)[0m top5: 0.8190298507462687
[2m[36m(func pid=54942)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=54942)[0m f1_macro: 0.2011839913458109
[2m[36m(func pid=54942)[0m f1_weighted: 0.23767004280673218
[2m[36m(func pid=54942)[0m f1_per_class: [0.157, 0.497, 0.174, 0.26, 0.096, 0.216, 0.086, 0.398, 0.0, 0.129]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6822 | Steps: 4 | Val loss: 3.2337 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=61908)[0m top1: 0.21828358208955223
[2m[36m(func pid=61908)[0m top5: 0.4771455223880597
[2m[36m(func pid=61908)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=61908)[0m f1_macro: 0.06430067253247979
[2m[36m(func pid=61908)[0m f1_weighted: 0.16092745056101285
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.005, 0.029, 0.0, 0.052, 0.0, 0.532, 0.0, 0.024, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6967 | Steps: 4 | Val loss: 1.8884 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.3158 | Steps: 4 | Val loss: 2.0525 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=56437)[0m top1: 0.17957089552238806
[2m[36m(func pid=56437)[0m top5: 0.738339552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=56437)[0m f1_macro: 0.16370464528504675
[2m[36m(func pid=56437)[0m f1_weighted: 0.1464513434020347
[2m[36m(func pid=56437)[0m f1_per_class: [0.13, 0.074, 0.117, 0.216, 0.098, 0.307, 0.006, 0.495, 0.075, 0.119]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.292910447761194
[2m[36m(func pid=56305)[0m top5: 0.8833955223880597
[2m[36m(func pid=56305)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=56305)[0m f1_macro: 0.27058924606291024
[2m[36m(func pid=56305)[0m f1_weighted: 0.30251079988257407
[2m[36m(func pid=56305)[0m f1_per_class: [0.32, 0.472, 0.131, 0.235, 0.077, 0.303, 0.274, 0.373, 0.169, 0.351]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.2291 | Steps: 4 | Val loss: 58.6010 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 13:34:35 (running for 00:41:27.89)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.316 |      0.211 |                   36 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.697 |      0.271 |                   31 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.682 |      0.164 |                   30 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  4.023 |      0.064 |                    8 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.29011194029850745
[2m[36m(func pid=54942)[0m top5: 0.8381529850746269
[2m[36m(func pid=54942)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=54942)[0m f1_macro: 0.21095775484965876
[2m[36m(func pid=54942)[0m f1_weighted: 0.24396846284922935
[2m[36m(func pid=54942)[0m f1_per_class: [0.188, 0.521, 0.194, 0.346, 0.103, 0.227, 0.018, 0.32, 0.0, 0.194]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0642 | Steps: 4 | Val loss: 2.8153 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=61908)[0m top1: 0.21361940298507462
[2m[36m(func pid=61908)[0m top5: 0.632929104477612
[2m[36m(func pid=61908)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=61908)[0m f1_macro: 0.06560207684025846
[2m[36m(func pid=61908)[0m f1_weighted: 0.16618685258872104
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.071, 0.0, 0.555, 0.0, 0.0, 0.03]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4357 | Steps: 4 | Val loss: 1.8471 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3303 | Steps: 4 | Val loss: 2.0335 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.21735074626865672
[2m[36m(func pid=56437)[0m top5: 0.800839552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=56437)[0m f1_macro: 0.15899365815568095
[2m[36m(func pid=56437)[0m f1_weighted: 0.18889443024701383
[2m[36m(func pid=56437)[0m f1_per_class: [0.14, 0.026, 0.0, 0.403, 0.084, 0.301, 0.012, 0.46, 0.086, 0.077]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.31902985074626866
[2m[36m(func pid=56305)[0m top5: 0.9029850746268657
[2m[36m(func pid=56305)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=56305)[0m f1_macro: 0.2784475973189643
[2m[36m(func pid=56305)[0m f1_weighted: 0.3462466021454876
[2m[36m(func pid=56305)[0m f1_per_class: [0.253, 0.453, 0.161, 0.362, 0.068, 0.252, 0.334, 0.417, 0.108, 0.375]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.7010 | Steps: 4 | Val loss: 6.7194 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:34:41 (running for 00:41:33.47)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.33  |      0.219 |                   37 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.436 |      0.278 |                   32 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.064 |      0.159 |                   31 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.229 |      0.066 |                    9 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.29850746268656714
[2m[36m(func pid=54942)[0m top5: 0.847481343283582
[2m[36m(func pid=54942)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=54942)[0m f1_macro: 0.21939549962799726
[2m[36m(func pid=54942)[0m f1_weighted: 0.26228458463913534
[2m[36m(func pid=54942)[0m f1_per_class: [0.204, 0.521, 0.218, 0.411, 0.081, 0.187, 0.03, 0.327, 0.0, 0.214]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9456 | Steps: 4 | Val loss: 2.8954 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=61908)[0m top1: 0.08675373134328358
[2m[36m(func pid=61908)[0m top5: 0.6268656716417911
[2m[36m(func pid=61908)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=61908)[0m f1_macro: 0.05075162579230874
[2m[36m(func pid=61908)[0m f1_weighted: 0.04399925855523519
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.079, 0.0, 0.028, 0.0, 0.09, 0.246, 0.064, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2564 | Steps: 4 | Val loss: 1.9748 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2704 | Steps: 4 | Val loss: 2.0121 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m top1: 0.16511194029850745
[2m[36m(func pid=56437)[0m top5: 0.7779850746268657
[2m[36m(func pid=56437)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=56437)[0m f1_macro: 0.13049379873566036
[2m[36m(func pid=56437)[0m f1_weighted: 0.1915710388022256
[2m[36m(func pid=56437)[0m f1_per_class: [0.156, 0.093, 0.0, 0.409, 0.038, 0.038, 0.125, 0.186, 0.117, 0.143]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.34095149253731344
[2m[36m(func pid=56305)[0m top5: 0.8824626865671642
[2m[36m(func pid=56305)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=56305)[0m f1_macro: 0.301032852616648
[2m[36m(func pid=56305)[0m f1_weighted: 0.38872700025879464
[2m[36m(func pid=56305)[0m f1_per_class: [0.222, 0.445, 0.262, 0.408, 0.085, 0.321, 0.391, 0.535, 0.155, 0.187]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 5.0998 | Steps: 4 | Val loss: 3.2055 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:34:46 (running for 00:41:38.79)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.27  |      0.226 |                   38 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.256 |      0.301 |                   33 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.946 |      0.13  |                   32 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.701 |      0.051 |                   10 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.30970149253731344
[2m[36m(func pid=54942)[0m top5: 0.8521455223880597
[2m[36m(func pid=54942)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=54942)[0m f1_macro: 0.22553338788144855
[2m[36m(func pid=54942)[0m f1_weighted: 0.2779038872779717
[2m[36m(func pid=54942)[0m f1_per_class: [0.248, 0.534, 0.209, 0.445, 0.08, 0.17, 0.05, 0.312, 0.0, 0.207]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8388 | Steps: 4 | Val loss: 2.9721 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=61908)[0m top1: 0.11986940298507463
[2m[36m(func pid=61908)[0m top5: 0.7952425373134329
[2m[36m(func pid=61908)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=61908)[0m f1_macro: 0.08090650664878135
[2m[36m(func pid=61908)[0m f1_weighted: 0.07039713642474109
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.296, 0.179, 0.003, 0.0, 0.0, 0.0, 0.262, 0.068, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3112 | Steps: 4 | Val loss: 1.7293 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3093 | Steps: 4 | Val loss: 2.0063 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56437)[0m top1: 0.14319029850746268
[2m[36m(func pid=56437)[0m top5: 0.7551305970149254
[2m[36m(func pid=56437)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=56437)[0m f1_macro: 0.10367699751015133
[2m[36m(func pid=56437)[0m f1_weighted: 0.17471102265952293
[2m[36m(func pid=56437)[0m f1_per_class: [0.113, 0.123, 0.0, 0.193, 0.031, 0.081, 0.276, 0.016, 0.106, 0.098]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.4076492537313433
[2m[36m(func pid=56305)[0m top5: 0.9043843283582089
[2m[36m(func pid=56305)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=56305)[0m f1_macro: 0.35597731206044825
[2m[36m(func pid=56305)[0m f1_weighted: 0.4428791756665418
[2m[36m(func pid=56305)[0m f1_per_class: [0.283, 0.503, 0.392, 0.503, 0.078, 0.354, 0.429, 0.493, 0.205, 0.319]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.3687 | Steps: 4 | Val loss: 2.1164 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:34:51 (running for 00:41:44.17)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.309 |      0.227 |                   39 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.311 |      0.356 |                   34 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.839 |      0.104 |                   33 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  5.1   |      0.081 |                   11 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3050373134328358
[2m[36m(func pid=54942)[0m top5: 0.8488805970149254
[2m[36m(func pid=54942)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=54942)[0m f1_macro: 0.226958845693854
[2m[36m(func pid=54942)[0m f1_weighted: 0.27730363746865605
[2m[36m(func pid=54942)[0m f1_per_class: [0.24, 0.525, 0.212, 0.443, 0.077, 0.154, 0.058, 0.324, 0.0, 0.235]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.9446 | Steps: 4 | Val loss: 2.9093 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=61908)[0m top1: 0.2798507462686567
[2m[36m(func pid=61908)[0m top5: 0.8111007462686567
[2m[36m(func pid=61908)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=61908)[0m f1_macro: 0.17822790453019927
[2m[36m(func pid=61908)[0m f1_weighted: 0.21147589021074537
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.466, 0.558, 0.389, 0.0, 0.0, 0.0, 0.29, 0.08, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.1376 | Steps: 4 | Val loss: 1.6717 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2365 | Steps: 4 | Val loss: 1.9832 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=56437)[0m top1: 0.1791044776119403
[2m[36m(func pid=56437)[0m top5: 0.7607276119402985
[2m[36m(func pid=56437)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=56437)[0m f1_macro: 0.1856891510790285
[2m[36m(func pid=56437)[0m f1_weighted: 0.2001633265123732
[2m[36m(func pid=56437)[0m f1_per_class: [0.093, 0.242, 0.186, 0.134, 0.114, 0.257, 0.193, 0.454, 0.09, 0.093]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.42257462686567165
[2m[36m(func pid=56305)[0m top5: 0.9011194029850746
[2m[36m(func pid=56305)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=56305)[0m f1_macro: 0.38088538310578374
[2m[36m(func pid=56305)[0m f1_weighted: 0.44848434838771506
[2m[36m(func pid=56305)[0m f1_per_class: [0.381, 0.559, 0.564, 0.464, 0.077, 0.341, 0.463, 0.395, 0.207, 0.356]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.5525 | Steps: 4 | Val loss: 2.0874 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:34:57 (running for 00:41:49.64)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.237 |      0.258 |                   40 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.138 |      0.381 |                   35 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.945 |      0.186 |                   34 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.369 |      0.178 |                   12 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.33115671641791045
[2m[36m(func pid=54942)[0m top5: 0.855410447761194
[2m[36m(func pid=54942)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=54942)[0m f1_macro: 0.25805286947660805
[2m[36m(func pid=54942)[0m f1_weighted: 0.3134513754794988
[2m[36m(func pid=54942)[0m f1_per_class: [0.297, 0.51, 0.293, 0.493, 0.063, 0.268, 0.087, 0.362, 0.0, 0.207]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7205 | Steps: 4 | Val loss: 4.8039 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2586 | Steps: 4 | Val loss: 1.7403 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=61908)[0m top1: 0.2042910447761194
[2m[36m(func pid=61908)[0m top5: 0.7765858208955224
[2m[36m(func pid=61908)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=61908)[0m f1_macro: 0.0968671870272663
[2m[36m(func pid=61908)[0m f1_weighted: 0.1266845350594572
[2m[36m(func pid=61908)[0m f1_per_class: [0.062, 0.456, 0.0, 0.103, 0.0, 0.0, 0.0, 0.304, 0.0, 0.043]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.1914 | Steps: 4 | Val loss: 1.9854 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=56437)[0m top1: 0.07276119402985075
[2m[36m(func pid=56437)[0m top5: 0.6175373134328358
[2m[36m(func pid=56437)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=56437)[0m f1_macro: 0.1509183129450494
[2m[36m(func pid=56437)[0m f1_weighted: 0.06394651262775011
[2m[36m(func pid=56437)[0m f1_per_class: [0.087, 0.032, 0.541, 0.023, 0.028, 0.0, 0.048, 0.476, 0.078, 0.196]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3885261194029851
[2m[36m(func pid=56305)[0m top5: 0.8875932835820896
[2m[36m(func pid=56305)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=56305)[0m f1_macro: 0.32294246935609505
[2m[36m(func pid=56305)[0m f1_weighted: 0.40193542851624237
[2m[36m(func pid=56305)[0m f1_per_class: [0.368, 0.51, 0.296, 0.313, 0.077, 0.351, 0.511, 0.216, 0.242, 0.345]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5902 | Steps: 4 | Val loss: 2.0845 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:35:02 (running for 00:41:55.10)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.191 |      0.264 |                   41 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.259 |      0.323 |                   36 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.721 |      0.151 |                   35 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  3.552 |      0.097 |                   13 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.33722014925373134
[2m[36m(func pid=54942)[0m top5: 0.8535447761194029
[2m[36m(func pid=54942)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=54942)[0m f1_macro: 0.2639546565621931
[2m[36m(func pid=54942)[0m f1_weighted: 0.32075956804702216
[2m[36m(func pid=54942)[0m f1_per_class: [0.309, 0.51, 0.282, 0.491, 0.056, 0.252, 0.107, 0.417, 0.0, 0.214]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1760 | Steps: 4 | Val loss: 3.3051 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.5803 | Steps: 4 | Val loss: 1.8060 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=61908)[0m top1: 0.1669776119402985
[2m[36m(func pid=61908)[0m top5: 0.7966417910447762
[2m[36m(func pid=61908)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=61908)[0m f1_macro: 0.09623401965107045
[2m[36m(func pid=61908)[0m f1_weighted: 0.10634551705948714
[2m[36m(func pid=61908)[0m f1_per_class: [0.106, 0.483, 0.0, 0.0, 0.0, 0.0, 0.006, 0.285, 0.083, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2488 | Steps: 4 | Val loss: 1.9930 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m top1: 0.16930970149253732
[2m[36m(func pid=56437)[0m top5: 0.8232276119402985
[2m[36m(func pid=56437)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=56437)[0m f1_macro: 0.20202232869465023
[2m[36m(func pid=56437)[0m f1_weighted: 0.1990460249090031
[2m[36m(func pid=56437)[0m f1_per_class: [0.122, 0.173, 0.4, 0.138, 0.031, 0.016, 0.291, 0.533, 0.124, 0.192]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.36007462686567165
[2m[36m(func pid=56305)[0m top5: 0.8927238805970149
[2m[36m(func pid=56305)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=56305)[0m f1_macro: 0.34916402989951606
[2m[36m(func pid=56305)[0m f1_weighted: 0.3954564126564333
[2m[36m(func pid=56305)[0m f1_per_class: [0.265, 0.462, 0.5, 0.339, 0.058, 0.287, 0.474, 0.441, 0.231, 0.434]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4670 | Steps: 4 | Val loss: 2.1030 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:35:08 (running for 00:42:00.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.249 |      0.256 |                   42 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.58  |      0.349 |                   37 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.176 |      0.202 |                   36 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.59  |      0.096 |                   14 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3208955223880597
[2m[36m(func pid=54942)[0m top5: 0.8563432835820896
[2m[36m(func pid=54942)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=54942)[0m f1_macro: 0.25601578487635546
[2m[36m(func pid=54942)[0m f1_weighted: 0.3126461935211127
[2m[36m(func pid=54942)[0m f1_per_class: [0.265, 0.485, 0.289, 0.48, 0.054, 0.236, 0.112, 0.432, 0.0, 0.207]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.0353 | Steps: 4 | Val loss: 2.2209 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6881 | Steps: 4 | Val loss: 1.8508 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=61908)[0m top1: 0.17350746268656717
[2m[36m(func pid=61908)[0m top5: 0.7444029850746269
[2m[36m(func pid=61908)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=61908)[0m f1_macro: 0.17162442118215768
[2m[36m(func pid=61908)[0m f1_weighted: 0.11004090036760064
[2m[36m(func pid=61908)[0m f1_per_class: [0.123, 0.469, 0.647, 0.0, 0.067, 0.008, 0.0, 0.329, 0.074, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2644 | Steps: 4 | Val loss: 1.9962 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.31529850746268656
[2m[36m(func pid=56437)[0m top5: 0.863339552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=56437)[0m f1_macro: 0.317848514452002
[2m[36m(func pid=56437)[0m f1_weighted: 0.32944031059140155
[2m[36m(func pid=56437)[0m f1_per_class: [0.172, 0.443, 0.632, 0.313, 0.096, 0.361, 0.271, 0.468, 0.165, 0.256]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3493470149253731
[2m[36m(func pid=56305)[0m top5: 0.9291044776119403
[2m[36m(func pid=56305)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=56305)[0m f1_macro: 0.3656844763156091
[2m[36m(func pid=56305)[0m f1_weighted: 0.3948038689953216
[2m[36m(func pid=56305)[0m f1_per_class: [0.27, 0.364, 0.69, 0.49, 0.058, 0.292, 0.384, 0.436, 0.206, 0.466]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6376 | Steps: 4 | Val loss: 2.1026 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 13:35:13 (running for 00:42:06.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.264 |      0.258 |                   43 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.688 |      0.366 |                   38 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.035 |      0.318 |                   37 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.467 |      0.172 |                   15 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3125
[2m[36m(func pid=54942)[0m top5: 0.8586753731343284
[2m[36m(func pid=54942)[0m f1_micro: 0.3125
[2m[36m(func pid=54942)[0m f1_macro: 0.2583555080090341
[2m[36m(func pid=54942)[0m f1_weighted: 0.3063742463787502
[2m[36m(func pid=54942)[0m f1_per_class: [0.257, 0.505, 0.361, 0.474, 0.046, 0.165, 0.107, 0.456, 0.0, 0.214]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8113 | Steps: 4 | Val loss: 1.9869 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4788 | Steps: 4 | Val loss: 1.7609 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=61908)[0m top1: 0.15205223880597016
[2m[36m(func pid=61908)[0m top5: 0.679570895522388
[2m[36m(func pid=61908)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=61908)[0m f1_macro: 0.18060736124523782
[2m[36m(func pid=61908)[0m f1_weighted: 0.16202496062155006
[2m[36m(func pid=61908)[0m f1_per_class: [0.038, 0.005, 0.688, 0.0, 0.023, 0.206, 0.348, 0.499, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4315 | Steps: 4 | Val loss: 1.9954 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m top1: 0.31763059701492535
[2m[36m(func pid=56437)[0m top5: 0.871268656716418
[2m[36m(func pid=56437)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=56437)[0m f1_macro: 0.23974992315465588
[2m[36m(func pid=56437)[0m f1_weighted: 0.2812231454132237
[2m[36m(func pid=56437)[0m f1_per_class: [0.224, 0.408, 0.0, 0.455, 0.09, 0.361, 0.006, 0.469, 0.19, 0.194]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.37080223880597013
[2m[36m(func pid=56305)[0m top5: 0.9440298507462687
[2m[36m(func pid=56305)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=56305)[0m f1_macro: 0.3896720714685815
[2m[36m(func pid=56305)[0m f1_weighted: 0.4011091610452737
[2m[36m(func pid=56305)[0m f1_per_class: [0.314, 0.391, 0.8, 0.527, 0.063, 0.306, 0.334, 0.534, 0.103, 0.525]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6231 | Steps: 4 | Val loss: 2.0612 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:35:19 (running for 00:42:11.75)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.432 |      0.26  |                   44 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.479 |      0.39  |                   39 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.811 |      0.24  |                   38 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.638 |      0.181 |                   16 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31716417910447764
[2m[36m(func pid=54942)[0m top5: 0.8698694029850746
[2m[36m(func pid=54942)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=54942)[0m f1_macro: 0.26049701575308354
[2m[36m(func pid=54942)[0m f1_weighted: 0.3303602607602423
[2m[36m(func pid=54942)[0m f1_per_class: [0.252, 0.473, 0.308, 0.462, 0.046, 0.198, 0.208, 0.445, 0.0, 0.214]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3574 | Steps: 4 | Val loss: 2.1153 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9936 | Steps: 4 | Val loss: 1.7462 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=61908)[0m top1: 0.21222014925373134
[2m[36m(func pid=61908)[0m top5: 0.6030783582089553
[2m[36m(func pid=61908)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=61908)[0m f1_macro: 0.1398486214839204
[2m[36m(func pid=61908)[0m f1_weighted: 0.1914393915301551
[2m[36m(func pid=61908)[0m f1_per_class: [0.04, 0.0, 0.609, 0.0, 0.024, 0.161, 0.565, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2364 | Steps: 4 | Val loss: 1.9849 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=56437)[0m top1: 0.2966417910447761
[2m[36m(func pid=56437)[0m top5: 0.835820895522388
[2m[36m(func pid=56437)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=56437)[0m f1_macro: 0.20229241660821962
[2m[36m(func pid=56437)[0m f1_weighted: 0.2358712616070102
[2m[36m(func pid=56437)[0m f1_per_class: [0.232, 0.524, 0.0, 0.271, 0.076, 0.309, 0.0, 0.448, 0.071, 0.091]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.38199626865671643
[2m[36m(func pid=56305)[0m top5: 0.9463619402985075
[2m[36m(func pid=56305)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=56305)[0m f1_macro: 0.3841649641850603
[2m[36m(func pid=56305)[0m f1_weighted: 0.40591428588234124
[2m[36m(func pid=56305)[0m f1_per_class: [0.278, 0.458, 0.667, 0.538, 0.086, 0.312, 0.293, 0.536, 0.219, 0.455]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5470 | Steps: 4 | Val loss: 2.0530 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 13:35:24 (running for 00:42:17.09)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.236 |      0.273 |                   45 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.994 |      0.384 |                   40 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.357 |      0.202 |                   39 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.623 |      0.14  |                   17 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31343283582089554
[2m[36m(func pid=54942)[0m top5: 0.8726679104477612
[2m[36m(func pid=54942)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=54942)[0m f1_macro: 0.2734414295709474
[2m[36m(func pid=54942)[0m f1_weighted: 0.33824667027827604
[2m[36m(func pid=54942)[0m f1_per_class: [0.249, 0.435, 0.333, 0.426, 0.051, 0.267, 0.261, 0.426, 0.027, 0.258]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6052 | Steps: 4 | Val loss: 2.2166 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.4388 | Steps: 4 | Val loss: 1.6093 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=61908)[0m top1: 0.20569029850746268
[2m[36m(func pid=61908)[0m top5: 0.6604477611940298
[2m[36m(func pid=61908)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=61908)[0m f1_macro: 0.1349161921326618
[2m[36m(func pid=61908)[0m f1_weighted: 0.17390704838703883
[2m[36m(func pid=61908)[0m f1_per_class: [0.065, 0.0, 0.552, 0.0, 0.107, 0.0, 0.558, 0.0, 0.068, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2897 | Steps: 4 | Val loss: 2.0000 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56437)[0m top1: 0.20102611940298507
[2m[36m(func pid=56437)[0m top5: 0.8362873134328358
[2m[36m(func pid=56437)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=56437)[0m f1_macro: 0.17444167845617858
[2m[36m(func pid=56437)[0m f1_weighted: 0.18092846005969426
[2m[36m(func pid=56437)[0m f1_per_class: [0.078, 0.185, 0.033, 0.276, 0.066, 0.368, 0.012, 0.325, 0.025, 0.375]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.41511194029850745
[2m[36m(func pid=56305)[0m top5: 0.9449626865671642
[2m[36m(func pid=56305)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=56305)[0m f1_macro: 0.40987454808696483
[2m[36m(func pid=56305)[0m f1_weighted: 0.4327295742091006
[2m[36m(func pid=56305)[0m f1_per_class: [0.304, 0.526, 0.87, 0.529, 0.086, 0.336, 0.352, 0.491, 0.167, 0.438]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:30 (running for 00:42:22.52)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.29  |      0.257 |                   46 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.439 |      0.41  |                   41 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.605 |      0.174 |                   40 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.547 |      0.135 |                   18 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.29850746268656714
[2m[36m(func pid=54942)[0m top5: 0.8582089552238806
[2m[36m(func pid=54942)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=54942)[0m f1_macro: 0.25684575655816116
[2m[36m(func pid=54942)[0m f1_weighted: 0.30740408772356115
[2m[36m(func pid=54942)[0m f1_per_class: [0.242, 0.483, 0.242, 0.377, 0.051, 0.247, 0.175, 0.464, 0.079, 0.207]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8884 | Steps: 4 | Val loss: 2.1607 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.8454 | Steps: 4 | Val loss: 2.1170 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3531 | Steps: 4 | Val loss: 1.7650 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=61908)[0m top1: 0.12639925373134328
[2m[36m(func pid=61908)[0m top5: 0.7229477611940298
[2m[36m(func pid=61908)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=61908)[0m f1_macro: 0.11921214958614453
[2m[36m(func pid=61908)[0m f1_weighted: 0.10750908690350505
[2m[36m(func pid=61908)[0m f1_per_class: [0.065, 0.0, 0.421, 0.0, 0.0, 0.0, 0.271, 0.374, 0.02, 0.04]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1892 | Steps: 4 | Val loss: 1.9839 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=56437)[0m top1: 0.3316231343283582
[2m[36m(func pid=56437)[0m top5: 0.8031716417910447
[2m[36m(func pid=56437)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=56437)[0m f1_macro: 0.26541008611210615
[2m[36m(func pid=56437)[0m f1_weighted: 0.3410136501141663
[2m[36m(func pid=56437)[0m f1_per_class: [0.245, 0.132, 0.036, 0.462, 0.088, 0.401, 0.341, 0.476, 0.148, 0.327]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3871268656716418
[2m[36m(func pid=56305)[0m top5: 0.9095149253731343
[2m[36m(func pid=56305)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=56305)[0m f1_macro: 0.33884898434723787
[2m[36m(func pid=56305)[0m f1_weighted: 0.3862091734042808
[2m[36m(func pid=56305)[0m f1_per_class: [0.383, 0.555, 0.558, 0.487, 0.114, 0.169, 0.31, 0.401, 0.133, 0.277]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:35 (running for 00:42:27.88)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.189 |      0.268 |                   47 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.353 |      0.339 |                   42 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.845 |      0.265 |                   41 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.888 |      0.119 |                   19 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31156716417910446
[2m[36m(func pid=54942)[0m top5: 0.8675373134328358
[2m[36m(func pid=54942)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=54942)[0m f1_macro: 0.2679411879793924
[2m[36m(func pid=54942)[0m f1_weighted: 0.3191083952411449
[2m[36m(func pid=54942)[0m f1_per_class: [0.215, 0.484, 0.267, 0.333, 0.062, 0.297, 0.241, 0.443, 0.051, 0.286]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8395 | Steps: 4 | Val loss: 2.0597 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3574 | Steps: 4 | Val loss: 2.2701 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0720 | Steps: 4 | Val loss: 2.8653 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2640 | Steps: 4 | Val loss: 1.9915 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=61908)[0m top1: 0.16324626865671643
[2m[36m(func pid=61908)[0m top5: 0.7873134328358209
[2m[36m(func pid=61908)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=61908)[0m f1_macro: 0.1678455372355781
[2m[36m(func pid=61908)[0m f1_weighted: 0.14251224101615306
[2m[36m(func pid=61908)[0m f1_per_class: [0.114, 0.375, 0.538, 0.195, 0.128, 0.0, 0.0, 0.284, 0.0, 0.043]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.28544776119402987
[2m[36m(func pid=56437)[0m top5: 0.7994402985074627
[2m[36m(func pid=56437)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=56437)[0m f1_macro: 0.21431744563308386
[2m[36m(func pid=56437)[0m f1_weighted: 0.3181117188121372
[2m[36m(func pid=56437)[0m f1_per_class: [0.213, 0.068, 0.238, 0.444, 0.056, 0.315, 0.446, 0.015, 0.127, 0.222]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.29011194029850745
[2m[36m(func pid=56305)[0m top5: 0.8334888059701493
[2m[36m(func pid=56305)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=56305)[0m f1_macro: 0.23268743983637888
[2m[36m(func pid=56305)[0m f1_weighted: 0.31322706283453816
[2m[36m(func pid=56305)[0m f1_per_class: [0.31, 0.53, 0.312, 0.496, 0.105, 0.053, 0.174, 0.26, 0.031, 0.057]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:41 (running for 00:42:33.38)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.264 |      0.276 |                   48 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.072 |      0.233 |                   43 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.357 |      0.214 |                   42 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.84  |      0.168 |                   20 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31669776119402987
[2m[36m(func pid=54942)[0m top5: 0.8726679104477612
[2m[36m(func pid=54942)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=54942)[0m f1_macro: 0.27634587259346677
[2m[36m(func pid=54942)[0m f1_weighted: 0.3364561906228149
[2m[36m(func pid=54942)[0m f1_per_class: [0.183, 0.496, 0.231, 0.298, 0.066, 0.286, 0.322, 0.459, 0.105, 0.318]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3340 | Steps: 4 | Val loss: 1.9933 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6743 | Steps: 4 | Val loss: 2.0080 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.0533 | Steps: 4 | Val loss: 2.4694 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1535 | Steps: 4 | Val loss: 1.9812 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=61908)[0m top1: 0.23460820895522388
[2m[36m(func pid=61908)[0m top5: 0.8031716417910447
[2m[36m(func pid=61908)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=61908)[0m f1_macro: 0.16250641532058635
[2m[36m(func pid=61908)[0m f1_weighted: 0.2229171989788169
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.388, 0.471, 0.319, 0.0, 0.0, 0.165, 0.255, 0.0, 0.027]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.36800373134328357
[2m[36m(func pid=56437)[0m top5: 0.8297574626865671
[2m[36m(func pid=56437)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=56437)[0m f1_macro: 0.2722873808163677
[2m[36m(func pid=56437)[0m f1_weighted: 0.3773895827326066
[2m[36m(func pid=56437)[0m f1_per_class: [0.217, 0.059, 0.4, 0.483, 0.11, 0.275, 0.568, 0.284, 0.172, 0.154]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.30177238805970147
[2m[36m(func pid=56305)[0m top5: 0.8675373134328358
[2m[36m(func pid=56305)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=56305)[0m f1_macro: 0.23970883386233638
[2m[36m(func pid=56305)[0m f1_weighted: 0.3380488923159134
[2m[36m(func pid=56305)[0m f1_per_class: [0.196, 0.524, 0.258, 0.417, 0.096, 0.125, 0.291, 0.381, 0.036, 0.073]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:46 (running for 00:42:38.76)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.154 |      0.289 |                   49 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.053 |      0.24  |                   44 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.674 |      0.272 |                   43 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.334 |      0.163 |                   21 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.33302238805970147
[2m[36m(func pid=54942)[0m top5: 0.8754664179104478
[2m[36m(func pid=54942)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=54942)[0m f1_macro: 0.2889869258110651
[2m[36m(func pid=54942)[0m f1_weighted: 0.36148738570986894
[2m[36m(func pid=54942)[0m f1_per_class: [0.182, 0.486, 0.25, 0.259, 0.065, 0.266, 0.447, 0.498, 0.093, 0.341]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3534 | Steps: 4 | Val loss: 1.8239 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2299 | Steps: 4 | Val loss: 1.9454 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0942 | Steps: 4 | Val loss: 1.9800 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2122 | Steps: 4 | Val loss: 1.9754 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=61908)[0m top1: 0.25
[2m[36m(func pid=61908)[0m top5: 0.8176305970149254
[2m[36m(func pid=61908)[0m f1_micro: 0.25
[2m[36m(func pid=61908)[0m f1_macro: 0.1790797180295302
[2m[36m(func pid=61908)[0m f1_weighted: 0.14429915330094462
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.412, 0.545, 0.054, 0.0, 0.286, 0.0, 0.357, 0.0, 0.136]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.345615671641791
[2m[36m(func pid=56437)[0m top5: 0.8535447761194029
[2m[36m(func pid=56437)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=56437)[0m f1_macro: 0.2913628788610425
[2m[36m(func pid=56437)[0m f1_weighted: 0.3804062896476123
[2m[36m(func pid=56437)[0m f1_per_class: [0.143, 0.306, 0.333, 0.442, 0.169, 0.316, 0.433, 0.491, 0.073, 0.21]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3736007462686567
[2m[36m(func pid=56305)[0m top5: 0.8805970149253731
[2m[36m(func pid=56305)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=56305)[0m f1_macro: 0.2910964717724374
[2m[36m(func pid=56305)[0m f1_weighted: 0.4165094882501009
[2m[36m(func pid=56305)[0m f1_per_class: [0.306, 0.537, 0.143, 0.404, 0.082, 0.31, 0.466, 0.446, 0.055, 0.164]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:51 (running for 00:42:44.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.212 |      0.282 |                   50 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.094 |      0.291 |                   45 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.23  |      0.291 |                   44 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.353 |      0.179 |                   22 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3316231343283582
[2m[36m(func pid=54942)[0m top5: 0.8726679104477612
[2m[36m(func pid=54942)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=54942)[0m f1_macro: 0.2821886354935492
[2m[36m(func pid=54942)[0m f1_weighted: 0.3576757299587893
[2m[36m(func pid=54942)[0m f1_per_class: [0.196, 0.482, 0.27, 0.193, 0.062, 0.265, 0.514, 0.406, 0.135, 0.3]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4890 | Steps: 4 | Val loss: 1.8267 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5876 | Steps: 4 | Val loss: 2.2727 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1791 | Steps: 4 | Val loss: 2.2469 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0804 | Steps: 4 | Val loss: 1.9841 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=61908)[0m top1: 0.3498134328358209
[2m[36m(func pid=61908)[0m top5: 0.8246268656716418
[2m[36m(func pid=61908)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=61908)[0m f1_macro: 0.23322864927493248
[2m[36m(func pid=61908)[0m f1_weighted: 0.27866548911061667
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.395, 0.632, 0.56, 0.118, 0.261, 0.0, 0.312, 0.055, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.2523320895522388
[2m[36m(func pid=56437)[0m top5: 0.8283582089552238
[2m[36m(func pid=56437)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=56437)[0m f1_macro: 0.2555479919482764
[2m[36m(func pid=56437)[0m f1_weighted: 0.2235467139105773
[2m[36m(func pid=56437)[0m f1_per_class: [0.149, 0.143, 0.606, 0.408, 0.146, 0.353, 0.012, 0.48, 0.11, 0.147]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.30363805970149255
[2m[36m(func pid=56305)[0m top5: 0.8666044776119403
[2m[36m(func pid=56305)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=56305)[0m f1_macro: 0.2592470071175562
[2m[36m(func pid=56305)[0m f1_weighted: 0.32727065498426217
[2m[36m(func pid=56305)[0m f1_per_class: [0.291, 0.551, 0.096, 0.332, 0.081, 0.257, 0.246, 0.391, 0.155, 0.193]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:35:57 (running for 00:42:49.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.08  |      0.297 |                   51 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.179 |      0.259 |                   46 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.588 |      0.256 |                   45 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.489 |      0.233 |                   23 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.32136194029850745
[2m[36m(func pid=54942)[0m top5: 0.8582089552238806
[2m[36m(func pid=54942)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=54942)[0m f1_macro: 0.2970445202093165
[2m[36m(func pid=54942)[0m f1_weighted: 0.34319939402191024
[2m[36m(func pid=54942)[0m f1_per_class: [0.233, 0.462, 0.333, 0.146, 0.064, 0.284, 0.497, 0.456, 0.143, 0.353]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4887 | Steps: 4 | Val loss: 2.2977 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3490 | Steps: 4 | Val loss: 2.0017 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8384 | Steps: 4 | Val loss: 1.8900 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1889 | Steps: 4 | Val loss: 1.9776 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=56437)[0m top1: 0.24440298507462688
[2m[36m(func pid=56437)[0m top5: 0.8521455223880597
[2m[36m(func pid=56437)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=56437)[0m f1_macro: 0.25539242969435644
[2m[36m(func pid=56437)[0m f1_weighted: 0.276829728890054
[2m[36m(func pid=56437)[0m f1_per_class: [0.167, 0.073, 0.581, 0.434, 0.065, 0.241, 0.259, 0.422, 0.148, 0.165]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.291044776119403
[2m[36m(func pid=61908)[0m top5: 0.8022388059701493
[2m[36m(func pid=61908)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=61908)[0m f1_macro: 0.16483873342518357
[2m[36m(func pid=61908)[0m f1_weighted: 0.18263766845329826
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.692, 0.571, 0.0, 0.0, 0.0, 0.294, 0.055, 0.037]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3493470149253731
[2m[36m(func pid=56305)[0m top5: 0.8917910447761194
[2m[36m(func pid=56305)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=56305)[0m f1_macro: 0.32280374333063083
[2m[36m(func pid=56305)[0m f1_weighted: 0.3459121700254986
[2m[36m(func pid=56305)[0m f1_per_class: [0.328, 0.551, 0.149, 0.29, 0.069, 0.345, 0.27, 0.5, 0.217, 0.508]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:02 (running for 00:42:54.92)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.189 |      0.292 |                   52 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.838 |      0.323 |                   47 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.489 |      0.255 |                   46 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.349 |      0.165 |                   24 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2943097014925373
[2m[36m(func pid=54942)[0m top5: 0.84375
[2m[36m(func pid=54942)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=54942)[0m f1_macro: 0.2915034017533379
[2m[36m(func pid=54942)[0m f1_weighted: 0.30003509644665743
[2m[36m(func pid=54942)[0m f1_per_class: [0.293, 0.426, 0.4, 0.062, 0.061, 0.282, 0.444, 0.472, 0.147, 0.328]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.8430 | Steps: 4 | Val loss: 2.3784 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4912 | Steps: 4 | Val loss: 2.0103 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1819 | Steps: 4 | Val loss: 1.9148 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1443 | Steps: 4 | Val loss: 1.9892 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=56437)[0m top1: 0.26632462686567165
[2m[36m(func pid=56437)[0m top5: 0.8544776119402985
[2m[36m(func pid=56437)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=56437)[0m f1_macro: 0.22313037693713839
[2m[36m(func pid=56437)[0m f1_weighted: 0.30384324025385434
[2m[36m(func pid=56437)[0m f1_per_class: [0.236, 0.159, 0.263, 0.432, 0.057, 0.0, 0.411, 0.346, 0.133, 0.195]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.27472014925373134
[2m[36m(func pid=61908)[0m top5: 0.7318097014925373
[2m[36m(func pid=61908)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=61908)[0m f1_macro: 0.19550392446691828
[2m[36m(func pid=61908)[0m f1_weighted: 0.1897180455826676
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.8, 0.581, 0.178, 0.0, 0.009, 0.285, 0.055, 0.047]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.37033582089552236
[2m[36m(func pid=56305)[0m top5: 0.8903917910447762
[2m[36m(func pid=56305)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=56305)[0m f1_macro: 0.3439750318469672
[2m[36m(func pid=56305)[0m f1_weighted: 0.35632443944841213
[2m[36m(func pid=56305)[0m f1_per_class: [0.37, 0.522, 0.393, 0.249, 0.076, 0.341, 0.355, 0.51, 0.232, 0.391]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:08 (running for 00:43:00.30)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.144 |      0.297 |                   53 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.182 |      0.344 |                   48 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.843 |      0.223 |                   47 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.491 |      0.196 |                   25 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3045708955223881
[2m[36m(func pid=54942)[0m top5: 0.8390858208955224
[2m[36m(func pid=54942)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=54942)[0m f1_macro: 0.29704122374947073
[2m[36m(func pid=54942)[0m f1_weighted: 0.31561520212929217
[2m[36m(func pid=54942)[0m f1_per_class: [0.28, 0.472, 0.364, 0.089, 0.066, 0.303, 0.431, 0.505, 0.152, 0.308]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1298 | Steps: 4 | Val loss: 2.3149 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1965 | Steps: 4 | Val loss: 1.9208 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2330 | Steps: 4 | Val loss: 1.9854 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0800 | Steps: 4 | Val loss: 1.9881 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56437)[0m top1: 0.30830223880597013
[2m[36m(func pid=56437)[0m top5: 0.8269589552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=56437)[0m f1_macro: 0.2703843831136856
[2m[36m(func pid=56437)[0m f1_weighted: 0.2720656442955764
[2m[36m(func pid=56437)[0m f1_per_class: [0.203, 0.469, 0.69, 0.466, 0.063, 0.066, 0.045, 0.455, 0.103, 0.143]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.38199626865671643
[2m[36m(func pid=61908)[0m top5: 0.7975746268656716
[2m[36m(func pid=61908)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=61908)[0m f1_macro: 0.18846101802327914
[2m[36m(func pid=61908)[0m f1_weighted: 0.3324418411435125
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.0, 0.6, 0.548, 0.069, 0.0, 0.582, 0.0, 0.055, 0.031]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3628731343283582
[2m[36m(func pid=56305)[0m top5: 0.8931902985074627
[2m[36m(func pid=56305)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=56305)[0m f1_macro: 0.35569655575347275
[2m[36m(func pid=56305)[0m f1_weighted: 0.35707831645001814
[2m[36m(func pid=56305)[0m f1_per_class: [0.392, 0.503, 0.415, 0.248, 0.133, 0.345, 0.364, 0.528, 0.18, 0.448]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:13 (running for 00:43:05.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.08  |      0.302 |                   54 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.233 |      0.356 |                   49 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.13  |      0.27  |                   48 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.197 |      0.188 |                   26 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3045708955223881
[2m[36m(func pid=54942)[0m top5: 0.8344216417910447
[2m[36m(func pid=54942)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=54942)[0m f1_macro: 0.3017152390250472
[2m[36m(func pid=54942)[0m f1_weighted: 0.3213519065000197
[2m[36m(func pid=54942)[0m f1_per_class: [0.297, 0.499, 0.367, 0.083, 0.06, 0.284, 0.45, 0.488, 0.141, 0.348]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5528 | Steps: 4 | Val loss: 2.7409 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4754 | Steps: 4 | Val loss: 1.9156 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8004 | Steps: 4 | Val loss: 1.8644 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0461 | Steps: 4 | Val loss: 1.9712 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=56437)[0m top1: 0.2826492537313433
[2m[36m(func pid=56437)[0m top5: 0.8013059701492538
[2m[36m(func pid=56437)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=56437)[0m f1_macro: 0.20797412297216863
[2m[36m(func pid=56437)[0m f1_weighted: 0.2456880794544084
[2m[36m(func pid=56437)[0m f1_per_class: [0.149, 0.526, 0.0, 0.29, 0.154, 0.335, 0.006, 0.455, 0.073, 0.091]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.28171641791044777
[2m[36m(func pid=61908)[0m top5: 0.8050373134328358
[2m[36m(func pid=61908)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=61908)[0m f1_macro: 0.175749307609464
[2m[36m(func pid=61908)[0m f1_weighted: 0.26752561119183926
[2m[36m(func pid=61908)[0m f1_per_class: [0.08, 0.0, 0.632, 0.302, 0.061, 0.022, 0.579, 0.0, 0.054, 0.028]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3908582089552239
[2m[36m(func pid=56305)[0m top5: 0.9011194029850746
[2m[36m(func pid=56305)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=56305)[0m f1_macro: 0.3530664858929258
[2m[36m(func pid=56305)[0m f1_weighted: 0.40940763702887295
[2m[36m(func pid=56305)[0m f1_per_class: [0.41, 0.574, 0.152, 0.427, 0.113, 0.333, 0.337, 0.537, 0.209, 0.441]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:18 (running for 00:43:11.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.046 |      0.296 |                   55 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.8   |      0.353 |                   50 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.553 |      0.208 |                   49 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.475 |      0.176 |                   27 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3125
[2m[36m(func pid=54942)[0m top5: 0.8479477611940298
[2m[36m(func pid=54942)[0m f1_micro: 0.3125
[2m[36m(func pid=54942)[0m f1_macro: 0.29602228634756356
[2m[36m(func pid=54942)[0m f1_weighted: 0.3267963849710677
[2m[36m(func pid=54942)[0m f1_per_class: [0.261, 0.482, 0.253, 0.069, 0.064, 0.309, 0.482, 0.507, 0.133, 0.4]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1657 | Steps: 4 | Val loss: 2.7844 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4457 | Steps: 4 | Val loss: 2.0093 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.3754 | Steps: 4 | Val loss: 1.9638 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.0616 | Steps: 4 | Val loss: 1.9814 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.25326492537313433
[2m[36m(func pid=56437)[0m top5: 0.8796641791044776
[2m[36m(func pid=56437)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=56437)[0m f1_macro: 0.1665400879830911
[2m[36m(func pid=56437)[0m f1_weighted: 0.20231127898268805
[2m[36m(func pid=56437)[0m f1_per_class: [0.137, 0.477, 0.0, 0.185, 0.0, 0.341, 0.009, 0.391, 0.0, 0.126]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.14039179104477612
[2m[36m(func pid=61908)[0m top5: 0.800839552238806
[2m[36m(func pid=61908)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=61908)[0m f1_macro: 0.19872631596084928
[2m[36m(func pid=61908)[0m f1_weighted: 0.12971828317300318
[2m[36m(func pid=61908)[0m f1_per_class: [0.083, 0.084, 0.7, 0.0, 0.073, 0.292, 0.148, 0.479, 0.099, 0.029]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.36800373134328357
[2m[36m(func pid=56305)[0m top5: 0.9109141791044776
[2m[36m(func pid=56305)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=56305)[0m f1_macro: 0.33847381191652154
[2m[36m(func pid=56305)[0m f1_weighted: 0.40180508836049994
[2m[36m(func pid=56305)[0m f1_per_class: [0.454, 0.499, 0.13, 0.468, 0.065, 0.306, 0.329, 0.536, 0.183, 0.415]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:24 (running for 00:43:16.66)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.062 |      0.28  |                   56 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.375 |      0.338 |                   51 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.166 |      0.167 |                   50 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.446 |      0.199 |                   28 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3125
[2m[36m(func pid=54942)[0m top5: 0.8460820895522388
[2m[36m(func pid=54942)[0m f1_micro: 0.3125
[2m[36m(func pid=54942)[0m f1_macro: 0.2798230913505937
[2m[36m(func pid=54942)[0m f1_weighted: 0.3271276561948953
[2m[36m(func pid=54942)[0m f1_per_class: [0.161, 0.498, 0.176, 0.072, 0.065, 0.309, 0.476, 0.528, 0.136, 0.377]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.6323 | Steps: 4 | Val loss: 2.3474 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4304 | Steps: 4 | Val loss: 2.1383 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8355 | Steps: 4 | Val loss: 2.0371 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3065 | Steps: 4 | Val loss: 1.9580 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.30783582089552236
[2m[36m(func pid=56437)[0m top5: 0.8264925373134329
[2m[36m(func pid=56437)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=56437)[0m f1_macro: 0.23568955608184675
[2m[36m(func pid=56437)[0m f1_weighted: 0.3036657733243571
[2m[36m(func pid=56437)[0m f1_per_class: [0.229, 0.493, 0.207, 0.167, 0.151, 0.213, 0.388, 0.4, 0.026, 0.082]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m top1: 0.11520522388059702
[2m[36m(func pid=61908)[0m top5: 0.7863805970149254
[2m[36m(func pid=61908)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=61908)[0m f1_macro: 0.16357249888381922
[2m[36m(func pid=61908)[0m f1_weighted: 0.07731346486953315
[2m[36m(func pid=61908)[0m f1_per_class: [0.086, 0.23, 0.632, 0.0, 0.149, 0.085, 0.0, 0.298, 0.104, 0.051]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m top1: 0.363339552238806
[2m[36m(func pid=56305)[0m top5: 0.9011194029850746
[2m[36m(func pid=56305)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=56305)[0m f1_macro: 0.34376863758613474
[2m[36m(func pid=56305)[0m f1_weighted: 0.38443223323947817
[2m[36m(func pid=56305)[0m f1_per_class: [0.335, 0.47, 0.426, 0.523, 0.07, 0.307, 0.244, 0.482, 0.244, 0.337]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:36:29 (running for 00:43:22.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.307 |      0.278 |                   57 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.836 |      0.344 |                   52 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.632 |      0.236 |                   51 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.43  |      0.164 |                   29 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2873134328358209
[2m[36m(func pid=54942)[0m top5: 0.8395522388059702
[2m[36m(func pid=54942)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=54942)[0m f1_macro: 0.27836634993333487
[2m[36m(func pid=54942)[0m f1_weighted: 0.30033141856662043
[2m[36m(func pid=54942)[0m f1_per_class: [0.253, 0.41, 0.218, 0.053, 0.057, 0.282, 0.467, 0.468, 0.131, 0.444]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3336 | Steps: 4 | Val loss: 1.9915 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3504 | Steps: 4 | Val loss: 2.1540 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.8646 | Steps: 4 | Val loss: 1.9431 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1544 | Steps: 4 | Val loss: 1.9596 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56437)[0m top1: 0.35447761194029853
[2m[36m(func pid=56437)[0m top5: 0.867070895522388
[2m[36m(func pid=56437)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=56437)[0m f1_macro: 0.3203128515346391
[2m[36m(func pid=56437)[0m f1_weighted: 0.3868892248355853
[2m[36m(func pid=56437)[0m f1_per_class: [0.252, 0.391, 0.647, 0.357, 0.062, 0.189, 0.521, 0.477, 0.117, 0.191]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3931902985074627
[2m[36m(func pid=56305)[0m top5: 0.9090485074626866
[2m[36m(func pid=56305)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=56305)[0m f1_macro: 0.3539151644839238
[2m[36m(func pid=56305)[0m f1_weighted: 0.4399745618548134
[2m[36m(func pid=56305)[0m f1_per_class: [0.332, 0.455, 0.267, 0.501, 0.056, 0.322, 0.452, 0.51, 0.224, 0.423]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.14272388059701493
[2m[36m(func pid=61908)[0m top5: 0.7691231343283582
[2m[36m(func pid=61908)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=61908)[0m f1_macro: 0.19373331821986334
[2m[36m(func pid=61908)[0m f1_weighted: 0.10565919930155206
[2m[36m(func pid=61908)[0m f1_per_class: [0.085, 0.264, 0.696, 0.0, 0.095, 0.23, 0.0, 0.378, 0.15, 0.039]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:36:35 (running for 00:43:27.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.154 |      0.279 |                   58 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.865 |      0.354 |                   53 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.334 |      0.32  |                   52 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.35  |      0.194 |                   30 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.29011194029850745
[2m[36m(func pid=54942)[0m top5: 0.8325559701492538
[2m[36m(func pid=54942)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=54942)[0m f1_macro: 0.27884349312368195
[2m[36m(func pid=54942)[0m f1_weighted: 0.3050996298939008
[2m[36m(func pid=54942)[0m f1_per_class: [0.27, 0.404, 0.207, 0.062, 0.057, 0.29, 0.473, 0.473, 0.143, 0.409]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.2098 | Steps: 4 | Val loss: 2.3094 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0528 | Steps: 4 | Val loss: 1.9591 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0892 | Steps: 4 | Val loss: 2.1411 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=56437)[0m top1: 0.2728544776119403
[2m[36m(func pid=56437)[0m top5: 0.863339552238806
[2m[36m(func pid=56437)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=56437)[0m f1_macro: 0.2751501815701764
[2m[36m(func pid=56437)[0m f1_weighted: 0.2545617537848659
[2m[36m(func pid=56437)[0m f1_per_class: [0.224, 0.199, 0.7, 0.539, 0.053, 0.007, 0.092, 0.403, 0.139, 0.395]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0475 | Steps: 4 | Val loss: 1.9683 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=56305)[0m top1: 0.3894589552238806
[2m[36m(func pid=56305)[0m top5: 0.8871268656716418
[2m[36m(func pid=56305)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=56305)[0m f1_macro: 0.33383388909511086
[2m[36m(func pid=56305)[0m f1_weighted: 0.43994312574282374
[2m[36m(func pid=56305)[0m f1_per_class: [0.283, 0.477, 0.147, 0.427, 0.058, 0.275, 0.536, 0.495, 0.201, 0.438]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.2989738805970149
[2m[36m(func pid=61908)[0m top5: 0.7583955223880597
[2m[36m(func pid=61908)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=61908)[0m f1_macro: 0.2463778968991472
[2m[36m(func pid=61908)[0m f1_weighted: 0.2566768744334214
[2m[36m(func pid=61908)[0m f1_per_class: [0.061, 0.239, 0.769, 0.552, 0.066, 0.246, 0.0, 0.391, 0.141, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.28031716417910446
[2m[36m(func pid=54942)[0m top5: 0.8348880597014925
[2m[36m(func pid=54942)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=54942)[0m f1_macro: 0.26287306716324943
[2m[36m(func pid=54942)[0m f1_weighted: 0.2934990648102038
[2m[36m(func pid=54942)[0m f1_per_class: [0.193, 0.4, 0.164, 0.063, 0.062, 0.278, 0.447, 0.466, 0.147, 0.408]
== Status ==
Current time: 2024-01-07 13:36:40 (running for 00:43:33.09)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.048 |      0.263 |                   59 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.053 |      0.334 |                   54 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.21  |      0.275 |                   53 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.089 |      0.246 |                   31 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0950 | Steps: 4 | Val loss: 3.6125 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9399 | Steps: 4 | Val loss: 2.1516 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.1997 | Steps: 4 | Val loss: 2.1848 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.1489 | Steps: 4 | Val loss: 1.9992 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=56437)[0m top1: 0.18190298507462688
[2m[36m(func pid=56437)[0m top5: 0.7355410447761194
[2m[36m(func pid=56437)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=56437)[0m f1_macro: 0.22340160273163479
[2m[36m(func pid=56437)[0m f1_weighted: 0.20069604041565214
[2m[36m(func pid=56437)[0m f1_per_class: [0.306, 0.203, 0.526, 0.391, 0.039, 0.0, 0.063, 0.392, 0.087, 0.228]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.3460820895522388
[2m[36m(func pid=56305)[0m top5: 0.8675373134328358
[2m[36m(func pid=56305)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=56305)[0m f1_macro: 0.3181856004685458
[2m[36m(func pid=56305)[0m f1_weighted: 0.39854387650770234
[2m[36m(func pid=56305)[0m f1_per_class: [0.241, 0.395, 0.182, 0.411, 0.065, 0.324, 0.438, 0.514, 0.229, 0.383]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.27705223880597013
[2m[36m(func pid=61908)[0m top5: 0.7639925373134329
[2m[36m(func pid=61908)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=61908)[0m f1_macro: 0.1948678779654645
[2m[36m(func pid=61908)[0m f1_weighted: 0.242811254293675
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.207, 0.375, 0.533, 0.051, 0.247, 0.0, 0.406, 0.13, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:36:46 (running for 00:43:38.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.149 |      0.259 |                   60 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.94  |      0.318 |                   55 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.095 |      0.223 |                   54 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.2   |      0.195 |                   32 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.26865671641791045
[2m[36m(func pid=54942)[0m top5: 0.8390858208955224
[2m[36m(func pid=54942)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=54942)[0m f1_macro: 0.25879154667412574
[2m[36m(func pid=54942)[0m f1_weighted: 0.28516723484495665
[2m[36m(func pid=54942)[0m f1_per_class: [0.185, 0.42, 0.154, 0.06, 0.061, 0.309, 0.396, 0.498, 0.129, 0.375]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8354 | Steps: 4 | Val loss: 3.4441 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9398 | Steps: 4 | Val loss: 2.1885 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.1271 | Steps: 4 | Val loss: 2.0543 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.2051 | Steps: 4 | Val loss: 1.9905 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=56437)[0m top1: 0.16884328358208955
[2m[36m(func pid=56437)[0m top5: 0.792910447761194
[2m[36m(func pid=56437)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=56437)[0m f1_macro: 0.16124306009866987
[2m[36m(func pid=56437)[0m f1_weighted: 0.17457296779164663
[2m[36m(func pid=56437)[0m f1_per_class: [0.283, 0.44, 0.261, 0.12, 0.028, 0.014, 0.171, 0.0, 0.095, 0.202]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=56305)[0m top1: 0.310634328358209
[2m[36m(func pid=56305)[0m top5: 0.8647388059701493
[2m[36m(func pid=56305)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=56305)[0m f1_macro: 0.28852900389283215
[2m[36m(func pid=56305)[0m f1_weighted: 0.34404308738260636
[2m[36m(func pid=56305)[0m f1_per_class: [0.225, 0.366, 0.2, 0.394, 0.071, 0.33, 0.295, 0.518, 0.157, 0.328]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.38992537313432835
[2m[36m(func pid=61908)[0m top5: 0.7728544776119403
[2m[36m(func pid=61908)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=61908)[0m f1_macro: 0.1500241184730765
[2m[36m(func pid=61908)[0m f1_weighted: 0.3637890214010411
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.37, 0.0, 0.564, 0.011, 0.0, 0.47, 0.0, 0.086, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:36:52 (running for 00:43:44.50)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.205 |      0.263 |                   61 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.94  |      0.289 |                   56 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.835 |      0.161 |                   55 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.127 |      0.15  |                   33 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.26632462686567165
[2m[36m(func pid=54942)[0m top5: 0.8428171641791045
[2m[36m(func pid=54942)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=54942)[0m f1_macro: 0.26336264064125403
[2m[36m(func pid=54942)[0m f1_weighted: 0.2839384114361993
[2m[36m(func pid=54942)[0m f1_per_class: [0.222, 0.403, 0.178, 0.092, 0.058, 0.308, 0.371, 0.489, 0.136, 0.377]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.6488 | Steps: 4 | Val loss: 2.7063 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0908 | Steps: 4 | Val loss: 1.9598 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4160 | Steps: 4 | Val loss: 1.9322 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=56437)[0m top1: 0.2560634328358209
[2m[36m(func pid=56437)[0m top5: 0.8465485074626866
[2m[36m(func pid=56437)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=56437)[0m f1_macro: 0.1635939460016934
[2m[36m(func pid=56437)[0m f1_weighted: 0.26243537673024586
[2m[36m(func pid=56437)[0m f1_per_class: [0.22, 0.532, 0.065, 0.323, 0.017, 0.03, 0.225, 0.0, 0.127, 0.097]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1250 | Steps: 4 | Val loss: 1.9750 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=56305)[0m top1: 0.36240671641791045
[2m[36m(func pid=56305)[0m top5: 0.8773320895522388
[2m[36m(func pid=56305)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=56305)[0m f1_macro: 0.33193317701768343
[2m[36m(func pid=56305)[0m f1_weighted: 0.3993047763703255
[2m[36m(func pid=56305)[0m f1_per_class: [0.42, 0.5, 0.156, 0.463, 0.057, 0.325, 0.33, 0.494, 0.159, 0.417]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.4141791044776119
[2m[36m(func pid=61908)[0m top5: 0.8078358208955224
[2m[36m(func pid=61908)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=61908)[0m f1_macro: 0.1909935160363317
[2m[36m(func pid=61908)[0m f1_weighted: 0.39375622974321906
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.337, 0.143, 0.569, 0.024, 0.21, 0.495, 0.0, 0.131, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:36:57 (running for 00:43:49.85)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.125 |      0.273 |                   62 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.091 |      0.332 |                   57 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.649 |      0.164 |                   56 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.416 |      0.191 |                   34 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.28078358208955223
[2m[36m(func pid=54942)[0m top5: 0.8577425373134329
[2m[36m(func pid=54942)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=54942)[0m f1_macro: 0.2727089580320383
[2m[36m(func pid=54942)[0m f1_weighted: 0.3050452123981425
[2m[36m(func pid=54942)[0m f1_per_class: [0.213, 0.416, 0.189, 0.13, 0.058, 0.308, 0.4, 0.48, 0.133, 0.4]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3574 | Steps: 4 | Val loss: 2.6063 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0853 | Steps: 4 | Val loss: 1.7063 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5204 | Steps: 4 | Val loss: 2.0294 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.314365671641791
[2m[36m(func pid=56437)[0m top5: 0.8316231343283582
[2m[36m(func pid=56437)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=56437)[0m f1_macro: 0.21671247327985563
[2m[36m(func pid=56437)[0m f1_weighted: 0.3248167641865885
[2m[36m(func pid=56437)[0m f1_per_class: [0.261, 0.429, 0.115, 0.549, 0.042, 0.098, 0.194, 0.31, 0.113, 0.055]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9873 | Steps: 4 | Val loss: 1.9573 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=56305)[0m top1: 0.41324626865671643
[2m[36m(func pid=56305)[0m top5: 0.9039179104477612
[2m[36m(func pid=56305)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=56305)[0m f1_macro: 0.3349905307686111
[2m[36m(func pid=56305)[0m f1_weighted: 0.4243576491818273
[2m[36m(func pid=56305)[0m f1_per_class: [0.128, 0.577, 0.387, 0.411, 0.072, 0.283, 0.449, 0.485, 0.204, 0.355]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.18190298507462688
[2m[36m(func pid=61908)[0m top5: 0.8036380597014925
[2m[36m(func pid=61908)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=61908)[0m f1_macro: 0.16678482559600552
[2m[36m(func pid=61908)[0m f1_weighted: 0.13266706005266382
[2m[36m(func pid=61908)[0m f1_per_class: [0.08, 0.379, 0.333, 0.0, 0.094, 0.327, 0.0, 0.431, 0.023, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:37:03 (running for 00:43:55.39)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.987 |      0.294 |                   63 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.085 |      0.335 |                   58 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.357 |      0.217 |                   57 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.52  |      0.167 |                   35 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3003731343283582
[2m[36m(func pid=54942)[0m top5: 0.8610074626865671
[2m[36m(func pid=54942)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=54942)[0m f1_macro: 0.29372222017811994
[2m[36m(func pid=54942)[0m f1_weighted: 0.32402024970336746
[2m[36m(func pid=54942)[0m f1_per_class: [0.225, 0.458, 0.22, 0.155, 0.067, 0.319, 0.405, 0.492, 0.141, 0.455]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.5069 | Steps: 4 | Val loss: 3.8526 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.3531 | Steps: 4 | Val loss: 1.8795 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2460 | Steps: 4 | Val loss: 2.0974 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=56437)[0m top1: 0.12733208955223882
[2m[36m(func pid=56437)[0m top5: 0.7798507462686567
[2m[36m(func pid=56437)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=56437)[0m f1_macro: 0.13939091800256892
[2m[36m(func pid=56437)[0m f1_weighted: 0.14686269981534286
[2m[36m(func pid=56437)[0m f1_per_class: [0.237, 0.205, 0.021, 0.205, 0.087, 0.013, 0.066, 0.417, 0.083, 0.059]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.0907 | Steps: 4 | Val loss: 1.9372 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=56305)[0m top1: 0.404384328358209
[2m[36m(func pid=56305)[0m top5: 0.8889925373134329
[2m[36m(func pid=56305)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=56305)[0m f1_macro: 0.34966422735092073
[2m[36m(func pid=56305)[0m f1_weighted: 0.3954924187333332
[2m[36m(func pid=56305)[0m f1_per_class: [0.14, 0.516, 0.533, 0.302, 0.078, 0.242, 0.482, 0.541, 0.256, 0.406]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.228544776119403
[2m[36m(func pid=61908)[0m top5: 0.7765858208955224
[2m[36m(func pid=61908)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=61908)[0m f1_macro: 0.16086810884561692
[2m[36m(func pid=61908)[0m f1_weighted: 0.11506315006440779
[2m[36m(func pid=61908)[0m f1_per_class: [0.107, 0.432, 0.727, 0.003, 0.053, 0.286, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:37:08 (running for 00:44:00.86)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.091 |      0.29  |                   64 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.353 |      0.35  |                   59 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.507 |      0.139 |                   58 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.246 |      0.161 |                   36 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.27705223880597013
[2m[36m(func pid=54942)[0m top5: 0.8610074626865671
[2m[36m(func pid=54942)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=54942)[0m f1_macro: 0.29026452399469865
[2m[36m(func pid=54942)[0m f1_weighted: 0.29683854013808664
[2m[36m(func pid=54942)[0m f1_per_class: [0.243, 0.399, 0.423, 0.162, 0.058, 0.313, 0.349, 0.445, 0.158, 0.353]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.9304 | Steps: 4 | Val loss: 4.3826 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3080 | Steps: 4 | Val loss: 1.9474 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=56437)[0m top1: 0.1044776119402985
[2m[36m(func pid=56437)[0m top5: 0.7145522388059702
[2m[36m(func pid=56437)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=56437)[0m f1_macro: 0.09993015045109474
[2m[36m(func pid=56437)[0m f1_weighted: 0.10802184691839782
[2m[36m(func pid=56437)[0m f1_per_class: [0.141, 0.204, 0.03, 0.013, 0.062, 0.038, 0.17, 0.106, 0.077, 0.158]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3205 | Steps: 4 | Val loss: 2.0954 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0831 | Steps: 4 | Val loss: 1.9531 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=56305)[0m top1: 0.38619402985074625
[2m[36m(func pid=56305)[0m top5: 0.8805970149253731
[2m[36m(func pid=56305)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=56305)[0m f1_macro: 0.3595264333394487
[2m[36m(func pid=56305)[0m f1_weighted: 0.40446628739147006
[2m[36m(func pid=56305)[0m f1_per_class: [0.338, 0.513, 0.585, 0.485, 0.08, 0.312, 0.318, 0.514, 0.205, 0.246]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.2966417910447761
[2m[36m(func pid=61908)[0m top5: 0.7831156716417911
[2m[36m(func pid=61908)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=61908)[0m f1_macro: 0.13190209893159155
[2m[36m(func pid=61908)[0m f1_weighted: 0.24423734087491725
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.461, 0.087, 0.485, 0.034, 0.253, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:37:13 (running for 00:44:06.21)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.083 |      0.297 |                   65 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.308 |      0.36  |                   60 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.93  |      0.1   |                   59 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.321 |      0.132 |                   37 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.28171641791044777
[2m[36m(func pid=54942)[0m top5: 0.8488805970149254
[2m[36m(func pid=54942)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=54942)[0m f1_macro: 0.29711249180554156
[2m[36m(func pid=54942)[0m f1_weighted: 0.29817443409786215
[2m[36m(func pid=54942)[0m f1_per_class: [0.232, 0.443, 0.386, 0.204, 0.059, 0.295, 0.287, 0.49, 0.157, 0.419]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.4014 | Steps: 4 | Val loss: 5.0413 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.9577 | Steps: 4 | Val loss: 2.6189 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=56437)[0m top1: 0.09281716417910447
[2m[36m(func pid=56437)[0m top5: 0.6268656716417911
[2m[36m(func pid=56437)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=56437)[0m f1_macro: 0.0985587933726759
[2m[36m(func pid=56437)[0m f1_weighted: 0.0681643478615807
[2m[36m(func pid=56437)[0m f1_per_class: [0.129, 0.102, 0.19, 0.0, 0.169, 0.181, 0.074, 0.0, 0.059, 0.081]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4740 | Steps: 4 | Val loss: 4.3289 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0227 | Steps: 4 | Val loss: 1.9534 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=56305)[0m top1: 0.2826492537313433
[2m[36m(func pid=56305)[0m top5: 0.8353544776119403
[2m[36m(func pid=56305)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=56305)[0m f1_macro: 0.2753867725272557
[2m[36m(func pid=56305)[0m f1_weighted: 0.2835705845384507
[2m[36m(func pid=56305)[0m f1_per_class: [0.386, 0.389, 0.571, 0.544, 0.083, 0.104, 0.037, 0.438, 0.11, 0.092]
[2m[36m(func pid=56305)[0m 
== Status ==
Current time: 2024-01-07 13:37:18 (running for 00:44:11.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.083 |      0.297 |                   65 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.958 |      0.275 |                   61 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.401 |      0.099 |                   60 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.474 |      0.122 |                   38 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.1767723880597015
[2m[36m(func pid=61908)[0m top5: 0.7826492537313433
[2m[36m(func pid=61908)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=61908)[0m f1_macro: 0.12234702960548324
[2m[36m(func pid=61908)[0m f1_weighted: 0.15793542272090066
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.26, 0.0, 0.238, 0.055, 0.191, 0.0, 0.368, 0.095, 0.016]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.2826492537313433
[2m[36m(func pid=54942)[0m top5: 0.8488805970149254
[2m[36m(func pid=54942)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=54942)[0m f1_macro: 0.29678702037798954
[2m[36m(func pid=54942)[0m f1_weighted: 0.29316004501537013
[2m[36m(func pid=54942)[0m f1_per_class: [0.216, 0.451, 0.414, 0.175, 0.062, 0.301, 0.29, 0.485, 0.174, 0.4]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9125 | Steps: 4 | Val loss: 2.6394 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.8575 | Steps: 4 | Val loss: 2.3980 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=56437)[0m top1: 0.30223880597014924
[2m[36m(func pid=56437)[0m top5: 0.8666044776119403
[2m[36m(func pid=56437)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=56437)[0m f1_macro: 0.2369879132691551
[2m[36m(func pid=56437)[0m f1_weighted: 0.3061842420282574
[2m[36m(func pid=56437)[0m f1_per_class: [0.161, 0.418, 0.0, 0.013, 0.079, 0.26, 0.546, 0.487, 0.062, 0.343]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3193 | Steps: 4 | Val loss: 3.1505 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1090 | Steps: 4 | Val loss: 1.9902 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=56305)[0m top1: 0.2728544776119403
[2m[36m(func pid=56305)[0m top5: 0.8535447761194029
[2m[36m(func pid=56305)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=56305)[0m f1_macro: 0.32697515688585954
[2m[36m(func pid=56305)[0m f1_weighted: 0.2714511923597704
[2m[36m(func pid=56305)[0m f1_per_class: [0.505, 0.437, 0.727, 0.407, 0.071, 0.19, 0.04, 0.445, 0.132, 0.316]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m top1: 0.21128731343283583
[2m[36m(func pid=61908)[0m top5: 0.7975746268656716
[2m[36m(func pid=61908)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=61908)[0m f1_macro: 0.13022523974466904
[2m[36m(func pid=61908)[0m f1_weighted: 0.22180141220211178
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.196, 0.031, 0.27, 0.06, 0.0, 0.302, 0.334, 0.068, 0.04]
== Status ==
Current time: 2024-01-07 13:37:24 (running for 00:44:16.84)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.023 |      0.297 |                   66 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.857 |      0.327 |                   62 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.912 |      0.237 |                   61 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.319 |      0.13  |                   39 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9927 | Steps: 4 | Val loss: 2.3711 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=54942)[0m top1: 0.27238805970149255
[2m[36m(func pid=54942)[0m top5: 0.8418843283582089
[2m[36m(func pid=54942)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=54942)[0m f1_macro: 0.28406019643887354
[2m[36m(func pid=54942)[0m f1_weighted: 0.28788847969165343
[2m[36m(func pid=54942)[0m f1_per_class: [0.198, 0.468, 0.364, 0.171, 0.06, 0.246, 0.289, 0.492, 0.18, 0.373]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0242 | Steps: 4 | Val loss: 2.1516 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=56437)[0m top1: 0.240205223880597
[2m[36m(func pid=56437)[0m top5: 0.8969216417910447
[2m[36m(func pid=56437)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=56437)[0m f1_macro: 0.18734765301404094
[2m[36m(func pid=56437)[0m f1_weighted: 0.21309097020438908
[2m[36m(func pid=56437)[0m f1_per_class: [0.213, 0.47, 0.1, 0.185, 0.082, 0.183, 0.113, 0.286, 0.06, 0.182]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2493 | Steps: 4 | Val loss: 2.7685 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8799 | Steps: 4 | Val loss: 1.9979 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=56305)[0m top1: 0.3087686567164179
[2m[36m(func pid=56305)[0m top5: 0.875
[2m[36m(func pid=56305)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=56305)[0m f1_macro: 0.35733041238628743
[2m[36m(func pid=56305)[0m f1_weighted: 0.31063971729638445
[2m[36m(func pid=56305)[0m f1_per_class: [0.496, 0.473, 0.7, 0.396, 0.072, 0.252, 0.122, 0.482, 0.191, 0.39]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.3004 | Steps: 4 | Val loss: 1.9220 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 13:37:30 (running for 00:44:22.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.109 |      0.284 |                   67 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.024 |      0.357 |                   63 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.993 |      0.187 |                   62 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.249 |      0.135 |                   40 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.34654850746268656
[2m[36m(func pid=61908)[0m top5: 0.8069029850746269
[2m[36m(func pid=61908)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=61908)[0m f1_macro: 0.13545705979627962
[2m[36m(func pid=61908)[0m f1_weighted: 0.32057926225866995
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.201, 0.0, 0.388, 0.082, 0.0, 0.583, 0.0, 0.1, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.2691231343283582
[2m[36m(func pid=54942)[0m top5: 0.8334888059701493
[2m[36m(func pid=54942)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=54942)[0m f1_macro: 0.2765683598535835
[2m[36m(func pid=54942)[0m f1_weighted: 0.28175315702699977
[2m[36m(func pid=54942)[0m f1_per_class: [0.172, 0.481, 0.369, 0.187, 0.062, 0.281, 0.234, 0.531, 0.127, 0.32]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7623 | Steps: 4 | Val loss: 2.4154 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=56437)[0m top1: 0.332089552238806
[2m[36m(func pid=56437)[0m top5: 0.9090485074626866
[2m[36m(func pid=56437)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=56437)[0m f1_macro: 0.25250357738249907
[2m[36m(func pid=56437)[0m f1_weighted: 0.33921169259362627
[2m[36m(func pid=56437)[0m f1_per_class: [0.235, 0.321, 0.407, 0.505, 0.073, 0.21, 0.295, 0.364, 0.046, 0.069]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4323 | Steps: 4 | Val loss: 2.5016 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0950 | Steps: 4 | Val loss: 1.9766 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=56305)[0m top1: 0.30736940298507465
[2m[36m(func pid=56305)[0m top5: 0.8558768656716418
[2m[36m(func pid=56305)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=56305)[0m f1_macro: 0.3187205657252442
[2m[36m(func pid=56305)[0m f1_weighted: 0.2999027373837388
[2m[36m(func pid=56305)[0m f1_per_class: [0.236, 0.556, 0.615, 0.4, 0.03, 0.237, 0.074, 0.383, 0.235, 0.421]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.7345 | Steps: 4 | Val loss: 2.2188 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 13:37:36 (running for 00:44:28.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.88  |      0.277 |                   68 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.762 |      0.319 |                   64 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.3   |      0.253 |                   63 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.432 |      0.131 |                   41 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.3344216417910448
[2m[36m(func pid=61908)[0m top5: 0.7798507462686567
[2m[36m(func pid=61908)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=61908)[0m f1_macro: 0.1312885137653621
[2m[36m(func pid=61908)[0m f1_weighted: 0.3249145831134619
[2m[36m(func pid=61908)[0m f1_per_class: [0.034, 0.245, 0.0, 0.388, 0.046, 0.0, 0.582, 0.0, 0.0, 0.018]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.2667910447761194
[2m[36m(func pid=54942)[0m top5: 0.840018656716418
[2m[36m(func pid=54942)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=54942)[0m f1_macro: 0.2671694396379777
[2m[36m(func pid=54942)[0m f1_weighted: 0.26485553273424545
[2m[36m(func pid=54942)[0m f1_per_class: [0.169, 0.497, 0.393, 0.184, 0.07, 0.285, 0.184, 0.465, 0.107, 0.318]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8216 | Steps: 4 | Val loss: 2.0300 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=56437)[0m top1: 0.32742537313432835
[2m[36m(func pid=56437)[0m top5: 0.9067164179104478
[2m[36m(func pid=56437)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=56437)[0m f1_macro: 0.2668530200427801
[2m[36m(func pid=56437)[0m f1_weighted: 0.36659370117088086
[2m[36m(func pid=56437)[0m f1_per_class: [0.221, 0.047, 0.186, 0.484, 0.048, 0.299, 0.49, 0.491, 0.195, 0.207]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.2739 | Steps: 4 | Val loss: 2.3978 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9384 | Steps: 4 | Val loss: 1.9351 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=56305)[0m top1: 0.38013059701492535
[2m[36m(func pid=56305)[0m top5: 0.8777985074626866
[2m[36m(func pid=56305)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=56305)[0m f1_macro: 0.36051057552225674
[2m[36m(func pid=56305)[0m f1_weighted: 0.37698427495836984
[2m[36m(func pid=56305)[0m f1_per_class: [0.349, 0.581, 0.537, 0.45, 0.033, 0.273, 0.225, 0.525, 0.233, 0.4]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8421 | Steps: 4 | Val loss: 2.6474 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 13:37:41 (running for 00:44:34.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.095 |      0.267 |                   69 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.822 |      0.361 |                   65 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.735 |      0.267 |                   64 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.274 |      0.117 |                   42 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3003731343283582
[2m[36m(func pid=54942)[0m top5: 0.8572761194029851
[2m[36m(func pid=54942)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=54942)[0m f1_macro: 0.2935166611268561
[2m[36m(func pid=54942)[0m f1_weighted: 0.3092624483949062
[2m[36m(func pid=54942)[0m f1_per_class: [0.186, 0.499, 0.338, 0.27, 0.076, 0.311, 0.237, 0.454, 0.131, 0.433]
[2m[36m(func pid=61908)[0m top1: 0.2579291044776119
[2m[36m(func pid=61908)[0m top5: 0.7667910447761194
[2m[36m(func pid=61908)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=61908)[0m f1_macro: 0.11675531438013816
[2m[36m(func pid=61908)[0m f1_weighted: 0.2503261571661983
[2m[36m(func pid=61908)[0m f1_per_class: [0.085, 0.18, 0.023, 0.151, 0.063, 0.0, 0.577, 0.0, 0.089, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.7877 | Steps: 4 | Val loss: 1.7928 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=56437)[0m top1: 0.22761194029850745
[2m[36m(func pid=56437)[0m top5: 0.8479477611940298
[2m[36m(func pid=56437)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=56437)[0m f1_macro: 0.2567328175019621
[2m[36m(func pid=56437)[0m f1_weighted: 0.2698992318057706
[2m[36m(func pid=56437)[0m f1_per_class: [0.138, 0.13, 0.247, 0.251, 0.046, 0.261, 0.345, 0.49, 0.188, 0.471]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0133 | Steps: 4 | Val loss: 1.9186 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4923 | Steps: 4 | Val loss: 2.3912 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=56305)[0m top1: 0.4291044776119403
[2m[36m(func pid=56305)[0m top5: 0.8913246268656716
[2m[36m(func pid=56305)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=56305)[0m f1_macro: 0.3931279969591782
[2m[36m(func pid=56305)[0m f1_weighted: 0.441625822838626
[2m[36m(func pid=56305)[0m f1_per_class: [0.474, 0.567, 0.453, 0.435, 0.086, 0.297, 0.442, 0.543, 0.23, 0.404]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.5476 | Steps: 4 | Val loss: 2.4771 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:37:47 (running for 00:44:39.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.013 |      0.291 |                   71 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.788 |      0.393 |                   66 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.842 |      0.257 |                   65 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.274 |      0.117 |                   42 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.30783582089552236
[2m[36m(func pid=54942)[0m top5: 0.8596082089552238
[2m[36m(func pid=54942)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=54942)[0m f1_macro: 0.29051043155533846
[2m[36m(func pid=54942)[0m f1_weighted: 0.3222957655085942
[2m[36m(func pid=54942)[0m f1_per_class: [0.196, 0.486, 0.34, 0.32, 0.077, 0.309, 0.247, 0.438, 0.128, 0.364]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.08069029850746269
[2m[36m(func pid=61908)[0m top5: 0.7490671641791045
[2m[36m(func pid=61908)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=61908)[0m f1_macro: 0.07324050415780184
[2m[36m(func pid=61908)[0m f1_weighted: 0.03500022715214941
[2m[36m(func pid=61908)[0m f1_per_class: [0.082, 0.055, 0.085, 0.0, 0.071, 0.0, 0.0, 0.347, 0.079, 0.013]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5888 | Steps: 4 | Val loss: 1.7278 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=56437)[0m top1: 0.32369402985074625
[2m[36m(func pid=56437)[0m top5: 0.8041044776119403
[2m[36m(func pid=56437)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=56437)[0m f1_macro: 0.25398077801082203
[2m[36m(func pid=56437)[0m f1_weighted: 0.2771157285963038
[2m[36m(func pid=56437)[0m f1_per_class: [0.275, 0.466, 0.393, 0.0, 0.0, 0.248, 0.426, 0.472, 0.146, 0.114]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1812 | Steps: 4 | Val loss: 1.9321 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=56305)[0m top1: 0.4505597014925373
[2m[36m(func pid=56305)[0m top5: 0.9057835820895522
[2m[36m(func pid=56305)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=56305)[0m f1_macro: 0.4191347339426773
[2m[36m(func pid=56305)[0m f1_weighted: 0.4796002107444824
[2m[36m(func pid=56305)[0m f1_per_class: [0.545, 0.56, 0.667, 0.543, 0.106, 0.287, 0.487, 0.48, 0.196, 0.321]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3903 | Steps: 4 | Val loss: 2.3917 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5153 | Steps: 4 | Val loss: 2.4763 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=54942)[0m top1: 0.29384328358208955
[2m[36m(func pid=54942)[0m top5: 0.8432835820895522
[2m[36m(func pid=54942)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=54942)[0m f1_macro: 0.2808863264133173
[2m[36m(func pid=54942)[0m f1_weighted: 0.2969193001461445
[2m[36m(func pid=54942)[0m f1_per_class: [0.203, 0.48, 0.393, 0.324, 0.078, 0.303, 0.163, 0.449, 0.13, 0.286]
== Status ==
Current time: 2024-01-07 13:37:52 (running for 00:44:44.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.181 |      0.281 |                   72 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.589 |      0.419 |                   67 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.548 |      0.254 |                   66 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.492 |      0.073 |                   43 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.15111940298507462
[2m[36m(func pid=61908)[0m top5: 0.730410447761194
[2m[36m(func pid=61908)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=61908)[0m f1_macro: 0.12849912025721272
[2m[36m(func pid=61908)[0m f1_weighted: 0.12792682096876076
[2m[36m(func pid=61908)[0m f1_per_class: [0.092, 0.437, 0.16, 0.082, 0.037, 0.0, 0.0, 0.46, 0.0, 0.017]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9799 | Steps: 4 | Val loss: 1.7259 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=56437)[0m top1: 0.3138992537313433
[2m[36m(func pid=56437)[0m top5: 0.8194962686567164
[2m[36m(func pid=56437)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=56437)[0m f1_macro: 0.2348504958484924
[2m[36m(func pid=56437)[0m f1_weighted: 0.2721837573050779
[2m[36m(func pid=56437)[0m f1_per_class: [0.207, 0.446, 0.182, 0.051, 0.053, 0.321, 0.366, 0.374, 0.173, 0.175]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.9749 | Steps: 4 | Val loss: 1.9417 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=56305)[0m top1: 0.4141791044776119
[2m[36m(func pid=56305)[0m top5: 0.9183768656716418
[2m[36m(func pid=56305)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=56305)[0m f1_macro: 0.3729992851661465
[2m[36m(func pid=56305)[0m f1_weighted: 0.4505786430440679
[2m[36m(func pid=56305)[0m f1_per_class: [0.56, 0.36, 0.324, 0.531, 0.076, 0.29, 0.51, 0.509, 0.235, 0.333]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.2083 | Steps: 4 | Val loss: 2.2262 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3273 | Steps: 4 | Val loss: 2.6667 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 13:37:57 (running for 00:44:50.20)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.975 |      0.284 |                   73 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.98  |      0.373 |                   68 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.515 |      0.235 |                   67 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.39  |      0.128 |                   44 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.2980410447761194
[2m[36m(func pid=54942)[0m top5: 0.8381529850746269
[2m[36m(func pid=54942)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=54942)[0m f1_macro: 0.2836097082205341
[2m[36m(func pid=54942)[0m f1_weighted: 0.29679169926415333
[2m[36m(func pid=54942)[0m f1_per_class: [0.219, 0.496, 0.387, 0.382, 0.07, 0.293, 0.095, 0.483, 0.137, 0.275]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.269589552238806
[2m[36m(func pid=61908)[0m top5: 0.7747201492537313
[2m[36m(func pid=61908)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=61908)[0m f1_macro: 0.16701727937077757
[2m[36m(func pid=61908)[0m f1_weighted: 0.23735150130020313
[2m[36m(func pid=61908)[0m f1_per_class: [0.039, 0.437, 0.24, 0.477, 0.025, 0.0, 0.003, 0.444, 0.0, 0.005]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0448 | Steps: 4 | Val loss: 1.7878 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=56437)[0m top1: 0.24347014925373134
[2m[36m(func pid=56437)[0m top5: 0.8213619402985075
[2m[36m(func pid=56437)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=56437)[0m f1_macro: 0.24758004357636168
[2m[36m(func pid=56437)[0m f1_weighted: 0.2160031757898194
[2m[36m(func pid=56437)[0m f1_per_class: [0.247, 0.139, 0.545, 0.465, 0.055, 0.32, 0.0, 0.12, 0.154, 0.43]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.8162 | Steps: 4 | Val loss: 1.9050 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=56305)[0m top1: 0.40205223880597013
[2m[36m(func pid=56305)[0m top5: 0.9132462686567164
[2m[36m(func pid=56305)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=56305)[0m f1_macro: 0.39185985777130405
[2m[36m(func pid=56305)[0m f1_weighted: 0.43745655126379307
[2m[36m(func pid=56305)[0m f1_per_class: [0.537, 0.505, 0.436, 0.374, 0.077, 0.307, 0.527, 0.465, 0.221, 0.469]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1713 | Steps: 4 | Val loss: 2.0918 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.1535 | Steps: 4 | Val loss: 2.4343 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 13:38:03 (running for 00:44:55.75)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.816 |      0.301 |                   74 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.045 |      0.392 |                   69 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.327 |      0.248 |                   68 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.208 |      0.167 |                   45 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.32322761194029853
[2m[36m(func pid=54942)[0m top5: 0.8498134328358209
[2m[36m(func pid=54942)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=54942)[0m f1_macro: 0.3005843527088063
[2m[36m(func pid=54942)[0m f1_weighted: 0.31878214088068874
[2m[36m(func pid=54942)[0m f1_per_class: [0.24, 0.524, 0.329, 0.425, 0.07, 0.294, 0.106, 0.47, 0.171, 0.377]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.3255597014925373
[2m[36m(func pid=61908)[0m top5: 0.8050373134328358
[2m[36m(func pid=61908)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=61908)[0m f1_macro: 0.1415988082220539
[2m[36m(func pid=61908)[0m f1_weighted: 0.3403635032405292
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.161, 0.0, 0.51, 0.044, 0.17, 0.499, 0.032, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.0938 | Steps: 4 | Val loss: 2.0735 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=56437)[0m top1: 0.27238805970149255
[2m[36m(func pid=56437)[0m top5: 0.8666044776119403
[2m[36m(func pid=56437)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=56437)[0m f1_macro: 0.2425957911831206
[2m[36m(func pid=56437)[0m f1_weighted: 0.23900857886891483
[2m[36m(func pid=56437)[0m f1_per_class: [0.161, 0.055, 0.269, 0.537, 0.067, 0.332, 0.0, 0.455, 0.168, 0.382]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0008 | Steps: 4 | Val loss: 1.9201 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=56305)[0m top1: 0.3316231343283582
[2m[36m(func pid=56305)[0m top5: 0.9137126865671642
[2m[36m(func pid=56305)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=56305)[0m f1_macro: 0.347002968606563
[2m[36m(func pid=56305)[0m f1_weighted: 0.35015583590417293
[2m[36m(func pid=56305)[0m f1_per_class: [0.548, 0.565, 0.818, 0.358, 0.096, 0.245, 0.315, 0.103, 0.156, 0.265]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4288 | Steps: 4 | Val loss: 2.0466 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.6066 | Steps: 4 | Val loss: 1.9869 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 13:38:08 (running for 00:45:01.07)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.298
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.001 |      0.299 |                   75 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  1.094 |      0.347 |                   70 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.154 |      0.243 |                   69 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.171 |      0.142 |                   46 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=54942)[0m top1: 0.31716417910447764

[2m[36m(func pid=54942)[0m top5: 0.8418843283582089
[2m[36m(func pid=54942)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=54942)[0m f1_macro: 0.29934981796726984
[2m[36m(func pid=54942)[0m f1_weighted: 0.31555283636250675
[2m[36m(func pid=54942)[0m f1_per_class: [0.25, 0.5, 0.329, 0.42, 0.067, 0.284, 0.112, 0.493, 0.193, 0.346]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.2994402985074627
[2m[36m(func pid=61908)[0m top5: 0.8115671641791045
[2m[36m(func pid=61908)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=61908)[0m f1_macro: 0.1685049609631159
[2m[36m(func pid=61908)[0m f1_weighted: 0.3063599608932202
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.016, 0.311, 0.523, 0.047, 0.271, 0.398, 0.077, 0.042, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7285 | Steps: 4 | Val loss: 1.7679 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=56437)[0m top1: 0.33675373134328357
[2m[36m(func pid=56437)[0m top5: 0.8950559701492538
[2m[36m(func pid=56437)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=56437)[0m f1_macro: 0.29483818407904694
[2m[36m(func pid=56437)[0m f1_weighted: 0.3661521704226676
[2m[36m(func pid=56437)[0m f1_per_class: [0.182, 0.197, 0.286, 0.53, 0.075, 0.304, 0.351, 0.519, 0.155, 0.35]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.9352 | Steps: 4 | Val loss: 1.8890 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56305)[0m top1: 0.4085820895522388
[2m[36m(func pid=56305)[0m top5: 0.9347014925373134
[2m[36m(func pid=56305)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=56305)[0m f1_macro: 0.33914141945630133
[2m[36m(func pid=56305)[0m f1_weighted: 0.43123380166112096
[2m[36m(func pid=56305)[0m f1_per_class: [0.547, 0.519, 0.0, 0.536, 0.11, 0.304, 0.359, 0.48, 0.218, 0.318]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1897 | Steps: 4 | Val loss: 2.0694 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 13:38:14 (running for 00:45:06.36)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.298
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.935 |      0.293 |                   76 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.729 |      0.339 |                   71 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.607 |      0.295 |                   70 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.429 |      0.169 |                   47 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3069029850746269
[2m[36m(func pid=54942)[0m top5: 0.8484141791044776
[2m[36m(func pid=54942)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=54942)[0m f1_macro: 0.2933855998487795
[2m[36m(func pid=54942)[0m f1_weighted: 0.2980814495691879
[2m[36m(func pid=54942)[0m f1_per_class: [0.258, 0.475, 0.407, 0.404, 0.071, 0.3, 0.088, 0.45, 0.151, 0.33]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5810 | Steps: 4 | Val loss: 1.9915 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.8433 | Steps: 4 | Val loss: 1.6757 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=61908)[0m top1: 0.15904850746268656
[2m[36m(func pid=61908)[0m top5: 0.8036380597014925
[2m[36m(func pid=61908)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=61908)[0m f1_macro: 0.12493180910596935
[2m[36m(func pid=61908)[0m f1_weighted: 0.12752636480556648
[2m[36m(func pid=61908)[0m f1_per_class: [0.076, 0.005, 0.091, 0.237, 0.069, 0.283, 0.0, 0.378, 0.11, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.3591417910447761
[2m[36m(func pid=56437)[0m top5: 0.8955223880597015
[2m[36m(func pid=56437)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=56437)[0m f1_macro: 0.3233860605943376
[2m[36m(func pid=56437)[0m f1_weighted: 0.40378559342460196
[2m[36m(func pid=56437)[0m f1_per_class: [0.235, 0.39, 0.6, 0.494, 0.089, 0.257, 0.437, 0.389, 0.173, 0.17]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.9076 | Steps: 4 | Val loss: 1.8956 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=56305)[0m top1: 0.4295708955223881
[2m[36m(func pid=56305)[0m top5: 0.9351679104477612
[2m[36m(func pid=56305)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=56305)[0m f1_macro: 0.43670565607066114
[2m[36m(func pid=56305)[0m f1_weighted: 0.44034703839849715
[2m[36m(func pid=56305)[0m f1_per_class: [0.471, 0.466, 0.833, 0.563, 0.089, 0.343, 0.363, 0.47, 0.197, 0.571]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1535 | Steps: 4 | Val loss: 2.1053 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:38:19 (running for 00:45:11.69)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.298
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.908 |      0.293 |                   77 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.843 |      0.437 |                   72 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.581 |      0.323 |                   71 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.19  |      0.125 |                   48 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31716417910447764
[2m[36m(func pid=54942)[0m top5: 0.8470149253731343
[2m[36m(func pid=54942)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=54942)[0m f1_macro: 0.2933773374955598
[2m[36m(func pid=54942)[0m f1_weighted: 0.3151580088596354
[2m[36m(func pid=54942)[0m f1_per_class: [0.253, 0.474, 0.414, 0.422, 0.077, 0.307, 0.127, 0.467, 0.139, 0.254]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.5105 | Steps: 4 | Val loss: 2.4867 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5840 | Steps: 4 | Val loss: 1.9287 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=61908)[0m top1: 0.08861940298507463
[2m[36m(func pid=61908)[0m top5: 0.8180970149253731
[2m[36m(func pid=61908)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=61908)[0m f1_macro: 0.07813861261838767
[2m[36m(func pid=61908)[0m f1_weighted: 0.032282712878062766
[2m[36m(func pid=61908)[0m f1_per_class: [0.068, 0.016, 0.136, 0.007, 0.086, 0.0, 0.0, 0.375, 0.093, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.324160447761194
[2m[36m(func pid=56437)[0m top5: 0.8978544776119403
[2m[36m(func pid=56437)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=56437)[0m f1_macro: 0.2387303102233261
[2m[36m(func pid=56437)[0m f1_weighted: 0.3684619208119697
[2m[36m(func pid=56437)[0m f1_per_class: [0.186, 0.482, 0.0, 0.479, 0.12, 0.092, 0.353, 0.422, 0.187, 0.066]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.8756 | Steps: 4 | Val loss: 1.8813 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=56305)[0m top1: 0.35774253731343286
[2m[36m(func pid=56305)[0m top5: 0.90625
[2m[36m(func pid=56305)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=56305)[0m f1_macro: 0.3102866378647794
[2m[36m(func pid=56305)[0m f1_weighted: 0.3436028870879832
[2m[36m(func pid=56305)[0m f1_per_class: [0.416, 0.532, 0.197, 0.492, 0.096, 0.126, 0.19, 0.373, 0.182, 0.5]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2097 | Steps: 4 | Val loss: 2.1312 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 13:38:24 (running for 00:45:17.08)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.298
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.876 |      0.281 |                   78 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.584 |      0.31  |                   73 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.51  |      0.239 |                   72 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.154 |      0.078 |                   49 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.31763059701492535
[2m[36m(func pid=54942)[0m top5: 0.855410447761194
[2m[36m(func pid=54942)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=54942)[0m f1_macro: 0.2808347436215322
[2m[36m(func pid=54942)[0m f1_weighted: 0.31218248806206383
[2m[36m(func pid=54942)[0m f1_per_class: [0.268, 0.467, 0.293, 0.429, 0.081, 0.322, 0.117, 0.431, 0.143, 0.258]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6156 | Steps: 4 | Val loss: 2.0787 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6796 | Steps: 4 | Val loss: 2.3519 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=61908)[0m top1: 0.10587686567164178
[2m[36m(func pid=61908)[0m top5: 0.8083022388059702
[2m[36m(func pid=61908)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=61908)[0m f1_macro: 0.10762872557704159
[2m[36m(func pid=61908)[0m f1_weighted: 0.06652809356942735
[2m[36m(func pid=61908)[0m f1_per_class: [0.072, 0.214, 0.213, 0.0, 0.082, 0.0, 0.0, 0.402, 0.094, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.35867537313432835
[2m[36m(func pid=56437)[0m top5: 0.8978544776119403
[2m[36m(func pid=56437)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=56437)[0m f1_macro: 0.30443301840743686
[2m[36m(func pid=56437)[0m f1_weighted: 0.37594010295784436
[2m[36m(func pid=56437)[0m f1_per_class: [0.215, 0.51, 0.379, 0.409, 0.13, 0.279, 0.336, 0.456, 0.201, 0.13]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.9058 | Steps: 4 | Val loss: 1.8757 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=56305)[0m top1: 0.3125
[2m[36m(func pid=56305)[0m top5: 0.878731343283582
[2m[36m(func pid=56305)[0m f1_micro: 0.3125
[2m[36m(func pid=56305)[0m f1_macro: 0.29197555520817947
[2m[36m(func pid=56305)[0m f1_weighted: 0.32176942291753285
[2m[36m(func pid=56305)[0m f1_per_class: [0.265, 0.524, 0.073, 0.346, 0.104, 0.349, 0.176, 0.413, 0.211, 0.459]
[2m[36m(func pid=56305)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.2796 | Steps: 4 | Val loss: 2.0458 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 13:38:30 (running for 00:45:22.41)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.298
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.906 |      0.28  |                   79 |
| train_52b21_00021 | RUNNING    | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.68  |      0.292 |                   74 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  2.616 |      0.304 |                   73 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.21  |      0.108 |                   50 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.33302238805970147
[2m[36m(func pid=54942)[0m top5: 0.8591417910447762
[2m[36m(func pid=54942)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=54942)[0m f1_macro: 0.2798887603241578
[2m[36m(func pid=54942)[0m f1_weighted: 0.3244595680874131
[2m[36m(func pid=54942)[0m f1_per_class: [0.262, 0.524, 0.25, 0.443, 0.081, 0.3, 0.122, 0.462, 0.075, 0.28]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.9958 | Steps: 4 | Val loss: 2.6514 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=56305)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9943 | Steps: 4 | Val loss: 3.5513 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=61908)[0m top1: 0.13712686567164178
[2m[36m(func pid=61908)[0m top5: 0.8036380597014925
[2m[36m(func pid=61908)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=61908)[0m f1_macro: 0.12428649097041208
[2m[36m(func pid=61908)[0m f1_weighted: 0.09614169914709957
[2m[36m(func pid=61908)[0m f1_per_class: [0.077, 0.375, 0.269, 0.0, 0.043, 0.0, 0.012, 0.362, 0.105, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.19916044776119404
[2m[36m(func pid=56437)[0m top5: 0.8283582089552238
[2m[36m(func pid=56437)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=56437)[0m f1_macro: 0.18934303001837235
[2m[36m(func pid=56437)[0m f1_weighted: 0.1621647685852611
[2m[36m(func pid=56437)[0m f1_per_class: [0.12, 0.44, 0.061, 0.003, 0.069, 0.271, 0.064, 0.384, 0.204, 0.277]
[2m[36m(func pid=56437)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.8231 | Steps: 4 | Val loss: 1.8282 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=56305)[0m top1: 0.20055970149253732
[2m[36m(func pid=56305)[0m top5: 0.8115671641791045
[2m[36m(func pid=56305)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=56305)[0m f1_macro: 0.1896787070541821
[2m[36m(func pid=56305)[0m f1_weighted: 0.2309261039358504
[2m[36m(func pid=56305)[0m f1_per_class: [0.134, 0.383, 0.137, 0.316, 0.114, 0.124, 0.109, 0.364, 0.106, 0.11]
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1483 | Steps: 4 | Val loss: 1.9477 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:38:35 (running for 00:45:27.95)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.29725
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.823 |      0.297 |                   80 |
| train_52b21_00022 | RUNNING    | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.996 |      0.189 |                   74 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.28  |      0.124 |                   51 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.34888059701492535
[2m[36m(func pid=54942)[0m top5: 0.8754664179104478
[2m[36m(func pid=54942)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=54942)[0m f1_macro: 0.29717435283343996
[2m[36m(func pid=54942)[0m f1_weighted: 0.34235396508916477
[2m[36m(func pid=54942)[0m f1_per_class: [0.273, 0.499, 0.353, 0.478, 0.083, 0.32, 0.16, 0.422, 0.076, 0.308]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=56437)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9415 | Steps: 4 | Val loss: 3.0337 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=61908)[0m top1: 0.2733208955223881
[2m[36m(func pid=61908)[0m top5: 0.8083022388059702
[2m[36m(func pid=61908)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=61908)[0m f1_macro: 0.1964607579410423
[2m[36m(func pid=61908)[0m f1_weighted: 0.2599370418870614
[2m[36m(func pid=61908)[0m f1_per_class: [0.127, 0.425, 0.348, 0.453, 0.04, 0.008, 0.104, 0.339, 0.121, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=56437)[0m top1: 0.22061567164179105
[2m[36m(func pid=56437)[0m top5: 0.7756529850746269
[2m[36m(func pid=56437)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=56437)[0m f1_macro: 0.1968555804039561
[2m[36m(func pid=56437)[0m f1_weighted: 0.1653140487295447
[2m[36m(func pid=56437)[0m f1_per_class: [0.181, 0.44, 0.0, 0.0, 0.072, 0.248, 0.068, 0.447, 0.224, 0.288]
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.8922 | Steps: 4 | Val loss: 1.8183 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2202 | Steps: 4 | Val loss: 1.9663 | Batch size: 32 | lr: 0.1 | Duration: 3.41s
== Status ==
Current time: 2024-01-07 13:38:41 (running for 00:45:33.63)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.892 |      0.295 |                   81 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.148 |      0.196 |                   52 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3493470149253731
[2m[36m(func pid=54942)[0m top5: 0.8703358208955224
[2m[36m(func pid=54942)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=54942)[0m f1_macro: 0.294928444839612
[2m[36m(func pid=54942)[0m f1_weighted: 0.34364802916815856
[2m[36m(func pid=54942)[0m f1_per_class: [0.31, 0.509, 0.353, 0.464, 0.08, 0.308, 0.18, 0.426, 0.026, 0.295]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.29617537313432835
[2m[36m(func pid=61908)[0m top5: 0.8152985074626866
[2m[36m(func pid=61908)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=61908)[0m f1_macro: 0.23618764469133527
[2m[36m(func pid=61908)[0m f1_weighted: 0.3148610837179351
[2m[36m(func pid=61908)[0m f1_per_class: [0.181, 0.437, 0.522, 0.505, 0.02, 0.0, 0.231, 0.313, 0.134, 0.018]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.8993 | Steps: 4 | Val loss: 1.8073 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 13:38:47 (running for 00:45:39.32)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.899 |      0.289 |                   82 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.22  |      0.236 |                   53 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3530783582089552
[2m[36m(func pid=54942)[0m top5: 0.8680037313432836
[2m[36m(func pid=54942)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=54942)[0m f1_macro: 0.28923499766383437
[2m[36m(func pid=54942)[0m f1_weighted: 0.349124570827965
[2m[36m(func pid=54942)[0m f1_per_class: [0.313, 0.526, 0.273, 0.449, 0.089, 0.297, 0.207, 0.431, 0.025, 0.283]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3925 | Steps: 4 | Val loss: 1.9415 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=61908)[0m top1: 0.30597014925373134
[2m[36m(func pid=61908)[0m top5: 0.820429104477612
[2m[36m(func pid=61908)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=61908)[0m f1_macro: 0.23141310736968398
[2m[36m(func pid=61908)[0m f1_weighted: 0.33228360001380347
[2m[36m(func pid=61908)[0m f1_per_class: [0.205, 0.284, 0.276, 0.521, 0.106, 0.008, 0.334, 0.485, 0.079, 0.016]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.9433 | Steps: 4 | Val loss: 1.8222 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 13:38:52 (running for 00:45:45.02)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.943 |      0.297 |                   83 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.392 |      0.231 |                   54 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.365205223880597
[2m[36m(func pid=54942)[0m top5: 0.8666044776119403
[2m[36m(func pid=54942)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=54942)[0m f1_macro: 0.2965062781278252
[2m[36m(func pid=54942)[0m f1_weighted: 0.3766931827054802
[2m[36m(func pid=54942)[0m f1_per_class: [0.265, 0.526, 0.238, 0.432, 0.083, 0.292, 0.306, 0.485, 0.071, 0.267]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0382 | Steps: 4 | Val loss: 1.9491 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=61908)[0m top1: 0.31669776119402987
[2m[36m(func pid=61908)[0m top5: 0.8222947761194029
[2m[36m(func pid=61908)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=61908)[0m f1_macro: 0.197793594217685
[2m[36m(func pid=61908)[0m f1_weighted: 0.2847181959377718
[2m[36m(func pid=61908)[0m f1_per_class: [0.077, 0.08, 0.273, 0.536, 0.098, 0.015, 0.276, 0.532, 0.076, 0.016]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.7651 | Steps: 4 | Val loss: 1.8386 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 13:38:58 (running for 00:45:50.51)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.765 |      0.302 |                   84 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.038 |      0.198 |                   55 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.365205223880597
[2m[36m(func pid=54942)[0m top5: 0.8586753731343284
[2m[36m(func pid=54942)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=54942)[0m f1_macro: 0.30176295710198275
[2m[36m(func pid=54942)[0m f1_weighted: 0.3836040413392211
[2m[36m(func pid=54942)[0m f1_per_class: [0.266, 0.523, 0.229, 0.407, 0.083, 0.292, 0.344, 0.509, 0.141, 0.224]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6492 | Steps: 4 | Val loss: 1.9519 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.7701 | Steps: 4 | Val loss: 1.8411 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=61908)[0m top1: 0.30970149253731344
[2m[36m(func pid=61908)[0m top5: 0.816231343283582
[2m[36m(func pid=61908)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=61908)[0m f1_macro: 0.20102205787440056
[2m[36m(func pid=61908)[0m f1_weighted: 0.2275996249843837
[2m[36m(func pid=61908)[0m f1_per_class: [0.034, 0.069, 0.5, 0.545, 0.074, 0.276, 0.0, 0.437, 0.075, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:03 (running for 00:45:55.97)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.77  |      0.303 |                   85 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.649 |      0.201 |                   56 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3670708955223881
[2m[36m(func pid=54942)[0m top5: 0.8568097014925373
[2m[36m(func pid=54942)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=54942)[0m f1_macro: 0.3029343004309685
[2m[36m(func pid=54942)[0m f1_weighted: 0.38710924706124566
[2m[36m(func pid=54942)[0m f1_per_class: [0.265, 0.529, 0.218, 0.386, 0.085, 0.3, 0.368, 0.519, 0.126, 0.232]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3516 | Steps: 4 | Val loss: 1.9356 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.8807 | Steps: 4 | Val loss: 1.8555 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=61908)[0m top1: 0.29990671641791045
[2m[36m(func pid=61908)[0m top5: 0.8166977611940298
[2m[36m(func pid=61908)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=61908)[0m f1_macro: 0.2030081207924897
[2m[36m(func pid=61908)[0m f1_weighted: 0.22980643043752944
[2m[36m(func pid=61908)[0m f1_per_class: [0.154, 0.116, 0.444, 0.531, 0.099, 0.296, 0.0, 0.34, 0.049, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:09 (running for 00:46:01.56)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.881 |      0.307 |                   86 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.352 |      0.203 |                   57 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.363339552238806
[2m[36m(func pid=54942)[0m top5: 0.8540111940298507
[2m[36m(func pid=54942)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=54942)[0m f1_macro: 0.30661738903508445
[2m[36m(func pid=54942)[0m f1_weighted: 0.3839708297319326
[2m[36m(func pid=54942)[0m f1_per_class: [0.257, 0.538, 0.261, 0.374, 0.08, 0.285, 0.366, 0.53, 0.13, 0.246]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1298 | Steps: 4 | Val loss: 1.9272 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.7503 | Steps: 4 | Val loss: 1.8581 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=61908)[0m top1: 0.2994402985074627
[2m[36m(func pid=61908)[0m top5: 0.8185634328358209
[2m[36m(func pid=61908)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=61908)[0m f1_macro: 0.21968504784135162
[2m[36m(func pid=61908)[0m f1_weighted: 0.23504779879045304
[2m[36m(func pid=61908)[0m f1_per_class: [0.098, 0.141, 0.526, 0.502, 0.08, 0.308, 0.0, 0.476, 0.065, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:14 (running for 00:46:07.02)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.75  |      0.309 |                   87 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.13  |      0.22  |                   58 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.36613805970149255
[2m[36m(func pid=54942)[0m top5: 0.8530783582089553
[2m[36m(func pid=54942)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=54942)[0m f1_macro: 0.30911595449536033
[2m[36m(func pid=54942)[0m f1_weighted: 0.38626856943525006
[2m[36m(func pid=54942)[0m f1_per_class: [0.239, 0.547, 0.316, 0.374, 0.082, 0.276, 0.372, 0.539, 0.125, 0.221]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0986 | Steps: 4 | Val loss: 1.9547 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.9355 | Steps: 4 | Val loss: 1.8414 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=61908)[0m top1: 0.30550373134328357
[2m[36m(func pid=61908)[0m top5: 0.8260261194029851
[2m[36m(func pid=61908)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=61908)[0m f1_macro: 0.22547696009031942
[2m[36m(func pid=61908)[0m f1_weighted: 0.2580950652541953
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.379, 0.562, 0.431, 0.087, 0.319, 0.019, 0.457, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:20 (running for 00:46:12.31)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.935 |      0.324 |                   88 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.099 |      0.225 |                   59 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.38386194029850745
[2m[36m(func pid=54942)[0m top5: 0.8661380597014925
[2m[36m(func pid=54942)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=54942)[0m f1_macro: 0.32364832679597433
[2m[36m(func pid=54942)[0m f1_weighted: 0.41766011690511473
[2m[36m(func pid=54942)[0m f1_per_class: [0.205, 0.55, 0.346, 0.386, 0.078, 0.29, 0.459, 0.547, 0.112, 0.263]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.0210 | Steps: 4 | Val loss: 2.0332 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 2.0188 | Steps: 4 | Val loss: 1.8506 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=61908)[0m top1: 0.3362873134328358
[2m[36m(func pid=61908)[0m top5: 0.8302238805970149
[2m[36m(func pid=61908)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=61908)[0m f1_macro: 0.24056573157671593
[2m[36m(func pid=61908)[0m f1_weighted: 0.30228714506024873
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.437, 0.545, 0.268, 0.034, 0.315, 0.275, 0.53, 0.0, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:25 (running for 00:46:17.76)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  2.019 |      0.323 |                   89 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.021 |      0.241 |                   60 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3843283582089552
[2m[36m(func pid=54942)[0m top5: 0.8647388059701493
[2m[36m(func pid=54942)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=54942)[0m f1_macro: 0.322986463232636
[2m[36m(func pid=54942)[0m f1_weighted: 0.4182210943018675
[2m[36m(func pid=54942)[0m f1_per_class: [0.216, 0.58, 0.339, 0.417, 0.077, 0.269, 0.437, 0.46, 0.119, 0.317]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.9260 | Steps: 4 | Val loss: 2.1827 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.7735 | Steps: 4 | Val loss: 1.7984 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=61908)[0m top1: 0.30130597014925375
[2m[36m(func pid=61908)[0m top5: 0.8404850746268657
[2m[36m(func pid=61908)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=61908)[0m f1_macro: 0.21489590730244004
[2m[36m(func pid=61908)[0m f1_weighted: 0.323384524499604
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.325, 0.348, 0.263, 0.062, 0.284, 0.456, 0.383, 0.027, 0.0]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:31 (running for 00:46:23.39)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.773 |      0.336 |                   90 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  1.926 |      0.215 |                   61 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.39132462686567165
[2m[36m(func pid=54942)[0m top5: 0.8824626865671642
[2m[36m(func pid=54942)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=54942)[0m f1_macro: 0.3364103848228737
[2m[36m(func pid=54942)[0m f1_weighted: 0.42707995607139
[2m[36m(func pid=54942)[0m f1_per_class: [0.219, 0.524, 0.462, 0.425, 0.078, 0.298, 0.473, 0.51, 0.074, 0.302]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.0021 | Steps: 4 | Val loss: 2.2022 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.8165 | Steps: 4 | Val loss: 1.7898 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=61908)[0m top1: 0.30923507462686567
[2m[36m(func pid=61908)[0m top5: 0.8404850746268657
[2m[36m(func pid=61908)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=61908)[0m f1_macro: 0.17531710979019194
[2m[36m(func pid=61908)[0m f1_weighted: 0.3363220163661363
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.207, 0.2, 0.425, 0.05, 0.2, 0.511, 0.062, 0.027, 0.071]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:36 (running for 00:46:29.11)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.817 |      0.333 |                   91 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.002 |      0.175 |                   62 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.38992537313432835
[2m[36m(func pid=54942)[0m top5: 0.8796641791044776
[2m[36m(func pid=54942)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=54942)[0m f1_macro: 0.33258822307886604
[2m[36m(func pid=54942)[0m f1_weighted: 0.42410146048702363
[2m[36m(func pid=54942)[0m f1_per_class: [0.267, 0.521, 0.439, 0.439, 0.071, 0.302, 0.452, 0.504, 0.051, 0.281]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0591 | Steps: 4 | Val loss: 2.1938 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.6490 | Steps: 4 | Val loss: 1.7876 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=61908)[0m top1: 0.23973880597014927
[2m[36m(func pid=61908)[0m top5: 0.8213619402985075
[2m[36m(func pid=61908)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=61908)[0m f1_macro: 0.1762018814886975
[2m[36m(func pid=61908)[0m f1_weighted: 0.22914591888642438
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.133, 0.316, 0.495, 0.054, 0.228, 0.048, 0.417, 0.025, 0.047]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:42 (running for 00:46:34.71)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.649 |      0.333 |                   92 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.059 |      0.176 |                   63 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3983208955223881
[2m[36m(func pid=54942)[0m top5: 0.8791977611940298
[2m[36m(func pid=54942)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=54942)[0m f1_macro: 0.33340462141913285
[2m[36m(func pid=54942)[0m f1_weighted: 0.4339541469862234
[2m[36m(func pid=54942)[0m f1_per_class: [0.259, 0.518, 0.419, 0.432, 0.074, 0.317, 0.489, 0.49, 0.076, 0.26]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3472 | Steps: 4 | Val loss: 2.1673 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.8685 | Steps: 4 | Val loss: 1.7700 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=61908)[0m top1: 0.21175373134328357
[2m[36m(func pid=61908)[0m top5: 0.8213619402985075
[2m[36m(func pid=61908)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=61908)[0m f1_macro: 0.18284053970472583
[2m[36m(func pid=61908)[0m f1_weighted: 0.20672391463583678
[2m[36m(func pid=61908)[0m f1_per_class: [0.0, 0.115, 0.444, 0.491, 0.06, 0.16, 0.0, 0.446, 0.075, 0.038]
[2m[36m(func pid=61908)[0m 
== Status ==
Current time: 2024-01-07 13:39:47 (running for 00:46:40.27)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.869 |      0.323 |                   93 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.347 |      0.183 |                   64 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.4006529850746269
[2m[36m(func pid=54942)[0m top5: 0.8810634328358209
[2m[36m(func pid=54942)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=54942)[0m f1_macro: 0.32334521469870386
[2m[36m(func pid=54942)[0m f1_weighted: 0.43187729084352844
[2m[36m(func pid=54942)[0m f1_per_class: [0.275, 0.492, 0.333, 0.43, 0.071, 0.306, 0.501, 0.524, 0.049, 0.252]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1117 | Steps: 4 | Val loss: 2.2221 | Batch size: 32 | lr: 0.1 | Duration: 3.30s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.7402 | Steps: 4 | Val loss: 1.7950 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 13:39:53 (running for 00:46:45.28)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.869 |      0.323 |                   93 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.347 |      0.183 |                   64 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.18610074626865672
[2m[36m(func pid=61908)[0m top5: 0.8073694029850746
[2m[36m(func pid=61908)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=61908)[0m f1_macro: 0.18135600762661
[2m[36m(func pid=61908)[0m f1_weighted: 0.19807512715235542
[2m[36m(func pid=61908)[0m f1_per_class: [0.084, 0.171, 0.222, 0.402, 0.055, 0.144, 0.0, 0.52, 0.186, 0.028]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.38992537313432835
[2m[36m(func pid=54942)[0m top5: 0.8740671641791045
[2m[36m(func pid=54942)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=54942)[0m f1_macro: 0.31363527072683095
[2m[36m(func pid=54942)[0m f1_weighted: 0.4251892161626028
[2m[36m(func pid=54942)[0m f1_per_class: [0.264, 0.489, 0.324, 0.431, 0.074, 0.3, 0.484, 0.525, 0.051, 0.194]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0043 | Steps: 4 | Val loss: 2.3191 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.6758 | Steps: 4 | Val loss: 1.7920 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 13:39:58 (running for 00:46:50.91)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.74  |      0.314 |                   94 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.112 |      0.181 |                   65 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.15345149253731344
[2m[36m(func pid=61908)[0m top5: 0.7817164179104478
[2m[36m(func pid=61908)[0m f1_micro: 0.15345149253731344
[2m[36m(func pid=61908)[0m f1_macro: 0.1535199257466447
[2m[36m(func pid=61908)[0m f1_weighted: 0.18179667379800601
[2m[36m(func pid=61908)[0m f1_per_class: [0.068, 0.271, 0.121, 0.257, 0.024, 0.0, 0.083, 0.541, 0.149, 0.021]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.39972014925373134
[2m[36m(func pid=54942)[0m top5: 0.8731343283582089
[2m[36m(func pid=54942)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=54942)[0m f1_macro: 0.31106086717343945
[2m[36m(func pid=54942)[0m f1_weighted: 0.43453410339337517
[2m[36m(func pid=54942)[0m f1_per_class: [0.264, 0.526, 0.264, 0.438, 0.078, 0.312, 0.494, 0.494, 0.0, 0.241]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8996 | Steps: 4 | Val loss: 2.3194 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.8385 | Steps: 4 | Val loss: 1.7865 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 13:40:04 (running for 00:46:56.39)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.676 |      0.311 |                   95 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.004 |      0.154 |                   66 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.16744402985074627
[2m[36m(func pid=61908)[0m top5: 0.7691231343283582
[2m[36m(func pid=61908)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=61908)[0m f1_macro: 0.11998909732533043
[2m[36m(func pid=61908)[0m f1_weighted: 0.20300351355942545
[2m[36m(func pid=61908)[0m f1_per_class: [0.065, 0.269, 0.042, 0.06, 0.021, 0.0, 0.414, 0.162, 0.157, 0.008]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m top1: 0.4039179104477612
[2m[36m(func pid=54942)[0m top5: 0.8708022388059702
[2m[36m(func pid=54942)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=54942)[0m f1_macro: 0.32507673525793845
[2m[36m(func pid=54942)[0m f1_weighted: 0.4384037884765018
[2m[36m(func pid=54942)[0m f1_per_class: [0.26, 0.527, 0.261, 0.452, 0.083, 0.309, 0.478, 0.514, 0.098, 0.268]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.8070 | Steps: 4 | Val loss: 1.7837 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.9834 | Steps: 4 | Val loss: 2.2688 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 13:40:09 (running for 00:47:01.89)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.838 |      0.325 |                   96 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  1.9   |      0.12  |                   67 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.4025186567164179
[2m[36m(func pid=54942)[0m top5: 0.8763992537313433
[2m[36m(func pid=54942)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=54942)[0m f1_macro: 0.3427484177893941
[2m[36m(func pid=54942)[0m f1_weighted: 0.4373423195472678
[2m[36m(func pid=54942)[0m f1_per_class: [0.269, 0.522, 0.4, 0.449, 0.079, 0.304, 0.472, 0.528, 0.14, 0.264]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.20755597014925373
[2m[36m(func pid=61908)[0m top5: 0.7915111940298507
[2m[36m(func pid=61908)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=61908)[0m f1_macro: 0.16056459536429146
[2m[36m(func pid=61908)[0m f1_weighted: 0.24018830102113642
[2m[36m(func pid=61908)[0m f1_per_class: [0.075, 0.226, 0.0, 0.095, 0.05, 0.0, 0.46, 0.525, 0.157, 0.017]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.7589 | Steps: 4 | Val loss: 1.7855 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2248 | Steps: 4 | Val loss: 2.0925 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 13:40:15 (running for 00:47:07.47)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.807 |      0.343 |                   97 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  1.983 |      0.161 |                   68 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.39365671641791045
[2m[36m(func pid=54942)[0m top5: 0.8680037313432836
[2m[36m(func pid=54942)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=54942)[0m f1_macro: 0.34124765784324695
[2m[36m(func pid=54942)[0m f1_weighted: 0.42666411028601187
[2m[36m(func pid=54942)[0m f1_per_class: [0.306, 0.51, 0.431, 0.437, 0.079, 0.313, 0.451, 0.531, 0.115, 0.238]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.25466417910447764
[2m[36m(func pid=61908)[0m top5: 0.8106343283582089
[2m[36m(func pid=61908)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=61908)[0m f1_macro: 0.2268206950521844
[2m[36m(func pid=61908)[0m f1_weighted: 0.303043954284681
[2m[36m(func pid=61908)[0m f1_per_class: [0.085, 0.323, 0.222, 0.178, 0.065, 0.264, 0.436, 0.491, 0.154, 0.05]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.8893 | Steps: 4 | Val loss: 1.7863 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9177 | Steps: 4 | Val loss: 2.1033 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:40:20 (running for 00:47:13.19)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.759 |      0.341 |                   98 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.225 |      0.227 |                   69 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.3787313432835821
[2m[36m(func pid=54942)[0m top5: 0.8656716417910447
[2m[36m(func pid=54942)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=54942)[0m f1_macro: 0.3307021146627084
[2m[36m(func pid=54942)[0m f1_weighted: 0.41232390134568625
[2m[36m(func pid=54942)[0m f1_per_class: [0.305, 0.471, 0.419, 0.429, 0.082, 0.324, 0.431, 0.519, 0.13, 0.198]
[2m[36m(func pid=54942)[0m 
[2m[36m(func pid=61908)[0m top1: 0.2513992537313433
[2m[36m(func pid=61908)[0m top5: 0.8111007462686567
[2m[36m(func pid=61908)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=61908)[0m f1_macro: 0.19701772606621057
[2m[36m(func pid=61908)[0m f1_weighted: 0.27460450265718866
[2m[36m(func pid=61908)[0m f1_per_class: [0.102, 0.329, 0.324, 0.374, 0.087, 0.291, 0.233, 0.0, 0.164, 0.066]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=54942)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.8108 | Steps: 4 | Val loss: 1.7871 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.8902 | Steps: 4 | Val loss: 2.1445 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 13:40:26 (running for 00:47:18.86)
Memory usage on this node: 20.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2965
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00020 | RUNNING    | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.889 |      0.331 |                   99 |
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  1.918 |      0.197 |                   70 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=54942)[0m top1: 0.37406716417910446
[2m[36m(func pid=54942)[0m top5: 0.8600746268656716
[2m[36m(func pid=54942)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=54942)[0m f1_macro: 0.33801833281943067
[2m[36m(func pid=54942)[0m f1_weighted: 0.408948262315649
[2m[36m(func pid=54942)[0m f1_per_class: [0.299, 0.491, 0.429, 0.419, 0.07, 0.301, 0.419, 0.522, 0.174, 0.256]
[2m[36m(func pid=61908)[0m top1: 0.240205223880597
[2m[36m(func pid=61908)[0m top5: 0.8125
[2m[36m(func pid=61908)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=61908)[0m f1_macro: 0.1888160320919422
[2m[36m(func pid=61908)[0m f1_weighted: 0.22802344940831323
[2m[36m(func pid=61908)[0m f1_per_class: [0.114, 0.413, 0.21, 0.358, 0.077, 0.285, 0.0, 0.25, 0.182, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0354 | Steps: 4 | Val loss: 2.3101 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:40:32 (running for 00:47:24.53)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.2965
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  1.89  |      0.189 |                   71 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
| train_52b21_00018 | TERMINATED | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  1.912 |      0.235 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.2439365671641791
[2m[36m(func pid=61908)[0m top5: 0.8232276119402985
[2m[36m(func pid=61908)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=61908)[0m f1_macro: 0.20037840625807365
[2m[36m(func pid=61908)[0m f1_weighted: 0.22340753203666494
[2m[36m(func pid=61908)[0m f1_per_class: [0.119, 0.291, 0.212, 0.35, 0.06, 0.337, 0.0, 0.479, 0.155, 0.0]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3789 | Steps: 4 | Val loss: 2.3595 | Batch size: 32 | lr: 0.1 | Duration: 3.37s
== Status ==
Current time: 2024-01-07 13:40:37 (running for 00:47:30.21)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.2965
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.035 |      0.2   |                   72 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
| train_52b21_00018 | TERMINATED | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  1.912 |      0.235 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.21128731343283583
[2m[36m(func pid=61908)[0m top5: 0.8180970149253731
[2m[36m(func pid=61908)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=61908)[0m f1_macro: 0.1653492435306185
[2m[36m(func pid=61908)[0m f1_weighted: 0.1858477584961634
[2m[36m(func pid=61908)[0m f1_per_class: [0.093, 0.144, 0.16, 0.344, 0.071, 0.319, 0.0, 0.358, 0.123, 0.041]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0732 | Steps: 4 | Val loss: 2.2875 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 13:40:43 (running for 00:47:36.22)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.2965
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.379 |      0.165 |                   73 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
| train_52b21_00018 | TERMINATED | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  1.912 |      0.235 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.21455223880597016
[2m[36m(func pid=61908)[0m top5: 0.7910447761194029
[2m[36m(func pid=61908)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=61908)[0m f1_macro: 0.13556409959847393
[2m[36m(func pid=61908)[0m f1_weighted: 0.249360873957778
[2m[36m(func pid=61908)[0m f1_per_class: [0.066, 0.124, 0.144, 0.247, 0.098, 0.049, 0.492, 0.0, 0.114, 0.023]
[2m[36m(func pid=61908)[0m 
[2m[36m(func pid=61908)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2365 | Steps: 4 | Val loss: 2.3466 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 13:40:49 (running for 00:47:41.81)
Memory usage on this node: 17.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.2965
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00023 | RUNNING    | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.073 |      0.136 |                   74 |
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
| train_52b21_00018 | TERMINATED | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  1.912 |      0.235 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=61908)[0m top1: 0.11800373134328358
[2m[36m(func pid=61908)[0m top5: 0.7229477611940298
[2m[36m(func pid=61908)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=61908)[0m f1_macro: 0.12959039350221507
[2m[36m(func pid=61908)[0m f1_weighted: 0.12929330283073207
[2m[36m(func pid=61908)[0m f1_per_class: [0.095, 0.17, 0.522, 0.109, 0.081, 0.0, 0.203, 0.0, 0.088, 0.027]
== Status ==
Current time: 2024-01-07 13:40:50 (running for 00:47:42.68)
Memory usage on this node: 16.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.29574999999999996
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/119.99 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_52b21_00000 | TERMINATED | 192.168.7.53:143647 | 0.0001 |       0.99 |         0      |  1.205 |      0.309 |                  100 |
| train_52b21_00001 | TERMINATED | 192.168.7.53:144027 | 0.001  |       0.99 |         0      |  0.848 |      0.306 |                   75 |
| train_52b21_00002 | TERMINATED | 192.168.7.53:144451 | 0.01   |       0.99 |         0      |  1.625 |      0.249 |                  100 |
| train_52b21_00003 | TERMINATED | 192.168.7.53:144880 | 0.1    |       0.99 |         0      |  3.351 |      0.08  |                   75 |
| train_52b21_00004 | TERMINATED | 192.168.7.53:162589 | 0.0001 |       0.9  |         0      |  1.834 |      0.275 |                   75 |
| train_52b21_00005 | TERMINATED | 192.168.7.53:163372 | 0.001  |       0.9  |         0      |  1.079 |      0.344 |                  100 |
| train_52b21_00006 | TERMINATED | 192.168.7.53:168552 | 0.01   |       0.9  |         0      |  1.123 |      0.205 |                   75 |
| train_52b21_00007 | TERMINATED | 192.168.7.53:168563 | 0.1    |       0.9  |         0      |  2.4   |      0.138 |                   75 |
| train_52b21_00008 | TERMINATED | 192.168.7.53:181384 | 0.0001 |       0.99 |         0.0001 |  0.982 |      0.325 |                  100 |
| train_52b21_00009 | TERMINATED | 192.168.7.53:186833 | 0.001  |       0.99 |         0.0001 |  1.257 |      0.199 |                   75 |
| train_52b21_00010 | TERMINATED | 192.168.7.53:187437 | 0.01   |       0.99 |         0.0001 |  2.259 |      0.102 |                   75 |
| train_52b21_00011 | TERMINATED | 192.168.7.53:549    | 0.1    |       0.99 |         0.0001 |  3.034 |      0.04  |                   75 |
| train_52b21_00012 | TERMINATED | 192.168.7.53:17787  | 0.0001 |       0.9  |         0.0001 |  1.977 |      0.251 |                   75 |
| train_52b21_00013 | TERMINATED | 192.168.7.53:17891  | 0.001  |       0.9  |         0.0001 |  1.238 |      0.295 |                   75 |
| train_52b21_00014 | TERMINATED | 192.168.7.53:18906  | 0.01   |       0.9  |         0.0001 |  1.872 |      0.282 |                   75 |
| train_52b21_00015 | TERMINATED | 192.168.7.53:19545  | 0.1    |       0.9  |         0.0001 |  1.915 |      0.178 |                   75 |
| train_52b21_00016 | TERMINATED | 192.168.7.53:36467  | 0.0001 |       0.99 |         1e-05  |  0.876 |      0.376 |                  100 |
| train_52b21_00017 | TERMINATED | 192.168.7.53:36555  | 0.001  |       0.99 |         1e-05  |  0.562 |      0.267 |                   75 |
| train_52b21_00018 | TERMINATED | 192.168.7.53:37814  | 0.01   |       0.99 |         1e-05  |  1.912 |      0.235 |                   75 |
| train_52b21_00019 | TERMINATED | 192.168.7.53:37884  | 0.1    |       0.99 |         1e-05  |  2.867 |      0.082 |                   75 |
| train_52b21_00020 | TERMINATED | 192.168.7.53:54942  | 0.0001 |       0.9  |         1e-05  |  1.811 |      0.338 |                  100 |
| train_52b21_00021 | TERMINATED | 192.168.7.53:56305  | 0.001  |       0.9  |         1e-05  |  0.994 |      0.19  |                   75 |
| train_52b21_00022 | TERMINATED | 192.168.7.53:56437  | 0.01   |       0.9  |         1e-05  |  1.941 |      0.197 |                   75 |
| train_52b21_00023 | TERMINATED | 192.168.7.53:61908  | 0.1    |       0.9  |         1e-05  |  2.237 |      0.13  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 13:40:50,408	INFO tune.py:798 -- Total run time: 2863.79 seconds (2862.66 seconds for the tuning loop).
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341348.1 ON aap04 CANCELLED AT 2024-01-07T13:40:55 ***
srun: error: aap04: task 0: Exited with exit code 1

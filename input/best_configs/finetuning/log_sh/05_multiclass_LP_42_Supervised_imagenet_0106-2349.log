IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 00:42:21,906	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 00:42:21,906	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 00:42:24,341	SUCC scripts.py:747 -- --------------------
2024-01-07 00:42:24,341	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 00:42:24,341	SUCC scripts.py:749 -- --------------------
2024-01-07 00:42:24,342	INFO scripts.py:751 -- Next steps
2024-01-07 00:42:24,342	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 00:42:24,342	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 00:42:24,342	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 00:42:24,342	INFO scripts.py:773 -- import ray
2024-01-07 00:42:24,342	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 00:42:24,342	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 00:42:24,342	INFO scripts.py:791 --   ray status
2024-01-07 00:42:24,342	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 00:42:24,342	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 00:42:24,342	INFO scripts.py:810 --   ray stop
2024-01-07 00:42:24,343	INFO scripts.py:891 -- --block
2024-01-07 00:42:24,343	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 00:42:24,343	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              8885778803418199643
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f4fc43bb100>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         imagenet
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [5 5 5 5 5 5 5 5 5 5]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.94
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [5 5 5 5 5 5 5 5 5 5]
Done!
Using ImageNet weights

Supervised model resnet18 with imagenet weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 00:43:06,832	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 00:43:06,844	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 00:43:27,087	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
[2m[36m(func pid=183140)[0m Dataloader to compute accuracy: val== Status ==
Current time: 2024-01-07 00:43:27 (running for 00:00:19.54)
Memory usage on this node: 12.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |
| train_57e67_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_57e67_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_57e67_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183140)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183140)[0m Configuration completed!
[2m[36m(func pid=183140)[0m New optimizer parameters:
[2m[36m(func pid=183140)[0m SGD (
[2m[36m(func pid=183140)[0m Parameter Group 0
[2m[36m(func pid=183140)[0m     dampening: 0
[2m[36m(func pid=183140)[0m     differentiable: False
[2m[36m(func pid=183140)[0m     foreach: None
[2m[36m(func pid=183140)[0m     lr: 0.0001
[2m[36m(func pid=183140)[0m     maximize: False
[2m[36m(func pid=183140)[0m     momentum: 0.99
[2m[36m(func pid=183140)[0m     nesterov: False
[2m[36m(func pid=183140)[0m     weight_decay: 0
[2m[36m(func pid=183140)[0m )
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1580 | Steps: 2 | Val loss: 2.5139 | Batch size: 32 | lr: 0.0001 | Duration: 4.25s
[2m[36m(func pid=183140)[0m top1: 0.06716417910447761
[2m[36m(func pid=183140)[0m top5: 0.4878731343283582
[2m[36m(func pid=183140)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=183140)[0m f1_macro: 0.04018681520435255
[2m[36m(func pid=183140)[0m f1_weighted: 0.038249481724392714
[2m[36m(func pid=183140)[0m f1_per_class: [0.122, 0.01, 0.0, 0.088, 0.0, 0.019, 0.0, 0.105, 0.022, 0.035]
== Status ==
Current time: 2024-01-07 00:43:37 (running for 00:00:29.68)
Memory usage on this node: 15.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |
| train_57e67_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_57e67_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183515)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183515)[0m Configuration completed!
[2m[36m(func pid=183515)[0m New optimizer parameters:
[2m[36m(func pid=183515)[0m SGD (
[2m[36m(func pid=183515)[0m Parameter Group 0
[2m[36m(func pid=183515)[0m     dampening: 0
[2m[36m(func pid=183515)[0m     differentiable: False
[2m[36m(func pid=183515)[0m     foreach: None
[2m[36m(func pid=183515)[0m     lr: 0.001
[2m[36m(func pid=183515)[0m     maximize: False
[2m[36m(func pid=183515)[0m     momentum: 0.99
[2m[36m(func pid=183515)[0m     nesterov: False
[2m[36m(func pid=183515)[0m     weight_decay: 0
[2m[36m(func pid=183515)[0m )
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1831 | Steps: 2 | Val loss: 2.4910 | Batch size: 32 | lr: 0.001 | Duration: 4.58s
[2m[36m(func pid=183515)[0m top1: 0.06763059701492537
[2m[36m(func pid=183515)[0m top5: 0.4808768656716418
[2m[36m(func pid=183515)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=183515)[0m f1_macro: 0.04277117713972649
[2m[36m(func pid=183515)[0m f1_weighted: 0.04288454675816041
[2m[36m(func pid=183515)[0m f1_per_class: [0.138, 0.01, 0.0, 0.105, 0.0, 0.017, 0.0, 0.102, 0.021, 0.034]
== Status ==
Current time: 2024-01-07 00:43:46 (running for 00:00:38.87)
Memory usage on this node: 17.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |
| train_57e67_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183934)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183934)[0m Configuration completed!
[2m[36m(func pid=183934)[0m New optimizer parameters:
[2m[36m(func pid=183934)[0m SGD (
[2m[36m(func pid=183934)[0m Parameter Group 0
[2m[36m(func pid=183934)[0m     dampening: 0
[2m[36m(func pid=183934)[0m     differentiable: False
[2m[36m(func pid=183934)[0m     foreach: None
[2m[36m(func pid=183934)[0m     lr: 0.01
[2m[36m(func pid=183934)[0m     maximize: False
[2m[36m(func pid=183934)[0m     momentum: 0.99
[2m[36m(func pid=183934)[0m     nesterov: False
[2m[36m(func pid=183934)[0m     weight_decay: 0
[2m[36m(func pid=183934)[0m )
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2072 | Steps: 2 | Val loss: 2.3884 | Batch size: 32 | lr: 0.01 | Duration: 4.50s
[2m[36m(func pid=183934)[0m top1: 0.06110074626865672
[2m[36m(func pid=183934)[0m top5: 0.46455223880597013
[2m[36m(func pid=183934)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=183934)[0m f1_macro: 0.06583395882703212
[2m[36m(func pid=183934)[0m f1_weighted: 0.06261834340306677
[2m[36m(func pid=183934)[0m f1_per_class: [0.118, 0.106, 0.044, 0.064, 0.01, 0.048, 0.034, 0.059, 0.129, 0.045]
== Status ==
Current time: 2024-01-07 00:43:54 (running for 00:00:47.10)
Memory usage on this node: 20.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 00:44:03 (running for 00:00:55.77)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |        |            |                      |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  3.207 |      0.066 |                    1 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |        |            |                      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=184353)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=184353)[0m Configuration completed!
[2m[36m(func pid=184353)[0m New optimizer parameters:
[2m[36m(func pid=184353)[0m SGD (
[2m[36m(func pid=184353)[0m Parameter Group 0
[2m[36m(func pid=184353)[0m     dampening: 0
[2m[36m(func pid=184353)[0m     differentiable: False
[2m[36m(func pid=184353)[0m     foreach: None
[2m[36m(func pid=184353)[0m     lr: 0.1
[2m[36m(func pid=184353)[0m     maximize: False
[2m[36m(func pid=184353)[0m     momentum: 0.99
[2m[36m(func pid=184353)[0m     nesterov: False
[2m[36m(func pid=184353)[0m     weight_decay: 0
[2m[36m(func pid=184353)[0m )
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1301 | Steps: 2 | Val loss: 2.5376 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1262 | Steps: 2 | Val loss: 2.4488 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8324 | Steps: 2 | Val loss: 2.4424 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.1691 | Steps: 2 | Val loss: 7.6061 | Batch size: 32 | lr: 0.1 | Duration: 4.54s
== Status ==
Current time: 2024-01-07 00:44:08 (running for 00:01:00.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.158 |      0.04  |                    1 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  3.183 |      0.043 |                    1 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  3.207 |      0.066 |                    1 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |        |            |                      |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.06343283582089553
[2m[36m(func pid=183140)[0m top5: 0.478544776119403
[2m[36m(func pid=183140)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=183140)[0m f1_macro: 0.036707410963900344
[2m[36m(func pid=183140)[0m f1_weighted: 0.035098008802594644
[2m[36m(func pid=183140)[0m f1_per_class: [0.103, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.102, 0.023, 0.031]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.06763059701492537
[2m[36m(func pid=183515)[0m top5: 0.47901119402985076
[2m[36m(func pid=183515)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=183515)[0m f1_macro: 0.04831996643064834
[2m[36m(func pid=183515)[0m f1_weighted: 0.053780145740220865
[2m[36m(func pid=183515)[0m f1_per_class: [0.076, 0.057, 0.0, 0.113, 0.0, 0.026, 0.0, 0.092, 0.059, 0.06]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.08022388059701492
[2m[36m(func pid=183934)[0m top5: 0.4939365671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=183934)[0m f1_macro: 0.06150255096784557
[2m[36m(func pid=183934)[0m f1_weighted: 0.0710985967612463
[2m[36m(func pid=183934)[0m f1_per_class: [0.106, 0.005, 0.143, 0.0, 0.033, 0.049, 0.197, 0.0, 0.082, 0.0]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.10121268656716417
[2m[36m(func pid=184353)[0m top5: 0.31902985074626866
[2m[36m(func pid=184353)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=184353)[0m f1_macro: 0.1187449512494307
[2m[36m(func pid=184353)[0m f1_weighted: 0.03931852655720324
[2m[36m(func pid=184353)[0m f1_per_class: [0.121, 0.0, 0.696, 0.0, 0.0, 0.251, 0.0, 0.0, 0.121, 0.0]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9990 | Steps: 2 | Val loss: 2.4031 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1061 | Steps: 2 | Val loss: 2.5547 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4898 | Steps: 2 | Val loss: 2.1171 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 8.6394 | Steps: 2 | Val loss: 17.7199 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=183515)[0m top1: 0.0708955223880597
[2m[36m(func pid=183515)[0m top5: 0.47154850746268656
[2m[36m(func pid=183515)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.056233945426477985
[2m[36m(func pid=183515)[0m f1_weighted: 0.0733071295915411
[2m[36m(func pid=183515)[0m f1_per_class: [0.083, 0.11, 0.0, 0.13, 0.0, 0.032, 0.018, 0.084, 0.067, 0.038]
== Status ==
Current time: 2024-01-07 00:44:13 (running for 00:01:06.00)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.13  |      0.037 |                    2 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.999 |      0.056 |                    3 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  2.832 |      0.062 |                    2 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  4.169 |      0.119 |                    1 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.06110074626865672
[2m[36m(func pid=183140)[0m top5: 0.46968283582089554
[2m[36m(func pid=183140)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=183140)[0m f1_macro: 0.03049259568181074
[2m[36m(func pid=183140)[0m f1_weighted: 0.033821015488528
[2m[36m(func pid=183140)[0m f1_per_class: [0.053, 0.015, 0.0, 0.078, 0.0, 0.02, 0.0, 0.099, 0.0, 0.041]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.2677238805970149
[2m[36m(func pid=183934)[0m top5: 0.7327425373134329
[2m[36m(func pid=183934)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=183934)[0m f1_macro: 0.1536408446056739
[2m[36m(func pid=183934)[0m f1_weighted: 0.19257091729444595
[2m[36m(func pid=183934)[0m f1_per_class: [0.235, 0.176, 0.253, 0.047, 0.027, 0.045, 0.441, 0.0, 0.118, 0.195]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.15391791044776118
[2m[36m(func pid=184353)[0m top5: 0.3917910447761194
[2m[36m(func pid=184353)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=184353)[0m f1_macro: 0.0924469886501097
[2m[36m(func pid=184353)[0m f1_weighted: 0.09053178429196636
[2m[36m(func pid=184353)[0m f1_per_class: [0.0, 0.38, 0.293, 0.0, 0.051, 0.201, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8284 | Steps: 2 | Val loss: 2.3615 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1118 | Steps: 2 | Val loss: 2.5587 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.7887 | Steps: 2 | Val loss: 1.8578 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 14.2587 | Steps: 2 | Val loss: 26.1694 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 00:44:18 (running for 00:01:11.03)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.106 |      0.03  |                    3 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.828 |      0.062 |                    4 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  2.49  |      0.154 |                    3 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  8.639 |      0.092 |                    2 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.07136194029850747
[2m[36m(func pid=183515)[0m top5: 0.49580223880597013
[2m[36m(func pid=183515)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=183515)[0m f1_macro: 0.0616920036765682
[2m[36m(func pid=183515)[0m f1_weighted: 0.08308702907571847
[2m[36m(func pid=183515)[0m f1_per_class: [0.062, 0.139, 0.0, 0.104, 0.005, 0.053, 0.051, 0.073, 0.095, 0.036]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.061567164179104475
[2m[36m(func pid=183140)[0m top5: 0.46548507462686567
[2m[36m(func pid=183140)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=183140)[0m f1_macro: 0.03318170107377807
[2m[36m(func pid=183140)[0m f1_weighted: 0.039343383843794426
[2m[36m(func pid=183140)[0m f1_per_class: [0.048, 0.036, 0.0, 0.085, 0.0, 0.02, 0.0, 0.096, 0.0, 0.047]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.33115671641791045
[2m[36m(func pid=183934)[0m top5: 0.8661380597014925
[2m[36m(func pid=183934)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=183934)[0m f1_macro: 0.24017113123932124
[2m[36m(func pid=183934)[0m f1_weighted: 0.2902634072556943
[2m[36m(func pid=183934)[0m f1_per_class: [0.347, 0.17, 0.524, 0.52, 0.094, 0.109, 0.274, 0.141, 0.0, 0.222]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.08861940298507463
[2m[36m(func pid=184353)[0m top5: 0.37220149253731344
[2m[36m(func pid=184353)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=184353)[0m f1_macro: 0.08765095433756316
[2m[36m(func pid=184353)[0m f1_weighted: 0.025572759790609353
[2m[36m(func pid=184353)[0m f1_per_class: [0.191, 0.031, 0.26, 0.0, 0.0, 0.015, 0.0, 0.141, 0.095, 0.143]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0744 | Steps: 2 | Val loss: 2.5598 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7323 | Steps: 2 | Val loss: 2.3444 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.3437 | Steps: 2 | Val loss: 1.9854 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 11.7569 | Steps: 2 | Val loss: 23.7670 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183515)[0m top1: 0.09095149253731344
[2m[36m(func pid=183515)[0m top5: 0.5303171641791045
[2m[36m(func pid=183515)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=183515)[0m f1_macro: 0.06991568116756512
[2m[36m(func pid=183515)[0m f1_weighted: 0.10339684221608864
[2m[36m(func pid=183515)[0m f1_per_class: [0.096, 0.097, 0.031, 0.064, 0.009, 0.062, 0.181, 0.034, 0.095, 0.03]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:44:23 (running for 00:01:16.14)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.074 |      0.034 |                    5 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.732 |      0.07  |                    5 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  1.789 |      0.24  |                    4 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      | 14.259 |      0.088 |                    3 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.061567164179104475
[2m[36m(func pid=183140)[0m top5: 0.458955223880597
[2m[36m(func pid=183140)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=183140)[0m f1_macro: 0.03421433350923724
[2m[36m(func pid=183140)[0m f1_weighted: 0.04414103266806487
[2m[36m(func pid=183140)[0m f1_per_class: [0.04, 0.055, 0.0, 0.092, 0.0, 0.019, 0.0, 0.093, 0.0, 0.042]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.3031716417910448
[2m[36m(func pid=183934)[0m top5: 0.7971082089552238
[2m[36m(func pid=183934)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=183934)[0m f1_macro: 0.28319720064712606
[2m[36m(func pid=183934)[0m f1_weighted: 0.26610464170703
[2m[36m(func pid=183934)[0m f1_per_class: [0.299, 0.35, 0.815, 0.537, 0.096, 0.225, 0.003, 0.257, 0.0, 0.25]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.17537313432835822
[2m[36m(func pid=184353)[0m top5: 0.45475746268656714
[2m[36m(func pid=184353)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=184353)[0m f1_macro: 0.17716374390753792
[2m[36m(func pid=184353)[0m f1_weighted: 0.11455632400473559
[2m[36m(func pid=184353)[0m f1_per_class: [0.375, 0.254, 0.16, 0.03, 0.0, 0.305, 0.0, 0.194, 0.121, 0.333]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6177 | Steps: 2 | Val loss: 2.3311 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0295 | Steps: 2 | Val loss: 2.5504 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9685 | Steps: 2 | Val loss: 2.1796 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 9.0663 | Steps: 2 | Val loss: 19.4725 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=183515)[0m top1: 0.1296641791044776
[2m[36m(func pid=183515)[0m top5: 0.550839552238806
[2m[36m(func pid=183515)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=183515)[0m f1_macro: 0.07973024894093637
[2m[36m(func pid=183515)[0m f1_weighted: 0.12043270437276982
[2m[36m(func pid=183515)[0m f1_per_class: [0.114, 0.056, 0.093, 0.038, 0.011, 0.099, 0.276, 0.0, 0.109, 0.0]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.0625
[2m[36m(func pid=183140)[0m top5: 0.45009328358208955
[2m[36m(func pid=183140)[0m f1_micro: 0.0625
[2m[36m(func pid=183140)[0m f1_macro: 0.03916702199681546
[2m[36m(func pid=183140)[0m f1_weighted: 0.04929432472958863
[2m[36m(func pid=183140)[0m f1_per_class: [0.049, 0.071, 0.0, 0.098, 0.0, 0.019, 0.0, 0.092, 0.025, 0.038]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:44:29 (running for 00:01:21.92)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.03  |      0.039 |                    6 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.618 |      0.08  |                    6 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.968 |      0.277 |                    6 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      | 11.757 |      0.177 |                    4 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.259794776119403
[2m[36m(func pid=183934)[0m top5: 0.7803171641791045
[2m[36m(func pid=183934)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=183934)[0m f1_macro: 0.27652940240007423
[2m[36m(func pid=183934)[0m f1_weighted: 0.23357803623667475
[2m[36m(func pid=183934)[0m f1_per_class: [0.3, 0.445, 0.846, 0.353, 0.103, 0.251, 0.0, 0.247, 0.043, 0.176]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.314365671641791
[2m[36m(func pid=184353)[0m top5: 0.613339552238806
[2m[36m(func pid=184353)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=184353)[0m f1_macro: 0.22376950084163444
[2m[36m(func pid=184353)[0m f1_weighted: 0.22336399871126938
[2m[36m(func pid=184353)[0m f1_per_class: [0.455, 0.267, 0.191, 0.484, 0.191, 0.197, 0.0, 0.0, 0.124, 0.328]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4677 | Steps: 2 | Val loss: 2.3324 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0369 | Steps: 2 | Val loss: 2.5453 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4933 | Steps: 2 | Val loss: 2.2098 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.0750 | Steps: 2 | Val loss: 15.0922 | Batch size: 32 | lr: 0.1 | Duration: 2.52s
[2m[36m(func pid=183515)[0m top1: 0.15205223880597016
[2m[36m(func pid=183515)[0m top5: 0.5522388059701493
[2m[36m(func pid=183515)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=183515)[0m f1_macro: 0.08705991016330586
[2m[36m(func pid=183515)[0m f1_weighted: 0.12404791500319869
[2m[36m(func pid=183515)[0m f1_per_class: [0.128, 0.044, 0.126, 0.013, 0.023, 0.111, 0.311, 0.0, 0.115, 0.0]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.06296641791044776
[2m[36m(func pid=183140)[0m top5: 0.44449626865671643
[2m[36m(func pid=183140)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=183140)[0m f1_macro: 0.04301503679649597
[2m[36m(func pid=183140)[0m f1_weighted: 0.05546932663167291
[2m[36m(func pid=183140)[0m f1_per_class: [0.05, 0.072, 0.0, 0.115, 0.0, 0.024, 0.0, 0.089, 0.048, 0.032]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:44:35 (running for 00:01:27.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  3.037 |      0.043 |                    7 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.468 |      0.087 |                    7 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.493 |      0.31  |                    7 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  9.066 |      0.224 |                    5 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.23041044776119404
[2m[36m(func pid=183934)[0m top5: 0.8069029850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=183934)[0m f1_macro: 0.3096586313260237
[2m[36m(func pid=183934)[0m f1_weighted: 0.2194714338468944
[2m[36m(func pid=183934)[0m f1_per_class: [0.439, 0.353, 0.8, 0.275, 0.087, 0.22, 0.051, 0.281, 0.192, 0.4]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.27472014925373134
[2m[36m(func pid=184353)[0m top5: 0.6529850746268657
[2m[36m(func pid=184353)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=184353)[0m f1_macro: 0.19281706996536377
[2m[36m(func pid=184353)[0m f1_weighted: 0.20954288894565753
[2m[36m(func pid=184353)[0m f1_per_class: [0.553, 0.27, 0.429, 0.513, 0.038, 0.0, 0.012, 0.0, 0.045, 0.069]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4176 | Steps: 2 | Val loss: 2.3251 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9398 | Steps: 2 | Val loss: 2.5328 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.2499 | Steps: 2 | Val loss: 2.5038 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3282 | Steps: 2 | Val loss: 10.4007 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=183515)[0m top1: 0.1646455223880597
[2m[36m(func pid=183515)[0m top5: 0.5583022388059702
[2m[36m(func pid=183515)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.09874697487889424
[2m[36m(func pid=183515)[0m f1_weighted: 0.13228751352826254
[2m[36m(func pid=183515)[0m f1_per_class: [0.169, 0.055, 0.122, 0.01, 0.025, 0.121, 0.328, 0.0, 0.107, 0.051]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.0625
[2m[36m(func pid=183140)[0m top5: 0.42677238805970147
[2m[36m(func pid=183140)[0m f1_micro: 0.0625
[2m[36m(func pid=183140)[0m f1_macro: 0.04358644632026183
[2m[36m(func pid=183140)[0m f1_weighted: 0.05857210486963236
[2m[36m(func pid=183140)[0m f1_per_class: [0.058, 0.069, 0.0, 0.128, 0.0, 0.017, 0.003, 0.09, 0.042, 0.028]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:44:40 (running for 00:01:32.60)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.94  |      0.044 |                    8 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.418 |      0.099 |                    8 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.25  |      0.286 |                    8 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  4.075 |      0.193 |                    6 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.21455223880597016
[2m[36m(func pid=183934)[0m top5: 0.8432835820895522
[2m[36m(func pid=183934)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=183934)[0m f1_macro: 0.28586296514682685
[2m[36m(func pid=183934)[0m f1_weighted: 0.2618843262479895
[2m[36m(func pid=183934)[0m f1_per_class: [0.485, 0.224, 0.727, 0.236, 0.084, 0.131, 0.368, 0.186, 0.115, 0.303]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.37779850746268656
[2m[36m(func pid=184353)[0m top5: 0.8889925373134329
[2m[36m(func pid=184353)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=184353)[0m f1_macro: 0.25294666729000803
[2m[36m(func pid=184353)[0m f1_weighted: 0.4048936606002123
[2m[36m(func pid=184353)[0m f1_per_class: [0.316, 0.447, 0.533, 0.5, 0.038, 0.0, 0.593, 0.0, 0.025, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2844 | Steps: 2 | Val loss: 2.2846 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9654 | Steps: 2 | Val loss: 2.5130 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.2488 | Steps: 2 | Val loss: 2.6079 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.7980 | Steps: 2 | Val loss: 12.3564 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=183515)[0m top1: 0.1707089552238806
[2m[36m(func pid=183515)[0m top5: 0.5951492537313433
[2m[36m(func pid=183515)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=183515)[0m f1_macro: 0.10835998500572375
[2m[36m(func pid=183515)[0m f1_weighted: 0.1424270418359758
[2m[36m(func pid=183515)[0m f1_per_class: [0.189, 0.059, 0.118, 0.023, 0.03, 0.126, 0.343, 0.0, 0.101, 0.095]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.06529850746268656
[2m[36m(func pid=183140)[0m top5: 0.41884328358208955
[2m[36m(func pid=183140)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=183140)[0m f1_macro: 0.049564896811621094
[2m[36m(func pid=183140)[0m f1_weighted: 0.06692080785185144
[2m[36m(func pid=183140)[0m f1_per_class: [0.054, 0.083, 0.0, 0.134, 0.0, 0.042, 0.006, 0.086, 0.071, 0.02]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:44:45 (running for 00:01:37.64)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.965 |      0.05  |                    9 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.284 |      0.108 |                    9 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.249 |      0.292 |                    9 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  2.328 |      0.253 |                    7 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.2751865671641791
[2m[36m(func pid=183934)[0m top5: 0.8540111940298507
[2m[36m(func pid=183934)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=183934)[0m f1_macro: 0.2918384617673341
[2m[36m(func pid=183934)[0m f1_weighted: 0.3116015298161926
[2m[36m(func pid=183934)[0m f1_per_class: [0.5, 0.205, 0.727, 0.265, 0.088, 0.077, 0.559, 0.069, 0.121, 0.308]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.36473880597014924
[2m[36m(func pid=184353)[0m top5: 0.8227611940298507
[2m[36m(func pid=184353)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=184353)[0m f1_macro: 0.22647995703627438
[2m[36m(func pid=184353)[0m f1_weighted: 0.3291873972005336
[2m[36m(func pid=184353)[0m f1_per_class: [0.269, 0.417, 0.629, 0.214, 0.049, 0.015, 0.619, 0.013, 0.041, 0.0]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0696 | Steps: 2 | Val loss: 2.2258 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8797 | Steps: 2 | Val loss: 2.5004 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2854 | Steps: 2 | Val loss: 2.4106 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=183515)[0m top1: 0.177705223880597
[2m[36m(func pid=183515)[0m top5: 0.6487873134328358
[2m[36m(func pid=183515)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.12634543670211246
[2m[36m(func pid=183515)[0m f1_weighted: 0.16974546638867857
[2m[36m(func pid=183515)[0m f1_per_class: [0.198, 0.092, 0.143, 0.093, 0.029, 0.132, 0.345, 0.0, 0.101, 0.131]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.3483 | Steps: 2 | Val loss: 24.5758 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=183140)[0m top1: 0.06809701492537314
[2m[36m(func pid=183140)[0m top5: 0.4155783582089552
[2m[36m(func pid=183140)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=183140)[0m f1_macro: 0.05212966805627276
[2m[36m(func pid=183140)[0m f1_weighted: 0.07216586744243447
[2m[36m(func pid=183140)[0m f1_per_class: [0.07, 0.093, 0.0, 0.145, 0.0, 0.04, 0.009, 0.078, 0.065, 0.021]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:44:50 (running for 00:01:42.91)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.88  |      0.052 |                   10 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  2.07  |      0.126 |                   10 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.285 |      0.34  |                   10 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  3.798 |      0.226 |                    8 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.36427238805970147
[2m[36m(func pid=183934)[0m top5: 0.8708022388059702
[2m[36m(func pid=183934)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=183934)[0m f1_macro: 0.3402597830850241
[2m[36m(func pid=183934)[0m f1_weighted: 0.39262381024211324
[2m[36m(func pid=183934)[0m f1_per_class: [0.578, 0.301, 0.783, 0.413, 0.074, 0.067, 0.624, 0.103, 0.162, 0.298]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.09888059701492537
[2m[36m(func pid=184353)[0m top5: 0.7784514925373134
[2m[36m(func pid=184353)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=184353)[0m f1_macro: 0.14901456216511313
[2m[36m(func pid=184353)[0m f1_weighted: 0.07070587687331145
[2m[36m(func pid=184353)[0m f1_per_class: [0.044, 0.031, 0.727, 0.013, 0.154, 0.192, 0.08, 0.118, 0.054, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.7870 | Steps: 2 | Val loss: 2.1460 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8225 | Steps: 2 | Val loss: 2.4909 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0841 | Steps: 2 | Val loss: 2.2149 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=183515)[0m top1: 0.19916044776119404
[2m[36m(func pid=183515)[0m top5: 0.7234141791044776
[2m[36m(func pid=183515)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=183515)[0m f1_macro: 0.15644364377256376
[2m[36m(func pid=183515)[0m f1_weighted: 0.22417196147087404
[2m[36m(func pid=183515)[0m f1_per_class: [0.211, 0.135, 0.229, 0.268, 0.033, 0.136, 0.333, 0.0, 0.132, 0.087]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1934 | Steps: 2 | Val loss: 32.5424 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=183140)[0m top1: 0.06763059701492537
[2m[36m(func pid=183140)[0m top5: 0.40625
[2m[36m(func pid=183140)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=183140)[0m f1_macro: 0.053432379223376736
[2m[36m(func pid=183140)[0m f1_weighted: 0.07208913255816378
[2m[36m(func pid=183140)[0m f1_per_class: [0.075, 0.104, 0.0, 0.136, 0.0, 0.044, 0.009, 0.077, 0.068, 0.022]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.3824626865671642
[2m[36m(func pid=183934)[0m top5: 0.8903917910447762
[2m[36m(func pid=183934)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=183934)[0m f1_macro: 0.35653093408640935
[2m[36m(func pid=183934)[0m f1_weighted: 0.41401790425473844
[2m[36m(func pid=183934)[0m f1_per_class: [0.476, 0.362, 0.769, 0.479, 0.069, 0.081, 0.573, 0.212, 0.204, 0.34]
[2m[36m(func pid=183934)[0m 
== Status ==
Current time: 2024-01-07 00:44:56 (running for 00:01:48.45)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.822 |      0.053 |                   11 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.787 |      0.156 |                   11 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.084 |      0.357 |                   11 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.193 |      0.181 |                   10 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.13432835820895522
[2m[36m(func pid=184353)[0m top5: 0.6497201492537313
[2m[36m(func pid=184353)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=184353)[0m f1_macro: 0.1810299578980169
[2m[36m(func pid=184353)[0m f1_weighted: 0.06115271421603015
[2m[36m(func pid=184353)[0m f1_per_class: [0.128, 0.031, 0.696, 0.01, 0.219, 0.217, 0.006, 0.204, 0.124, 0.176]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.6295 | Steps: 2 | Val loss: 2.0842 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8129 | Steps: 2 | Val loss: 2.4807 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0281 | Steps: 2 | Val loss: 2.1989 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=183515)[0m top1: 0.23041044776119404
[2m[36m(func pid=183515)[0m top5: 0.7686567164179104
[2m[36m(func pid=183515)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=183515)[0m f1_macro: 0.1947880906283163
[2m[36m(func pid=183515)[0m f1_weighted: 0.2601941166958406
[2m[36m(func pid=183515)[0m f1_per_class: [0.267, 0.173, 0.373, 0.411, 0.038, 0.129, 0.289, 0.0, 0.173, 0.095]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4652 | Steps: 2 | Val loss: 40.8328 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=183140)[0m top1: 0.06716417910447761
[2m[36m(func pid=183140)[0m top5: 0.4039179104477612
[2m[36m(func pid=183140)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=183140)[0m f1_macro: 0.05780146590528561
[2m[36m(func pid=183140)[0m f1_weighted: 0.07351089840083437
[2m[36m(func pid=183140)[0m f1_per_class: [0.066, 0.108, 0.029, 0.128, 0.0, 0.047, 0.018, 0.063, 0.091, 0.028]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.37080223880597013
[2m[36m(func pid=183934)[0m top5: 0.898320895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=183934)[0m f1_macro: 0.3787515501365547
[2m[36m(func pid=183934)[0m f1_weighted: 0.3977442556260255
[2m[36m(func pid=183934)[0m f1_per_class: [0.497, 0.419, 0.815, 0.523, 0.084, 0.101, 0.419, 0.252, 0.238, 0.441]
[2m[36m(func pid=183934)[0m 
== Status ==
Current time: 2024-01-07 00:45:01 (running for 00:01:53.78)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.813 |      0.058 |                   12 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.63  |      0.195 |                   12 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.028 |      0.379 |                   12 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.465 |      0.223 |                   11 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.4079 | Steps: 2 | Val loss: 2.0445 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=184353)[0m top1: 0.1553171641791045
[2m[36m(func pid=184353)[0m top5: 0.53125
[2m[36m(func pid=184353)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=184353)[0m f1_macro: 0.2234687115279767
[2m[36m(func pid=184353)[0m f1_weighted: 0.07439198565399702
[2m[36m(func pid=184353)[0m f1_per_class: [0.444, 0.031, 0.667, 0.013, 0.182, 0.234, 0.0, 0.292, 0.135, 0.237]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7810 | Steps: 2 | Val loss: 2.4713 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0200 | Steps: 2 | Val loss: 2.3220 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=183515)[0m top1: 0.25513059701492535
[2m[36m(func pid=183515)[0m top5: 0.7653917910447762
[2m[36m(func pid=183515)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=183515)[0m f1_macro: 0.22825409177450165
[2m[36m(func pid=183515)[0m f1_weighted: 0.2665613676792819
[2m[36m(func pid=183515)[0m f1_per_class: [0.35, 0.206, 0.489, 0.497, 0.04, 0.088, 0.189, 0.151, 0.17, 0.102]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0903 | Steps: 2 | Val loss: 40.2109 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183140)[0m top1: 0.06389925373134328
[2m[36m(func pid=183140)[0m top5: 0.39925373134328357
[2m[36m(func pid=183140)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=183140)[0m f1_macro: 0.05390981554012401
[2m[36m(func pid=183140)[0m f1_weighted: 0.0710162101568136
[2m[36m(func pid=183140)[0m f1_per_class: [0.071, 0.108, 0.019, 0.111, 0.0, 0.041, 0.032, 0.049, 0.082, 0.027]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.35867537313432835
[2m[36m(func pid=183934)[0m top5: 0.8992537313432836
[2m[36m(func pid=183934)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=183934)[0m f1_macro: 0.365937001398736
[2m[36m(func pid=183934)[0m f1_weighted: 0.3737471198099877
[2m[36m(func pid=183934)[0m f1_per_class: [0.444, 0.444, 0.815, 0.557, 0.094, 0.133, 0.286, 0.255, 0.206, 0.424]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.3178 | Steps: 2 | Val loss: 2.0122 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:45:06 (running for 00:01:59.20)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.781 |      0.054 |                   13 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.408 |      0.228 |                   13 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.02  |      0.366 |                   13 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  2.09  |      0.239 |                   12 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.17210820895522388
[2m[36m(func pid=184353)[0m top5: 0.5461753731343284
[2m[36m(func pid=184353)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=184353)[0m f1_macro: 0.23944030662844362
[2m[36m(func pid=184353)[0m f1_weighted: 0.12199013587032742
[2m[36m(func pid=184353)[0m f1_per_class: [0.595, 0.102, 0.556, 0.11, 0.2, 0.28, 0.0, 0.323, 0.159, 0.07]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7276 | Steps: 2 | Val loss: 2.4657 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=183515)[0m top1: 0.2789179104477612
[2m[36m(func pid=183515)[0m top5: 0.7527985074626866
[2m[36m(func pid=183515)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=183515)[0m f1_macro: 0.25615308256565766
[2m[36m(func pid=183515)[0m f1_weighted: 0.257182051608323
[2m[36m(func pid=183515)[0m f1_per_class: [0.42, 0.251, 0.579, 0.513, 0.05, 0.071, 0.087, 0.286, 0.196, 0.108]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0203 | Steps: 2 | Val loss: 2.5229 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=183140)[0m top1: 0.06389925373134328
[2m[36m(func pid=183140)[0m top5: 0.39738805970149255
[2m[36m(func pid=183140)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=183140)[0m f1_macro: 0.05529214865108125
[2m[36m(func pid=183140)[0m f1_weighted: 0.07251878794453843
[2m[36m(func pid=183140)[0m f1_per_class: [0.064, 0.101, 0.042, 0.096, 0.0, 0.061, 0.051, 0.036, 0.072, 0.031]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.2490 | Steps: 2 | Val loss: 37.4497 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=183934)[0m top1: 0.3451492537313433
[2m[36m(func pid=183934)[0m top5: 0.8969216417910447
[2m[36m(func pid=183934)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=183934)[0m f1_macro: 0.36079769110715765
[2m[36m(func pid=183934)[0m f1_weighted: 0.34623147279801625
[2m[36m(func pid=183934)[0m f1_per_class: [0.424, 0.467, 0.815, 0.562, 0.104, 0.132, 0.177, 0.24, 0.226, 0.462]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.1892 | Steps: 2 | Val loss: 1.9900 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 00:45:12 (running for 00:02:04.41)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.728 |      0.055 |                   14 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.318 |      0.256 |                   14 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.02  |      0.361 |                   14 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.249 |      0.256 |                   13 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.21455223880597016
[2m[36m(func pid=184353)[0m top5: 0.5778917910447762
[2m[36m(func pid=184353)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=184353)[0m f1_macro: 0.2562149518030299
[2m[36m(func pid=184353)[0m f1_weighted: 0.19989016111405866
[2m[36m(func pid=184353)[0m f1_per_class: [0.42, 0.274, 0.471, 0.274, 0.218, 0.329, 0.0, 0.318, 0.194, 0.065]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7366 | Steps: 2 | Val loss: 2.4585 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=183515)[0m top1: 0.29151119402985076
[2m[36m(func pid=183515)[0m top5: 0.7430037313432836
[2m[36m(func pid=183515)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=183515)[0m f1_macro: 0.2627897824904409
[2m[36m(func pid=183515)[0m f1_weighted: 0.24799327680634659
[2m[36m(func pid=183515)[0m f1_per_class: [0.42, 0.284, 0.647, 0.512, 0.055, 0.06, 0.039, 0.308, 0.171, 0.132]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0901 | Steps: 2 | Val loss: 2.6774 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=183140)[0m top1: 0.06203358208955224
[2m[36m(func pid=183140)[0m top5: 0.404384328358209
[2m[36m(func pid=183140)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=183140)[0m f1_macro: 0.05436197066408843
[2m[36m(func pid=183140)[0m f1_weighted: 0.06975663547451441
[2m[36m(func pid=183140)[0m f1_per_class: [0.066, 0.091, 0.043, 0.075, 0.0, 0.059, 0.067, 0.022, 0.09, 0.029]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6443 | Steps: 2 | Val loss: 35.1704 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.0242 | Steps: 2 | Val loss: 1.9801 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=183934)[0m top1: 0.35074626865671643
[2m[36m(func pid=183934)[0m top5: 0.8992537313432836
[2m[36m(func pid=183934)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=183934)[0m f1_macro: 0.36337167843235907
[2m[36m(func pid=183934)[0m f1_weighted: 0.3462978209865339
[2m[36m(func pid=183934)[0m f1_per_class: [0.441, 0.487, 0.815, 0.573, 0.11, 0.142, 0.152, 0.243, 0.211, 0.462]
[2m[36m(func pid=183934)[0m 
== Status ==
Current time: 2024-01-07 00:45:17 (running for 00:02:09.48)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.737 |      0.054 |                   15 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.189 |      0.263 |                   15 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.09  |      0.363 |                   15 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.644 |      0.225 |                   14 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.2555970149253731
[2m[36m(func pid=184353)[0m top5: 0.6114738805970149
[2m[36m(func pid=184353)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=184353)[0m f1_macro: 0.2253273143590165
[2m[36m(func pid=184353)[0m f1_weighted: 0.23046566660678883
[2m[36m(func pid=184353)[0m f1_per_class: [0.274, 0.335, 0.143, 0.349, 0.163, 0.332, 0.009, 0.325, 0.208, 0.115]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7162 | Steps: 2 | Val loss: 2.4519 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.2966417910447761
[2m[36m(func pid=183515)[0m top5: 0.7509328358208955
[2m[36m(func pid=183515)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=183515)[0m f1_macro: 0.2692634439812465
[2m[36m(func pid=183515)[0m f1_weighted: 0.2499743717578389
[2m[36m(func pid=183515)[0m f1_per_class: [0.414, 0.342, 0.667, 0.51, 0.067, 0.045, 0.025, 0.268, 0.183, 0.173]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0208 | Steps: 2 | Val loss: 2.8858 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=183140)[0m top1: 0.06902985074626866
[2m[36m(func pid=183140)[0m top5: 0.41324626865671643
[2m[36m(func pid=183140)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=183140)[0m f1_macro: 0.05696289797097105
[2m[36m(func pid=183140)[0m f1_weighted: 0.07972982299045285
[2m[36m(func pid=183140)[0m f1_per_class: [0.074, 0.088, 0.034, 0.066, 0.0, 0.065, 0.11, 0.013, 0.091, 0.029]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.4717 | Steps: 2 | Val loss: 35.1662 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8499 | Steps: 2 | Val loss: 1.9667 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=183934)[0m top1: 0.3516791044776119
[2m[36m(func pid=183934)[0m top5: 0.8931902985074627
[2m[36m(func pid=183934)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=183934)[0m f1_macro: 0.36277947494442697
[2m[36m(func pid=183934)[0m f1_weighted: 0.3429220272559327
[2m[36m(func pid=183934)[0m f1_per_class: [0.437, 0.505, 0.786, 0.57, 0.137, 0.193, 0.117, 0.236, 0.193, 0.455]
[2m[36m(func pid=183934)[0m 
== Status ==
Current time: 2024-01-07 00:45:22 (running for 00:02:14.52)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.716 |      0.057 |                   16 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  1.024 |      0.269 |                   16 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.021 |      0.363 |                   16 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.472 |      0.219 |                   15 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.2574626865671642
[2m[36m(func pid=184353)[0m top5: 0.6534514925373134
[2m[36m(func pid=184353)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=184353)[0m f1_macro: 0.2192665687854999
[2m[36m(func pid=184353)[0m f1_weighted: 0.22709114703730862
[2m[36m(func pid=184353)[0m f1_per_class: [0.221, 0.324, 0.143, 0.356, 0.059, 0.244, 0.03, 0.33, 0.208, 0.278]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6681 | Steps: 2 | Val loss: 2.4457 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.30597014925373134
[2m[36m(func pid=183515)[0m top5: 0.769589552238806
[2m[36m(func pid=183515)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=183515)[0m f1_macro: 0.29714944801777976
[2m[36m(func pid=183515)[0m f1_weighted: 0.260244633294582
[2m[36m(func pid=183515)[0m f1_per_class: [0.452, 0.4, 0.786, 0.509, 0.09, 0.064, 0.012, 0.255, 0.206, 0.199]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0125 | Steps: 2 | Val loss: 3.0669 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1682 | Steps: 2 | Val loss: 38.2186 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=183140)[0m top1: 0.06949626865671642
[2m[36m(func pid=183140)[0m top5: 0.4216417910447761
[2m[36m(func pid=183140)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=183140)[0m f1_macro: 0.054492914169663974
[2m[36m(func pid=183140)[0m f1_weighted: 0.0807033426266425
[2m[36m(func pid=183140)[0m f1_per_class: [0.067, 0.08, 0.038, 0.054, 0.0, 0.064, 0.135, 0.0, 0.073, 0.033]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7525 | Steps: 2 | Val loss: 1.9598 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=183934)[0m top1: 0.3512126865671642
[2m[36m(func pid=183934)[0m top5: 0.8894589552238806
[2m[36m(func pid=183934)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=183934)[0m f1_macro: 0.36384484937917116
[2m[36m(func pid=183934)[0m f1_weighted: 0.34006760369276207
[2m[36m(func pid=183934)[0m f1_per_class: [0.447, 0.501, 0.786, 0.563, 0.147, 0.237, 0.098, 0.235, 0.213, 0.412]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.24720149253731344
[2m[36m(func pid=184353)[0m top5: 0.6870335820895522
[2m[36m(func pid=184353)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=184353)[0m f1_macro: 0.20272951109181978
[2m[36m(func pid=184353)[0m f1_weighted: 0.20868951432031763
[2m[36m(func pid=184353)[0m f1_per_class: [0.241, 0.311, 0.267, 0.342, 0.067, 0.117, 0.053, 0.244, 0.219, 0.167]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6301 | Steps: 2 | Val loss: 2.4356 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:45:28 (running for 00:02:20.86)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.668 |      0.054 |                   17 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.752 |      0.313 |                   18 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.013 |      0.364 |                   17 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.168 |      0.203 |                   16 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.3050373134328358
[2m[36m(func pid=183515)[0m top5: 0.7943097014925373
[2m[36m(func pid=183515)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=183515)[0m f1_macro: 0.31330723672924504
[2m[36m(func pid=183515)[0m f1_weighted: 0.26606595595750865
[2m[36m(func pid=183515)[0m f1_per_class: [0.424, 0.435, 0.846, 0.503, 0.095, 0.07, 0.012, 0.241, 0.223, 0.283]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0033 | Steps: 2 | Val loss: 3.2308 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=183140)[0m top1: 0.07835820895522388
[2m[36m(func pid=183140)[0m top5: 0.44263059701492535
[2m[36m(func pid=183140)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=183140)[0m f1_macro: 0.05884832279653559
[2m[36m(func pid=183140)[0m f1_weighted: 0.0884056892747078
[2m[36m(func pid=183140)[0m f1_per_class: [0.073, 0.067, 0.043, 0.042, 0.0, 0.076, 0.173, 0.0, 0.075, 0.038]
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2447 | Steps: 2 | Val loss: 37.4776 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6260 | Steps: 2 | Val loss: 1.9322 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=183934)[0m top1: 0.35401119402985076
[2m[36m(func pid=183934)[0m top5: 0.8894589552238806
[2m[36m(func pid=183934)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=183934)[0m f1_macro: 0.3683248812741877
[2m[36m(func pid=183934)[0m f1_weighted: 0.3393792237845305
[2m[36m(func pid=183934)[0m f1_per_class: [0.452, 0.496, 0.786, 0.559, 0.143, 0.276, 0.085, 0.241, 0.221, 0.424]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.26865671641791045
[2m[36m(func pid=184353)[0m top5: 0.7280783582089553
[2m[36m(func pid=184353)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=184353)[0m f1_macro: 0.21183431856374374
[2m[36m(func pid=184353)[0m f1_weighted: 0.23158550665159114
[2m[36m(func pid=184353)[0m f1_per_class: [0.227, 0.326, 0.471, 0.412, 0.065, 0.071, 0.089, 0.167, 0.222, 0.069]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6167 | Steps: 2 | Val loss: 2.4304 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:45:33 (running for 00:02:26.15)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.63  |      0.059 |                   18 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.626 |      0.317 |                   19 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.003 |      0.368 |                   18 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  2.245 |      0.212 |                   17 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.2994402985074627
[2m[36m(func pid=183515)[0m top5: 0.8222947761194029
[2m[36m(func pid=183515)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=183515)[0m f1_macro: 0.3168692007335072
[2m[36m(func pid=183515)[0m f1_weighted: 0.26721729756338564
[2m[36m(func pid=183515)[0m f1_per_class: [0.411, 0.434, 0.846, 0.5, 0.103, 0.077, 0.018, 0.232, 0.21, 0.337]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0048 | Steps: 2 | Val loss: 3.3968 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=183140)[0m top1: 0.08488805970149253
[2m[36m(func pid=183140)[0m top5: 0.4496268656716418
[2m[36m(func pid=183140)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=183140)[0m f1_macro: 0.06006040101086718
[2m[36m(func pid=183140)[0m f1_weighted: 0.09329858020014664
[2m[36m(func pid=183140)[0m f1_per_class: [0.078, 0.063, 0.046, 0.028, 0.0, 0.081, 0.204, 0.0, 0.071, 0.03]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0000 | Steps: 2 | Val loss: 34.4389 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5490 | Steps: 2 | Val loss: 1.8802 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=183934)[0m top1: 0.3516791044776119
[2m[36m(func pid=183934)[0m top5: 0.8922574626865671
[2m[36m(func pid=183934)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=183934)[0m f1_macro: 0.3693629353438813
[2m[36m(func pid=183934)[0m f1_weighted: 0.3360177562587977
[2m[36m(func pid=183934)[0m f1_per_class: [0.44, 0.493, 0.786, 0.548, 0.156, 0.292, 0.079, 0.242, 0.22, 0.438]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.3041044776119403
[2m[36m(func pid=184353)[0m top5: 0.7667910447761194
[2m[36m(func pid=184353)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=184353)[0m f1_macro: 0.23920033493218282
[2m[36m(func pid=184353)[0m f1_weighted: 0.27988167338242187
[2m[36m(func pid=184353)[0m f1_per_class: [0.187, 0.337, 0.667, 0.498, 0.051, 0.084, 0.169, 0.126, 0.202, 0.071]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5323 | Steps: 2 | Val loss: 2.4228 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:45:39 (running for 00:02:31.27)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.617 |      0.06  |                   19 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.549 |      0.331 |                   20 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.005 |      0.369 |                   19 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.239 |                   18 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.3050373134328358
[2m[36m(func pid=183515)[0m top5: 0.8568097014925373
[2m[36m(func pid=183515)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=183515)[0m f1_macro: 0.3311319617135561
[2m[36m(func pid=183515)[0m f1_weighted: 0.2830438057328505
[2m[36m(func pid=183515)[0m f1_per_class: [0.413, 0.445, 0.846, 0.49, 0.115, 0.147, 0.045, 0.231, 0.237, 0.343]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0077 | Steps: 2 | Val loss: 3.5410 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=183140)[0m top1: 0.09701492537313433
[2m[36m(func pid=183140)[0m top5: 0.45475746268656714
[2m[36m(func pid=183140)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=183140)[0m f1_macro: 0.065870125005417
[2m[36m(func pid=183140)[0m f1_weighted: 0.10143360371399576
[2m[36m(func pid=183140)[0m f1_per_class: [0.074, 0.066, 0.063, 0.016, 0.0, 0.089, 0.236, 0.0, 0.083, 0.031]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0475 | Steps: 2 | Val loss: 33.8215 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5788 | Steps: 2 | Val loss: 1.8238 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=183934)[0m top1: 0.3516791044776119
[2m[36m(func pid=183934)[0m top5: 0.8936567164179104
[2m[36m(func pid=183934)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=183934)[0m f1_macro: 0.36759468726351685
[2m[36m(func pid=183934)[0m f1_weighted: 0.33554094939753015
[2m[36m(func pid=183934)[0m f1_per_class: [0.448, 0.489, 0.786, 0.545, 0.121, 0.301, 0.079, 0.241, 0.217, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.32276119402985076
[2m[36m(func pid=184353)[0m top5: 0.7798507462686567
[2m[36m(func pid=184353)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=184353)[0m f1_macro: 0.28021634232037895
[2m[36m(func pid=184353)[0m f1_weighted: 0.3030125210224854
[2m[36m(func pid=184353)[0m f1_per_class: [0.187, 0.329, 0.833, 0.507, 0.125, 0.093, 0.235, 0.099, 0.193, 0.2]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5143 | Steps: 2 | Val loss: 2.4120 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 00:45:44 (running for 00:02:36.34)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.532 |      0.066 |                   20 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.579 |      0.345 |                   21 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.008 |      0.368 |                   20 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.048 |      0.28  |                   19 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.3162313432835821
[2m[36m(func pid=183515)[0m top5: 0.8768656716417911
[2m[36m(func pid=183515)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=183515)[0m f1_macro: 0.34504789190789364
[2m[36m(func pid=183515)[0m f1_weighted: 0.3082727449418982
[2m[36m(func pid=183515)[0m f1_per_class: [0.408, 0.449, 0.846, 0.468, 0.111, 0.183, 0.131, 0.245, 0.221, 0.387]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0377 | Steps: 2 | Val loss: 3.6210 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=183140)[0m top1: 0.11333955223880597
[2m[36m(func pid=183140)[0m top5: 0.4664179104477612
[2m[36m(func pid=183140)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=183140)[0m f1_macro: 0.07272254803905404
[2m[36m(func pid=183140)[0m f1_weighted: 0.11361257406741983
[2m[36m(func pid=183140)[0m f1_per_class: [0.089, 0.061, 0.061, 0.013, 0.0, 0.109, 0.274, 0.0, 0.086, 0.035]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0011 | Steps: 2 | Val loss: 35.2406 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4785 | Steps: 2 | Val loss: 1.7804 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=183934)[0m top1: 0.35494402985074625
[2m[36m(func pid=183934)[0m top5: 0.8955223880597015
[2m[36m(func pid=183934)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=183934)[0m f1_macro: 0.3715210793984755
[2m[36m(func pid=183934)[0m f1_weighted: 0.3385581165716253
[2m[36m(func pid=183934)[0m f1_per_class: [0.457, 0.484, 0.786, 0.539, 0.136, 0.327, 0.087, 0.242, 0.216, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.33675373134328357
[2m[36m(func pid=184353)[0m top5: 0.792910447761194
[2m[36m(func pid=184353)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=184353)[0m f1_macro: 0.28248303195949653
[2m[36m(func pid=184353)[0m f1_weighted: 0.30951333951325294
[2m[36m(func pid=184353)[0m f1_per_class: [0.22, 0.263, 0.759, 0.51, 0.171, 0.059, 0.303, 0.096, 0.187, 0.258]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5198 | Steps: 2 | Val loss: 2.4031 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:45:49 (running for 00:02:41.48)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.514 |      0.073 |                   21 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.479 |      0.358 |                   22 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.038 |      0.372 |                   21 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.001 |      0.282 |                   20 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.3316231343283582
[2m[36m(func pid=183515)[0m top5: 0.8922574626865671
[2m[36m(func pid=183515)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=183515)[0m f1_macro: 0.3578042299165377
[2m[36m(func pid=183515)[0m f1_weighted: 0.33884374461691336
[2m[36m(func pid=183515)[0m f1_per_class: [0.402, 0.431, 0.846, 0.468, 0.104, 0.231, 0.225, 0.252, 0.218, 0.4]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0255 | Steps: 2 | Val loss: 3.7174 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=183140)[0m top1: 0.12686567164179105
[2m[36m(func pid=183140)[0m top5: 0.4808768656716418
[2m[36m(func pid=183140)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=183140)[0m f1_macro: 0.07770385768690706
[2m[36m(func pid=183140)[0m f1_weighted: 0.12227539280342009
[2m[36m(func pid=183140)[0m f1_per_class: [0.103, 0.059, 0.061, 0.013, 0.008, 0.11, 0.303, 0.0, 0.081, 0.04]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0052 | Steps: 2 | Val loss: 37.6592 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3623 | Steps: 2 | Val loss: 1.7562 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=183934)[0m top1: 0.35634328358208955
[2m[36m(func pid=183934)[0m top5: 0.8917910447761194
[2m[36m(func pid=183934)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=183934)[0m f1_macro: 0.37617429432369687
[2m[36m(func pid=183934)[0m f1_weighted: 0.34031935654062634
[2m[36m(func pid=183934)[0m f1_per_class: [0.465, 0.491, 0.786, 0.522, 0.141, 0.332, 0.101, 0.247, 0.216, 0.462]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.34048507462686567
[2m[36m(func pid=184353)[0m top5: 0.7966417910447762
[2m[36m(func pid=184353)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=184353)[0m f1_macro: 0.27875029274579893
[2m[36m(func pid=184353)[0m f1_weighted: 0.3055911210276129
[2m[36m(func pid=184353)[0m f1_per_class: [0.242, 0.189, 0.733, 0.502, 0.225, 0.045, 0.349, 0.071, 0.173, 0.258]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4344 | Steps: 2 | Val loss: 2.3886 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:45:54 (running for 00:02:46.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.52  |      0.078 |                   22 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.362 |      0.365 |                   23 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.025 |      0.376 |                   22 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.005 |      0.279 |                   21 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.34328358208955223
[2m[36m(func pid=183515)[0m top5: 0.9015858208955224
[2m[36m(func pid=183515)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=183515)[0m f1_macro: 0.36462110254129937
[2m[36m(func pid=183515)[0m f1_weighted: 0.3640627875756349
[2m[36m(func pid=183515)[0m f1_per_class: [0.373, 0.409, 0.846, 0.478, 0.099, 0.249, 0.31, 0.246, 0.195, 0.44]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0009 | Steps: 2 | Val loss: 3.7455 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.1115 | Steps: 2 | Val loss: 41.3722 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=183140)[0m top1: 0.14412313432835822
[2m[36m(func pid=183140)[0m top5: 0.49720149253731344
[2m[36m(func pid=183140)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=183140)[0m f1_macro: 0.0829673297500588
[2m[36m(func pid=183140)[0m f1_weighted: 0.13196274876521238
[2m[36m(func pid=183140)[0m f1_per_class: [0.111, 0.056, 0.064, 0.01, 0.009, 0.11, 0.338, 0.0, 0.09, 0.042]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2703 | Steps: 2 | Val loss: 1.7474 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=183934)[0m top1: 0.36800373134328357
[2m[36m(func pid=183934)[0m top5: 0.8931902985074627
[2m[36m(func pid=183934)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=183934)[0m f1_macro: 0.3788921646136276
[2m[36m(func pid=183934)[0m f1_weighted: 0.35550477057222984
[2m[36m(func pid=183934)[0m f1_per_class: [0.446, 0.493, 0.786, 0.536, 0.133, 0.329, 0.137, 0.258, 0.233, 0.438]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.3512126865671642
[2m[36m(func pid=184353)[0m top5: 0.7975746268656716
[2m[36m(func pid=184353)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=184353)[0m f1_macro: 0.26986395226217763
[2m[36m(func pid=184353)[0m f1_weighted: 0.2997836517164679
[2m[36m(func pid=184353)[0m f1_per_class: [0.32, 0.123, 0.632, 0.514, 0.24, 0.031, 0.361, 0.057, 0.162, 0.258]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4334 | Steps: 2 | Val loss: 2.3714 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=183515)[0m top1: 0.35447761194029853
[2m[36m(func pid=183515)[0m top5: 0.9057835820895522
[2m[36m(func pid=183515)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=183515)[0m f1_macro: 0.3669046599941407
[2m[36m(func pid=183515)[0m f1_weighted: 0.3805628978247925
[2m[36m(func pid=183515)[0m f1_per_class: [0.387, 0.39, 0.815, 0.469, 0.101, 0.27, 0.385, 0.2, 0.193, 0.458]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0028 | Steps: 2 | Val loss: 3.8167 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3232 | Steps: 2 | Val loss: 43.3435 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:46:00 (running for 00:02:53.01)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.433 |      0.088 |                   24 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.27  |      0.367 |                   24 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.379 |                   23 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.111 |      0.27  |                   22 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.15811567164179105
[2m[36m(func pid=183140)[0m top5: 0.5139925373134329
[2m[36m(func pid=183140)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=183140)[0m f1_macro: 0.08833965982752222
[2m[36m(func pid=183140)[0m f1_weighted: 0.13879566686579908
[2m[36m(func pid=183140)[0m f1_per_class: [0.123, 0.057, 0.065, 0.01, 0.02, 0.106, 0.36, 0.0, 0.098, 0.045]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2566 | Steps: 2 | Val loss: 1.7435 | Batch size: 32 | lr: 0.001 | Duration: 2.53s
[2m[36m(func pid=183934)[0m top1: 0.37173507462686567
[2m[36m(func pid=183934)[0m top5: 0.8950559701492538
[2m[36m(func pid=183934)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=183934)[0m f1_macro: 0.383219410316766
[2m[36m(func pid=183934)[0m f1_weighted: 0.3604053983385549
[2m[36m(func pid=183934)[0m f1_per_class: [0.465, 0.484, 0.786, 0.538, 0.127, 0.338, 0.152, 0.262, 0.227, 0.455]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.35867537313432835
[2m[36m(func pid=184353)[0m top5: 0.7905783582089553
[2m[36m(func pid=184353)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=184353)[0m f1_macro: 0.2564409187807811
[2m[36m(func pid=184353)[0m f1_weighted: 0.3016985844848082
[2m[36m(func pid=184353)[0m f1_per_class: [0.373, 0.075, 0.5, 0.522, 0.231, 0.024, 0.395, 0.029, 0.164, 0.25]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3683 | Steps: 2 | Val loss: 2.3545 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=183515)[0m top1: 0.3712686567164179
[2m[36m(func pid=183515)[0m top5: 0.9071828358208955
[2m[36m(func pid=183515)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=183515)[0m f1_macro: 0.36603021923861656
[2m[36m(func pid=183515)[0m f1_weighted: 0.3967739654314185
[2m[36m(func pid=183515)[0m f1_per_class: [0.398, 0.384, 0.815, 0.471, 0.114, 0.282, 0.449, 0.137, 0.194, 0.417]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0004 | Steps: 2 | Val loss: 3.8577 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:46:05 (running for 00:02:58.04)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.368 |      0.089 |                   25 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.257 |      0.366 |                   25 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.003 |      0.383 |                   24 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.323 |      0.256 |                   23 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.166044776119403
[2m[36m(func pid=183140)[0m top5: 0.5265858208955224
[2m[36m(func pid=183140)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=183140)[0m f1_macro: 0.08932996567534571
[2m[36m(func pid=183140)[0m f1_weighted: 0.14262842680697854
[2m[36m(func pid=183140)[0m f1_per_class: [0.125, 0.061, 0.065, 0.01, 0.021, 0.098, 0.374, 0.0, 0.096, 0.044]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0547 | Steps: 2 | Val loss: 45.0406 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2476 | Steps: 2 | Val loss: 1.7642 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=183934)[0m top1: 0.37826492537313433
[2m[36m(func pid=183934)[0m top5: 0.8978544776119403
[2m[36m(func pid=183934)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=183934)[0m f1_macro: 0.3874010085614621
[2m[36m(func pid=183934)[0m f1_weighted: 0.3719630433052535
[2m[36m(func pid=183934)[0m f1_per_class: [0.471, 0.484, 0.786, 0.539, 0.13, 0.337, 0.189, 0.263, 0.227, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.3694029850746269
[2m[36m(func pid=184353)[0m top5: 0.7840485074626866
[2m[36m(func pid=184353)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=184353)[0m f1_macro: 0.25256345123901414
[2m[36m(func pid=184353)[0m f1_weighted: 0.31112186993883467
[2m[36m(func pid=184353)[0m f1_per_class: [0.404, 0.057, 0.421, 0.532, 0.235, 0.016, 0.435, 0.015, 0.162, 0.25]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3691 | Steps: 2 | Val loss: 2.3358 | Batch size: 32 | lr: 0.0001 | Duration: 2.66s
[2m[36m(func pid=183515)[0m top1: 0.37453358208955223
[2m[36m(func pid=183515)[0m top5: 0.8992537313432836
[2m[36m(func pid=183515)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=183515)[0m f1_macro: 0.36507482758945276
[2m[36m(func pid=183515)[0m f1_weighted: 0.39921979415812425
[2m[36m(func pid=183515)[0m f1_per_class: [0.407, 0.369, 0.815, 0.46, 0.108, 0.284, 0.482, 0.102, 0.19, 0.435]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.8885 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7686 | Steps: 2 | Val loss: 44.5069 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
== Status ==
Current time: 2024-01-07 00:46:10 (running for 00:03:03.09)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.369 |      0.095 |                   26 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.248 |      0.365 |                   26 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.387 |                   25 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.055 |      0.253 |                   24 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.17583955223880596
[2m[36m(func pid=183140)[0m top5: 0.5419776119402985
[2m[36m(func pid=183140)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=183140)[0m f1_macro: 0.09494077124560077
[2m[36m(func pid=183140)[0m f1_weighted: 0.14842676151717624
[2m[36m(func pid=183140)[0m f1_per_class: [0.142, 0.073, 0.073, 0.01, 0.023, 0.101, 0.384, 0.0, 0.096, 0.048]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.1926 | Steps: 2 | Val loss: 1.7750 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=183934)[0m top1: 0.38152985074626866
[2m[36m(func pid=183934)[0m top5: 0.8987873134328358
[2m[36m(func pid=183934)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=183934)[0m f1_macro: 0.3906521728527604
[2m[36m(func pid=183934)[0m f1_weighted: 0.37724585329600197
[2m[36m(func pid=183934)[0m f1_per_class: [0.477, 0.484, 0.786, 0.534, 0.128, 0.335, 0.211, 0.263, 0.226, 0.462]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.3763992537313433
[2m[36m(func pid=184353)[0m top5: 0.7835820895522388
[2m[36m(func pid=184353)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=184353)[0m f1_macro: 0.24973224056799442
[2m[36m(func pid=184353)[0m f1_weighted: 0.32886777191517536
[2m[36m(func pid=184353)[0m f1_per_class: [0.388, 0.08, 0.308, 0.538, 0.208, 0.016, 0.477, 0.015, 0.155, 0.312]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.2546 | Steps: 2 | Val loss: 2.3148 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=183515)[0m top1: 0.3810634328358209
[2m[36m(func pid=183515)[0m top5: 0.8978544776119403
[2m[36m(func pid=183515)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=183515)[0m f1_macro: 0.3677649522869696
[2m[36m(func pid=183515)[0m f1_weighted: 0.40527391926860007
[2m[36m(func pid=183515)[0m f1_per_class: [0.428, 0.364, 0.786, 0.461, 0.109, 0.29, 0.501, 0.099, 0.181, 0.458]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0002 | Steps: 2 | Val loss: 3.9420 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0000 | Steps: 2 | Val loss: 44.7215 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:46:16 (running for 00:03:08.26)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.255 |      0.101 |                   27 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.193 |      0.368 |                   27 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.391 |                   26 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.769 |      0.25  |                   25 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.1814365671641791
[2m[36m(func pid=183140)[0m top5: 0.5615671641791045
[2m[36m(func pid=183140)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=183140)[0m f1_macro: 0.10133127873652774
[2m[36m(func pid=183140)[0m f1_weighted: 0.1541835303532748
[2m[36m(func pid=183140)[0m f1_per_class: [0.146, 0.091, 0.079, 0.016, 0.025, 0.107, 0.383, 0.0, 0.099, 0.067]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1955 | Steps: 2 | Val loss: 1.7925 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=183934)[0m top1: 0.38572761194029853
[2m[36m(func pid=183934)[0m top5: 0.898320895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=183934)[0m f1_macro: 0.39461913835617024
[2m[36m(func pid=183934)[0m f1_weighted: 0.3860239201967358
[2m[36m(func pid=183934)[0m f1_per_class: [0.503, 0.487, 0.786, 0.532, 0.125, 0.328, 0.244, 0.256, 0.223, 0.462]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.37779850746268656
[2m[36m(func pid=184353)[0m top5: 0.7840485074626866
[2m[36m(func pid=184353)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=184353)[0m f1_macro: 0.25210169176049735
[2m[36m(func pid=184353)[0m f1_weighted: 0.33665588053434836
[2m[36m(func pid=184353)[0m f1_per_class: [0.404, 0.099, 0.25, 0.534, 0.196, 0.016, 0.495, 0.015, 0.148, 0.364]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2454 | Steps: 2 | Val loss: 2.2926 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.3805970149253731
[2m[36m(func pid=183515)[0m top5: 0.8969216417910447
[2m[36m(func pid=183515)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=183515)[0m f1_macro: 0.3596179915466388
[2m[36m(func pid=183515)[0m f1_weighted: 0.40263360767229994
[2m[36m(func pid=183515)[0m f1_per_class: [0.428, 0.354, 0.733, 0.455, 0.11, 0.287, 0.509, 0.09, 0.182, 0.449]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:46:21 (running for 00:03:13.44)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.245 |      0.108 |                   28 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.195 |      0.36  |                   28 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.395 |                   27 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.252 |                   26 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.18703358208955223
[2m[36m(func pid=183140)[0m top5: 0.5834888059701493
[2m[36m(func pid=183140)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=183140)[0m f1_macro: 0.10775716004951863
[2m[36m(func pid=183140)[0m f1_weighted: 0.15934503486222043
[2m[36m(func pid=183140)[0m f1_per_class: [0.153, 0.097, 0.084, 0.023, 0.014, 0.113, 0.386, 0.0, 0.102, 0.106]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.0241 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4912 | Steps: 2 | Val loss: 45.5937 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1888 | Steps: 2 | Val loss: 1.8090 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=184353)[0m top1: 0.37779850746268656
[2m[36m(func pid=184353)[0m top5: 0.7803171641791045
[2m[36m(func pid=184353)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=184353)[0m f1_macro: 0.24463885184365125
[2m[36m(func pid=184353)[0m f1_weighted: 0.34574724492639036
[2m[36m(func pid=184353)[0m f1_per_class: [0.333, 0.117, 0.215, 0.537, 0.191, 0.016, 0.519, 0.015, 0.14, 0.364]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m top1: 0.3903917910447761
[2m[36m(func pid=183934)[0m top5: 0.9001865671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=183934)[0m f1_macro: 0.3995611043765436
[2m[36m(func pid=183934)[0m f1_weighted: 0.3920914663440877
[2m[36m(func pid=183934)[0m f1_per_class: [0.521, 0.489, 0.786, 0.526, 0.125, 0.335, 0.264, 0.264, 0.218, 0.469]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.2535 | Steps: 2 | Val loss: 2.2737 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=183515)[0m top1: 0.38619402985074625
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=183515)[0m f1_macro: 0.3578699009773124
[2m[36m(func pid=183515)[0m f1_weighted: 0.4090386603933822
[2m[36m(func pid=183515)[0m f1_per_class: [0.434, 0.339, 0.71, 0.471, 0.109, 0.29, 0.523, 0.089, 0.177, 0.436]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:46:26 (running for 00:03:18.55)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.254 |      0.111 |                   29 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.189 |      0.358 |                   29 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   28 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.491 |      0.245 |                   27 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.1912313432835821
[2m[36m(func pid=183140)[0m top5: 0.5998134328358209
[2m[36m(func pid=183140)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=183140)[0m f1_macro: 0.11089458705110906
[2m[36m(func pid=183140)[0m f1_weighted: 0.16446821540127482
[2m[36m(func pid=183140)[0m f1_per_class: [0.144, 0.103, 0.091, 0.029, 0.014, 0.118, 0.392, 0.0, 0.101, 0.118]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1922 | Steps: 2 | Val loss: 46.1824 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0006 | Steps: 2 | Val loss: 4.1057 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1955 | Steps: 2 | Val loss: 1.8410 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=184353)[0m top1: 0.37779850746268656
[2m[36m(func pid=184353)[0m top5: 0.7798507462686567
[2m[36m(func pid=184353)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=184353)[0m f1_macro: 0.23982025214211117
[2m[36m(func pid=184353)[0m f1_weighted: 0.3527622007151275
[2m[36m(func pid=184353)[0m f1_per_class: [0.286, 0.133, 0.181, 0.545, 0.171, 0.016, 0.526, 0.042, 0.137, 0.364]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1974 | Steps: 2 | Val loss: 2.2472 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=183934)[0m top1: 0.38992537313432835
[2m[36m(func pid=183934)[0m top5: 0.9006529850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=183934)[0m f1_macro: 0.39558858296636834
[2m[36m(func pid=183934)[0m f1_weighted: 0.39266282301963223
[2m[36m(func pid=183934)[0m f1_per_class: [0.518, 0.488, 0.759, 0.519, 0.123, 0.341, 0.274, 0.251, 0.213, 0.469]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3829291044776119
[2m[36m(func pid=183515)[0m top5: 0.8917910447761194
[2m[36m(func pid=183515)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=183515)[0m f1_macro: 0.3545048431792035
[2m[36m(func pid=183515)[0m f1_weighted: 0.4075540633379399
[2m[36m(func pid=183515)[0m f1_per_class: [0.432, 0.323, 0.71, 0.486, 0.124, 0.282, 0.516, 0.102, 0.183, 0.387]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:46:31 (running for 00:03:23.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.197 |      0.118 |                   30 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.195 |      0.355 |                   30 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.396 |                   29 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.192 |      0.24  |                   28 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.19869402985074627
[2m[36m(func pid=183140)[0m top5: 0.6217350746268657
[2m[36m(func pid=183140)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=183140)[0m f1_macro: 0.11764354070357164
[2m[36m(func pid=183140)[0m f1_weighted: 0.17593231217718494
[2m[36m(func pid=183140)[0m f1_per_class: [0.143, 0.132, 0.102, 0.044, 0.028, 0.124, 0.397, 0.0, 0.103, 0.103]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5905 | Steps: 2 | Val loss: 47.0006 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.1943 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1219 | Steps: 2 | Val loss: 1.8556 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=184353)[0m top1: 0.3712686567164179
[2m[36m(func pid=184353)[0m top5: 0.777518656716418
[2m[36m(func pid=184353)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=184353)[0m f1_macro: 0.2291952263852258
[2m[36m(func pid=184353)[0m f1_weighted: 0.35618968839349846
[2m[36m(func pid=184353)[0m f1_per_class: [0.2, 0.153, 0.149, 0.537, 0.159, 0.016, 0.531, 0.099, 0.136, 0.312]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0991 | Steps: 2 | Val loss: 2.2171 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=183934)[0m top1: 0.3903917910447761
[2m[36m(func pid=183934)[0m top5: 0.9020522388059702
[2m[36m(func pid=183934)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=183934)[0m f1_macro: 0.39572818494721507
[2m[36m(func pid=183934)[0m f1_weighted: 0.393936203099209
[2m[36m(func pid=183934)[0m f1_per_class: [0.522, 0.486, 0.759, 0.512, 0.119, 0.339, 0.287, 0.243, 0.213, 0.476]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3843283582089552
[2m[36m(func pid=183515)[0m top5: 0.8908582089552238
[2m[36m(func pid=183515)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=183515)[0m f1_macro: 0.35475560612290247
[2m[36m(func pid=183515)[0m f1_weighted: 0.4090988539092858
[2m[36m(func pid=183515)[0m f1_per_class: [0.431, 0.328, 0.71, 0.488, 0.121, 0.281, 0.517, 0.096, 0.188, 0.388]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:46:36 (running for 00:03:28.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.099 |      0.128 |                   31 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.122 |      0.355 |                   31 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.396 |                   30 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.59  |      0.229 |                   29 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.20708955223880596
[2m[36m(func pid=183140)[0m top5: 0.6478544776119403
[2m[36m(func pid=183140)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=183140)[0m f1_macro: 0.12772051333557072
[2m[36m(func pid=183140)[0m f1_weighted: 0.19008343178704734
[2m[36m(func pid=183140)[0m f1_per_class: [0.15, 0.174, 0.113, 0.065, 0.028, 0.123, 0.399, 0.0, 0.109, 0.116]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.2143 | Steps: 2 | Val loss: 46.8221 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0002 | Steps: 2 | Val loss: 4.1740 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1088 | Steps: 2 | Val loss: 1.8796 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=184353)[0m top1: 0.3656716417910448
[2m[36m(func pid=184353)[0m top5: 0.7784514925373134
[2m[36m(func pid=184353)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=184353)[0m f1_macro: 0.23605934051437943
[2m[36m(func pid=184353)[0m f1_weighted: 0.36349874327786785
[2m[36m(func pid=184353)[0m f1_per_class: [0.164, 0.199, 0.138, 0.53, 0.152, 0.024, 0.526, 0.138, 0.137, 0.353]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m top1: 0.39925373134328357
[2m[36m(func pid=183934)[0m top5: 0.9053171641791045
[2m[36m(func pid=183934)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=183934)[0m f1_macro: 0.3945832590444692
[2m[36m(func pid=183934)[0m f1_weighted: 0.40688202261819534
[2m[36m(func pid=183934)[0m f1_per_class: [0.515, 0.489, 0.733, 0.515, 0.127, 0.341, 0.328, 0.24, 0.212, 0.444]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0427 | Steps: 2 | Val loss: 2.1913 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=183515)[0m top1: 0.38526119402985076
[2m[36m(func pid=183515)[0m top5: 0.8885261194029851
[2m[36m(func pid=183515)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=183515)[0m f1_macro: 0.36058378716634387
[2m[36m(func pid=183515)[0m f1_weighted: 0.41174377844851917
[2m[36m(func pid=183515)[0m f1_per_class: [0.453, 0.332, 0.688, 0.491, 0.125, 0.28, 0.511, 0.138, 0.194, 0.395]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0000 | Steps: 2 | Val loss: 45.1933 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 00:46:42 (running for 00:03:34.34)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  2.043 |      0.136 |                   32 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.109 |      0.361 |                   32 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.395 |                   31 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.214 |      0.236 |                   30 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.20755597014925373
[2m[36m(func pid=183140)[0m top5: 0.6697761194029851
[2m[36m(func pid=183140)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=183140)[0m f1_macro: 0.13569426170158705
[2m[36m(func pid=183140)[0m f1_weighted: 0.198743116625044
[2m[36m(func pid=183140)[0m f1_per_class: [0.165, 0.199, 0.134, 0.098, 0.025, 0.122, 0.381, 0.0, 0.113, 0.119]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.2644 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0892 | Steps: 2 | Val loss: 1.8885 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=184353)[0m top1: 0.37779850746268656
[2m[36m(func pid=184353)[0m top5: 0.7798507462686567
[2m[36m(func pid=184353)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=184353)[0m f1_macro: 0.25728496322885575
[2m[36m(func pid=184353)[0m f1_weighted: 0.39125108985400125
[2m[36m(func pid=184353)[0m f1_per_class: [0.194, 0.305, 0.123, 0.532, 0.152, 0.023, 0.549, 0.152, 0.143, 0.4]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m top1: 0.40345149253731344
[2m[36m(func pid=183934)[0m top5: 0.9029850746268657
[2m[36m(func pid=183934)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=183934)[0m f1_macro: 0.39847041144123724
[2m[36m(func pid=183934)[0m f1_weighted: 0.41276719820494845
[2m[36m(func pid=183934)[0m f1_per_class: [0.519, 0.484, 0.733, 0.517, 0.12, 0.343, 0.345, 0.251, 0.222, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9786 | Steps: 2 | Val loss: 2.1668 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=183515)[0m top1: 0.38386194029850745
[2m[36m(func pid=183515)[0m top5: 0.8899253731343284
[2m[36m(func pid=183515)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=183515)[0m f1_macro: 0.3650641493336264
[2m[36m(func pid=183515)[0m f1_weighted: 0.41385849336951075
[2m[36m(func pid=183515)[0m f1_per_class: [0.463, 0.341, 0.688, 0.503, 0.127, 0.27, 0.494, 0.197, 0.193, 0.375]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.0872 | Steps: 2 | Val loss: 44.6878 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:46:47 (running for 00:03:39.47)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.979 |      0.141 |                   33 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.089 |      0.365 |                   33 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.398 |                   32 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.257 |                   31 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.20988805970149255
[2m[36m(func pid=183140)[0m top5: 0.6870335820895522
[2m[36m(func pid=183140)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=183140)[0m f1_macro: 0.14140969842497503
[2m[36m(func pid=183140)[0m f1_weighted: 0.20677926607257202
[2m[36m(func pid=183140)[0m f1_per_class: [0.167, 0.213, 0.15, 0.126, 0.024, 0.113, 0.376, 0.0, 0.126, 0.118]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0988 | Steps: 2 | Val loss: 1.9028 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.3721 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184353)[0m top1: 0.373134328358209
[2m[36m(func pid=184353)[0m top5: 0.78125
[2m[36m(func pid=184353)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=184353)[0m f1_macro: 0.26403338375331764
[2m[36m(func pid=184353)[0m f1_weighted: 0.3989568947298314
[2m[36m(func pid=184353)[0m f1_per_class: [0.197, 0.362, 0.113, 0.523, 0.14, 0.037, 0.54, 0.173, 0.156, 0.4]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m top1: 0.40718283582089554
[2m[36m(func pid=183934)[0m top5: 0.902518656716418
[2m[36m(func pid=183934)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=183934)[0m f1_macro: 0.4025259890875067
[2m[36m(func pid=183934)[0m f1_weighted: 0.4176848760915556
[2m[36m(func pid=183934)[0m f1_per_class: [0.531, 0.49, 0.733, 0.512, 0.119, 0.341, 0.363, 0.253, 0.219, 0.464]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.38199626865671643
[2m[36m(func pid=183515)[0m top5: 0.8889925373134329
[2m[36m(func pid=183515)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=183515)[0m f1_macro: 0.3627657555249004
[2m[36m(func pid=183515)[0m f1_weighted: 0.4145086591928435
[2m[36m(func pid=183515)[0m f1_per_class: [0.483, 0.358, 0.647, 0.517, 0.117, 0.261, 0.471, 0.23, 0.199, 0.345]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8648 | Steps: 2 | Val loss: 2.1398 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0000 | Steps: 2 | Val loss: 46.0620 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:46:52 (running for 00:03:44.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.865 |      0.152 |                   34 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.099 |      0.363 |                   34 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   33 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.087 |      0.264 |                   32 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.21595149253731344
[2m[36m(func pid=183140)[0m top5: 0.7122201492537313
[2m[36m(func pid=183140)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=183140)[0m f1_macro: 0.15207404692782797
[2m[36m(func pid=183140)[0m f1_weighted: 0.2192042452966015
[2m[36m(func pid=183140)[0m f1_per_class: [0.188, 0.235, 0.177, 0.16, 0.023, 0.121, 0.367, 0.0, 0.142, 0.109]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.5039 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0762 | Steps: 2 | Val loss: 1.9310 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=184353)[0m top1: 0.341884328358209
[2m[36m(func pid=184353)[0m top5: 0.7779850746268657
[2m[36m(func pid=184353)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=184353)[0m f1_macro: 0.2567362282959005
[2m[36m(func pid=184353)[0m f1_weighted: 0.37580917487695054
[2m[36m(func pid=184353)[0m f1_per_class: [0.194, 0.38, 0.105, 0.475, 0.119, 0.063, 0.475, 0.248, 0.156, 0.353]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8660 | Steps: 2 | Val loss: 2.1180 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=183515)[0m top1: 0.37220149253731344
[2m[36m(func pid=183515)[0m top5: 0.8861940298507462
[2m[36m(func pid=183515)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=183515)[0m f1_macro: 0.3555815214439401
[2m[36m(func pid=183515)[0m f1_weighted: 0.404878791811017
[2m[36m(func pid=183515)[0m f1_per_class: [0.467, 0.359, 0.629, 0.513, 0.115, 0.257, 0.443, 0.241, 0.208, 0.326]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4076492537313433
[2m[36m(func pid=183934)[0m top5: 0.9034514925373134
[2m[36m(func pid=183934)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=183934)[0m f1_macro: 0.40273159994365715
[2m[36m(func pid=183934)[0m f1_weighted: 0.4171300066648237
[2m[36m(func pid=183934)[0m f1_per_class: [0.531, 0.492, 0.733, 0.504, 0.12, 0.343, 0.366, 0.254, 0.219, 0.464]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 48.3529 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 00:46:57 (running for 00:03:49.68)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.866 |      0.163 |                   35 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.076 |      0.356 |                   35 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   34 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.257 |                   33 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.22154850746268656
[2m[36m(func pid=183140)[0m top5: 0.7276119402985075
[2m[36m(func pid=183140)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=183140)[0m f1_macro: 0.16259520467699828
[2m[36m(func pid=183140)[0m f1_weighted: 0.2316682142739155
[2m[36m(func pid=183140)[0m f1_per_class: [0.212, 0.24, 0.204, 0.212, 0.021, 0.114, 0.356, 0.0, 0.149, 0.118]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0003 | Steps: 2 | Val loss: 4.5291 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1018 | Steps: 2 | Val loss: 1.9665 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=184353)[0m top1: 0.3255597014925373
[2m[36m(func pid=184353)[0m top5: 0.7770522388059702
[2m[36m(func pid=184353)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=184353)[0m f1_macro: 0.2537761951005166
[2m[36m(func pid=184353)[0m f1_weighted: 0.35814629189792596
[2m[36m(func pid=184353)[0m f1_per_class: [0.19, 0.4, 0.11, 0.451, 0.104, 0.083, 0.415, 0.266, 0.166, 0.353]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7743 | Steps: 2 | Val loss: 2.0972 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=183934)[0m top1: 0.41091417910447764
[2m[36m(func pid=183934)[0m top5: 0.9048507462686567
[2m[36m(func pid=183934)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=183934)[0m f1_macro: 0.40119124998462985
[2m[36m(func pid=183934)[0m f1_weighted: 0.42135505616100416
[2m[36m(func pid=183934)[0m f1_per_class: [0.531, 0.493, 0.71, 0.508, 0.12, 0.342, 0.378, 0.254, 0.212, 0.464]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3596082089552239
[2m[36m(func pid=183515)[0m top5: 0.8824626865671642
[2m[36m(func pid=183515)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=183515)[0m f1_macro: 0.3490260396328819
[2m[36m(func pid=183515)[0m f1_weighted: 0.39036942251927254
[2m[36m(func pid=183515)[0m f1_per_class: [0.471, 0.371, 0.611, 0.512, 0.103, 0.262, 0.386, 0.238, 0.209, 0.327]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2831 | Steps: 2 | Val loss: 52.3199 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:47:02 (running for 00:03:54.77)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.774 |      0.173 |                   36 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.102 |      0.349 |                   36 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   35 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.254 |                   34 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.22901119402985073
[2m[36m(func pid=183140)[0m top5: 0.7476679104477612
[2m[36m(func pid=183140)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=183140)[0m f1_macro: 0.1730713813833676
[2m[36m(func pid=183140)[0m f1_weighted: 0.24494103899266959
[2m[36m(func pid=183140)[0m f1_per_class: [0.215, 0.255, 0.242, 0.263, 0.02, 0.121, 0.34, 0.0, 0.155, 0.121]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.6041 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0347 | Steps: 2 | Val loss: 1.9913 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=184353)[0m top1: 0.291044776119403
[2m[36m(func pid=184353)[0m top5: 0.7560634328358209
[2m[36m(func pid=184353)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=184353)[0m f1_macro: 0.2443000316669531
[2m[36m(func pid=184353)[0m f1_weighted: 0.31776855315671854
[2m[36m(func pid=184353)[0m f1_per_class: [0.209, 0.384, 0.107, 0.375, 0.098, 0.138, 0.334, 0.284, 0.161, 0.353]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7797 | Steps: 2 | Val loss: 2.0765 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=183515)[0m top1: 0.3582089552238806
[2m[36m(func pid=183515)[0m top5: 0.8847947761194029
[2m[36m(func pid=183515)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=183515)[0m f1_macro: 0.3478517083604986
[2m[36m(func pid=183515)[0m f1_weighted: 0.38441502505540076
[2m[36m(func pid=183515)[0m f1_per_class: [0.474, 0.377, 0.595, 0.533, 0.104, 0.249, 0.344, 0.256, 0.221, 0.327]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.408115671641791
[2m[36m(func pid=183934)[0m top5: 0.9048507462686567
[2m[36m(func pid=183934)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=183934)[0m f1_macro: 0.4002932097190345
[2m[36m(func pid=183934)[0m f1_weighted: 0.42046710377047486
[2m[36m(func pid=183934)[0m f1_per_class: [0.535, 0.482, 0.71, 0.501, 0.11, 0.339, 0.389, 0.249, 0.206, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 56.7965 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:47:07 (running for 00:03:59.86)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.78  |      0.187 |                   37 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.035 |      0.348 |                   37 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   36 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.283 |      0.244 |                   35 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.23787313432835822
[2m[36m(func pid=183140)[0m top5: 0.7677238805970149
[2m[36m(func pid=183140)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=183140)[0m f1_macro: 0.18737829457480143
[2m[36m(func pid=183140)[0m f1_weighted: 0.2551949813754124
[2m[36m(func pid=183140)[0m f1_per_class: [0.238, 0.263, 0.319, 0.299, 0.028, 0.113, 0.335, 0.0, 0.16, 0.119]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0436 | Steps: 2 | Val loss: 2.0285 | Batch size: 32 | lr: 0.001 | Duration: 2.56s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.6528 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=184353)[0m top1: 0.27705223880597013
[2m[36m(func pid=184353)[0m top5: 0.7313432835820896
[2m[36m(func pid=184353)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=184353)[0m f1_macro: 0.25060383319131074
[2m[36m(func pid=184353)[0m f1_weighted: 0.2924397702886131
[2m[36m(func pid=184353)[0m f1_per_class: [0.38, 0.393, 0.12, 0.318, 0.093, 0.163, 0.279, 0.273, 0.174, 0.312]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3493470149253731
[2m[36m(func pid=183515)[0m top5: 0.8824626865671642
[2m[36m(func pid=183515)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=183515)[0m f1_macro: 0.3339382567681211
[2m[36m(func pid=183515)[0m f1_weighted: 0.37269893678480354
[2m[36m(func pid=183515)[0m f1_per_class: [0.474, 0.386, 0.524, 0.532, 0.105, 0.238, 0.308, 0.248, 0.212, 0.312]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7429 | Steps: 2 | Val loss: 2.0582 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=183934)[0m top1: 0.4123134328358209
[2m[36m(func pid=183934)[0m top5: 0.9081156716417911
[2m[36m(func pid=183934)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=183934)[0m f1_macro: 0.4021558071317847
[2m[36m(func pid=183934)[0m f1_weighted: 0.4240918652046796
[2m[36m(func pid=183934)[0m f1_per_class: [0.535, 0.487, 0.71, 0.501, 0.115, 0.343, 0.397, 0.253, 0.199, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0000 | Steps: 2 | Val loss: 61.3789 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 00:47:12 (running for 00:04:05.05)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.743 |      0.196 |                   38 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.044 |      0.334 |                   38 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   37 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.251 |                   36 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2462686567164179
[2m[36m(func pid=183140)[0m top5: 0.7868470149253731
[2m[36m(func pid=183140)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=183140)[0m f1_macro: 0.19572952406222305
[2m[36m(func pid=183140)[0m f1_weighted: 0.26317237541694594
[2m[36m(func pid=183140)[0m f1_per_class: [0.268, 0.288, 0.349, 0.337, 0.027, 0.113, 0.31, 0.0, 0.147, 0.118]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0347 | Steps: 2 | Val loss: 2.0614 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0026 | Steps: 2 | Val loss: 4.7590 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=184353)[0m top1: 0.2728544776119403
[2m[36m(func pid=184353)[0m top5: 0.7103544776119403
[2m[36m(func pid=184353)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=184353)[0m f1_macro: 0.2515070394132632
[2m[36m(func pid=184353)[0m f1_weighted: 0.2718905602489229
[2m[36m(func pid=184353)[0m f1_per_class: [0.449, 0.407, 0.138, 0.27, 0.095, 0.206, 0.224, 0.278, 0.19, 0.258]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m top1: 0.35074626865671643
[2m[36m(func pid=183515)[0m top5: 0.8782649253731343
[2m[36m(func pid=183515)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=183515)[0m f1_macro: 0.33883512898232804
[2m[36m(func pid=183515)[0m f1_weighted: 0.36832049839771575
[2m[36m(func pid=183515)[0m f1_per_class: [0.468, 0.39, 0.558, 0.542, 0.104, 0.235, 0.278, 0.272, 0.215, 0.327]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6637 | Steps: 2 | Val loss: 2.0449 | Batch size: 32 | lr: 0.0001 | Duration: 2.68s
[2m[36m(func pid=183934)[0m top1: 0.41277985074626866
[2m[36m(func pid=183934)[0m top5: 0.909981343283582
[2m[36m(func pid=183934)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=183934)[0m f1_macro: 0.4003296060259925
[2m[36m(func pid=183934)[0m f1_weighted: 0.42452528074049817
[2m[36m(func pid=183934)[0m f1_per_class: [0.535, 0.491, 0.71, 0.502, 0.112, 0.342, 0.399, 0.237, 0.202, 0.473]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0343 | Steps: 2 | Val loss: 2.0943 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 68.3094 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:47:17 (running for 00:04:10.15)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.664 |      0.2   |                   39 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.035 |      0.339 |                   39 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.003 |      0.4   |                   38 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.252 |                   37 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.25
[2m[36m(func pid=183140)[0m top5: 0.7873134328358209
[2m[36m(func pid=183140)[0m f1_micro: 0.25
[2m[36m(func pid=183140)[0m f1_macro: 0.20031271761845812
[2m[36m(func pid=183140)[0m f1_weighted: 0.26331457225714405
[2m[36m(func pid=183140)[0m f1_per_class: [0.272, 0.301, 0.386, 0.373, 0.027, 0.119, 0.266, 0.0, 0.152, 0.107]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.8758 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.34701492537313433
[2m[36m(func pid=183515)[0m top5: 0.8759328358208955
[2m[36m(func pid=183515)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=183515)[0m f1_macro: 0.3332216496440882
[2m[36m(func pid=183515)[0m f1_weighted: 0.3612318998831368
[2m[36m(func pid=183515)[0m f1_per_class: [0.468, 0.395, 0.545, 0.546, 0.103, 0.23, 0.252, 0.266, 0.206, 0.322]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.2644589552238806
[2m[36m(func pid=184353)[0m top5: 0.6842350746268657
[2m[36m(func pid=184353)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=184353)[0m f1_macro: 0.24109739602868432
[2m[36m(func pid=184353)[0m f1_weighted: 0.24464048156047946
[2m[36m(func pid=184353)[0m f1_per_class: [0.455, 0.415, 0.144, 0.203, 0.1, 0.243, 0.177, 0.286, 0.182, 0.207]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6372 | Steps: 2 | Val loss: 2.0335 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=183934)[0m top1: 0.4141791044776119
[2m[36m(func pid=183934)[0m top5: 0.9090485074626866
[2m[36m(func pid=183934)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=183934)[0m f1_macro: 0.4005040132904222
[2m[36m(func pid=183934)[0m f1_weighted: 0.42604793054264906
[2m[36m(func pid=183934)[0m f1_per_class: [0.531, 0.492, 0.71, 0.498, 0.114, 0.348, 0.405, 0.236, 0.198, 0.473]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0449 | Steps: 2 | Val loss: 2.1224 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 73.9964 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:47:23 (running for 00:04:15.31)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.637 |      0.205 |                   40 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.034 |      0.333 |                   40 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   39 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.241 |                   38 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.25
[2m[36m(func pid=183140)[0m top5: 0.784981343283582
[2m[36m(func pid=183140)[0m f1_micro: 0.25
[2m[36m(func pid=183140)[0m f1_macro: 0.20464233006349625
[2m[36m(func pid=183140)[0m f1_weighted: 0.25854366999996
[2m[36m(func pid=183140)[0m f1_per_class: [0.302, 0.304, 0.407, 0.386, 0.027, 0.116, 0.233, 0.014, 0.148, 0.109]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.9802 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=183515)[0m top1: 0.3460820895522388
[2m[36m(func pid=183515)[0m top5: 0.8796641791044776
[2m[36m(func pid=183515)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=183515)[0m f1_macro: 0.33053571016602057
[2m[36m(func pid=183515)[0m f1_weighted: 0.35538590626316485
[2m[36m(func pid=183515)[0m f1_per_class: [0.477, 0.404, 0.533, 0.557, 0.095, 0.21, 0.224, 0.265, 0.207, 0.333]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.2593283582089552
[2m[36m(func pid=184353)[0m top5: 0.6613805970149254
[2m[36m(func pid=184353)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=184353)[0m f1_macro: 0.23014960270340978
[2m[36m(func pid=184353)[0m f1_weighted: 0.22174892060941706
[2m[36m(func pid=184353)[0m f1_per_class: [0.43, 0.421, 0.165, 0.151, 0.109, 0.268, 0.14, 0.276, 0.194, 0.148]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5260 | Steps: 2 | Val loss: 2.0286 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=183934)[0m top1: 0.41138059701492535
[2m[36m(func pid=183934)[0m top5: 0.9081156716417911
[2m[36m(func pid=183934)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=183934)[0m f1_macro: 0.40194596898134505
[2m[36m(func pid=183934)[0m f1_weighted: 0.4237345261619717
[2m[36m(func pid=183934)[0m f1_per_class: [0.535, 0.486, 0.733, 0.495, 0.111, 0.345, 0.407, 0.212, 0.213, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0229 | Steps: 2 | Val loss: 2.1752 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
== Status ==
Current time: 2024-01-07 00:47:28 (running for 00:04:20.31)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.526 |      0.216 |                   41 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.045 |      0.331 |                   41 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   40 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.23  |                   39 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.25466417910447764
[2m[36m(func pid=183140)[0m top5: 0.7887126865671642
[2m[36m(func pid=183140)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=183140)[0m f1_macro: 0.2163060135899729
[2m[36m(func pid=183140)[0m f1_weighted: 0.26015219250795824
[2m[36m(func pid=183140)[0m f1_per_class: [0.319, 0.317, 0.431, 0.393, 0.036, 0.122, 0.208, 0.079, 0.139, 0.119]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 80.3572 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.0189 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=183515)[0m top1: 0.341884328358209
[2m[36m(func pid=183515)[0m top5: 0.875
[2m[36m(func pid=183515)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=183515)[0m f1_macro: 0.32577529353723506
[2m[36m(func pid=183515)[0m f1_weighted: 0.3471869884924594
[2m[36m(func pid=183515)[0m f1_per_class: [0.47, 0.405, 0.533, 0.56, 0.096, 0.209, 0.195, 0.259, 0.209, 0.322]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.25513059701492535
[2m[36m(func pid=184353)[0m top5: 0.6408582089552238
[2m[36m(func pid=184353)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=184353)[0m f1_macro: 0.22623994666231034
[2m[36m(func pid=184353)[0m f1_weighted: 0.20612291125483237
[2m[36m(func pid=184353)[0m f1_per_class: [0.427, 0.42, 0.181, 0.113, 0.113, 0.279, 0.12, 0.279, 0.188, 0.143]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.4751 | Steps: 2 | Val loss: 2.0224 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=183934)[0m top1: 0.4146455223880597
[2m[36m(func pid=183934)[0m top5: 0.9085820895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=183934)[0m f1_macro: 0.4052720416762908
[2m[36m(func pid=183934)[0m f1_weighted: 0.4277865908639947
[2m[36m(func pid=183934)[0m f1_per_class: [0.557, 0.483, 0.733, 0.495, 0.112, 0.345, 0.422, 0.207, 0.217, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0334 | Steps: 2 | Val loss: 2.2254 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 87.7712 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:47:33 (running for 00:04:25.60)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.475 |      0.221 |                   42 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.023 |      0.326 |                   42 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.405 |                   41 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.226 |                   40 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2621268656716418
[2m[36m(func pid=183140)[0m top5: 0.7882462686567164
[2m[36m(func pid=183140)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=183140)[0m f1_macro: 0.22100741001015872
[2m[36m(func pid=183140)[0m f1_weighted: 0.2634653087818052
[2m[36m(func pid=183140)[0m f1_per_class: [0.326, 0.325, 0.44, 0.414, 0.036, 0.12, 0.192, 0.099, 0.14, 0.119]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0029 | Steps: 2 | Val loss: 5.0953 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=183515)[0m top1: 0.33348880597014924
[2m[36m(func pid=183515)[0m top5: 0.8726679104477612
[2m[36m(func pid=183515)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=183515)[0m f1_macro: 0.3192841835269154
[2m[36m(func pid=183515)[0m f1_weighted: 0.335289581708878
[2m[36m(func pid=183515)[0m f1_per_class: [0.467, 0.393, 0.533, 0.558, 0.094, 0.188, 0.174, 0.254, 0.209, 0.324]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.2462686567164179
[2m[36m(func pid=184353)[0m top5: 0.6217350746268657
[2m[36m(func pid=184353)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=184353)[0m f1_macro: 0.22159785123203352
[2m[36m(func pid=184353)[0m f1_weighted: 0.18213549975972215
[2m[36m(func pid=184353)[0m f1_per_class: [0.453, 0.421, 0.205, 0.074, 0.107, 0.28, 0.074, 0.28, 0.181, 0.143]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4078 | Steps: 2 | Val loss: 2.0185 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=183934)[0m top1: 0.4197761194029851
[2m[36m(func pid=183934)[0m top5: 0.9104477611940298
[2m[36m(func pid=183934)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183934)[0m f1_macro: 0.4101284233672244
[2m[36m(func pid=183934)[0m f1_weighted: 0.43192570843138867
[2m[36m(func pid=183934)[0m f1_per_class: [0.562, 0.489, 0.733, 0.489, 0.114, 0.348, 0.434, 0.213, 0.219, 0.5]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0289 | Steps: 2 | Val loss: 2.2659 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0032 | Steps: 2 | Val loss: 93.8247 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 00:47:38 (running for 00:04:30.70)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.408 |      0.235 |                   43 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.033 |      0.319 |                   43 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.003 |      0.41  |                   42 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.222 |                   41 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2719216417910448
[2m[36m(func pid=183140)[0m top5: 0.7770522388059702
[2m[36m(func pid=183140)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=183140)[0m f1_macro: 0.23464552676259612
[2m[36m(func pid=183140)[0m f1_weighted: 0.2682819439035077
[2m[36m(func pid=183140)[0m f1_per_class: [0.331, 0.337, 0.468, 0.435, 0.045, 0.115, 0.162, 0.204, 0.129, 0.119]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0023 | Steps: 2 | Val loss: 5.1123 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183515)[0m top1: 0.3306902985074627
[2m[36m(func pid=183515)[0m top5: 0.8722014925373134
[2m[36m(func pid=183515)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=183515)[0m f1_macro: 0.3136132738728786
[2m[36m(func pid=183515)[0m f1_weighted: 0.3272191696031784
[2m[36m(func pid=183515)[0m f1_per_class: [0.466, 0.403, 0.533, 0.56, 0.096, 0.181, 0.143, 0.253, 0.206, 0.295]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.24766791044776118
[2m[36m(func pid=184353)[0m top5: 0.6063432835820896
[2m[36m(func pid=184353)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=184353)[0m f1_macro: 0.2132291757608451
[2m[36m(func pid=184353)[0m f1_weighted: 0.17592358253467724
[2m[36m(func pid=184353)[0m f1_per_class: [0.44, 0.428, 0.216, 0.055, 0.112, 0.294, 0.065, 0.282, 0.162, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4007 | Steps: 2 | Val loss: 2.0190 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183934)[0m top1: 0.4244402985074627
[2m[36m(func pid=183934)[0m top5: 0.9104477611940298
[2m[36m(func pid=183934)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=183934)[0m f1_macro: 0.40993988831189
[2m[36m(func pid=183934)[0m f1_weighted: 0.43771675166664825
[2m[36m(func pid=183934)[0m f1_per_class: [0.567, 0.484, 0.733, 0.497, 0.114, 0.354, 0.449, 0.199, 0.22, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0169 | Steps: 2 | Val loss: 2.3065 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0000 | Steps: 2 | Val loss: 101.1756 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:47:43 (running for 00:04:35.76)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.401 |      0.237 |                   44 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.029 |      0.314 |                   44 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.002 |      0.41  |                   43 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.003 |      0.213 |                   42 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.27238805970149255
[2m[36m(func pid=183140)[0m top5: 0.7663246268656716
[2m[36m(func pid=183140)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=183140)[0m f1_macro: 0.2365597962938455
[2m[36m(func pid=183140)[0m f1_weighted: 0.2582624174920703
[2m[36m(func pid=183140)[0m f1_per_class: [0.33, 0.339, 0.478, 0.444, 0.047, 0.117, 0.109, 0.246, 0.134, 0.121]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.1044 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=183515)[0m top1: 0.3306902985074627
[2m[36m(func pid=183515)[0m top5: 0.8722014925373134
[2m[36m(func pid=183515)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=183515)[0m f1_macro: 0.3142560898519715
[2m[36m(func pid=183515)[0m f1_weighted: 0.32722111316524244
[2m[36m(func pid=183515)[0m f1_per_class: [0.472, 0.418, 0.545, 0.561, 0.104, 0.173, 0.138, 0.246, 0.199, 0.286]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.2439365671641791
[2m[36m(func pid=184353)[0m top5: 0.597481343283582
[2m[36m(func pid=184353)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=184353)[0m f1_macro: 0.20815657751001834
[2m[36m(func pid=184353)[0m f1_weighted: 0.1654247258814948
[2m[36m(func pid=184353)[0m f1_per_class: [0.432, 0.435, 0.245, 0.046, 0.111, 0.285, 0.042, 0.285, 0.123, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3393 | Steps: 2 | Val loss: 2.0155 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=183934)[0m top1: 0.42723880597014924
[2m[36m(func pid=183934)[0m top5: 0.9104477611940298
[2m[36m(func pid=183934)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=183934)[0m f1_macro: 0.41056813307634893
[2m[36m(func pid=183934)[0m f1_weighted: 0.4412168253008775
[2m[36m(func pid=183934)[0m f1_per_class: [0.567, 0.491, 0.733, 0.506, 0.114, 0.333, 0.458, 0.196, 0.218, 0.491]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0246 | Steps: 2 | Val loss: 2.3579 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=183140)[0m top1: 0.28125
[2m[36m(func pid=183140)[0m top5: 0.7635261194029851
[2m[36m(func pid=183140)[0m f1_micro: 0.28125
[2m[36m(func pid=183140)[0m f1_macro: 0.24300270447416242
[2m[36m(func pid=183140)[0m f1_weighted: 0.261842570387719
[2m[36m(func pid=183140)[0m f1_per_class: [0.335, 0.35, 0.5, 0.464, 0.047, 0.119, 0.092, 0.258, 0.132, 0.131]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 107.7881 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0006 | Steps: 2 | Val loss: 5.2076 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:47:50 (running for 00:04:42.65)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.339 |      0.243 |                   45 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.025 |      0.311 |                   46 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                   44 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.208 |                   43 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.32975746268656714
[2m[36m(func pid=183515)[0m top5: 0.8684701492537313
[2m[36m(func pid=183515)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=183515)[0m f1_macro: 0.31116080446593514
[2m[36m(func pid=183515)[0m f1_weighted: 0.3240707383010076
[2m[36m(func pid=183515)[0m f1_per_class: [0.486, 0.422, 0.545, 0.564, 0.096, 0.152, 0.13, 0.244, 0.199, 0.273]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.24580223880597016
[2m[36m(func pid=184353)[0m top5: 0.5876865671641791
[2m[36m(func pid=184353)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=184353)[0m f1_macro: 0.21087092208212047
[2m[36m(func pid=184353)[0m f1_weighted: 0.16082830910261023
[2m[36m(func pid=184353)[0m f1_per_class: [0.444, 0.443, 0.293, 0.036, 0.107, 0.287, 0.03, 0.288, 0.103, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3051 | Steps: 2 | Val loss: 2.0149 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=183934)[0m top1: 0.42490671641791045
[2m[36m(func pid=183934)[0m top5: 0.9095149253731343
[2m[36m(func pid=183934)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=183934)[0m f1_macro: 0.40630333072665736
[2m[36m(func pid=183934)[0m f1_weighted: 0.4374855876796038
[2m[36m(func pid=183934)[0m f1_per_class: [0.571, 0.486, 0.733, 0.5, 0.114, 0.336, 0.457, 0.166, 0.226, 0.473]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0127 | Steps: 2 | Val loss: 2.3937 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=183140)[0m top1: 0.28451492537313433
[2m[36m(func pid=183140)[0m top5: 0.7583955223880597
[2m[36m(func pid=183140)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=183140)[0m f1_macro: 0.24997678537643617
[2m[36m(func pid=183140)[0m f1_weighted: 0.26190848901577585
[2m[36m(func pid=183140)[0m f1_per_class: [0.345, 0.355, 0.537, 0.473, 0.05, 0.121, 0.076, 0.268, 0.144, 0.131]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 115.7851 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.3523 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=183515)[0m top1: 0.3278917910447761
[2m[36m(func pid=183515)[0m top5: 0.8680037313432836
[2m[36m(func pid=183515)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=183515)[0m f1_macro: 0.307413531936068
[2m[36m(func pid=183515)[0m f1_weighted: 0.3209451366002599
[2m[36m(func pid=183515)[0m f1_per_class: [0.476, 0.418, 0.533, 0.564, 0.096, 0.129, 0.13, 0.247, 0.209, 0.271]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m top1: 0.25046641791044777
[2m[36m(func pid=184353)[0m top5: 0.5853544776119403
[2m[36m(func pid=184353)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=184353)[0m f1_macro: 0.2141488296941619
[2m[36m(func pid=184353)[0m f1_weighted: 0.1607580539578915
[2m[36m(func pid=184353)[0m f1_per_class: [0.456, 0.452, 0.329, 0.03, 0.113, 0.29, 0.03, 0.298, 0.066, 0.077]
[2m[36m(func pid=184353)[0m 
== Status ==
Current time: 2024-01-07 00:47:56 (running for 00:04:48.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.305 |      0.25  |                   46 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.013 |      0.307 |                   47 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.406 |                   45 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.214 |                   45 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.2499 | Steps: 2 | Val loss: 2.0130 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=183934)[0m top1: 0.42350746268656714
[2m[36m(func pid=183934)[0m top5: 0.909981343283582
[2m[36m(func pid=183934)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=183934)[0m f1_macro: 0.40822548432689043
[2m[36m(func pid=183934)[0m f1_weighted: 0.4362863796370135
[2m[36m(func pid=183934)[0m f1_per_class: [0.571, 0.487, 0.733, 0.502, 0.112, 0.335, 0.45, 0.165, 0.225, 0.5]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0320 | Steps: 2 | Val loss: 2.4305 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=183140)[0m top1: 0.28591417910447764
[2m[36m(func pid=183140)[0m top5: 0.757929104477612
[2m[36m(func pid=183140)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=183140)[0m f1_macro: 0.25476794105535844
[2m[36m(func pid=183140)[0m f1_weighted: 0.2579428028963299
[2m[36m(func pid=183140)[0m f1_per_class: [0.351, 0.361, 0.55, 0.476, 0.051, 0.107, 0.057, 0.277, 0.159, 0.16]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0002 | Steps: 2 | Val loss: 123.5849 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183515)[0m top1: 0.32882462686567165
[2m[36m(func pid=183515)[0m top5: 0.867070895522388
[2m[36m(func pid=183515)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=183515)[0m f1_macro: 0.30515941645060785
[2m[36m(func pid=183515)[0m f1_weighted: 0.3207476766776041
[2m[36m(func pid=183515)[0m f1_per_class: [0.479, 0.43, 0.522, 0.567, 0.104, 0.127, 0.123, 0.247, 0.192, 0.261]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0011 | Steps: 2 | Val loss: 5.4047 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:48:01 (running for 00:04:53.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.25  |      0.255 |                   47 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.032 |      0.305 |                   48 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.408 |                   46 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.214 |                   46 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.2490671641791045
[2m[36m(func pid=184353)[0m top5: 0.5792910447761194
[2m[36m(func pid=184353)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=184353)[0m f1_macro: 0.2141694430474609
[2m[36m(func pid=184353)[0m f1_weighted: 0.15582022648813823
[2m[36m(func pid=184353)[0m f1_per_class: [0.453, 0.457, 0.375, 0.023, 0.116, 0.285, 0.022, 0.296, 0.038, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2965 | Steps: 2 | Val loss: 2.0129 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0188 | Steps: 2 | Val loss: 2.4564 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=183934)[0m top1: 0.42863805970149255
[2m[36m(func pid=183934)[0m top5: 0.9118470149253731
[2m[36m(func pid=183934)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=183934)[0m f1_macro: 0.4097457748239194
[2m[36m(func pid=183934)[0m f1_weighted: 0.4423575252320447
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.484, 0.733, 0.501, 0.114, 0.344, 0.47, 0.172, 0.217, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2887126865671642
[2m[36m(func pid=183140)[0m top5: 0.7560634328358209
[2m[36m(func pid=183140)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=183140)[0m f1_macro: 0.2574538474630246
[2m[36m(func pid=183140)[0m f1_weighted: 0.25905085493987423
[2m[36m(func pid=183140)[0m f1_per_class: [0.353, 0.362, 0.55, 0.483, 0.053, 0.115, 0.048, 0.282, 0.17, 0.159]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.1130 | Steps: 2 | Val loss: 130.3865 | Batch size: 32 | lr: 0.1 | Duration: 2.55s
[2m[36m(func pid=183515)[0m top1: 0.3292910447761194
[2m[36m(func pid=183515)[0m top5: 0.8694029850746269
[2m[36m(func pid=183515)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=183515)[0m f1_macro: 0.30543620505035757
[2m[36m(func pid=183515)[0m f1_weighted: 0.32048894323476496
[2m[36m(func pid=183515)[0m f1_per_class: [0.493, 0.428, 0.522, 0.565, 0.094, 0.115, 0.128, 0.249, 0.205, 0.257]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.4618 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:48:06 (running for 00:04:58.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.296 |      0.257 |                   48 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.019 |      0.305 |                   49 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.41  |                   47 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.113 |      0.217 |                   47 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=184353)[0m top1: 0.25093283582089554
[2m[36m(func pid=184353)[0m top5: 0.5750932835820896
[2m[36m(func pid=184353)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=184353)[0m f1_macro: 0.21733783872511442
[2m[36m(func pid=184353)[0m f1_weighted: 0.152900815012279
[2m[36m(func pid=184353)[0m f1_per_class: [0.463, 0.465, 0.414, 0.01, 0.122, 0.289, 0.018, 0.295, 0.02, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1720 | Steps: 2 | Val loss: 2.0163 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.4821 | Batch size: 32 | lr: 0.001 | Duration: 2.51s
[2m[36m(func pid=183934)[0m top1: 0.4300373134328358
[2m[36m(func pid=183934)[0m top5: 0.9090485074626866
[2m[36m(func pid=183934)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=183934)[0m f1_macro: 0.4101102646175798
[2m[36m(func pid=183934)[0m f1_weighted: 0.4439348712050619
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.486, 0.733, 0.502, 0.115, 0.341, 0.475, 0.173, 0.215, 0.481]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2905783582089552
[2m[36m(func pid=183140)[0m top5: 0.7495335820895522
[2m[36m(func pid=183140)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=183140)[0m f1_macro: 0.26199275711092396
[2m[36m(func pid=183140)[0m f1_weighted: 0.25983896391482325
[2m[36m(func pid=183140)[0m f1_per_class: [0.372, 0.368, 0.564, 0.484, 0.055, 0.117, 0.045, 0.271, 0.173, 0.171]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.9077 | Steps: 2 | Val loss: 136.9823 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=183515)[0m top1: 0.33115671641791045
[2m[36m(func pid=183515)[0m top5: 0.8684701492537313
[2m[36m(func pid=183515)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=183515)[0m f1_macro: 0.3069919168612943
[2m[36m(func pid=183515)[0m f1_weighted: 0.3210424453385124
[2m[36m(func pid=183515)[0m f1_per_class: [0.496, 0.425, 0.533, 0.567, 0.092, 0.106, 0.131, 0.251, 0.211, 0.257]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0052 | Steps: 2 | Val loss: 5.5510 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.2451 | Steps: 2 | Val loss: 2.0122 | Batch size: 32 | lr: 0.0001 | Duration: 2.57s
[2m[36m(func pid=184353)[0m top1: 0.24953358208955223
[2m[36m(func pid=184353)[0m top5: 0.5746268656716418
[2m[36m(func pid=184353)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=184353)[0m f1_macro: 0.21873848140255334
[2m[36m(func pid=184353)[0m f1_weighted: 0.1511407262418291
[2m[36m(func pid=184353)[0m f1_per_class: [0.456, 0.46, 0.444, 0.007, 0.122, 0.291, 0.018, 0.29, 0.022, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0173 | Steps: 2 | Val loss: 2.4929 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:48:13 (running for 00:05:05.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.172 |      0.262 |                   49 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.023 |      0.307 |                   50 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.005 |      0.405 |                   49 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  2.908 |      0.219 |                   48 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42630597014925375
[2m[36m(func pid=183934)[0m top5: 0.9076492537313433
[2m[36m(func pid=183934)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=183934)[0m f1_macro: 0.40460440415876314
[2m[36m(func pid=183934)[0m f1_weighted: 0.4390422118784856
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.483, 0.733, 0.493, 0.112, 0.338, 0.475, 0.148, 0.215, 0.473]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.29011194029850745
[2m[36m(func pid=183140)[0m top5: 0.7527985074626866
[2m[36m(func pid=183140)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=183140)[0m f1_macro: 0.26493219559623965
[2m[36m(func pid=183140)[0m f1_weighted: 0.2585703317111886
[2m[36m(func pid=183140)[0m f1_per_class: [0.368, 0.377, 0.595, 0.487, 0.058, 0.121, 0.033, 0.252, 0.181, 0.178]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.2582 | Steps: 2 | Val loss: 136.3762 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=183515)[0m top1: 0.3316231343283582
[2m[36m(func pid=183515)[0m top5: 0.8698694029850746
[2m[36m(func pid=183515)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=183515)[0m f1_macro: 0.3094370746973636
[2m[36m(func pid=183515)[0m f1_weighted: 0.32346546918983377
[2m[36m(func pid=183515)[0m f1_per_class: [0.486, 0.427, 0.533, 0.569, 0.093, 0.112, 0.133, 0.252, 0.211, 0.277]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0003 | Steps: 2 | Val loss: 5.5899 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=184353)[0m top1: 0.25093283582089554
[2m[36m(func pid=184353)[0m top5: 0.5722947761194029
[2m[36m(func pid=184353)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=184353)[0m f1_macro: 0.21736781771682212
[2m[36m(func pid=184353)[0m f1_weighted: 0.15119728370499938
[2m[36m(func pid=184353)[0m f1_per_class: [0.43, 0.462, 0.453, 0.007, 0.121, 0.291, 0.018, 0.293, 0.021, 0.077]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1253 | Steps: 2 | Val loss: 2.0150 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0180 | Steps: 2 | Val loss: 2.4923 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:48:18 (running for 00:05:10.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.245 |      0.265 |                   50 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.017 |      0.309 |                   51 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   50 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  3.258 |      0.217 |                   49 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4253731343283582
[2m[36m(func pid=183934)[0m top5: 0.9085820895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=183934)[0m f1_macro: 0.4003271996822706
[2m[36m(func pid=183934)[0m f1_weighted: 0.4394430716704936
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.478, 0.71, 0.496, 0.109, 0.338, 0.478, 0.143, 0.211, 0.464]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.28404850746268656
[2m[36m(func pid=183140)[0m top5: 0.7513992537313433
[2m[36m(func pid=183140)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=183140)[0m f1_macro: 0.26204342328919866
[2m[36m(func pid=183140)[0m f1_weighted: 0.25073833074292373
[2m[36m(func pid=183140)[0m f1_per_class: [0.388, 0.372, 0.595, 0.48, 0.049, 0.106, 0.022, 0.247, 0.178, 0.184]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0714 | Steps: 2 | Val loss: 133.1454 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=183515)[0m top1: 0.33348880597014924
[2m[36m(func pid=183515)[0m top5: 0.871268656716418
[2m[36m(func pid=183515)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=183515)[0m f1_macro: 0.31002357979947753
[2m[36m(func pid=183515)[0m f1_weighted: 0.3261040242122248
[2m[36m(func pid=183515)[0m f1_per_class: [0.511, 0.432, 0.533, 0.572, 0.09, 0.111, 0.138, 0.247, 0.196, 0.27]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0012 | Steps: 2 | Val loss: 5.6629 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1280 | Steps: 2 | Val loss: 2.0206 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=184353)[0m top1: 0.25326492537313433
[2m[36m(func pid=184353)[0m top5: 0.5769589552238806
[2m[36m(func pid=184353)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=184353)[0m f1_macro: 0.21984005679689472
[2m[36m(func pid=184353)[0m f1_weighted: 0.15346698548660762
[2m[36m(func pid=184353)[0m f1_per_class: [0.389, 0.455, 0.48, 0.013, 0.111, 0.297, 0.018, 0.301, 0.059, 0.074]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0162 | Steps: 2 | Val loss: 2.5359 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 00:48:23 (running for 00:05:16.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.125 |      0.262 |                   51 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.018 |      0.31  |                   52 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.4   |                   51 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.071 |      0.22  |                   50 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42677238805970147
[2m[36m(func pid=183934)[0m top5: 0.9090485074626866
[2m[36m(func pid=183934)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=183934)[0m f1_macro: 0.4003483629217234
[2m[36m(func pid=183934)[0m f1_weighted: 0.44010876415348477
[2m[36m(func pid=183934)[0m f1_per_class: [0.571, 0.482, 0.71, 0.493, 0.111, 0.338, 0.483, 0.135, 0.217, 0.464]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.27845149253731344
[2m[36m(func pid=183140)[0m top5: 0.7551305970149254
[2m[36m(func pid=183140)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=183140)[0m f1_macro: 0.25967974090418916
[2m[36m(func pid=183140)[0m f1_weighted: 0.24604942575790034
[2m[36m(func pid=183140)[0m f1_per_class: [0.395, 0.373, 0.595, 0.475, 0.049, 0.099, 0.015, 0.236, 0.171, 0.189]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6764 | Steps: 2 | Val loss: 129.5805 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=183515)[0m top1: 0.3344216417910448
[2m[36m(func pid=183515)[0m top5: 0.8689365671641791
[2m[36m(func pid=183515)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=183515)[0m f1_macro: 0.31143583470244074
[2m[36m(func pid=183515)[0m f1_weighted: 0.32624380942941106
[2m[36m(func pid=183515)[0m f1_per_class: [0.515, 0.433, 0.545, 0.572, 0.102, 0.107, 0.138, 0.248, 0.205, 0.248]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0005 | Steps: 2 | Val loss: 5.7008 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0306 | Steps: 2 | Val loss: 2.0164 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=184353)[0m top1: 0.25
[2m[36m(func pid=184353)[0m top5: 0.5774253731343284
[2m[36m(func pid=184353)[0m f1_micro: 0.25
[2m[36m(func pid=184353)[0m f1_macro: 0.21729063092916606
[2m[36m(func pid=184353)[0m f1_weighted: 0.15564585725834254
[2m[36m(func pid=184353)[0m f1_per_class: [0.347, 0.449, 0.471, 0.03, 0.113, 0.289, 0.018, 0.292, 0.09, 0.074]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0268 | Steps: 2 | Val loss: 2.5536 | Batch size: 32 | lr: 0.001 | Duration: 2.57s
== Status ==
Current time: 2024-01-07 00:48:28 (running for 00:05:21.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.128 |      0.26  |                   52 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.016 |      0.311 |                   53 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.399 |                   52 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.676 |      0.217 |                   51 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42677238805970147
[2m[36m(func pid=183934)[0m top5: 0.9090485074626866
[2m[36m(func pid=183934)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=183934)[0m f1_macro: 0.3992853366760981
[2m[36m(func pid=183934)[0m f1_weighted: 0.44159267878385816
[2m[36m(func pid=183934)[0m f1_per_class: [0.567, 0.475, 0.71, 0.496, 0.104, 0.335, 0.486, 0.162, 0.218, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.27845149253731344
[2m[36m(func pid=183140)[0m top5: 0.7630597014925373
[2m[36m(func pid=183140)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=183140)[0m f1_macro: 0.2644789256439962
[2m[36m(func pid=183140)[0m f1_weighted: 0.2472307710961703
[2m[36m(func pid=183140)[0m f1_per_class: [0.4, 0.374, 0.611, 0.481, 0.058, 0.099, 0.012, 0.224, 0.186, 0.199]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 126.6740 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.33675373134328357
[2m[36m(func pid=183515)[0m top5: 0.8694029850746269
[2m[36m(func pid=183515)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=183515)[0m f1_macro: 0.3122027066263414
[2m[36m(func pid=183515)[0m f1_weighted: 0.3280193834177407
[2m[36m(func pid=183515)[0m f1_per_class: [0.515, 0.43, 0.545, 0.572, 0.093, 0.113, 0.144, 0.25, 0.203, 0.257]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.7919 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0283 | Steps: 2 | Val loss: 2.0159 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=184353)[0m top1: 0.24673507462686567
[2m[36m(func pid=184353)[0m top5: 0.5764925373134329
[2m[36m(func pid=184353)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=184353)[0m f1_macro: 0.2381563251710203
[2m[36m(func pid=184353)[0m f1_weighted: 0.1575583647677507
[2m[36m(func pid=184353)[0m f1_per_class: [0.324, 0.453, 0.533, 0.033, 0.099, 0.272, 0.022, 0.274, 0.105, 0.267]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0311 | Steps: 2 | Val loss: 2.5846 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:48:34 (running for 00:05:26.43)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.031 |      0.264 |                   53 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.027 |      0.312 |                   54 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   53 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.238 |                   52 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2756529850746269
[2m[36m(func pid=183140)[0m top5: 0.7672574626865671
[2m[36m(func pid=183140)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=183140)[0m f1_macro: 0.2607550580552007
[2m[36m(func pid=183140)[0m f1_weighted: 0.24623431953782687
[2m[36m(func pid=183140)[0m f1_per_class: [0.386, 0.374, 0.595, 0.481, 0.062, 0.099, 0.012, 0.22, 0.174, 0.206]
[2m[36m(func pid=183934)[0m top1: 0.4291044776119403
[2m[36m(func pid=183934)[0m top5: 0.9085820895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=183934)[0m f1_macro: 0.4010797146651023
[2m[36m(func pid=183934)[0m f1_weighted: 0.4444264755175979
[2m[36m(func pid=183934)[0m f1_per_class: [0.567, 0.475, 0.71, 0.495, 0.098, 0.336, 0.496, 0.162, 0.217, 0.456]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.33722014925373134
[2m[36m(func pid=183515)[0m top5: 0.8726679104477612
[2m[36m(func pid=183515)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=183515)[0m f1_macro: 0.31061669577106815
[2m[36m(func pid=183515)[0m f1_weighted: 0.3293183025573827
[2m[36m(func pid=183515)[0m f1_per_class: [0.5, 0.435, 0.533, 0.57, 0.099, 0.107, 0.149, 0.253, 0.213, 0.247]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 123.1253 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0006 | Steps: 2 | Val loss: 5.9071 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9822 | Steps: 2 | Val loss: 2.0161 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=184353)[0m top1: 0.24580223880597016
[2m[36m(func pid=184353)[0m top5: 0.5788246268656716
[2m[36m(func pid=184353)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=184353)[0m f1_macro: 0.25589474057538114
[2m[36m(func pid=184353)[0m f1_weighted: 0.1630266046289926
[2m[36m(func pid=184353)[0m f1_per_class: [0.295, 0.451, 0.558, 0.049, 0.1, 0.24, 0.028, 0.273, 0.165, 0.4]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0102 | Steps: 2 | Val loss: 2.5873 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:48:39 (running for 00:05:31.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.028 |      0.261 |                   54 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.031 |      0.311 |                   55 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.4   |                   54 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.256 |                   53 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.425839552238806
[2m[36m(func pid=183934)[0m top5: 0.9076492537313433
[2m[36m(func pid=183934)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=183934)[0m f1_macro: 0.40040515743265975
[2m[36m(func pid=183934)[0m f1_weighted: 0.4417027959067473
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.472, 0.71, 0.492, 0.095, 0.328, 0.494, 0.162, 0.214, 0.456]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2719216417910448
[2m[36m(func pid=183140)[0m top5: 0.7761194029850746
[2m[36m(func pid=183140)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=183140)[0m f1_macro: 0.26298969486726154
[2m[36m(func pid=183140)[0m f1_weighted: 0.2435663964339544
[2m[36m(func pid=183140)[0m f1_per_class: [0.391, 0.372, 0.629, 0.472, 0.063, 0.097, 0.012, 0.22, 0.169, 0.204]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.34281716417910446
[2m[36m(func pid=183515)[0m top5: 0.8694029850746269
[2m[36m(func pid=183515)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=183515)[0m f1_macro: 0.31599097759726547
[2m[36m(func pid=183515)[0m f1_weighted: 0.33661532332986144
[2m[36m(func pid=183515)[0m f1_per_class: [0.511, 0.435, 0.545, 0.572, 0.097, 0.113, 0.167, 0.259, 0.209, 0.25]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 120.3538 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=184353)[0m top1: 0.23600746268656717
[2m[36m(func pid=184353)[0m top5: 0.590018656716418
[2m[36m(func pid=184353)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=184353)[0m f1_macro: 0.25152913033056856
[2m[36m(func pid=184353)[0m f1_weighted: 0.1603398300982097
[2m[36m(func pid=184353)[0m f1_per_class: [0.265, 0.438, 0.585, 0.07, 0.091, 0.195, 0.028, 0.272, 0.132, 0.439]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.0906 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0238 | Steps: 2 | Val loss: 2.0152 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0084 | Steps: 2 | Val loss: 2.5746 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:48:44 (running for 00:05:37.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.982 |      0.263 |                   55 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.008 |      0.315 |                   57 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.001 |      0.4   |                   54 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.252 |                   54 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.345615671641791
[2m[36m(func pid=183515)[0m top5: 0.8684701492537313
[2m[36m(func pid=183515)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=183515)[0m f1_macro: 0.31514721570949095
[2m[36m(func pid=183515)[0m f1_weighted: 0.34139119943194596
[2m[36m(func pid=183515)[0m f1_per_class: [0.496, 0.448, 0.533, 0.568, 0.083, 0.114, 0.181, 0.26, 0.209, 0.259]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.27098880597014924
[2m[36m(func pid=183140)[0m top5: 0.7807835820895522
[2m[36m(func pid=183140)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=183140)[0m f1_macro: 0.2624845204720585
[2m[36m(func pid=183140)[0m f1_weighted: 0.2442846252413866
[2m[36m(func pid=183140)[0m f1_per_class: [0.386, 0.371, 0.629, 0.477, 0.067, 0.086, 0.015, 0.217, 0.167, 0.209]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.41744402985074625
[2m[36m(func pid=183934)[0m top5: 0.9053171641791045
[2m[36m(func pid=183934)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=183934)[0m f1_macro: 0.3971625150986722
[2m[36m(func pid=183934)[0m f1_weighted: 0.43326025190958545
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.473, 0.71, 0.481, 0.105, 0.324, 0.477, 0.16, 0.218, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 118.5838 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=184353)[0m top1: 0.23460820895522388
[2m[36m(func pid=184353)[0m top5: 0.5988805970149254
[2m[36m(func pid=184353)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=184353)[0m f1_macro: 0.25038227006550634
[2m[36m(func pid=184353)[0m f1_weighted: 0.16320913485400848
[2m[36m(func pid=184353)[0m f1_per_class: [0.25, 0.437, 0.649, 0.1, 0.085, 0.147, 0.027, 0.271, 0.153, 0.385]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9596 | Steps: 2 | Val loss: 2.0117 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0240 | Steps: 2 | Val loss: 2.5683 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0002 | Steps: 2 | Val loss: 6.1135 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:48:50 (running for 00:05:42.31)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  1.024 |      0.262 |                   56 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.008 |      0.315 |                   57 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.399 |                   56 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.25  |                   55 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2691231343283582
[2m[36m(func pid=183140)[0m top5: 0.7807835820895522
[2m[36m(func pid=183140)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=183140)[0m f1_macro: 0.2610918179461351
[2m[36m(func pid=183140)[0m f1_weighted: 0.24260762624697477
[2m[36m(func pid=183140)[0m f1_per_class: [0.368, 0.375, 0.629, 0.468, 0.069, 0.092, 0.015, 0.219, 0.163, 0.214]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.35401119402985076
[2m[36m(func pid=183515)[0m top5: 0.8689365671641791
[2m[36m(func pid=183515)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=183515)[0m f1_macro: 0.3203749372006411
[2m[36m(func pid=183515)[0m f1_weighted: 0.3511148269764066
[2m[36m(func pid=183515)[0m f1_per_class: [0.511, 0.444, 0.533, 0.574, 0.092, 0.122, 0.204, 0.267, 0.211, 0.245]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.417910447761194
[2m[36m(func pid=183934)[0m top5: 0.9043843283582089
[2m[36m(func pid=183934)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.3986783415555639
[2m[36m(func pid=183934)[0m f1_weighted: 0.434622936549675
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.474, 0.71, 0.482, 0.089, 0.324, 0.475, 0.183, 0.218, 0.456]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 118.9154 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=184353)[0m top1: 0.2332089552238806
[2m[36m(func pid=184353)[0m top5: 0.6072761194029851
[2m[36m(func pid=184353)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=184353)[0m f1_macro: 0.25016914695577835
[2m[36m(func pid=184353)[0m f1_weighted: 0.16317274280991004
[2m[36m(func pid=184353)[0m f1_per_class: [0.25, 0.436, 0.649, 0.123, 0.083, 0.088, 0.028, 0.276, 0.148, 0.421]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.9295 | Steps: 2 | Val loss: 2.0060 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0099 | Steps: 2 | Val loss: 2.5783 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.1990 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:48:55 (running for 00:05:47.51)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.96  |      0.261 |                   57 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.024 |      0.32  |                   58 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.397 |                   57 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.25  |                   56 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.41511194029850745
[2m[36m(func pid=183934)[0m top5: 0.9034514925373134
[2m[36m(func pid=183934)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=183934)[0m f1_macro: 0.3974268072808647
[2m[36m(func pid=183934)[0m f1_weighted: 0.43124957350515336
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.469, 0.733, 0.482, 0.092, 0.321, 0.469, 0.176, 0.219, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2677238805970149
[2m[36m(func pid=183140)[0m top5: 0.7896455223880597
[2m[36m(func pid=183140)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=183140)[0m f1_macro: 0.2647386385057108
[2m[36m(func pid=183140)[0m f1_weighted: 0.24350613561219353
[2m[36m(func pid=183140)[0m f1_per_class: [0.382, 0.369, 0.595, 0.465, 0.087, 0.106, 0.015, 0.214, 0.194, 0.22]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3568097014925373
[2m[36m(func pid=183515)[0m top5: 0.8694029850746269
[2m[36m(func pid=183515)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=183515)[0m f1_macro: 0.31918556624383687
[2m[36m(func pid=183515)[0m f1_weighted: 0.35463178911498033
[2m[36m(func pid=183515)[0m f1_per_class: [0.518, 0.452, 0.511, 0.576, 0.09, 0.122, 0.211, 0.269, 0.2, 0.244]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.9202 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0002 | Steps: 2 | Val loss: 6.2453 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.8068 | Steps: 2 | Val loss: 1.9903 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0089 | Steps: 2 | Val loss: 2.5625 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=184353)[0m top1: 0.22667910447761194
[2m[36m(func pid=184353)[0m top5: 0.6114738805970149
[2m[36m(func pid=184353)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=184353)[0m f1_macro: 0.23834134604981522
[2m[36m(func pid=184353)[0m f1_weighted: 0.15881649428734668
[2m[36m(func pid=184353)[0m f1_per_class: [0.242, 0.426, 0.649, 0.136, 0.09, 0.052, 0.025, 0.267, 0.164, 0.333]
[2m[36m(func pid=184353)[0m 
== Status ==
Current time: 2024-01-07 00:49:00 (running for 00:05:52.76)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.807 |      0.271 |                   59 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.01  |      0.319 |                   59 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.397 |                   57 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.238 |                   57 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2667910447761194
[2m[36m(func pid=183140)[0m top5: 0.8013059701492538
[2m[36m(func pid=183140)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=183140)[0m f1_macro: 0.2711466180192076
[2m[36m(func pid=183140)[0m f1_weighted: 0.24492942791937478
[2m[36m(func pid=183140)[0m f1_per_class: [0.393, 0.371, 0.629, 0.463, 0.089, 0.11, 0.018, 0.209, 0.195, 0.233]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4193097014925373
[2m[36m(func pid=183934)[0m top5: 0.9039179104477612
[2m[36m(func pid=183934)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=183934)[0m f1_macro: 0.39835405640134464
[2m[36m(func pid=183934)[0m f1_weighted: 0.4353951461615969
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.471, 0.733, 0.494, 0.094, 0.327, 0.469, 0.178, 0.214, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.36007462686567165
[2m[36m(func pid=183515)[0m top5: 0.8703358208955224
[2m[36m(func pid=183515)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=183515)[0m f1_macro: 0.32259062723163834
[2m[36m(func pid=183515)[0m f1_weighted: 0.36008049369856887
[2m[36m(func pid=183515)[0m f1_per_class: [0.514, 0.453, 0.522, 0.57, 0.09, 0.125, 0.232, 0.273, 0.201, 0.245]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.0790 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0259 | Steps: 2 | Val loss: 2.5563 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8651 | Steps: 2 | Val loss: 1.9807 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.2564 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184353)[0m top1: 0.23087686567164178
[2m[36m(func pid=184353)[0m top5: 0.6152052238805971
[2m[36m(func pid=184353)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=184353)[0m f1_macro: 0.24687660979961898
[2m[36m(func pid=184353)[0m f1_weighted: 0.1702816032850189
[2m[36m(func pid=184353)[0m f1_per_class: [0.238, 0.422, 0.706, 0.183, 0.089, 0.035, 0.025, 0.273, 0.178, 0.321]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3605410447761194
[2m[36m(func pid=183515)[0m top5: 0.8694029850746269
[2m[36m(func pid=183515)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=183515)[0m f1_macro: 0.32438425692614026
[2m[36m(func pid=183515)[0m f1_weighted: 0.36417022616660266
[2m[36m(func pid=183515)[0m f1_per_class: [0.533, 0.44, 0.511, 0.567, 0.087, 0.13, 0.251, 0.28, 0.213, 0.232]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:49:05 (running for 00:05:57.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.865 |      0.278 |                   60 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.026 |      0.324 |                   61 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.398 |                   58 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.247 |                   58 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.2667910447761194
[2m[36m(func pid=183140)[0m top5: 0.8111007462686567
[2m[36m(func pid=183140)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=183140)[0m f1_macro: 0.27793097720775645
[2m[36m(func pid=183140)[0m f1_weighted: 0.2467464527502169
[2m[36m(func pid=183140)[0m f1_per_class: [0.398, 0.364, 0.667, 0.462, 0.085, 0.116, 0.025, 0.209, 0.209, 0.246]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4244402985074627
[2m[36m(func pid=183934)[0m top5: 0.9057835820895522
[2m[36m(func pid=183934)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=183934)[0m f1_macro: 0.4046057052307262
[2m[36m(func pid=183934)[0m f1_weighted: 0.44172941449646835
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.473, 0.733, 0.501, 0.097, 0.329, 0.479, 0.184, 0.218, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.4201 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0042 | Steps: 2 | Val loss: 2.5255 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8539 | Steps: 2 | Val loss: 1.9667 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3414 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=184353)[0m top1: 0.23694029850746268
[2m[36m(func pid=184353)[0m top5: 0.6175373134328358
[2m[36m(func pid=184353)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=184353)[0m f1_macro: 0.2579624225639827
[2m[36m(func pid=184353)[0m f1_weighted: 0.18045830325979761
[2m[36m(func pid=184353)[0m f1_per_class: [0.231, 0.418, 0.774, 0.216, 0.083, 0.036, 0.025, 0.282, 0.201, 0.312]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3675373134328358
[2m[36m(func pid=183515)[0m top5: 0.8740671641791045
[2m[36m(func pid=183515)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=183515)[0m f1_macro: 0.3326291592590776
[2m[36m(func pid=183515)[0m f1_weighted: 0.37293056636869987
[2m[36m(func pid=183515)[0m f1_per_class: [0.534, 0.448, 0.545, 0.572, 0.091, 0.139, 0.268, 0.271, 0.219, 0.239]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:49:10 (running for 00:06:02.95)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.854 |      0.277 |                   61 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.333 |                   62 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.405 |                   59 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.258 |                   59 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183140)[0m top1: 0.26492537313432835
[2m[36m(func pid=183140)[0m top5: 0.8208955223880597
[2m[36m(func pid=183140)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=183140)[0m f1_macro: 0.27700851178921526
[2m[36m(func pid=183140)[0m f1_weighted: 0.2469663854784801
[2m[36m(func pid=183140)[0m f1_per_class: [0.387, 0.363, 0.647, 0.462, 0.088, 0.114, 0.027, 0.203, 0.214, 0.264]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4230410447761194
[2m[36m(func pid=183934)[0m top5: 0.9081156716417911
[2m[36m(func pid=183934)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.4049846333210527
[2m[36m(func pid=183934)[0m f1_weighted: 0.44098964585531153
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.473, 0.733, 0.506, 0.093, 0.325, 0.472, 0.184, 0.215, 0.456]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.0180 | Steps: 2 | Val loss: 118.2462 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0112 | Steps: 2 | Val loss: 2.5258 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7545 | Steps: 2 | Val loss: 1.9505 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.4174 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=184353)[0m top1: 0.24067164179104478
[2m[36m(func pid=184353)[0m top5: 0.6208022388059702
[2m[36m(func pid=184353)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=184353)[0m f1_macro: 0.26263292658851606
[2m[36m(func pid=184353)[0m f1_weighted: 0.18755874831148794
[2m[36m(func pid=184353)[0m f1_per_class: [0.223, 0.412, 0.774, 0.246, 0.085, 0.03, 0.025, 0.289, 0.2, 0.343]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183140)[0m top1: 0.269589552238806
[2m[36m(func pid=183140)[0m top5: 0.8288246268656716
[2m[36m(func pid=183140)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=183140)[0m f1_macro: 0.2850986896552693
[2m[36m(func pid=183140)[0m f1_weighted: 0.25406585346805616
[2m[36m(func pid=183140)[0m f1_per_class: [0.387, 0.372, 0.688, 0.463, 0.088, 0.117, 0.042, 0.207, 0.219, 0.269]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.3694029850746269
[2m[36m(func pid=183515)[0m top5: 0.8745335820895522
[2m[36m(func pid=183515)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=183515)[0m f1_macro: 0.33208516027910384
[2m[36m(func pid=183515)[0m f1_weighted: 0.37685011901713766
[2m[36m(func pid=183515)[0m f1_per_class: [0.538, 0.451, 0.533, 0.568, 0.08, 0.144, 0.282, 0.27, 0.213, 0.241]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:49:16 (running for 00:06:08.38)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.755 |      0.285 |                   62 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.011 |      0.332 |                   63 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   61 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.018 |      0.263 |                   60 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4221082089552239
[2m[36m(func pid=183934)[0m top5: 0.90625
[2m[36m(func pid=183934)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=183934)[0m f1_macro: 0.4024302313356999
[2m[36m(func pid=183934)[0m f1_weighted: 0.43957254762245046
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.476, 0.733, 0.508, 0.094, 0.325, 0.464, 0.19, 0.215, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 117.6385 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7947 | Steps: 2 | Val loss: 1.9363 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0059 | Steps: 2 | Val loss: 2.5362 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=184353)[0m top1: 0.2439365671641791
[2m[36m(func pid=184353)[0m top5: 0.6180037313432836
[2m[36m(func pid=184353)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=184353)[0m f1_macro: 0.25744360191180293
[2m[36m(func pid=184353)[0m f1_weighted: 0.19743939102560834
[2m[36m(func pid=184353)[0m f1_per_class: [0.216, 0.405, 0.733, 0.284, 0.087, 0.031, 0.028, 0.291, 0.208, 0.29]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.3752 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=183515)[0m top1: 0.37220149253731344
[2m[36m(func pid=183515)[0m top5: 0.8763992537313433
[2m[36m(func pid=183515)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=183515)[0m f1_macro: 0.3327894604639673
[2m[36m(func pid=183515)[0m f1_weighted: 0.3809870667143357
[2m[36m(func pid=183515)[0m f1_per_class: [0.551, 0.458, 0.5, 0.57, 0.076, 0.144, 0.29, 0.27, 0.212, 0.258]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2691231343283582
[2m[36m(func pid=183140)[0m top5: 0.8372201492537313
[2m[36m(func pid=183140)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=183140)[0m f1_macro: 0.2862266787700521
[2m[36m(func pid=183140)[0m f1_weighted: 0.2598379788233884
[2m[36m(func pid=183140)[0m f1_per_class: [0.387, 0.356, 0.688, 0.459, 0.085, 0.138, 0.067, 0.206, 0.207, 0.269]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:49:21 (running for 00:06:13.62)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.795 |      0.286 |                   63 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.006 |      0.333 |                   64 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.408 |                   62 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.257 |                   61 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4281716417910448
[2m[36m(func pid=183934)[0m top5: 0.9081156716417911
[2m[36m(func pid=183934)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=183934)[0m f1_macro: 0.40750389831689543
[2m[36m(func pid=183934)[0m f1_weighted: 0.4462704149713448
[2m[36m(func pid=183934)[0m f1_per_class: [0.584, 0.476, 0.733, 0.512, 0.115, 0.33, 0.481, 0.19, 0.22, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3118 | Steps: 2 | Val loss: 117.8349 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0052 | Steps: 2 | Val loss: 2.5004 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7322 | Steps: 2 | Val loss: 1.9274 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=184353)[0m top1: 0.24673507462686567
[2m[36m(func pid=184353)[0m top5: 0.6184701492537313
[2m[36m(func pid=184353)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=184353)[0m f1_macro: 0.26026668975213657
[2m[36m(func pid=184353)[0m f1_weighted: 0.20735863587763018
[2m[36m(func pid=184353)[0m f1_per_class: [0.204, 0.396, 0.759, 0.326, 0.093, 0.031, 0.028, 0.293, 0.21, 0.263]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.4594 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=183515)[0m top1: 0.37966417910447764
[2m[36m(func pid=183515)[0m top5: 0.8819962686567164
[2m[36m(func pid=183515)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=183515)[0m f1_macro: 0.3377136633794028
[2m[36m(func pid=183515)[0m f1_weighted: 0.38813628453986715
[2m[36m(func pid=183515)[0m f1_per_class: [0.556, 0.459, 0.522, 0.578, 0.073, 0.155, 0.301, 0.266, 0.208, 0.26]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.2691231343283582
[2m[36m(func pid=183140)[0m top5: 0.8423507462686567
[2m[36m(func pid=183140)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=183140)[0m f1_macro: 0.2838677164547724
[2m[36m(func pid=183140)[0m f1_weighted: 0.26142065718125257
[2m[36m(func pid=183140)[0m f1_per_class: [0.38, 0.363, 0.667, 0.456, 0.083, 0.127, 0.075, 0.209, 0.219, 0.259]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.1219 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=183934)[0m top1: 0.427705223880597
[2m[36m(func pid=183934)[0m top5: 0.90625
[2m[36m(func pid=183934)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=183934)[0m f1_macro: 0.4064631755015108
[2m[36m(func pid=183934)[0m f1_weighted: 0.4454315907462756
[2m[36m(func pid=183934)[0m f1_per_class: [0.596, 0.475, 0.71, 0.509, 0.113, 0.329, 0.48, 0.191, 0.228, 0.433]
== Status ==
Current time: 2024-01-07 00:49:26 (running for 00:06:18.76)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.732 |      0.284 |                   64 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.005 |      0.338 |                   65 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.406 |                   63 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.312 |      0.26  |                   62 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0066 | Steps: 2 | Val loss: 2.4894 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6702 | Steps: 2 | Val loss: 1.9132 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=184353)[0m top1: 0.2453358208955224
[2m[36m(func pid=184353)[0m top5: 0.6161380597014925
[2m[36m(func pid=184353)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=184353)[0m f1_macro: 0.26075215226455367
[2m[36m(func pid=184353)[0m f1_weighted: 0.2115916273336652
[2m[36m(func pid=184353)[0m f1_per_class: [0.195, 0.384, 0.786, 0.348, 0.089, 0.031, 0.028, 0.303, 0.221, 0.225]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0005 | Steps: 2 | Val loss: 6.4893 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=183515)[0m top1: 0.37966417910447764
[2m[36m(func pid=183515)[0m top5: 0.8833955223880597
[2m[36m(func pid=183515)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=183515)[0m f1_macro: 0.3365923277812985
[2m[36m(func pid=183515)[0m f1_weighted: 0.39095951867279105
[2m[36m(func pid=183515)[0m f1_per_class: [0.556, 0.454, 0.511, 0.57, 0.071, 0.149, 0.325, 0.261, 0.205, 0.265]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.27658582089552236
[2m[36m(func pid=183140)[0m top5: 0.8470149253731343
[2m[36m(func pid=183140)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=183140)[0m f1_macro: 0.2922010773822378
[2m[36m(func pid=183140)[0m f1_weighted: 0.2729439324355397
[2m[36m(func pid=183140)[0m f1_per_class: [0.391, 0.364, 0.688, 0.455, 0.095, 0.129, 0.11, 0.219, 0.216, 0.255]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.2096 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 00:49:31 (running for 00:06:23.91)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.67  |      0.292 |                   65 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.007 |      0.337 |                   66 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   64 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.261 |                   63 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4253731343283582
[2m[36m(func pid=183934)[0m top5: 0.9067164179104478
[2m[36m(func pid=183934)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=183934)[0m f1_macro: 0.4020137862933707
[2m[36m(func pid=183934)[0m f1_weighted: 0.44328126853924
[2m[36m(func pid=183934)[0m f1_per_class: [0.584, 0.472, 0.71, 0.506, 0.11, 0.328, 0.483, 0.174, 0.22, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0042 | Steps: 2 | Val loss: 2.4801 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.8084 | Steps: 2 | Val loss: 1.8924 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=184353)[0m top1: 0.24766791044776118
[2m[36m(func pid=184353)[0m top5: 0.6161380597014925
[2m[36m(func pid=184353)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=184353)[0m f1_macro: 0.2644101523213602
[2m[36m(func pid=184353)[0m f1_weighted: 0.21875858849598165
[2m[36m(func pid=184353)[0m f1_per_class: [0.192, 0.374, 0.815, 0.378, 0.086, 0.031, 0.028, 0.307, 0.231, 0.203]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6025 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=183515)[0m top1: 0.3824626865671642
[2m[36m(func pid=183515)[0m top5: 0.8824626865671642
[2m[36m(func pid=183515)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=183515)[0m f1_macro: 0.3384107083130049
[2m[36m(func pid=183515)[0m f1_weighted: 0.39635357879820043
[2m[36m(func pid=183515)[0m f1_per_class: [0.556, 0.457, 0.511, 0.572, 0.067, 0.147, 0.343, 0.249, 0.205, 0.28]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.28078358208955223
[2m[36m(func pid=183140)[0m top5: 0.8549440298507462
[2m[36m(func pid=183140)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=183140)[0m f1_macro: 0.2975282264042068
[2m[36m(func pid=183140)[0m f1_weighted: 0.2828640641729026
[2m[36m(func pid=183140)[0m f1_per_class: [0.4, 0.358, 0.688, 0.449, 0.092, 0.141, 0.148, 0.222, 0.211, 0.268]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.5446 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:49:36 (running for 00:06:29.03)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.808 |      0.298 |                   66 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.338 |                   67 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.404 |                   65 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.264 |                   64 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4239738805970149
[2m[36m(func pid=183934)[0m top5: 0.9076492537313433
[2m[36m(func pid=183934)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=183934)[0m f1_macro: 0.40358484565904096
[2m[36m(func pid=183934)[0m f1_weighted: 0.4424807838735295
[2m[36m(func pid=183934)[0m f1_per_class: [0.596, 0.472, 0.71, 0.497, 0.108, 0.322, 0.487, 0.19, 0.22, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0120 | Steps: 2 | Val loss: 2.4282 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5834 | Steps: 2 | Val loss: 1.8742 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=184353)[0m top1: 0.25652985074626866
[2m[36m(func pid=184353)[0m top5: 0.6128731343283582
[2m[36m(func pid=184353)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=184353)[0m f1_macro: 0.2666493562000539
[2m[36m(func pid=184353)[0m f1_weighted: 0.2320955485854753
[2m[36m(func pid=184353)[0m f1_per_class: [0.185, 0.379, 0.815, 0.423, 0.085, 0.024, 0.031, 0.313, 0.225, 0.189]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6590 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183515)[0m top1: 0.38992537313432835
[2m[36m(func pid=183515)[0m top5: 0.8885261194029851
[2m[36m(func pid=183515)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=183515)[0m f1_macro: 0.3442947463544679
[2m[36m(func pid=183515)[0m f1_weighted: 0.4051205437698937
[2m[36m(func pid=183515)[0m f1_per_class: [0.541, 0.459, 0.533, 0.575, 0.07, 0.159, 0.363, 0.252, 0.205, 0.286]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.28638059701492535
[2m[36m(func pid=183140)[0m top5: 0.8596082089552238
[2m[36m(func pid=183140)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=183140)[0m f1_macro: 0.3018376819147336
[2m[36m(func pid=183140)[0m f1_weighted: 0.28992478085308415
[2m[36m(func pid=183140)[0m f1_per_class: [0.407, 0.361, 0.688, 0.451, 0.091, 0.145, 0.164, 0.226, 0.213, 0.273]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:49:41 (running for 00:06:34.12)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.583 |      0.302 |                   67 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.012 |      0.344 |                   68 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   66 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.267 |                   65 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4239738805970149
[2m[36m(func pid=183934)[0m top5: 0.9071828358208955
[2m[36m(func pid=183934)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=183934)[0m f1_macro: 0.40258484079523055
[2m[36m(func pid=183934)[0m f1_weighted: 0.4426163916839569
[2m[36m(func pid=183934)[0m f1_per_class: [0.579, 0.48, 0.71, 0.496, 0.106, 0.315, 0.487, 0.197, 0.216, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.0460 | Steps: 2 | Val loss: 121.6671 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0046 | Steps: 2 | Val loss: 2.4398 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7201 | Steps: 2 | Val loss: 1.8618 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=184353)[0m top1: 0.25886194029850745
[2m[36m(func pid=184353)[0m top5: 0.6049440298507462
[2m[36m(func pid=184353)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=184353)[0m f1_macro: 0.2723052973443226
[2m[36m(func pid=184353)[0m f1_weighted: 0.2371466839583272
[2m[36m(func pid=184353)[0m f1_per_class: [0.189, 0.374, 0.846, 0.439, 0.082, 0.024, 0.033, 0.3, 0.255, 0.18]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.7466 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=183515)[0m top1: 0.38992537313432835
[2m[36m(func pid=183515)[0m top5: 0.8899253731343284
[2m[36m(func pid=183515)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=183515)[0m f1_macro: 0.3403459417258017
[2m[36m(func pid=183515)[0m f1_weighted: 0.40547004013404275
[2m[36m(func pid=183515)[0m f1_per_class: [0.545, 0.456, 0.511, 0.574, 0.069, 0.159, 0.37, 0.244, 0.187, 0.288]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.29197761194029853
[2m[36m(func pid=183140)[0m top5: 0.8596082089552238
[2m[36m(func pid=183140)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=183140)[0m f1_macro: 0.3118023440703616
[2m[36m(func pid=183140)[0m f1_weighted: 0.30095151115842844
[2m[36m(func pid=183140)[0m f1_per_class: [0.424, 0.37, 0.733, 0.439, 0.086, 0.147, 0.204, 0.226, 0.224, 0.265]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:49:47 (running for 00:06:39.38)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.72  |      0.312 |                   68 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.005 |      0.34  |                   69 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.405 |                   67 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  1.046 |      0.272 |                   66 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4239738805970149
[2m[36m(func pid=183934)[0m top5: 0.9029850746268657
[2m[36m(func pid=183934)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=183934)[0m f1_macro: 0.40516806011387246
[2m[36m(func pid=183934)[0m f1_weighted: 0.44294710560126505
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.48, 0.733, 0.499, 0.099, 0.32, 0.482, 0.196, 0.225, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 121.1724 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0029 | Steps: 2 | Val loss: 2.4421 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6338 | Steps: 2 | Val loss: 1.8469 | Batch size: 32 | lr: 0.0001 | Duration: 2.67s
[2m[36m(func pid=184353)[0m top1: 0.26259328358208955
[2m[36m(func pid=184353)[0m top5: 0.6049440298507462
[2m[36m(func pid=184353)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=184353)[0m f1_macro: 0.2726563929386628
[2m[36m(func pid=184353)[0m f1_weighted: 0.24281408144683303
[2m[36m(func pid=184353)[0m f1_per_class: [0.19, 0.375, 0.846, 0.452, 0.08, 0.024, 0.039, 0.315, 0.241, 0.165]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.7192 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=183515)[0m top1: 0.39925373134328357
[2m[36m(func pid=183515)[0m top5: 0.8899253731343284
[2m[36m(func pid=183515)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=183515)[0m f1_macro: 0.34711166939238913
[2m[36m(func pid=183515)[0m f1_weighted: 0.4120874904299314
[2m[36m(func pid=183515)[0m f1_per_class: [0.537, 0.47, 0.533, 0.582, 0.071, 0.155, 0.374, 0.247, 0.211, 0.29]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.29850746268656714
[2m[36m(func pid=183140)[0m top5: 0.8619402985074627
[2m[36m(func pid=183140)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=183140)[0m f1_macro: 0.3116500626244953
[2m[36m(func pid=183140)[0m f1_weighted: 0.31038584083879944
[2m[36m(func pid=183140)[0m f1_per_class: [0.429, 0.372, 0.688, 0.444, 0.085, 0.159, 0.225, 0.232, 0.213, 0.27]
[2m[36m(func pid=183140)[0m 
== Status ==
Current time: 2024-01-07 00:49:52 (running for 00:06:44.49)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.634 |      0.312 |                   69 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.347 |                   70 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   68 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.273 |                   67 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.425839552238806
[2m[36m(func pid=183934)[0m top5: 0.9048507462686567
[2m[36m(func pid=183934)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=183934)[0m f1_macro: 0.40328525442001784
[2m[36m(func pid=183934)[0m f1_weighted: 0.44401143836992435
[2m[36m(func pid=183934)[0m f1_per_class: [0.579, 0.479, 0.733, 0.499, 0.103, 0.319, 0.491, 0.174, 0.222, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0050 | Steps: 2 | Val loss: 120.4049 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0041 | Steps: 2 | Val loss: 2.4395 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5698 | Steps: 2 | Val loss: 1.8331 | Batch size: 32 | lr: 0.0001 | Duration: 2.65s
[2m[36m(func pid=184353)[0m top1: 0.2681902985074627
[2m[36m(func pid=184353)[0m top5: 0.6077425373134329
[2m[36m(func pid=184353)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=184353)[0m f1_macro: 0.2741250653946538
[2m[36m(func pid=184353)[0m f1_weighted: 0.24669355724707248
[2m[36m(func pid=184353)[0m f1_per_class: [0.201, 0.366, 0.846, 0.47, 0.089, 0.016, 0.042, 0.324, 0.236, 0.151]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.7753 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=183515)[0m top1: 0.39552238805970147
[2m[36m(func pid=183515)[0m top5: 0.8903917910447762
[2m[36m(func pid=183515)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=183515)[0m f1_macro: 0.34440500140457053
[2m[36m(func pid=183515)[0m f1_weighted: 0.41125038312694595
[2m[36m(func pid=183515)[0m f1_per_class: [0.541, 0.461, 0.522, 0.576, 0.067, 0.154, 0.386, 0.241, 0.196, 0.301]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.3069029850746269
[2m[36m(func pid=183140)[0m top5: 0.8708022388059702
[2m[36m(func pid=183140)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=183140)[0m f1_macro: 0.31307658871304433
[2m[36m(func pid=183140)[0m f1_weighted: 0.3253191223475913
[2m[36m(func pid=183140)[0m f1_per_class: [0.41, 0.371, 0.667, 0.445, 0.083, 0.17, 0.274, 0.221, 0.217, 0.273]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7723 | Steps: 2 | Val loss: 121.1440 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:49:57 (running for 00:06:49.85)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.57  |      0.313 |                   70 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.344 |                   71 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   69 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.005 |      0.274 |                   68 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4230410447761194
[2m[36m(func pid=183934)[0m top5: 0.9057835820895522
[2m[36m(func pid=183934)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.40199084922001693
[2m[36m(func pid=183934)[0m f1_weighted: 0.4403782791926138
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.481, 0.71, 0.503, 0.104, 0.323, 0.474, 0.165, 0.221, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0042 | Steps: 2 | Val loss: 2.4207 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5691 | Steps: 2 | Val loss: 1.8181 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=184353)[0m top1: 0.26725746268656714
[2m[36m(func pid=184353)[0m top5: 0.6124067164179104
[2m[36m(func pid=184353)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=184353)[0m f1_macro: 0.27239764199700445
[2m[36m(func pid=184353)[0m f1_weighted: 0.2468480732753974
[2m[36m(func pid=184353)[0m f1_per_class: [0.206, 0.355, 0.846, 0.476, 0.089, 0.016, 0.045, 0.317, 0.232, 0.142]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8144 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=183515)[0m top1: 0.40345149253731344
[2m[36m(func pid=183515)[0m top5: 0.8913246268656716
[2m[36m(func pid=183515)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=183515)[0m f1_macro: 0.3513634357925861
[2m[36m(func pid=183515)[0m f1_weighted: 0.4197399775013746
[2m[36m(func pid=183515)[0m f1_per_class: [0.55, 0.465, 0.533, 0.58, 0.068, 0.15, 0.407, 0.245, 0.208, 0.308]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.3199626865671642
[2m[36m(func pid=183140)[0m top5: 0.8731343283582089
[2m[36m(func pid=183140)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=183140)[0m f1_macro: 0.32134343122011294
[2m[36m(func pid=183140)[0m f1_weighted: 0.3432481204684465
[2m[36m(func pid=183140)[0m f1_per_class: [0.42, 0.363, 0.667, 0.445, 0.085, 0.19, 0.33, 0.226, 0.21, 0.278]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 119.4080 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 00:50:02 (running for 00:06:55.06)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.569 |      0.321 |                   71 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.351 |                   72 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   70 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.772 |      0.272 |                   69 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42490671641791045
[2m[36m(func pid=183934)[0m top5: 0.9067164179104478
[2m[36m(func pid=183934)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=183934)[0m f1_macro: 0.40251556514826137
[2m[36m(func pid=183934)[0m f1_weighted: 0.4425270729716049
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.476, 0.71, 0.506, 0.109, 0.328, 0.476, 0.182, 0.22, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6396 | Steps: 2 | Val loss: 1.8107 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0163 | Steps: 2 | Val loss: 2.3840 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=184353)[0m top1: 0.27472014925373134
[2m[36m(func pid=184353)[0m top5: 0.6184701492537313
[2m[36m(func pid=184353)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=184353)[0m f1_macro: 0.27560678228449326
[2m[36m(func pid=184353)[0m f1_weighted: 0.25478144711962486
[2m[36m(func pid=184353)[0m f1_per_class: [0.225, 0.361, 0.846, 0.485, 0.091, 0.016, 0.06, 0.323, 0.216, 0.135]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.8912 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183515)[0m top1: 0.41651119402985076
[2m[36m(func pid=183515)[0m top5: 0.8941231343283582
[2m[36m(func pid=183515)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=183515)[0m f1_macro: 0.3600548639227104
[2m[36m(func pid=183515)[0m f1_weighted: 0.43474475687338565
[2m[36m(func pid=183515)[0m f1_per_class: [0.564, 0.472, 0.533, 0.581, 0.071, 0.169, 0.441, 0.259, 0.206, 0.303]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.3246268656716418
[2m[36m(func pid=183140)[0m top5: 0.8736007462686567
[2m[36m(func pid=183140)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=183140)[0m f1_macro: 0.3200857750691754
[2m[36m(func pid=183140)[0m f1_weighted: 0.3517150133414098
[2m[36m(func pid=183140)[0m f1_per_class: [0.407, 0.362, 0.667, 0.443, 0.078, 0.183, 0.368, 0.205, 0.208, 0.278]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0749 | Steps: 2 | Val loss: 119.0977 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:50:07 (running for 00:07:00.22)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.64  |      0.32  |                   72 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.016 |      0.36  |                   73 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   71 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.276 |                   70 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42350746268656714
[2m[36m(func pid=183934)[0m top5: 0.9067164179104478
[2m[36m(func pid=183934)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=183934)[0m f1_macro: 0.4017940279396658
[2m[36m(func pid=183934)[0m f1_weighted: 0.4413525660049391
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.473, 0.71, 0.5, 0.091, 0.326, 0.48, 0.18, 0.226, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5473 | Steps: 2 | Val loss: 1.7982 | Batch size: 32 | lr: 0.0001 | Duration: 2.59s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.4201 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=184353)[0m top1: 0.27798507462686567
[2m[36m(func pid=184353)[0m top5: 0.6277985074626866
[2m[36m(func pid=184353)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=184353)[0m f1_macro: 0.27616538299792054
[2m[36m(func pid=184353)[0m f1_weighted: 0.25565373687168175
[2m[36m(func pid=184353)[0m f1_per_class: [0.234, 0.362, 0.846, 0.488, 0.092, 0.016, 0.06, 0.319, 0.211, 0.135]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.9831 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183140)[0m top1: 0.3316231343283582
[2m[36m(func pid=183140)[0m top5: 0.8759328358208955
[2m[36m(func pid=183140)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=183140)[0m f1_macro: 0.32229148465148383
[2m[36m(func pid=183140)[0m f1_weighted: 0.35908867909447095
[2m[36m(func pid=183140)[0m f1_per_class: [0.405, 0.362, 0.667, 0.434, 0.078, 0.179, 0.401, 0.222, 0.202, 0.273]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.41511194029850745
[2m[36m(func pid=183515)[0m top5: 0.8931902985074627
[2m[36m(func pid=183515)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=183515)[0m f1_macro: 0.35860916976832374
[2m[36m(func pid=183515)[0m f1_weighted: 0.43244481839690785
[2m[36m(func pid=183515)[0m f1_per_class: [0.564, 0.476, 0.522, 0.575, 0.07, 0.16, 0.439, 0.261, 0.209, 0.31]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:50:13 (running for 00:07:05.36)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.547 |      0.322 |                   73 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.359 |                   74 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   72 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.075 |      0.276 |                   71 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4216417910447761
[2m[36m(func pid=183934)[0m top5: 0.90625
[2m[36m(func pid=183934)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=183934)[0m f1_macro: 0.3995746534463418
[2m[36m(func pid=183934)[0m f1_weighted: 0.4401184694735596
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.469, 0.71, 0.505, 0.091, 0.328, 0.474, 0.176, 0.223, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 118.1429 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4858 | Steps: 2 | Val loss: 1.7868 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0021 | Steps: 2 | Val loss: 2.4360 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=184353)[0m top1: 0.2835820895522388
[2m[36m(func pid=184353)[0m top5: 0.6352611940298507
[2m[36m(func pid=184353)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=184353)[0m f1_macro: 0.2814138995226473
[2m[36m(func pid=184353)[0m f1_weighted: 0.2601238116751711
[2m[36m(func pid=184353)[0m f1_per_class: [0.252, 0.36, 0.88, 0.497, 0.092, 0.016, 0.068, 0.306, 0.209, 0.133]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.0827 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=183140)[0m top1: 0.34375
[2m[36m(func pid=183140)[0m top5: 0.8824626865671642
[2m[36m(func pid=183140)[0m f1_micro: 0.34375
[2m[36m(func pid=183140)[0m f1_macro: 0.32656859056338355
[2m[36m(func pid=183140)[0m f1_weighted: 0.37177580169626334
[2m[36m(func pid=183140)[0m f1_per_class: [0.4, 0.354, 0.667, 0.445, 0.075, 0.182, 0.434, 0.24, 0.205, 0.264]
[2m[36m(func pid=183140)[0m 
[2m[36m(func pid=183515)[0m top1: 0.41744402985074625
[2m[36m(func pid=183515)[0m top5: 0.8927238805970149
[2m[36m(func pid=183515)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=183515)[0m f1_macro: 0.35521999187523334
[2m[36m(func pid=183515)[0m f1_weighted: 0.43479958594780926
[2m[36m(func pid=183515)[0m f1_per_class: [0.559, 0.472, 0.5, 0.573, 0.071, 0.157, 0.454, 0.269, 0.192, 0.305]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:50:18 (running for 00:07:10.53)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | RUNNING  | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.486 |      0.327 |                   74 |
| train_57e67_00001 | RUNNING  | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.355 |                   75 |
| train_57e67_00002 | RUNNING  | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.399 |                   73 |
| train_57e67_00003 | RUNNING  | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.281 |                   72 |
| train_57e67_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.41651119402985076
[2m[36m(func pid=183934)[0m top5: 0.9057835820895522
[2m[36m(func pid=183934)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=183934)[0m f1_macro: 0.39942539488079115
[2m[36m(func pid=183934)[0m f1_weighted: 0.4340733109140979
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.47, 0.71, 0.491, 0.088, 0.323, 0.467, 0.175, 0.236, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 116.6365 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0056 | Steps: 2 | Val loss: 2.4508 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=183140)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4618 | Steps: 2 | Val loss: 1.7875 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=184353)[0m top1: 0.2891791044776119
[2m[36m(func pid=184353)[0m top5: 0.6431902985074627
[2m[36m(func pid=184353)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=184353)[0m f1_macro: 0.2845832642744394
[2m[36m(func pid=184353)[0m f1_weighted: 0.26434029600261005
[2m[36m(func pid=184353)[0m f1_per_class: [0.262, 0.36, 0.88, 0.501, 0.093, 0.016, 0.077, 0.315, 0.206, 0.137]
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1192 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=183515)[0m top1: 0.4193097014925373
[2m[36m(func pid=183515)[0m top5: 0.8936567164179104
[2m[36m(func pid=183515)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=183515)[0m f1_macro: 0.3573778690092455
[2m[36m(func pid=183515)[0m f1_weighted: 0.4357861303214717
[2m[36m(func pid=183515)[0m f1_per_class: [0.559, 0.475, 0.5, 0.573, 0.08, 0.163, 0.452, 0.277, 0.19, 0.305]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183140)[0m top1: 0.3460820895522388
[2m[36m(func pid=183140)[0m top5: 0.8782649253731343
[2m[36m(func pid=183140)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=183140)[0m f1_macro: 0.3274300077434273
[2m[36m(func pid=183140)[0m f1_weighted: 0.3732993326918604
[2m[36m(func pid=183140)[0m f1_per_class: [0.391, 0.358, 0.667, 0.439, 0.088, 0.181, 0.446, 0.225, 0.2, 0.28]
[2m[36m(func pid=183934)[0m top1: 0.41744402985074625
[2m[36m(func pid=183934)[0m top5: 0.9039179104477612
[2m[36m(func pid=183934)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=183934)[0m f1_macro: 0.39927626261475113
[2m[36m(func pid=183934)[0m f1_weighted: 0.4351486247917438
[2m[36m(func pid=183934)[0m f1_per_class: [0.596, 0.464, 0.688, 0.487, 0.089, 0.328, 0.474, 0.183, 0.234, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1560 | Steps: 2 | Val loss: 115.5818 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0121 | Steps: 2 | Val loss: 2.4592 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=184353)[0m top1: 0.29617537313432835
[2m[36m(func pid=184353)[0m top5: 0.659981343283582
[2m[36m(func pid=184353)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=184353)[0m f1_macro: 0.2893804985240412
[2m[36m(func pid=184353)[0m f1_weighted: 0.26817430436646184
[2m[36m(func pid=184353)[0m f1_per_class: [0.286, 0.364, 0.88, 0.504, 0.09, 0.016, 0.082, 0.318, 0.209, 0.146]
[2m[36m(func pid=183515)[0m top1: 0.42117537313432835
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=183515)[0m f1_macro: 0.35589332510677624
[2m[36m(func pid=183515)[0m f1_weighted: 0.43750834810310185
[2m[36m(func pid=183515)[0m f1_per_class: [0.555, 0.469, 0.5, 0.574, 0.081, 0.162, 0.462, 0.272, 0.187, 0.297]
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0002 | Steps: 2 | Val loss: 7.1556 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=183934)[0m top1: 0.417910447761194
[2m[36m(func pid=183934)[0m top5: 0.9034514925373134
[2m[36m(func pid=183934)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.40090806897327946
[2m[36m(func pid=183934)[0m f1_weighted: 0.4352594399896372
[2m[36m(func pid=183934)[0m f1_per_class: [0.596, 0.471, 0.688, 0.486, 0.101, 0.326, 0.471, 0.184, 0.237, 0.448]
== Status ==
Current time: 2024-01-07 00:50:23 (running for 00:07:15.62)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.006 |      0.357 |                   76 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.399 |                   74 |
| train_57e67_00003 | RUNNING    | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.285 |                   73 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 00:50:30 (running for 00:07:22.41)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.012 |      0.356 |                   77 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.399 |                   74 |
| train_57e67_00003 | RUNNING    | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0     |      0.285 |                   73 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m 
[2m[36m(func pid=12986)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=12986)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=12986)[0m Configuration completed!
[2m[36m(func pid=12986)[0m New optimizer parameters:
[2m[36m(func pid=12986)[0m SGD (
[2m[36m(func pid=12986)[0m Parameter Group 0
[2m[36m(func pid=12986)[0m     dampening: 0
[2m[36m(func pid=12986)[0m     differentiable: False
[2m[36m(func pid=12986)[0m     foreach: None
[2m[36m(func pid=12986)[0m     lr: 0.0001
[2m[36m(func pid=12986)[0m     maximize: False
[2m[36m(func pid=12986)[0m     momentum: 0.9
[2m[36m(func pid=12986)[0m     nesterov: False
[2m[36m(func pid=12986)[0m     weight_decay: 0
[2m[36m(func pid=12986)[0m )
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.4578 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=184353)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3685 | Steps: 2 | Val loss: 114.6493 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1317 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1403 | Steps: 2 | Val loss: 2.5102 | Batch size: 32 | lr: 0.0001 | Duration: 4.54s
== Status ==
Current time: 2024-01-07 00:50:35 (running for 00:07:27.44)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.378
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.012 |      0.356 |                   77 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   75 |
| train_57e67_00003 | RUNNING    | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.156 |      0.289 |                   74 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |        |            |                      |
| train_57e67_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.4221082089552239
[2m[36m(func pid=183515)[0m top5: 0.8969216417910447
[2m[36m(func pid=183515)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=183515)[0m f1_macro: 0.35850778210754497
[2m[36m(func pid=183515)[0m f1_weighted: 0.4393138610640379
[2m[36m(func pid=183515)[0m f1_per_class: [0.545, 0.47, 0.511, 0.573, 0.081, 0.172, 0.465, 0.266, 0.194, 0.308]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4253731343283582
[2m[36m(func pid=183934)[0m top5: 0.9043843283582089
[2m[36m(func pid=183934)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=183934)[0m f1_macro: 0.40361900539596895
[2m[36m(func pid=183934)[0m f1_weighted: 0.4440236960748792
[2m[36m(func pid=183934)[0m f1_per_class: [0.607, 0.47, 0.688, 0.49, 0.103, 0.324, 0.498, 0.196, 0.228, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=184353)[0m top1: 0.30130597014925375
[2m[36m(func pid=184353)[0m top5: 0.6665111940298507
[2m[36m(func pid=184353)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=184353)[0m f1_macro: 0.2901610641270604
[2m[36m(func pid=184353)[0m f1_weighted: 0.2702967894979071
[2m[36m(func pid=184353)[0m f1_per_class: [0.299, 0.353, 0.846, 0.509, 0.091, 0.024, 0.085, 0.327, 0.214, 0.155]
[2m[36m(func pid=12986)[0m top1: 0.06856343283582089
[2m[36m(func pid=12986)[0m top5: 0.4869402985074627
[2m[36m(func pid=12986)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=12986)[0m f1_macro: 0.04039230789344547
[2m[36m(func pid=12986)[0m f1_weighted: 0.04031851208521882
[2m[36m(func pid=12986)[0m f1_per_class: [0.117, 0.01, 0.0, 0.096, 0.0, 0.019, 0.0, 0.106, 0.022, 0.034]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0055 | Steps: 2 | Val loss: 2.4659 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1631 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1716 | Steps: 2 | Val loss: 2.5341 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=183515)[0m top1: 0.4230410447761194
[2m[36m(func pid=183515)[0m top5: 0.8959888059701493
[2m[36m(func pid=183515)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=183515)[0m f1_macro: 0.3607341264162202
[2m[36m(func pid=183515)[0m f1_weighted: 0.44090354480435673
[2m[36m(func pid=183515)[0m f1_per_class: [0.559, 0.467, 0.511, 0.573, 0.08, 0.175, 0.469, 0.272, 0.194, 0.308]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.4244402985074627
[2m[36m(func pid=183934)[0m top5: 0.9057835820895522
[2m[36m(func pid=183934)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=183934)[0m f1_macro: 0.4043107641580234
[2m[36m(func pid=183934)[0m f1_weighted: 0.44321186199646434
[2m[36m(func pid=183934)[0m f1_per_class: [0.596, 0.467, 0.688, 0.492, 0.117, 0.325, 0.493, 0.201, 0.232, 0.433]
[2m[36m(func pid=12986)[0m top1: 0.0625
[2m[36m(func pid=12986)[0m top5: 0.48134328358208955
[2m[36m(func pid=12986)[0m f1_micro: 0.0625
[2m[36m(func pid=12986)[0m f1_macro: 0.030680354002932016
[2m[36m(func pid=12986)[0m f1_weighted: 0.03372796234674975
[2m[36m(func pid=12986)[0m f1_per_class: [0.026, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.102, 0.023, 0.048]
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.4898 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 00:50:40 (running for 00:07:32.78)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.006 |      0.361 |                   79 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.404 |                   76 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.14  |      0.04  |                    1 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=13733)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=13733)[0m Configuration completed!
[2m[36m(func pid=13733)[0m New optimizer parameters:
[2m[36m(func pid=13733)[0m SGD (
[2m[36m(func pid=13733)[0m Parameter Group 0
[2m[36m(func pid=13733)[0m     dampening: 0
[2m[36m(func pid=13733)[0m     differentiable: False
[2m[36m(func pid=13733)[0m     foreach: None
[2m[36m(func pid=13733)[0m     lr: 0.001
[2m[36m(func pid=13733)[0m     maximize: False
[2m[36m(func pid=13733)[0m     momentum: 0.9
[2m[36m(func pid=13733)[0m     nesterov: False
[2m[36m(func pid=13733)[0m     weight_decay: 0
[2m[36m(func pid=13733)[0m )
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:50:46 (running for 00:07:38.31)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.358 |                   80 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.404 |                   77 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.172 |      0.031 |                    2 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |        |            |                      |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.4197761194029851
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183515)[0m f1_macro: 0.35836849880430927
[2m[36m(func pid=183515)[0m f1_weighted: 0.4369682856258254
[2m[36m(func pid=183515)[0m f1_per_class: [0.559, 0.467, 0.511, 0.572, 0.078, 0.173, 0.458, 0.268, 0.193, 0.304]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.2721 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1859 | Steps: 2 | Val loss: 2.5523 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.4906 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1131 | Steps: 2 | Val loss: 2.5001 | Batch size: 32 | lr: 0.001 | Duration: 4.69s
[2m[36m(func pid=183934)[0m top1: 0.41884328358208955
[2m[36m(func pid=183934)[0m top5: 0.9020522388059702
[2m[36m(func pid=183934)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=183934)[0m f1_macro: 0.40123033158679783
[2m[36m(func pid=183934)[0m f1_weighted: 0.4370755078673285
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.469, 0.688, 0.483, 0.116, 0.318, 0.483, 0.193, 0.238, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.061567164179104475
[2m[36m(func pid=12986)[0m top5: 0.4701492537313433
[2m[36m(func pid=12986)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=12986)[0m f1_macro: 0.030952098709105404
[2m[36m(func pid=12986)[0m f1_weighted: 0.03409084632787365
[2m[36m(func pid=12986)[0m f1_per_class: [0.055, 0.015, 0.0, 0.079, 0.0, 0.02, 0.0, 0.1, 0.0, 0.042]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4207089552238806
[2m[36m(func pid=183515)[0m top5: 0.8973880597014925
[2m[36m(func pid=183515)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=183515)[0m f1_macro: 0.36007513384441275
[2m[36m(func pid=183515)[0m f1_weighted: 0.43873203698968943
[2m[36m(func pid=183515)[0m f1_per_class: [0.555, 0.468, 0.511, 0.569, 0.077, 0.161, 0.47, 0.269, 0.201, 0.32]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:50:51 (running for 00:07:43.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.36  |                   81 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   78 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.186 |      0.031 |                    3 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  3.113 |      0.042 |                    1 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.06623134328358209
[2m[36m(func pid=13733)[0m top5: 0.4818097014925373
[2m[36m(func pid=13733)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=13733)[0m f1_macro: 0.04182488756678647
[2m[36m(func pid=13733)[0m f1_weighted: 0.04086114984688744
[2m[36m(func pid=13733)[0m f1_per_class: [0.137, 0.01, 0.0, 0.098, 0.0, 0.017, 0.0, 0.102, 0.021, 0.033]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0001 | Steps: 2 | Val loss: 7.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1045 | Steps: 2 | Val loss: 2.5632 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.5118 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1063 | Steps: 2 | Val loss: 2.4746 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=183934)[0m top1: 0.42257462686567165
[2m[36m(func pid=183934)[0m top5: 0.9029850746268657
[2m[36m(func pid=183934)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=183934)[0m f1_macro: 0.40294663684997933
[2m[36m(func pid=183934)[0m f1_weighted: 0.4410352466141728
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.471, 0.688, 0.494, 0.114, 0.318, 0.485, 0.193, 0.235, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06110074626865672
[2m[36m(func pid=12986)[0m top5: 0.470615671641791
[2m[36m(func pid=12986)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=12986)[0m f1_macro: 0.03230304689330431
[2m[36m(func pid=12986)[0m f1_weighted: 0.038020721390401274
[2m[36m(func pid=12986)[0m f1_per_class: [0.049, 0.032, 0.0, 0.083, 0.0, 0.019, 0.0, 0.096, 0.0, 0.043]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.42024253731343286
[2m[36m(func pid=183515)[0m top5: 0.898320895522388
[2m[36m(func pid=183515)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=183515)[0m f1_macro: 0.36076362672631346
[2m[36m(func pid=183515)[0m f1_weighted: 0.4382243050747466
[2m[36m(func pid=183515)[0m f1_per_class: [0.559, 0.476, 0.511, 0.563, 0.076, 0.165, 0.467, 0.271, 0.2, 0.319]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:50:56 (running for 00:07:48.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.361 |                   82 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   79 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.104 |      0.032 |                    4 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  3.106 |      0.046 |                    2 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.06389925373134328
[2m[36m(func pid=13733)[0m top5: 0.4626865671641791
[2m[36m(func pid=13733)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=13733)[0m f1_macro: 0.04634166959635454
[2m[36m(func pid=13733)[0m f1_weighted: 0.04778524886255871
[2m[36m(func pid=13733)[0m f1_per_class: [0.098, 0.047, 0.0, 0.098, 0.0, 0.022, 0.0, 0.093, 0.061, 0.045]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.3461 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0561 | Steps: 2 | Val loss: 2.5664 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0050 | Steps: 2 | Val loss: 2.5091 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0198 | Steps: 2 | Val loss: 2.4625 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=183934)[0m top1: 0.42117537313432835
[2m[36m(func pid=183934)[0m top5: 0.9015858208955224
[2m[36m(func pid=183934)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=183934)[0m f1_macro: 0.4019092642077619
[2m[36m(func pid=183934)[0m f1_weighted: 0.43890988261870056
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.469, 0.688, 0.489, 0.128, 0.322, 0.484, 0.187, 0.235, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06203358208955224
[2m[36m(func pid=12986)[0m top5: 0.46548507462686567
[2m[36m(func pid=12986)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=12986)[0m f1_macro: 0.03722177017570985
[2m[36m(func pid=12986)[0m f1_weighted: 0.04267135350014023
[2m[36m(func pid=12986)[0m f1_per_class: [0.047, 0.051, 0.0, 0.088, 0.0, 0.013, 0.0, 0.092, 0.025, 0.055]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4216417910447761
[2m[36m(func pid=183515)[0m top5: 0.8987873134328358
[2m[36m(func pid=183515)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=183515)[0m f1_macro: 0.36127558644621294
[2m[36m(func pid=183515)[0m f1_weighted: 0.44045546075183806
[2m[36m(func pid=183515)[0m f1_per_class: [0.564, 0.476, 0.5, 0.563, 0.069, 0.173, 0.472, 0.266, 0.199, 0.331]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:51:01 (running for 00:07:53.57)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.005 |      0.361 |                   83 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   80 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.056 |      0.037 |                    5 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  3.02  |      0.052 |                    3 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.06623134328358209
[2m[36m(func pid=13733)[0m top5: 0.44402985074626866
[2m[36m(func pid=13733)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=13733)[0m f1_macro: 0.05200697427191574
[2m[36m(func pid=13733)[0m f1_weighted: 0.060566880270810745
[2m[36m(func pid=13733)[0m f1_per_class: [0.105, 0.111, 0.0, 0.097, 0.0, 0.024, 0.009, 0.085, 0.036, 0.052]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.4378 | Batch size: 32 | lr: 0.01 | Duration: 2.65s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0583 | Steps: 2 | Val loss: 2.5650 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.5286 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8947 | Steps: 2 | Val loss: 2.4482 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=183934)[0m top1: 0.4197761194029851
[2m[36m(func pid=183934)[0m top5: 0.9001865671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183934)[0m f1_macro: 0.39987891184622104
[2m[36m(func pid=183934)[0m f1_weighted: 0.4382154000889998
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.474, 0.688, 0.486, 0.122, 0.32, 0.482, 0.194, 0.234, 0.413]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06389925373134328
[2m[36m(func pid=12986)[0m top5: 0.458955223880597
[2m[36m(func pid=12986)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=12986)[0m f1_macro: 0.038524413302095724
[2m[36m(func pid=12986)[0m f1_weighted: 0.04717121114000158
[2m[36m(func pid=12986)[0m f1_per_class: [0.043, 0.072, 0.0, 0.092, 0.0, 0.013, 0.0, 0.094, 0.025, 0.047]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4193097014925373
[2m[36m(func pid=183515)[0m top5: 0.8987873134328358
[2m[36m(func pid=183515)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=183515)[0m f1_macro: 0.3606588543501319
[2m[36m(func pid=183515)[0m f1_weighted: 0.43755396952365355
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.473, 0.5, 0.556, 0.069, 0.165, 0.471, 0.276, 0.199, 0.325]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:51:06 (running for 00:07:58.62)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.361 |                   84 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   81 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.058 |      0.039 |                    6 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.895 |      0.054 |                    4 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.06203358208955224
[2m[36m(func pid=13733)[0m top5: 0.4193097014925373
[2m[36m(func pid=13733)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=13733)[0m f1_macro: 0.054185233061729746
[2m[36m(func pid=13733)[0m f1_weighted: 0.06113473082802977
[2m[36m(func pid=13733)[0m f1_per_class: [0.099, 0.13, 0.0, 0.074, 0.0, 0.035, 0.015, 0.074, 0.09, 0.024]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.4871 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0777 | Steps: 2 | Val loss: 2.5559 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0035 | Steps: 2 | Val loss: 2.5434 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7577 | Steps: 2 | Val loss: 2.4386 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=183934)[0m top1: 0.417910447761194
[2m[36m(func pid=183934)[0m top5: 0.8997201492537313
[2m[36m(func pid=183934)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.39985397428333813
[2m[36m(func pid=183934)[0m f1_weighted: 0.4364591938070533
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.467, 0.688, 0.489, 0.12, 0.324, 0.477, 0.192, 0.228, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06576492537313433
[2m[36m(func pid=12986)[0m top5: 0.45615671641791045
[2m[36m(func pid=12986)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=12986)[0m f1_macro: 0.04330438406721405
[2m[36m(func pid=12986)[0m f1_weighted: 0.05285762372185946
[2m[36m(func pid=12986)[0m f1_per_class: [0.055, 0.091, 0.0, 0.095, 0.0, 0.019, 0.0, 0.094, 0.049, 0.03]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.417910447761194
[2m[36m(func pid=183515)[0m top5: 0.8973880597014925
[2m[36m(func pid=183515)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183515)[0m f1_macro: 0.36317073901469366
[2m[36m(func pid=183515)[0m f1_weighted: 0.43671738061019444
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.467, 0.511, 0.556, 0.083, 0.168, 0.471, 0.272, 0.199, 0.333]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:51:11 (running for 00:08:03.82)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.363 |                   85 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   82 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.078 |      0.043 |                    7 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.758 |      0.057 |                    5 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.05783582089552239
[2m[36m(func pid=13733)[0m top5: 0.41744402985074625
[2m[36m(func pid=13733)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=13733)[0m f1_macro: 0.05732775480542166
[2m[36m(func pid=13733)[0m f1_weighted: 0.059874881915394786
[2m[36m(func pid=13733)[0m f1_per_class: [0.086, 0.114, 0.028, 0.055, 0.008, 0.044, 0.033, 0.061, 0.123, 0.02]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.4620 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0232 | Steps: 2 | Val loss: 2.5485 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0046 | Steps: 2 | Val loss: 2.5744 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6856 | Steps: 2 | Val loss: 2.4142 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=183934)[0m top1: 0.42257462686567165
[2m[36m(func pid=183934)[0m top5: 0.9011194029850746
[2m[36m(func pid=183934)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=183934)[0m f1_macro: 0.4022694301169851
[2m[36m(func pid=183934)[0m f1_weighted: 0.4424238336242933
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.467, 0.688, 0.497, 0.12, 0.321, 0.488, 0.201, 0.228, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.0648320895522388
[2m[36m(func pid=12986)[0m top5: 0.45475746268656714
[2m[36m(func pid=12986)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=12986)[0m f1_macro: 0.04226483945604741
[2m[36m(func pid=12986)[0m f1_weighted: 0.053853117220269184
[2m[36m(func pid=12986)[0m f1_per_class: [0.047, 0.086, 0.0, 0.103, 0.0, 0.017, 0.0, 0.095, 0.048, 0.027]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.41744402985074625
[2m[36m(func pid=183515)[0m top5: 0.8955223880597015
[2m[36m(func pid=183515)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=183515)[0m f1_macro: 0.36162837384073143
[2m[36m(func pid=183515)[0m f1_weighted: 0.4369417006272931
[2m[36m(func pid=183515)[0m f1_per_class: [0.576, 0.467, 0.511, 0.557, 0.083, 0.164, 0.474, 0.27, 0.198, 0.317]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=13733)[0m top1: 0.06669776119402986
[2m[36m(func pid=13733)[0m top5: 0.449160447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=13733)[0m f1_macro: 0.06325742531318979
[2m[36m(func pid=13733)[0m f1_weighted: 0.07474454924524371
[2m[36m(func pid=13733)[0m f1_per_class: [0.09, 0.111, 0.055, 0.057, 0.011, 0.054, 0.083, 0.049, 0.111, 0.013]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:51:16 (running for 00:08:09.05)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.005 |      0.362 |                   86 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   83 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  3.023 |      0.042 |                    8 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.686 |      0.063 |                    6 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.5135 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9552 | Steps: 2 | Val loss: 2.5392 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0055 | Steps: 2 | Val loss: 2.5774 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6037 | Steps: 2 | Val loss: 2.3909 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=183934)[0m top1: 0.425839552238806
[2m[36m(func pid=183934)[0m top5: 0.9006529850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=183934)[0m f1_macro: 0.4070553992765591
[2m[36m(func pid=183934)[0m f1_weighted: 0.4457575675236899
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.468, 0.71, 0.497, 0.12, 0.325, 0.496, 0.203, 0.228, 0.438]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06763059701492537
[2m[36m(func pid=12986)[0m top5: 0.4510261194029851
[2m[36m(func pid=12986)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=12986)[0m f1_macro: 0.04531760712923942
[2m[36m(func pid=12986)[0m f1_weighted: 0.059165298972290954
[2m[36m(func pid=12986)[0m f1_per_class: [0.054, 0.095, 0.0, 0.117, 0.0, 0.017, 0.0, 0.093, 0.046, 0.032]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4146455223880597
[2m[36m(func pid=183515)[0m top5: 0.8941231343283582
[2m[36m(func pid=183515)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.36138902973429393
[2m[36m(func pid=183515)[0m f1_weighted: 0.43489957336482216
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.465, 0.511, 0.553, 0.082, 0.182, 0.464, 0.269, 0.201, 0.315]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:51:21 (running for 00:08:14.25)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.006 |      0.361 |                   87 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.407 |                   84 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.955 |      0.045 |                    9 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.604 |      0.075 |                    7 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.08488805970149253
[2m[36m(func pid=13733)[0m top5: 0.4780783582089552
[2m[36m(func pid=13733)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=13733)[0m f1_macro: 0.07454372066124816
[2m[36m(func pid=13733)[0m f1_weighted: 0.0937688317726449
[2m[36m(func pid=13733)[0m f1_per_class: [0.094, 0.091, 0.093, 0.04, 0.014, 0.06, 0.173, 0.037, 0.11, 0.034]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0002 | Steps: 2 | Val loss: 7.6068 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9746 | Steps: 2 | Val loss: 2.5329 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0010 | Steps: 2 | Val loss: 2.5869 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4794 | Steps: 2 | Val loss: 2.3508 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=183934)[0m top1: 0.4193097014925373
[2m[36m(func pid=183934)[0m top5: 0.8978544776119403
[2m[36m(func pid=183934)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=183934)[0m f1_macro: 0.40172920080771146
[2m[36m(func pid=183934)[0m f1_weighted: 0.439667978180878
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.468, 0.71, 0.496, 0.104, 0.322, 0.479, 0.2, 0.227, 0.431]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06809701492537314
[2m[36m(func pid=12986)[0m top5: 0.4449626865671642
[2m[36m(func pid=12986)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=12986)[0m f1_macro: 0.04812642025585927
[2m[36m(func pid=12986)[0m f1_weighted: 0.06294806267290114
[2m[36m(func pid=12986)[0m f1_per_class: [0.047, 0.102, 0.0, 0.12, 0.0, 0.032, 0.0, 0.085, 0.063, 0.032]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.41884328358208955
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=183515)[0m f1_macro: 0.36523458074832876
[2m[36m(func pid=183515)[0m f1_weighted: 0.43937610988498643
[2m[36m(func pid=183515)[0m f1_per_class: [0.581, 0.471, 0.522, 0.557, 0.076, 0.191, 0.468, 0.274, 0.2, 0.312]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=13733)[0m top1: 0.10820895522388059
[2m[36m(func pid=13733)[0m top5: 0.5247201492537313
[2m[36m(func pid=13733)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=13733)[0m f1_macro: 0.08566811244939265
[2m[36m(func pid=13733)[0m f1_weighted: 0.11501765552972101
[2m[36m(func pid=13733)[0m f1_per_class: [0.1, 0.094, 0.119, 0.048, 0.016, 0.072, 0.232, 0.013, 0.115, 0.048]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:51:27 (running for 00:08:19.28)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.365 |                   88 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   85 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.975 |      0.048 |                   10 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.479 |      0.086 |                    8 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0002 | Steps: 2 | Val loss: 7.6563 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9481 | Steps: 2 | Val loss: 2.5235 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0384 | Steps: 2 | Val loss: 2.5699 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4329 | Steps: 2 | Val loss: 2.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=183934)[0m top1: 0.4155783582089552
[2m[36m(func pid=183934)[0m top5: 0.9015858208955224
[2m[36m(func pid=183934)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=183934)[0m f1_macro: 0.3967860474799673
[2m[36m(func pid=183934)[0m f1_weighted: 0.43515626782667155
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.465, 0.688, 0.491, 0.105, 0.317, 0.476, 0.178, 0.227, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.06856343283582089
[2m[36m(func pid=12986)[0m top5: 0.44029850746268656
[2m[36m(func pid=12986)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=12986)[0m f1_macro: 0.04802708518446013
[2m[36m(func pid=12986)[0m f1_weighted: 0.06503482331209133
[2m[36m(func pid=12986)[0m f1_per_class: [0.041, 0.108, 0.0, 0.122, 0.0, 0.037, 0.0, 0.085, 0.06, 0.026]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4197761194029851
[2m[36m(func pid=183515)[0m top5: 0.8959888059701493
[2m[36m(func pid=183515)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183515)[0m f1_macro: 0.3636505753573529
[2m[36m(func pid=183515)[0m f1_weighted: 0.43989009346647
[2m[36m(func pid=183515)[0m f1_per_class: [0.569, 0.473, 0.533, 0.552, 0.087, 0.19, 0.478, 0.252, 0.204, 0.299]
[2m[36m(func pid=183515)[0m 
== Status ==
Current time: 2024-01-07 00:51:32 (running for 00:08:24.53)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.038 |      0.364 |                   89 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.397 |                   86 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.948 |      0.048 |                   11 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.433 |      0.098 |                    9 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.13386194029850745
[2m[36m(func pid=13733)[0m top5: 0.5643656716417911
[2m[36m(func pid=13733)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=13733)[0m f1_macro: 0.0982043314205577
[2m[36m(func pid=13733)[0m f1_weighted: 0.13529007892945474
[2m[36m(func pid=13733)[0m f1_per_class: [0.123, 0.101, 0.156, 0.06, 0.018, 0.075, 0.285, 0.0, 0.115, 0.051]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.6740 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0018 | Steps: 2 | Val loss: 2.5960 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9460 | Steps: 2 | Val loss: 2.5154 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3410 | Steps: 2 | Val loss: 2.2638 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=183934)[0m top1: 0.4193097014925373
[2m[36m(func pid=183934)[0m top5: 0.902518656716418
[2m[36m(func pid=183934)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=183934)[0m f1_macro: 0.4015020597384383
[2m[36m(func pid=183934)[0m f1_weighted: 0.4392705920897314
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.468, 0.71, 0.499, 0.105, 0.321, 0.476, 0.195, 0.233, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4197761194029851
[2m[36m(func pid=183515)[0m top5: 0.8959888059701493
[2m[36m(func pid=183515)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183515)[0m f1_macro: 0.3658419505175179
[2m[36m(func pid=183515)[0m f1_weighted: 0.44060491192255186
[2m[36m(func pid=183515)[0m f1_per_class: [0.576, 0.472, 0.533, 0.555, 0.084, 0.19, 0.474, 0.269, 0.198, 0.306]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.0708955223880597
[2m[36m(func pid=12986)[0m top5: 0.439365671641791
[2m[36m(func pid=12986)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=12986)[0m f1_macro: 0.05005220044286118
[2m[36m(func pid=12986)[0m f1_weighted: 0.06812099249883882
[2m[36m(func pid=12986)[0m f1_per_class: [0.046, 0.122, 0.0, 0.123, 0.0, 0.041, 0.0, 0.082, 0.058, 0.028]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:51:37 (running for 00:08:29.70)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.366 |                   90 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   87 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.946 |      0.05  |                   12 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.341 |      0.112 |                   10 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.15764925373134328
[2m[36m(func pid=13733)[0m top5: 0.6105410447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=13733)[0m f1_macro: 0.11247503794948395
[2m[36m(func pid=13733)[0m f1_weighted: 0.1586299072969833
[2m[36m(func pid=13733)[0m f1_per_class: [0.161, 0.119, 0.204, 0.088, 0.012, 0.078, 0.323, 0.0, 0.097, 0.043]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.7595 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.6137 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9452 | Steps: 2 | Val loss: 2.5073 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1843 | Steps: 2 | Val loss: 2.2134 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=183934)[0m top1: 0.4137126865671642
[2m[36m(func pid=183934)[0m top5: 0.8997201492537313
[2m[36m(func pid=183934)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=183934)[0m f1_macro: 0.39970420441138776
[2m[36m(func pid=183934)[0m f1_weighted: 0.4337503743086322
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.467, 0.71, 0.485, 0.104, 0.317, 0.473, 0.191, 0.226, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.417910447761194
[2m[36m(func pid=183515)[0m top5: 0.8955223880597015
[2m[36m(func pid=183515)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183515)[0m f1_macro: 0.36201337101755093
[2m[36m(func pid=183515)[0m f1_weighted: 0.43826008324183496
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.471, 0.511, 0.554, 0.075, 0.183, 0.474, 0.26, 0.197, 0.325]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.0708955223880597
[2m[36m(func pid=12986)[0m top5: 0.43703358208955223
[2m[36m(func pid=12986)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=12986)[0m f1_macro: 0.050646268289691466
[2m[36m(func pid=12986)[0m f1_weighted: 0.06948348639125716
[2m[36m(func pid=12986)[0m f1_per_class: [0.05, 0.134, 0.0, 0.12, 0.0, 0.04, 0.003, 0.077, 0.056, 0.027]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:51:42 (running for 00:08:34.93)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.362 |                   91 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.4   |                   88 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.945 |      0.051 |                   13 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.184 |      0.136 |                   11 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.18050373134328357
[2m[36m(func pid=13733)[0m top5: 0.65625
[2m[36m(func pid=13733)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=13733)[0m f1_macro: 0.13613812373540268
[2m[36m(func pid=13733)[0m f1_weighted: 0.18417366842575672
[2m[36m(func pid=13733)[0m f1_per_class: [0.217, 0.131, 0.259, 0.137, 0.016, 0.084, 0.343, 0.025, 0.103, 0.047]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.7534 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0036 | Steps: 2 | Val loss: 2.6468 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9101 | Steps: 2 | Val loss: 2.5022 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0884 | Steps: 2 | Val loss: 2.1694 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=183934)[0m top1: 0.417910447761194
[2m[36m(func pid=183934)[0m top5: 0.9006529850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.4011954493397007
[2m[36m(func pid=183934)[0m f1_weighted: 0.43864045899469684
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.471, 0.71, 0.493, 0.101, 0.314, 0.481, 0.192, 0.236, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4155783582089552
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=183515)[0m f1_macro: 0.35852206316541235
[2m[36m(func pid=183515)[0m f1_weighted: 0.43530530156981484
[2m[36m(func pid=183515)[0m f1_per_class: [0.553, 0.466, 0.5, 0.554, 0.076, 0.171, 0.47, 0.27, 0.206, 0.32]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07136194029850747
[2m[36m(func pid=12986)[0m top5: 0.43656716417910446
[2m[36m(func pid=12986)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=12986)[0m f1_macro: 0.05045590678699106
[2m[36m(func pid=12986)[0m f1_weighted: 0.07208556755620515
[2m[36m(func pid=12986)[0m f1_per_class: [0.048, 0.132, 0.0, 0.132, 0.0, 0.041, 0.003, 0.07, 0.052, 0.028]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:51:47 (running for 00:08:40.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.004 |      0.359 |                   92 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   89 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.91  |      0.05  |                   14 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.088 |      0.156 |                   12 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.18889925373134328
[2m[36m(func pid=13733)[0m top5: 0.7033582089552238
[2m[36m(func pid=13733)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=13733)[0m f1_macro: 0.15637021629384784
[2m[36m(func pid=13733)[0m f1_weighted: 0.20639128173701835
[2m[36m(func pid=13733)[0m f1_per_class: [0.236, 0.164, 0.301, 0.219, 0.019, 0.086, 0.311, 0.074, 0.088, 0.066]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.7317 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.6445 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8527 | Steps: 2 | Val loss: 2.4989 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0341 | Steps: 2 | Val loss: 2.1419 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=183934)[0m top1: 0.4221082089552239
[2m[36m(func pid=183934)[0m top5: 0.9006529850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=183934)[0m f1_macro: 0.4009308552287134
[2m[36m(func pid=183934)[0m f1_weighted: 0.4427582757592957
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.473, 0.71, 0.491, 0.102, 0.316, 0.496, 0.192, 0.23, 0.419]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4197761194029851
[2m[36m(func pid=183515)[0m top5: 0.8964552238805971
[2m[36m(func pid=183515)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183515)[0m f1_macro: 0.3650360392104169
[2m[36m(func pid=183515)[0m f1_weighted: 0.43947400505384787
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.465, 0.511, 0.559, 0.076, 0.18, 0.472, 0.273, 0.223, 0.32]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07136194029850747
[2m[36m(func pid=12986)[0m top5: 0.42677238805970147
[2m[36m(func pid=12986)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=12986)[0m f1_macro: 0.05041863213985964
[2m[36m(func pid=12986)[0m f1_weighted: 0.07245877835628638
[2m[36m(func pid=12986)[0m f1_per_class: [0.046, 0.133, 0.0, 0.132, 0.0, 0.04, 0.003, 0.074, 0.049, 0.027]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:51:53 (running for 00:08:45.34)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.365 |                   93 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   90 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.853 |      0.05  |                   15 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  2.034 |      0.177 |                   13 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.19869402985074627
[2m[36m(func pid=13733)[0m top5: 0.7229477611940298
[2m[36m(func pid=13733)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=13733)[0m f1_macro: 0.1766341095703717
[2m[36m(func pid=13733)[0m f1_weighted: 0.22298057633828852
[2m[36m(func pid=13733)[0m f1_per_class: [0.255, 0.184, 0.373, 0.277, 0.021, 0.092, 0.289, 0.092, 0.111, 0.073]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.7748 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0012 | Steps: 2 | Val loss: 2.6616 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8836 | Steps: 2 | Val loss: 2.4960 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9019 | Steps: 2 | Val loss: 2.1210 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=183934)[0m top1: 0.4221082089552239
[2m[36m(func pid=183934)[0m top5: 0.9006529850746269
[2m[36m(func pid=183934)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=183934)[0m f1_macro: 0.40110338931658224
[2m[36m(func pid=183934)[0m f1_weighted: 0.44287309884684606
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.475, 0.71, 0.491, 0.101, 0.316, 0.495, 0.191, 0.232, 0.419]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.4183768656716418
[2m[36m(func pid=183515)[0m top5: 0.8955223880597015
[2m[36m(func pid=183515)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=183515)[0m f1_macro: 0.36780729161446446
[2m[36m(func pid=183515)[0m f1_weighted: 0.4384767626294415
[2m[36m(func pid=183515)[0m f1_per_class: [0.581, 0.467, 0.511, 0.552, 0.075, 0.183, 0.471, 0.277, 0.222, 0.339]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07555970149253731
[2m[36m(func pid=12986)[0m top5: 0.42024253731343286
[2m[36m(func pid=12986)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=12986)[0m f1_macro: 0.05559621613765291
[2m[36m(func pid=12986)[0m f1_weighted: 0.0784332050118841
[2m[36m(func pid=12986)[0m f1_per_class: [0.051, 0.129, 0.0, 0.152, 0.0, 0.039, 0.003, 0.078, 0.079, 0.026]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:51:58 (running for 00:08:50.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.368 |                   94 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.401 |                   91 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.884 |      0.056 |                   16 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.902 |      0.196 |                   14 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.2103544776119403
[2m[36m(func pid=13733)[0m top5: 0.7402052238805971
[2m[36m(func pid=13733)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=13733)[0m f1_macro: 0.1962334339310758
[2m[36m(func pid=13733)[0m f1_weighted: 0.23424699858014697
[2m[36m(func pid=13733)[0m f1_per_class: [0.277, 0.222, 0.423, 0.33, 0.03, 0.097, 0.239, 0.138, 0.133, 0.073]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0020 | Steps: 2 | Val loss: 7.8146 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0063 | Steps: 2 | Val loss: 2.6975 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8203 | Steps: 2 | Val loss: 2.4930 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8851 | Steps: 2 | Val loss: 2.1217 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=183934)[0m top1: 0.42257462686567165
[2m[36m(func pid=183934)[0m top5: 0.8997201492537313
[2m[36m(func pid=183934)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=183934)[0m f1_macro: 0.40419403359070893
[2m[36m(func pid=183934)[0m f1_weighted: 0.4421389610334331
[2m[36m(func pid=183934)[0m f1_per_class: [0.591, 0.474, 0.71, 0.488, 0.115, 0.317, 0.493, 0.191, 0.236, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.416044776119403
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=183515)[0m f1_macro: 0.3646638531800058
[2m[36m(func pid=183515)[0m f1_weighted: 0.4363555064911702
[2m[36m(func pid=183515)[0m f1_per_class: [0.581, 0.467, 0.5, 0.547, 0.083, 0.189, 0.467, 0.281, 0.205, 0.325]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07602611940298508
[2m[36m(func pid=12986)[0m top5: 0.4183768656716418
[2m[36m(func pid=12986)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=12986)[0m f1_macro: 0.05665748227770266
[2m[36m(func pid=12986)[0m f1_weighted: 0.07962753989037137
[2m[36m(func pid=12986)[0m f1_per_class: [0.051, 0.127, 0.0, 0.156, 0.0, 0.043, 0.003, 0.073, 0.088, 0.027]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:52:03 (running for 00:08:55.83)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.006 |      0.365 |                   95 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0.002 |      0.404 |                   92 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.82  |      0.057 |                   17 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.885 |      0.206 |                   15 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.21735074626865672
[2m[36m(func pid=13733)[0m top5: 0.7364738805970149
[2m[36m(func pid=13733)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=13733)[0m f1_macro: 0.20619874114037023
[2m[36m(func pid=13733)[0m f1_weighted: 0.23669519608002146
[2m[36m(func pid=13733)[0m f1_per_class: [0.287, 0.252, 0.458, 0.37, 0.027, 0.118, 0.177, 0.166, 0.131, 0.074]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.8250 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.6961 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8388 | Steps: 2 | Val loss: 2.4876 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6921 | Steps: 2 | Val loss: 2.1152 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=183934)[0m top1: 0.4197761194029851
[2m[36m(func pid=183934)[0m top5: 0.9001865671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=183934)[0m f1_macro: 0.403206967989151
[2m[36m(func pid=183934)[0m f1_weighted: 0.43810825491322347
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.477, 0.71, 0.473, 0.116, 0.318, 0.492, 0.192, 0.235, 0.433]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=183515)[0m top1: 0.41651119402985076
[2m[36m(func pid=183515)[0m top5: 0.8950559701492538
[2m[36m(func pid=183515)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=183515)[0m f1_macro: 0.3641562906054321
[2m[36m(func pid=183515)[0m f1_weighted: 0.43691331234047237
[2m[36m(func pid=183515)[0m f1_per_class: [0.581, 0.46, 0.511, 0.553, 0.084, 0.185, 0.471, 0.273, 0.203, 0.32]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07555970149253731
[2m[36m(func pid=12986)[0m top5: 0.42024253731343286
[2m[36m(func pid=12986)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=12986)[0m f1_macro: 0.05838224625604336
[2m[36m(func pid=12986)[0m f1_weighted: 0.0786170313838271
[2m[36m(func pid=12986)[0m f1_per_class: [0.048, 0.128, 0.026, 0.151, 0.0, 0.043, 0.003, 0.076, 0.086, 0.023]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.22388059701492538
[2m[36m(func pid=13733)[0m top5: 0.7350746268656716
[2m[36m(func pid=13733)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=13733)[0m f1_macro: 0.20788179985059368
[2m[36m(func pid=13733)[0m f1_weighted: 0.22559811785193828
[2m[36m(func pid=13733)[0m f1_per_class: [0.275, 0.269, 0.44, 0.398, 0.042, 0.117, 0.097, 0.194, 0.151, 0.096]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:52:08 (running for 00:09:00.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.364 |                   96 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   93 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.839 |      0.058 |                   18 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.692 |      0.208 |                   16 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0012 | Steps: 2 | Val loss: 2.7087 | Batch size: 32 | lr: 0.001 | Duration: 2.53s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.8940 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7940 | Steps: 2 | Val loss: 2.4784 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6628 | Steps: 2 | Val loss: 2.1056 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=183515)[0m top1: 0.4123134328358209
[2m[36m(func pid=183515)[0m top5: 0.894589552238806
[2m[36m(func pid=183515)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=183515)[0m f1_macro: 0.3605071100626901
[2m[36m(func pid=183515)[0m f1_weighted: 0.4336671935900839
[2m[36m(func pid=183515)[0m f1_per_class: [0.571, 0.458, 0.49, 0.546, 0.083, 0.189, 0.468, 0.273, 0.211, 0.317]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.417910447761194
[2m[36m(func pid=183934)[0m top5: 0.9001865671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=183934)[0m f1_macro: 0.4026207450852987
[2m[36m(func pid=183934)[0m f1_weighted: 0.43693772216065824
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.477, 0.71, 0.48, 0.115, 0.314, 0.484, 0.191, 0.234, 0.441]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07649253731343283
[2m[36m(func pid=12986)[0m top5: 0.42117537313432835
[2m[36m(func pid=12986)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=12986)[0m f1_macro: 0.05878586814009612
[2m[36m(func pid=12986)[0m f1_weighted: 0.08078553773092229
[2m[36m(func pid=12986)[0m f1_per_class: [0.045, 0.132, 0.022, 0.149, 0.0, 0.047, 0.009, 0.075, 0.084, 0.024]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.23507462686567165
[2m[36m(func pid=13733)[0m top5: 0.730410447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=13733)[0m f1_macro: 0.2102576125081293
[2m[36m(func pid=13733)[0m f1_weighted: 0.223959094832282
[2m[36m(func pid=13733)[0m f1_per_class: [0.289, 0.293, 0.423, 0.409, 0.049, 0.103, 0.069, 0.212, 0.14, 0.115]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0020 | Steps: 2 | Val loss: 2.7167 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.9297 | Batch size: 32 | lr: 0.01 | Duration: 2.63s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.6152 | Steps: 2 | Val loss: 2.0904 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7977 | Steps: 2 | Val loss: 2.4734 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:52:16 (running for 00:09:09.12)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.365 |                   98 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.403 |                   94 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.794 |      0.059 |                   19 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.663 |      0.21  |                   17 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.4146455223880597
[2m[36m(func pid=183515)[0m top5: 0.8941231343283582
[2m[36m(func pid=183515)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.36509511149861246
[2m[36m(func pid=183515)[0m f1_weighted: 0.4360879636492454
[2m[36m(func pid=183515)[0m f1_per_class: [0.576, 0.455, 0.511, 0.554, 0.082, 0.187, 0.468, 0.273, 0.214, 0.331]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m top1: 0.416044776119403
[2m[36m(func pid=183934)[0m top5: 0.8992537313432836
[2m[36m(func pid=183934)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=183934)[0m f1_macro: 0.40046729660533725
[2m[36m(func pid=183934)[0m f1_weighted: 0.4345436634939377
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.473, 0.71, 0.472, 0.115, 0.315, 0.485, 0.194, 0.234, 0.426]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=13733)[0m top1: 0.24813432835820895
[2m[36m(func pid=13733)[0m top5: 0.7374067164179104
[2m[36m(func pid=13733)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=13733)[0m f1_macro: 0.22793244616529726
[2m[36m(func pid=13733)[0m f1_weighted: 0.23153258358929427
[2m[36m(func pid=13733)[0m f1_per_class: [0.281, 0.321, 0.489, 0.412, 0.058, 0.109, 0.064, 0.233, 0.159, 0.154]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07602611940298508
[2m[36m(func pid=12986)[0m top5: 0.42490671641791045
[2m[36m(func pid=12986)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=12986)[0m f1_macro: 0.05858811482928762
[2m[36m(func pid=12986)[0m f1_weighted: 0.081092736493706
[2m[36m(func pid=12986)[0m f1_per_class: [0.045, 0.135, 0.02, 0.143, 0.0, 0.055, 0.012, 0.07, 0.082, 0.025]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0010 | Steps: 2 | Val loss: 2.7237 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.9281 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.4595 | Steps: 2 | Val loss: 2.0908 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7825 | Steps: 2 | Val loss: 2.4653 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=183934)[0m top1: 0.41884328358208955
[2m[36m(func pid=183934)[0m top5: 0.898320895522388
[2m[36m(func pid=183934)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=183934)[0m f1_macro: 0.4021714642782574
[2m[36m(func pid=183934)[0m f1_weighted: 0.4383029483175231
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.471, 0.71, 0.484, 0.114, 0.316, 0.487, 0.193, 0.238, 0.433]
== Status ==
Current time: 2024-01-07 00:52:22 (running for 00:09:14.32)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.002 |      0.365 |                   98 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.402 |                   96 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.798 |      0.059 |                   20 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.615 |      0.228 |                   18 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.4123134328358209
[2m[36m(func pid=183515)[0m top5: 0.8941231343283582
[2m[36m(func pid=183515)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=183515)[0m f1_macro: 0.3615109090079808
[2m[36m(func pid=183515)[0m f1_weighted: 0.43232761307469847
[2m[36m(func pid=183515)[0m f1_per_class: [0.567, 0.454, 0.511, 0.546, 0.076, 0.179, 0.467, 0.275, 0.215, 0.325]
[2m[36m(func pid=183515)[0m 
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=13733)[0m top1: 0.24300373134328357
[2m[36m(func pid=13733)[0m top5: 0.7411380597014925
[2m[36m(func pid=13733)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=13733)[0m f1_macro: 0.22970470938265378
[2m[36m(func pid=13733)[0m f1_weighted: 0.22939380086359681
[2m[36m(func pid=13733)[0m f1_per_class: [0.276, 0.314, 0.5, 0.406, 0.045, 0.114, 0.064, 0.236, 0.161, 0.182]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07742537313432836
[2m[36m(func pid=12986)[0m top5: 0.42630597014925375
[2m[36m(func pid=12986)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=12986)[0m f1_macro: 0.062162985723730545
[2m[36m(func pid=12986)[0m f1_weighted: 0.08162629606961058
[2m[36m(func pid=12986)[0m f1_per_class: [0.045, 0.136, 0.048, 0.136, 0.0, 0.065, 0.015, 0.073, 0.078, 0.025]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.0061 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=183515)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0027 | Steps: 2 | Val loss: 2.7514 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7597 | Steps: 2 | Val loss: 2.4554 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4318 | Steps: 2 | Val loss: 2.0794 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 00:52:27 (running for 00:09:19.61)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00001 | RUNNING    | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.001 |      0.362 |                   99 |
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.404 |                   97 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.782 |      0.062 |                   21 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.46  |      0.23  |                   19 |
| train_57e67_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183515)[0m top1: 0.4146455223880597
[2m[36m(func pid=183515)[0m top5: 0.8931902985074627
[2m[36m(func pid=183515)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=183515)[0m f1_macro: 0.36287809854543385
[2m[36m(func pid=183515)[0m f1_weighted: 0.4358973160461498
[2m[36m(func pid=183515)[0m f1_per_class: [0.567, 0.455, 0.511, 0.561, 0.089, 0.193, 0.462, 0.264, 0.213, 0.315]
[2m[36m(func pid=183934)[0m top1: 0.41884328358208955
[2m[36m(func pid=183934)[0m top5: 0.8987873134328358
[2m[36m(func pid=183934)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=183934)[0m f1_macro: 0.40393324767675953
[2m[36m(func pid=183934)[0m f1_weighted: 0.43838178705156294
[2m[36m(func pid=183934)[0m f1_per_class: [0.571, 0.471, 0.71, 0.483, 0.126, 0.316, 0.487, 0.194, 0.232, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07695895522388059
[2m[36m(func pid=12986)[0m top5: 0.4295708955223881
[2m[36m(func pid=12986)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=12986)[0m f1_macro: 0.0612903898947636
[2m[36m(func pid=12986)[0m f1_weighted: 0.08234902397582423
[2m[36m(func pid=12986)[0m f1_per_class: [0.045, 0.135, 0.045, 0.143, 0.0, 0.059, 0.015, 0.069, 0.077, 0.024]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.24440298507462688
[2m[36m(func pid=13733)[0m top5: 0.7555970149253731
[2m[36m(func pid=13733)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=13733)[0m f1_macro: 0.2359366152429942
[2m[36m(func pid=13733)[0m f1_weighted: 0.23467366049596772
[2m[36m(func pid=13733)[0m f1_per_class: [0.289, 0.308, 0.5, 0.402, 0.058, 0.135, 0.078, 0.243, 0.155, 0.191]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.9482 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7490 | Steps: 2 | Val loss: 2.4537 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3297 | Steps: 2 | Val loss: 2.0737 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=183934)[0m top1: 0.42117537313432835
[2m[36m(func pid=183934)[0m top5: 0.9015858208955224
[2m[36m(func pid=183934)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=183934)[0m f1_macro: 0.4059588782014612
[2m[36m(func pid=183934)[0m f1_weighted: 0.44256638267343845
[2m[36m(func pid=183934)[0m f1_per_class: [0.576, 0.463, 0.71, 0.49, 0.115, 0.316, 0.496, 0.216, 0.23, 0.448]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m top1: 0.07649253731343283
[2m[36m(func pid=12986)[0m top5: 0.425839552238806
[2m[36m(func pid=12986)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=12986)[0m f1_macro: 0.06277053977592677
[2m[36m(func pid=12986)[0m f1_weighted: 0.08143212103648981
[2m[36m(func pid=12986)[0m f1_per_class: [0.058, 0.131, 0.053, 0.138, 0.0, 0.058, 0.018, 0.072, 0.076, 0.024]
[2m[36m(func pid=13733)[0m top1: 0.23787313432835822
[2m[36m(func pid=13733)[0m top5: 0.761660447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=13733)[0m f1_macro: 0.2297082664401635
[2m[36m(func pid=13733)[0m f1_weighted: 0.23688002910558717
[2m[36m(func pid=13733)[0m f1_per_class: [0.277, 0.297, 0.468, 0.391, 0.052, 0.147, 0.101, 0.235, 0.155, 0.173]
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.9653 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:52:32 (running for 00:09:25.14)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.406 |                   98 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.76  |      0.061 |                   22 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.432 |      0.236 |                   20 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=18554)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=18554)[0m Configuration completed!
[2m[36m(func pid=18554)[0m New optimizer parameters:
[2m[36m(func pid=18554)[0m SGD (
[2m[36m(func pid=18554)[0m Parameter Group 0
[2m[36m(func pid=18554)[0m     dampening: 0
[2m[36m(func pid=18554)[0m     differentiable: False
[2m[36m(func pid=18554)[0m     foreach: None
[2m[36m(func pid=18554)[0m     lr: 0.01
[2m[36m(func pid=18554)[0m     maximize: False
[2m[36m(func pid=18554)[0m     momentum: 0.9
[2m[36m(func pid=18554)[0m     nesterov: False
[2m[36m(func pid=18554)[0m     weight_decay: 0
[2m[36m(func pid=18554)[0m )
[2m[36m(func pid=18554)[0m 
== Status ==
Current time: 2024-01-07 00:52:38 (running for 00:09:30.38)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00002 | RUNNING    | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.408 |                   99 |
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.749 |      0.063 |                   23 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.33  |      0.23  |                   21 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.4221082089552239
[2m[36m(func pid=183934)[0m top5: 0.9011194029850746
[2m[36m(func pid=183934)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=183934)[0m f1_macro: 0.40842167371954685
[2m[36m(func pid=183934)[0m f1_weighted: 0.44388350693447
[2m[36m(func pid=183934)[0m f1_per_class: [0.581, 0.472, 0.71, 0.49, 0.113, 0.313, 0.497, 0.203, 0.23, 0.475]
[2m[36m(func pid=183934)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7374 | Steps: 2 | Val loss: 2.4456 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3596 | Steps: 2 | Val loss: 2.0649 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=183934)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.8434 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1531 | Steps: 2 | Val loss: 2.3927 | Batch size: 32 | lr: 0.01 | Duration: 4.45s
[2m[36m(func pid=12986)[0m top1: 0.07975746268656717
[2m[36m(func pid=12986)[0m top5: 0.42863805970149255
[2m[36m(func pid=12986)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=12986)[0m f1_macro: 0.06460025001234031
[2m[36m(func pid=12986)[0m f1_weighted: 0.08539422168340567
[2m[36m(func pid=12986)[0m f1_per_class: [0.057, 0.137, 0.051, 0.14, 0.0, 0.062, 0.024, 0.076, 0.075, 0.025]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.23367537313432835
[2m[36m(func pid=13733)[0m top5: 0.7784514925373134
[2m[36m(func pid=13733)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=13733)[0m f1_macro: 0.22330180062652297
[2m[36m(func pid=13733)[0m f1_weighted: 0.23978169456418263
[2m[36m(func pid=13733)[0m f1_per_class: [0.284, 0.286, 0.415, 0.387, 0.058, 0.138, 0.128, 0.223, 0.157, 0.156]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:52:43 (running for 00:09:35.59)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 3 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.737 |      0.065 |                   24 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.36  |      0.223 |                   22 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |        |            |                      |
| train_57e67_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=183934)[0m top1: 0.42863805970149255
[2m[36m(func pid=183934)[0m top5: 0.9001865671641791
[2m[36m(func pid=183934)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=183934)[0m f1_macro: 0.41098418091564515
[2m[36m(func pid=183934)[0m f1_weighted: 0.44921136581145
[2m[36m(func pid=183934)[0m f1_per_class: [0.586, 0.477, 0.71, 0.498, 0.115, 0.305, 0.506, 0.206, 0.24, 0.467]
[2m[36m(func pid=18554)[0m top1: 0.06203358208955224
[2m[36m(func pid=18554)[0m top5: 0.44822761194029853
[2m[36m(func pid=18554)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=18554)[0m f1_macro: 0.0685060529081684
[2m[36m(func pid=18554)[0m f1_weighted: 0.0628550994302455
[2m[36m(func pid=18554)[0m f1_per_class: [0.101, 0.107, 0.044, 0.069, 0.011, 0.038, 0.029, 0.06, 0.167, 0.058]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6971 | Steps: 2 | Val loss: 2.4418 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2412 | Steps: 2 | Val loss: 2.0388 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6799 | Steps: 2 | Val loss: 2.3636 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=12986)[0m top1: 0.07882462686567164
[2m[36m(func pid=12986)[0m top5: 0.43236940298507465
[2m[36m(func pid=12986)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=12986)[0m f1_macro: 0.06476031652338882
[2m[36m(func pid=12986)[0m f1_weighted: 0.08487398627683893
[2m[36m(func pid=12986)[0m f1_per_class: [0.058, 0.135, 0.059, 0.137, 0.0, 0.062, 0.026, 0.073, 0.073, 0.024]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.24440298507462688
[2m[36m(func pid=13733)[0m top5: 0.7966417910447762
[2m[36m(func pid=13733)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=13733)[0m f1_macro: 0.2299283488864094
[2m[36m(func pid=13733)[0m f1_weighted: 0.25807130901551856
[2m[36m(func pid=13733)[0m f1_per_class: [0.287, 0.298, 0.423, 0.387, 0.059, 0.139, 0.183, 0.22, 0.156, 0.146]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.09375
[2m[36m(func pid=18554)[0m top5: 0.5223880597014925
[2m[36m(func pid=18554)[0m f1_micro: 0.09375
[2m[36m(func pid=18554)[0m f1_macro: 0.06901642871356907
[2m[36m(func pid=18554)[0m f1_weighted: 0.0925555389291398
[2m[36m(func pid=18554)[0m f1_per_class: [0.103, 0.054, 0.112, 0.016, 0.021, 0.051, 0.221, 0.014, 0.098, 0.0]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7006 | Steps: 2 | Val loss: 2.4385 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2009 | Steps: 2 | Val loss: 2.0111 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5316 | Steps: 2 | Val loss: 2.1527 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:52:48 (running for 00:09:40.96)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.697 |      0.065 |                   25 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.241 |      0.23  |                   23 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  2.68  |      0.069 |                    2 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.08022388059701492
[2m[36m(func pid=12986)[0m top5: 0.4351679104477612
[2m[36m(func pid=12986)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=12986)[0m f1_macro: 0.06560959109304519
[2m[36m(func pid=12986)[0m f1_weighted: 0.08727222873721009
[2m[36m(func pid=12986)[0m f1_per_class: [0.058, 0.137, 0.057, 0.137, 0.0, 0.058, 0.035, 0.075, 0.069, 0.03]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=19406)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=19406)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=19406)[0m Configuration completed!
[2m[36m(func pid=19406)[0m New optimizer parameters:
[2m[36m(func pid=19406)[0m SGD (
[2m[36m(func pid=19406)[0m Parameter Group 0
[2m[36m(func pid=19406)[0m     dampening: 0
[2m[36m(func pid=19406)[0m     differentiable: False
[2m[36m(func pid=19406)[0m     foreach: None
[2m[36m(func pid=19406)[0m     lr: 0.1
[2m[36m(func pid=19406)[0m     maximize: False
[2m[36m(func pid=19406)[0m     momentum: 0.9
[2m[36m(func pid=19406)[0m     nesterov: False
[2m[36m(func pid=19406)[0m     weight_decay: 0
[2m[36m(func pid=19406)[0m )
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m top1: 0.25886194029850745
[2m[36m(func pid=13733)[0m top5: 0.8092350746268657
[2m[36m(func pid=13733)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=13733)[0m f1_macro: 0.23843683583229253
[2m[36m(func pid=13733)[0m f1_weighted: 0.2767002136083565
[2m[36m(func pid=13733)[0m f1_per_class: [0.274, 0.302, 0.44, 0.401, 0.061, 0.146, 0.227, 0.22, 0.161, 0.153]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:52:54 (running for 00:09:46.33)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.701 |      0.066 |                   26 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.201 |      0.238 |                   24 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  2.532 |      0.137 |                    3 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |        |            |                      |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.16930970149253732
[2m[36m(func pid=18554)[0m top5: 0.7360074626865671
[2m[36m(func pid=18554)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=18554)[0m f1_macro: 0.13670960094177825
[2m[36m(func pid=18554)[0m f1_weighted: 0.17698480608380887
[2m[36m(func pid=18554)[0m f1_per_class: [0.195, 0.277, 0.25, 0.119, 0.028, 0.113, 0.245, 0.0, 0.14, 0.0]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6637 | Steps: 2 | Val loss: 2.4340 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1852 | Steps: 2 | Val loss: 1.9908 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.4230 | Steps: 2 | Val loss: 4.1972 | Batch size: 32 | lr: 0.1 | Duration: 4.34s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.7600 | Steps: 2 | Val loss: 1.9800 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=12986)[0m top1: 0.08115671641791045
[2m[36m(func pid=12986)[0m top5: 0.43703358208955223
[2m[36m(func pid=12986)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=12986)[0m f1_macro: 0.06701928848557838
[2m[36m(func pid=12986)[0m f1_weighted: 0.09024070181392936
[2m[36m(func pid=12986)[0m f1_per_class: [0.058, 0.133, 0.056, 0.14, 0.0, 0.058, 0.043, 0.072, 0.079, 0.03]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.271455223880597
[2m[36m(func pid=13733)[0m top5: 0.820429104477612
[2m[36m(func pid=13733)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=13733)[0m f1_macro: 0.25179732434927427
[2m[36m(func pid=13733)[0m f1_weighted: 0.2907979562581888
[2m[36m(func pid=13733)[0m f1_per_class: [0.321, 0.314, 0.458, 0.412, 0.059, 0.143, 0.252, 0.224, 0.173, 0.161]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.10774253731343283
[2m[36m(func pid=19406)[0m top5: 0.425839552238806
[2m[36m(func pid=19406)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=19406)[0m f1_macro: 0.03889866155266958
[2m[36m(func pid=19406)[0m f1_weighted: 0.02581523143089334
[2m[36m(func pid=19406)[0m f1_per_class: [0.0, 0.0, 0.079, 0.0, 0.094, 0.216, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:52:59 (running for 00:09:51.66)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.664 |      0.067 |                   27 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.185 |      0.252 |                   25 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  1.76  |      0.288 |                    4 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  4.423 |      0.039 |                    1 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.2775186567164179
[2m[36m(func pid=18554)[0m top5: 0.8069029850746269
[2m[36m(func pid=18554)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=18554)[0m f1_macro: 0.2875881483264028
[2m[36m(func pid=18554)[0m f1_weighted: 0.25105025566979666
[2m[36m(func pid=18554)[0m f1_per_class: [0.475, 0.436, 0.759, 0.417, 0.056, 0.159, 0.03, 0.196, 0.113, 0.235]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6749 | Steps: 2 | Val loss: 2.4284 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1966 | Steps: 2 | Val loss: 1.9797 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.1416 | Steps: 2 | Val loss: 6.0009 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.3370 | Steps: 2 | Val loss: 2.0373 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=12986)[0m top1: 0.08208955223880597
[2m[36m(func pid=12986)[0m top5: 0.4398320895522388
[2m[36m(func pid=12986)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=12986)[0m f1_macro: 0.06774827228347509
[2m[36m(func pid=12986)[0m f1_weighted: 0.09119949875880148
[2m[36m(func pid=12986)[0m f1_per_class: [0.059, 0.133, 0.056, 0.134, 0.0, 0.055, 0.054, 0.069, 0.077, 0.041]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.27052238805970147
[2m[36m(func pid=13733)[0m top5: 0.824160447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=13733)[0m f1_macro: 0.25681846104088424
[2m[36m(func pid=13733)[0m f1_weighted: 0.29095035810635217
[2m[36m(func pid=13733)[0m f1_per_class: [0.318, 0.317, 0.512, 0.401, 0.068, 0.135, 0.261, 0.23, 0.183, 0.142]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.27845149253731344
[2m[36m(func pid=19406)[0m top5: 0.8013059701492538
[2m[36m(func pid=19406)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=19406)[0m f1_macro: 0.04828862250890177
[2m[36m(func pid=19406)[0m f1_weighted: 0.12927753073431522
[2m[36m(func pid=19406)[0m f1_per_class: [0.0, 0.051, 0.0, 0.432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:53:04 (running for 00:09:56.94)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.675 |      0.068 |                   28 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.197 |      0.257 |                   26 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  1.337 |      0.25  |                    5 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  7.142 |      0.048 |                    2 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.25326492537313433
[2m[36m(func pid=18554)[0m top5: 0.7644589552238806
[2m[36m(func pid=18554)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=18554)[0m f1_macro: 0.25016956411114694
[2m[36m(func pid=18554)[0m f1_weighted: 0.21668595428746476
[2m[36m(func pid=18554)[0m f1_per_class: [0.417, 0.141, 0.733, 0.525, 0.112, 0.134, 0.003, 0.199, 0.086, 0.152]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6875 | Steps: 2 | Val loss: 2.4210 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0112 | Steps: 2 | Val loss: 1.9486 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 11.9312 | Steps: 2 | Val loss: 15.3976 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.0302 | Steps: 2 | Val loss: 2.1871 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=12986)[0m top1: 0.08395522388059702
[2m[36m(func pid=12986)[0m top5: 0.4458955223880597
[2m[36m(func pid=12986)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=12986)[0m f1_macro: 0.06889883174055289
[2m[36m(func pid=12986)[0m f1_weighted: 0.09404700922396698
[2m[36m(func pid=12986)[0m f1_per_class: [0.059, 0.135, 0.056, 0.131, 0.0, 0.055, 0.065, 0.07, 0.075, 0.042]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.2849813432835821
[2m[36m(func pid=13733)[0m top5: 0.8348880597014925
[2m[36m(func pid=13733)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=13733)[0m f1_macro: 0.2659465993047875
[2m[36m(func pid=13733)[0m f1_weighted: 0.3080192381486782
[2m[36m(func pid=13733)[0m f1_per_class: [0.306, 0.324, 0.524, 0.395, 0.072, 0.139, 0.318, 0.226, 0.195, 0.16]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.19169776119402984
[2m[36m(func pid=19406)[0m top5: 0.5060634328358209
[2m[36m(func pid=19406)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=19406)[0m f1_macro: 0.1556066901776171
[2m[36m(func pid=19406)[0m f1_weighted: 0.16515620621886296
[2m[36m(func pid=19406)[0m f1_per_class: [0.102, 0.0, 0.632, 0.0, 0.182, 0.0, 0.503, 0.138, 0.0, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:53:09 (running for 00:10:02.17)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.687 |      0.069 |                   29 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  1.011 |      0.266 |                   27 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  1.03  |      0.255 |                    6 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      | 11.931 |      0.156 |                    3 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.228544776119403
[2m[36m(func pid=18554)[0m top5: 0.7136194029850746
[2m[36m(func pid=18554)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=18554)[0m f1_macro: 0.2554006613778212
[2m[36m(func pid=18554)[0m f1_weighted: 0.20214561596284206
[2m[36m(func pid=18554)[0m f1_per_class: [0.343, 0.032, 0.846, 0.523, 0.05, 0.123, 0.006, 0.257, 0.146, 0.227]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6679 | Steps: 2 | Val loss: 2.4154 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9906 | Steps: 2 | Val loss: 1.9223 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 18.3637 | Steps: 2 | Val loss: 30.4984 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.9025 | Steps: 2 | Val loss: 2.0496 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=12986)[0m top1: 0.08348880597014925
[2m[36m(func pid=12986)[0m top5: 0.44636194029850745
[2m[36m(func pid=12986)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=12986)[0m f1_macro: 0.06834075650385395
[2m[36m(func pid=12986)[0m f1_weighted: 0.09263799637182546
[2m[36m(func pid=12986)[0m f1_per_class: [0.053, 0.143, 0.058, 0.125, 0.0, 0.058, 0.062, 0.068, 0.074, 0.043]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.29617537313432835
[2m[36m(func pid=13733)[0m top5: 0.8418843283582089
[2m[36m(func pid=13733)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=13733)[0m f1_macro: 0.2720579630374885
[2m[36m(func pid=13733)[0m f1_weighted: 0.3203786043612283
[2m[36m(func pid=13733)[0m f1_per_class: [0.309, 0.327, 0.537, 0.414, 0.079, 0.143, 0.342, 0.214, 0.181, 0.175]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.10074626865671642
[2m[36m(func pid=19406)[0m top5: 0.47341417910447764
[2m[36m(func pid=19406)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=19406)[0m f1_macro: 0.09893470048075068
[2m[36m(func pid=19406)[0m f1_weighted: 0.04014025694079283
[2m[36m(func pid=19406)[0m f1_per_class: [0.0, 0.0, 0.588, 0.0, 0.0, 0.278, 0.0, 0.031, 0.093, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:53:15 (running for 00:10:07.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.668 |      0.068 |                   30 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.991 |      0.272 |                   28 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.902 |      0.263 |                    7 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      | 18.364 |      0.099 |                    4 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.2579291044776119
[2m[36m(func pid=18554)[0m top5: 0.8083022388059702
[2m[36m(func pid=18554)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=18554)[0m f1_macro: 0.2629152865139268
[2m[36m(func pid=18554)[0m f1_weighted: 0.2504660940913654
[2m[36m(func pid=18554)[0m f1_per_class: [0.383, 0.061, 0.579, 0.52, 0.064, 0.216, 0.115, 0.262, 0.182, 0.247]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6326 | Steps: 2 | Val loss: 2.4106 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9264 | Steps: 2 | Val loss: 1.9199 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 20.7201 | Steps: 2 | Val loss: 29.9553 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4702 | Steps: 2 | Val loss: 1.8901 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=12986)[0m top1: 0.08348880597014925
[2m[36m(func pid=12986)[0m top5: 0.4458955223880597
[2m[36m(func pid=12986)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=12986)[0m f1_macro: 0.06824703610853891
[2m[36m(func pid=12986)[0m f1_weighted: 0.09297815553176574
[2m[36m(func pid=12986)[0m f1_per_class: [0.054, 0.148, 0.057, 0.114, 0.0, 0.059, 0.069, 0.069, 0.074, 0.038]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.2971082089552239
[2m[36m(func pid=13733)[0m top5: 0.8451492537313433
[2m[36m(func pid=13733)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=13733)[0m f1_macro: 0.27465220163004817
[2m[36m(func pid=13733)[0m f1_weighted: 0.32203404533164254
[2m[36m(func pid=13733)[0m f1_per_class: [0.321, 0.32, 0.537, 0.425, 0.073, 0.135, 0.34, 0.22, 0.192, 0.184]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.05223880597014925
[2m[36m(func pid=19406)[0m top5: 0.4841417910447761
[2m[36m(func pid=19406)[0m f1_micro: 0.05223880597014925
[2m[36m(func pid=19406)[0m f1_macro: 0.05479851209327406
[2m[36m(func pid=19406)[0m f1_weighted: 0.03092222646591
[2m[36m(func pid=19406)[0m f1_per_class: [0.0, 0.0, 0.074, 0.0, 0.022, 0.074, 0.0, 0.378, 0.0, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:53:20 (running for 00:10:12.52)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.633 |      0.068 |                   31 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.926 |      0.275 |                   29 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.47  |      0.304 |                    8 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      | 20.72  |      0.055 |                    5 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.292910447761194
[2m[36m(func pid=18554)[0m top5: 0.8931902985074627
[2m[36m(func pid=18554)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.30379754056292413
[2m[36m(func pid=18554)[0m f1_weighted: 0.30717377662756934
[2m[36m(func pid=18554)[0m f1_per_class: [0.476, 0.314, 0.537, 0.464, 0.119, 0.274, 0.206, 0.146, 0.135, 0.367]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6372 | Steps: 2 | Val loss: 2.4066 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 12.5190 | Steps: 2 | Val loss: 12.6691 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9510 | Steps: 2 | Val loss: 1.9043 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3456 | Steps: 2 | Val loss: 1.7475 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=12986)[0m top1: 0.08442164179104478
[2m[36m(func pid=12986)[0m top5: 0.45009328358208955
[2m[36m(func pid=12986)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=12986)[0m f1_macro: 0.06924855886546488
[2m[36m(func pid=12986)[0m f1_weighted: 0.09360771257696576
[2m[36m(func pid=12986)[0m f1_per_class: [0.062, 0.148, 0.054, 0.115, 0.0, 0.059, 0.069, 0.074, 0.073, 0.038]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.302705223880597
[2m[36m(func pid=13733)[0m top5: 0.855410447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=13733)[0m f1_macro: 0.283423162068175
[2m[36m(func pid=13733)[0m f1_weighted: 0.3253885606471035
[2m[36m(func pid=13733)[0m f1_per_class: [0.332, 0.331, 0.564, 0.435, 0.075, 0.143, 0.327, 0.229, 0.205, 0.192]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.32975746268656714
[2m[36m(func pid=19406)[0m top5: 0.5909514925373134
[2m[36m(func pid=19406)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=19406)[0m f1_macro: 0.1542492515252734
[2m[36m(func pid=19406)[0m f1_weighted: 0.25444827624502897
[2m[36m(func pid=19406)[0m f1_per_class: [0.263, 0.326, 0.109, 0.0, 0.106, 0.037, 0.615, 0.042, 0.043, 0.0]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:53:25 (running for 00:10:17.76)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.637 |      0.069 |                   32 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.951 |      0.283 |                   30 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.346 |      0.34  |                    9 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      | 12.519 |      0.154 |                    6 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.345615671641791
[2m[36m(func pid=18554)[0m top5: 0.9267723880597015
[2m[36m(func pid=18554)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=18554)[0m f1_macro: 0.3401533921327386
[2m[36m(func pid=18554)[0m f1_weighted: 0.35719892916831575
[2m[36m(func pid=18554)[0m f1_per_class: [0.561, 0.422, 0.5, 0.48, 0.159, 0.306, 0.263, 0.219, 0.15, 0.343]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6493 | Steps: 2 | Val loss: 2.4030 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3547 | Steps: 2 | Val loss: 8.4563 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9081 | Steps: 2 | Val loss: 1.9049 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3740 | Steps: 2 | Val loss: 1.6615 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=12986)[0m top1: 0.08955223880597014
[2m[36m(func pid=12986)[0m top5: 0.45149253731343286
[2m[36m(func pid=12986)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=12986)[0m f1_macro: 0.07266319759277083
[2m[36m(func pid=12986)[0m f1_weighted: 0.10017062794672695
[2m[36m(func pid=12986)[0m f1_per_class: [0.062, 0.159, 0.055, 0.127, 0.0, 0.059, 0.072, 0.08, 0.073, 0.039]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=19406)[0m top1: 0.29244402985074625
[2m[36m(func pid=19406)[0m top5: 0.8661380597014925
[2m[36m(func pid=19406)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=19406)[0m f1_macro: 0.2743918008929408
[2m[36m(func pid=19406)[0m f1_weighted: 0.26612142864653937
[2m[36m(func pid=19406)[0m f1_per_class: [0.453, 0.33, 0.688, 0.241, 0.111, 0.117, 0.348, 0.015, 0.216, 0.225]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m top1: 0.29850746268656714
[2m[36m(func pid=13733)[0m top5: 0.8502798507462687
[2m[36m(func pid=13733)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=13733)[0m f1_macro: 0.28394061580351343
[2m[36m(func pid=13733)[0m f1_weighted: 0.3191665142161136
[2m[36m(func pid=13733)[0m f1_per_class: [0.358, 0.345, 0.564, 0.437, 0.068, 0.146, 0.293, 0.236, 0.206, 0.186]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:53:30 (running for 00:10:23.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.649 |      0.073 |                   33 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.908 |      0.284 |                   31 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.374 |      0.351 |                   10 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  2.355 |      0.274 |                    7 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.38013059701492535
[2m[36m(func pid=18554)[0m top5: 0.9323694029850746
[2m[36m(func pid=18554)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=18554)[0m f1_macro: 0.35095858159637355
[2m[36m(func pid=18554)[0m f1_weighted: 0.3856230507948808
[2m[36m(func pid=18554)[0m f1_per_class: [0.545, 0.463, 0.48, 0.497, 0.136, 0.281, 0.32, 0.257, 0.177, 0.353]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.6423 | Steps: 2 | Val loss: 16.1737 | Batch size: 32 | lr: 0.1 | Duration: 2.56s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5482 | Steps: 2 | Val loss: 2.3954 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8678 | Steps: 2 | Val loss: 1.8906 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2097 | Steps: 2 | Val loss: 1.6853 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=12986)[0m top1: 0.08955223880597014
[2m[36m(func pid=12986)[0m top5: 0.45755597014925375
[2m[36m(func pid=12986)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=12986)[0m f1_macro: 0.07315334722183864
[2m[36m(func pid=12986)[0m f1_weighted: 0.09835877234704422
[2m[36m(func pid=12986)[0m f1_per_class: [0.063, 0.16, 0.06, 0.117, 0.0, 0.059, 0.074, 0.08, 0.077, 0.041]
[2m[36m(func pid=19406)[0m top1: 0.25652985074626866
[2m[36m(func pid=19406)[0m top5: 0.7714552238805971
[2m[36m(func pid=19406)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=19406)[0m f1_macro: 0.20129015440271214
[2m[36m(func pid=19406)[0m f1_weighted: 0.18987775109952304
[2m[36m(func pid=19406)[0m f1_per_class: [0.327, 0.148, 0.762, 0.5, 0.0, 0.053, 0.003, 0.031, 0.126, 0.062]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.2989738805970149
[2m[36m(func pid=13733)[0m top5: 0.8540111940298507
[2m[36m(func pid=13733)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=13733)[0m f1_macro: 0.29065361993266114
[2m[36m(func pid=13733)[0m f1_weighted: 0.3199026663287847
[2m[36m(func pid=13733)[0m f1_per_class: [0.359, 0.338, 0.611, 0.456, 0.061, 0.147, 0.278, 0.243, 0.2, 0.213]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:53:36 (running for 00:10:28.28)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.548 |      0.073 |                   34 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.868 |      0.291 |                   32 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.21  |      0.35  |                   11 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  3.642 |      0.201 |                    8 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3829291044776119
[2m[36m(func pid=18554)[0m top5: 0.9095149253731343
[2m[36m(func pid=18554)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=18554)[0m f1_macro: 0.3496546354390688
[2m[36m(func pid=18554)[0m f1_weighted: 0.39132243154944885
[2m[36m(func pid=18554)[0m f1_per_class: [0.542, 0.485, 0.471, 0.536, 0.07, 0.188, 0.319, 0.286, 0.177, 0.423]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 5.3527 | Steps: 2 | Val loss: 16.5674 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5933 | Steps: 2 | Val loss: 2.3915 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7737 | Steps: 2 | Val loss: 1.8702 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1755 | Steps: 2 | Val loss: 1.8265 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=19406)[0m top1: 0.332089552238806
[2m[36m(func pid=19406)[0m top5: 0.7467350746268657
[2m[36m(func pid=19406)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=19406)[0m f1_macro: 0.2577798507344741
[2m[36m(func pid=19406)[0m f1_weighted: 0.23436211576038968
[2m[36m(func pid=19406)[0m f1_per_class: [0.478, 0.308, 0.375, 0.473, 0.1, 0.189, 0.0, 0.077, 0.185, 0.394]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.09001865671641791
[2m[36m(func pid=12986)[0m top5: 0.4594216417910448
[2m[36m(func pid=12986)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=12986)[0m f1_macro: 0.07340933643748666
[2m[36m(func pid=12986)[0m f1_weighted: 0.09853911984059213
[2m[36m(func pid=12986)[0m f1_per_class: [0.068, 0.16, 0.061, 0.113, 0.0, 0.06, 0.079, 0.076, 0.076, 0.042]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3064365671641791
[2m[36m(func pid=13733)[0m top5: 0.8610074626865671
[2m[36m(func pid=13733)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=13733)[0m f1_macro: 0.29652368778521954
[2m[36m(func pid=13733)[0m f1_weighted: 0.32394169448963245
[2m[36m(func pid=13733)[0m f1_per_class: [0.389, 0.361, 0.595, 0.462, 0.062, 0.145, 0.272, 0.241, 0.198, 0.24]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3498134328358209
[2m[36m(func pid=18554)[0m top5: 0.8880597014925373
[2m[36m(func pid=18554)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=18554)[0m f1_macro: 0.3266453048853331
[2m[36m(func pid=18554)[0m f1_weighted: 0.3651998618431713
[2m[36m(func pid=18554)[0m f1_per_class: [0.479, 0.45, 0.48, 0.537, 0.058, 0.094, 0.29, 0.293, 0.173, 0.412]
== Status ==
Current time: 2024-01-07 00:53:41 (running for 00:10:33.52)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.593 |      0.073 |                   35 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.774 |      0.297 |                   33 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.175 |      0.327 |                   12 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  5.353 |      0.258 |                    9 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2756 | Steps: 2 | Val loss: 13.6899 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6198 | Steps: 2 | Val loss: 2.3919 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7600 | Steps: 2 | Val loss: 1.8604 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1633 | Steps: 2 | Val loss: 1.9604 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=19406)[0m top1: 0.3064365671641791
[2m[36m(func pid=19406)[0m top5: 0.6828358208955224
[2m[36m(func pid=19406)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=19406)[0m f1_macro: 0.2588482360118297
[2m[36m(func pid=19406)[0m f1_weighted: 0.27864767204081586
[2m[36m(func pid=19406)[0m f1_per_class: [0.557, 0.432, 0.143, 0.479, 0.076, 0.246, 0.003, 0.356, 0.22, 0.077]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.09328358208955224
[2m[36m(func pid=12986)[0m top5: 0.46222014925373134
[2m[36m(func pid=12986)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=12986)[0m f1_macro: 0.0763493873454097
[2m[36m(func pid=12986)[0m f1_weighted: 0.10131842012316568
[2m[36m(func pid=12986)[0m f1_per_class: [0.069, 0.169, 0.061, 0.111, 0.0, 0.061, 0.082, 0.085, 0.086, 0.04]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.30783582089552236
[2m[36m(func pid=13733)[0m top5: 0.8624067164179104
[2m[36m(func pid=13733)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=13733)[0m f1_macro: 0.29847325323844515
[2m[36m(func pid=13733)[0m f1_weighted: 0.322524762870417
[2m[36m(func pid=13733)[0m f1_per_class: [0.393, 0.376, 0.595, 0.456, 0.062, 0.155, 0.258, 0.251, 0.205, 0.235]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:53:46 (running for 00:10:38.81)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.62  |      0.076 |                   36 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.76  |      0.298 |                   34 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.163 |      0.308 |                   13 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  1.276 |      0.259 |                   10 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.322294776119403
[2m[36m(func pid=18554)[0m top5: 0.8763992537313433
[2m[36m(func pid=18554)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=18554)[0m f1_macro: 0.30780028768576906
[2m[36m(func pid=18554)[0m f1_weighted: 0.34147434566170554
[2m[36m(func pid=18554)[0m f1_per_class: [0.465, 0.355, 0.511, 0.541, 0.052, 0.086, 0.27, 0.295, 0.142, 0.361]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8286 | Steps: 2 | Val loss: 20.1808 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5421 | Steps: 2 | Val loss: 2.3874 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7573 | Steps: 2 | Val loss: 1.8596 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1216 | Steps: 2 | Val loss: 1.9586 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=19406)[0m top1: 0.17490671641791045
[2m[36m(func pid=19406)[0m top5: 0.6455223880597015
[2m[36m(func pid=19406)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=19406)[0m f1_macro: 0.21789373878511692
[2m[36m(func pid=19406)[0m f1_weighted: 0.16128660630438002
[2m[36m(func pid=19406)[0m f1_per_class: [0.734, 0.41, 0.267, 0.117, 0.034, 0.16, 0.003, 0.273, 0.18, 0.0]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.09188432835820895
[2m[36m(func pid=12986)[0m top5: 0.4626865671641791
[2m[36m(func pid=12986)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=12986)[0m f1_macro: 0.07641844472477961
[2m[36m(func pid=12986)[0m f1_weighted: 0.10032432417877525
[2m[36m(func pid=12986)[0m f1_per_class: [0.068, 0.162, 0.067, 0.106, 0.0, 0.063, 0.086, 0.085, 0.088, 0.04]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.30783582089552236
[2m[36m(func pid=13733)[0m top5: 0.8642723880597015
[2m[36m(func pid=13733)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=13733)[0m f1_macro: 0.2965636824363746
[2m[36m(func pid=13733)[0m f1_weighted: 0.32173099945342404
[2m[36m(func pid=13733)[0m f1_per_class: [0.385, 0.374, 0.595, 0.466, 0.062, 0.15, 0.249, 0.256, 0.201, 0.23]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:53:51 (running for 00:10:44.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.542 |      0.076 |                   37 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.757 |      0.297 |                   35 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.122 |      0.309 |                   14 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  2.829 |      0.218 |                   11 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.33255597014925375
[2m[36m(func pid=18554)[0m top5: 0.8736007462686567
[2m[36m(func pid=18554)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=18554)[0m f1_macro: 0.309370607636099
[2m[36m(func pid=18554)[0m f1_weighted: 0.3570621193217128
[2m[36m(func pid=18554)[0m f1_per_class: [0.456, 0.336, 0.522, 0.548, 0.054, 0.076, 0.333, 0.28, 0.17, 0.319]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.7949 | Steps: 2 | Val loss: 20.8767 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5336 | Steps: 2 | Val loss: 2.3812 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7019 | Steps: 2 | Val loss: 1.8588 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=19406)[0m top1: 0.1515858208955224
[2m[36m(func pid=19406)[0m top5: 0.7117537313432836
[2m[36m(func pid=19406)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=19406)[0m f1_macro: 0.2343739317591666
[2m[36m(func pid=19406)[0m f1_weighted: 0.14678631625878769
[2m[36m(func pid=19406)[0m f1_per_class: [0.642, 0.388, 0.696, 0.076, 0.03, 0.156, 0.021, 0.236, 0.099, 0.0]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1123 | Steps: 2 | Val loss: 1.8861 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=12986)[0m top1: 0.09794776119402986
[2m[36m(func pid=12986)[0m top5: 0.47154850746268656
[2m[36m(func pid=12986)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=12986)[0m f1_macro: 0.08098179004288693
[2m[36m(func pid=12986)[0m f1_weighted: 0.10759260717987108
[2m[36m(func pid=12986)[0m f1_per_class: [0.082, 0.17, 0.074, 0.123, 0.0, 0.065, 0.088, 0.084, 0.09, 0.034]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31156716417910446
[2m[36m(func pid=13733)[0m top5: 0.8628731343283582
[2m[36m(func pid=13733)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=13733)[0m f1_macro: 0.2957148770179888
[2m[36m(func pid=13733)[0m f1_weighted: 0.3194230232395682
[2m[36m(func pid=13733)[0m f1_per_class: [0.389, 0.384, 0.55, 0.471, 0.065, 0.163, 0.222, 0.27, 0.211, 0.233]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:53:57 (running for 00:10:49.33)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.534 |      0.081 |                   38 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.702 |      0.296 |                   36 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.112 |      0.318 |                   15 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  1.795 |      0.234 |                   12 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.355410447761194
[2m[36m(func pid=18554)[0m top5: 0.8847947761194029
[2m[36m(func pid=18554)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.31804705550025125
[2m[36m(func pid=18554)[0m f1_weighted: 0.38085565452256054
[2m[36m(func pid=18554)[0m f1_per_class: [0.456, 0.306, 0.533, 0.562, 0.064, 0.104, 0.408, 0.269, 0.174, 0.305]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4341 | Steps: 2 | Val loss: 18.3713 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5438 | Steps: 2 | Val loss: 2.3770 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7295 | Steps: 2 | Val loss: 1.8572 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=19406)[0m top1: 0.1553171641791045
[2m[36m(func pid=19406)[0m top5: 0.7299440298507462
[2m[36m(func pid=19406)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=19406)[0m f1_macro: 0.231621562860526
[2m[36m(func pid=19406)[0m f1_weighted: 0.15005713262144843
[2m[36m(func pid=19406)[0m f1_per_class: [0.522, 0.373, 0.8, 0.064, 0.043, 0.132, 0.084, 0.152, 0.069, 0.077]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0966 | Steps: 2 | Val loss: 1.8296 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=12986)[0m top1: 0.09981343283582089
[2m[36m(func pid=12986)[0m top5: 0.4766791044776119
[2m[36m(func pid=12986)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=12986)[0m f1_macro: 0.08253595837979608
[2m[36m(func pid=12986)[0m f1_weighted: 0.11001178921895893
[2m[36m(func pid=12986)[0m f1_per_class: [0.088, 0.172, 0.074, 0.121, 0.0, 0.069, 0.096, 0.083, 0.089, 0.035]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31203358208955223
[2m[36m(func pid=13733)[0m top5: 0.8628731343283582
[2m[36m(func pid=13733)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=13733)[0m f1_macro: 0.29516671241287434
[2m[36m(func pid=13733)[0m f1_weighted: 0.3205538038833246
[2m[36m(func pid=13733)[0m f1_per_class: [0.374, 0.384, 0.55, 0.475, 0.064, 0.156, 0.224, 0.275, 0.215, 0.234]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:02 (running for 00:10:54.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.544 |      0.083 |                   39 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.73  |      0.295 |                   37 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.097 |      0.332 |                   16 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.434 |      0.232 |                   13 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3805970149253731
[2m[36m(func pid=18554)[0m top5: 0.8861940298507462
[2m[36m(func pid=18554)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=18554)[0m f1_macro: 0.3324120925050189
[2m[36m(func pid=18554)[0m f1_weighted: 0.40969743866959907
[2m[36m(func pid=18554)[0m f1_per_class: [0.468, 0.32, 0.512, 0.556, 0.08, 0.18, 0.478, 0.235, 0.192, 0.305]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0001 | Steps: 2 | Val loss: 18.8954 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5194 | Steps: 2 | Val loss: 2.3675 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7561 | Steps: 2 | Val loss: 1.8517 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=19406)[0m top1: 0.15065298507462688
[2m[36m(func pid=19406)[0m top5: 0.7206156716417911
[2m[36m(func pid=19406)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=19406)[0m f1_macro: 0.18773306097520126
[2m[36m(func pid=19406)[0m f1_weighted: 0.15254872879906556
[2m[36m(func pid=19406)[0m f1_per_class: [0.5, 0.275, 0.511, 0.058, 0.067, 0.044, 0.207, 0.12, 0.022, 0.074]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0630 | Steps: 2 | Val loss: 1.8844 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=12986)[0m top1: 0.10307835820895522
[2m[36m(func pid=12986)[0m top5: 0.48740671641791045
[2m[36m(func pid=12986)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=12986)[0m f1_macro: 0.0840692146824962
[2m[36m(func pid=12986)[0m f1_weighted: 0.11531024040265905
[2m[36m(func pid=12986)[0m f1_per_class: [0.088, 0.174, 0.075, 0.125, 0.0, 0.065, 0.11, 0.081, 0.087, 0.036]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.314365671641791
[2m[36m(func pid=13733)[0m top5: 0.8652052238805971
[2m[36m(func pid=13733)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=13733)[0m f1_macro: 0.2960829722851271
[2m[36m(func pid=13733)[0m f1_weighted: 0.32028924435764305
[2m[36m(func pid=13733)[0m f1_per_class: [0.354, 0.394, 0.55, 0.475, 0.069, 0.169, 0.213, 0.278, 0.214, 0.246]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:07 (running for 00:10:59.72)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.519 |      0.084 |                   40 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.756 |      0.296 |                   38 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.063 |      0.345 |                   17 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.188 |                   14 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3773320895522388
[2m[36m(func pid=18554)[0m top5: 0.8782649253731343
[2m[36m(func pid=18554)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=18554)[0m f1_macro: 0.34483876475422903
[2m[36m(func pid=18554)[0m f1_weighted: 0.4092819913174967
[2m[36m(func pid=18554)[0m f1_per_class: [0.449, 0.328, 0.579, 0.542, 0.107, 0.238, 0.458, 0.248, 0.191, 0.308]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0003 | Steps: 2 | Val loss: 20.7708 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5284 | Steps: 2 | Val loss: 2.3619 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7263 | Steps: 2 | Val loss: 1.8478 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=19406)[0m top1: 0.15298507462686567
[2m[36m(func pid=19406)[0m top5: 0.6884328358208955
[2m[36m(func pid=19406)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=19406)[0m f1_macro: 0.1714596853082317
[2m[36m(func pid=19406)[0m f1_weighted: 0.15462964347023084
[2m[36m(func pid=19406)[0m f1_per_class: [0.511, 0.172, 0.3, 0.042, 0.125, 0.036, 0.297, 0.103, 0.0, 0.129]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0965 | Steps: 2 | Val loss: 1.9281 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=12986)[0m top1: 0.1044776119402985
[2m[36m(func pid=12986)[0m top5: 0.49533582089552236
[2m[36m(func pid=12986)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=12986)[0m f1_macro: 0.08584583300261919
[2m[36m(func pid=12986)[0m f1_weighted: 0.11724309566148251
[2m[36m(func pid=12986)[0m f1_per_class: [0.082, 0.177, 0.081, 0.126, 0.0, 0.066, 0.114, 0.078, 0.091, 0.043]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31343283582089554
[2m[36m(func pid=13733)[0m top5: 0.8610074626865671
[2m[36m(func pid=13733)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=13733)[0m f1_macro: 0.29682598176137587
[2m[36m(func pid=13733)[0m f1_weighted: 0.3156287585653829
[2m[36m(func pid=13733)[0m f1_per_class: [0.372, 0.393, 0.55, 0.47, 0.073, 0.173, 0.2, 0.281, 0.216, 0.242]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0165 | Steps: 2 | Val loss: 22.0134 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:54:12 (running for 00:11:04.98)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.528 |      0.086 |                   41 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.726 |      0.297 |                   39 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.096 |      0.341 |                   18 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.171 |                   15 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3619402985074627
[2m[36m(func pid=18554)[0m top5: 0.8777985074626866
[2m[36m(func pid=18554)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=18554)[0m f1_macro: 0.3412599800451178
[2m[36m(func pid=18554)[0m f1_weighted: 0.3967070089352055
[2m[36m(func pid=18554)[0m f1_per_class: [0.424, 0.339, 0.595, 0.524, 0.09, 0.258, 0.424, 0.232, 0.179, 0.348]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4788 | Steps: 2 | Val loss: 2.3559 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6516 | Steps: 2 | Val loss: 1.8397 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=19406)[0m top1: 0.17490671641791045
[2m[36m(func pid=19406)[0m top5: 0.6492537313432836
[2m[36m(func pid=19406)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=19406)[0m f1_macro: 0.17582430141849006
[2m[36m(func pid=19406)[0m f1_weighted: 0.1710565466082346
[2m[36m(func pid=19406)[0m f1_per_class: [0.515, 0.111, 0.148, 0.036, 0.159, 0.023, 0.395, 0.102, 0.0, 0.27]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0467 | Steps: 2 | Val loss: 1.9326 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=12986)[0m top1: 0.10867537313432836
[2m[36m(func pid=12986)[0m top5: 0.5013992537313433
[2m[36m(func pid=12986)[0m f1_micro: 0.10867537313432836
[2m[36m(func pid=12986)[0m f1_macro: 0.08920903707146711
[2m[36m(func pid=12986)[0m f1_weighted: 0.12125286159301524
[2m[36m(func pid=12986)[0m f1_per_class: [0.082, 0.182, 0.088, 0.127, 0.0, 0.069, 0.121, 0.081, 0.092, 0.05]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3162313432835821
[2m[36m(func pid=13733)[0m top5: 0.8647388059701493
[2m[36m(func pid=13733)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=13733)[0m f1_macro: 0.2972208456964345
[2m[36m(func pid=13733)[0m f1_weighted: 0.32017509251724074
[2m[36m(func pid=13733)[0m f1_per_class: [0.372, 0.391, 0.55, 0.473, 0.074, 0.17, 0.215, 0.278, 0.211, 0.238]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1447 | Steps: 2 | Val loss: 21.7664 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=18554)[0m top1: 0.36100746268656714
[2m[36m(func pid=18554)[0m top5: 0.8824626865671642
[2m[36m(func pid=18554)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=18554)[0m f1_macro: 0.34413101915523947
[2m[36m(func pid=18554)[0m f1_weighted: 0.3935649011767342
[2m[36m(func pid=18554)[0m f1_per_class: [0.419, 0.335, 0.611, 0.53, 0.116, 0.268, 0.405, 0.231, 0.181, 0.345]
== Status ==
Current time: 2024-01-07 00:54:17 (running for 00:11:10.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.479 |      0.089 |                   42 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.652 |      0.297 |                   40 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.047 |      0.344 |                   19 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.017 |      0.176 |                   16 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.5062 | Steps: 2 | Val loss: 2.3477 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6114 | Steps: 2 | Val loss: 1.8342 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=19406)[0m top1: 0.23647388059701493
[2m[36m(func pid=19406)[0m top5: 0.628731343283582
[2m[36m(func pid=19406)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=19406)[0m f1_macro: 0.18447109110375623
[2m[36m(func pid=19406)[0m f1_weighted: 0.2096694833858542
[2m[36m(func pid=19406)[0m f1_per_class: [0.44, 0.084, 0.075, 0.058, 0.186, 0.031, 0.52, 0.096, 0.0, 0.356]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0589 | Steps: 2 | Val loss: 1.9342 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=12986)[0m top1: 0.10914179104477612
[2m[36m(func pid=12986)[0m top5: 0.5130597014925373
[2m[36m(func pid=12986)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=12986)[0m f1_macro: 0.08893908607160862
[2m[36m(func pid=12986)[0m f1_weighted: 0.12212104554805624
[2m[36m(func pid=12986)[0m f1_per_class: [0.086, 0.183, 0.091, 0.121, 0.0, 0.064, 0.131, 0.084, 0.086, 0.044]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3162313432835821
[2m[36m(func pid=13733)[0m top5: 0.8661380597014925
[2m[36m(func pid=13733)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=13733)[0m f1_macro: 0.2977519064098645
[2m[36m(func pid=13733)[0m f1_weighted: 0.323752189216643
[2m[36m(func pid=13733)[0m f1_per_class: [0.378, 0.389, 0.55, 0.474, 0.075, 0.164, 0.232, 0.266, 0.213, 0.236]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0722 | Steps: 2 | Val loss: 21.2536 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 00:54:23 (running for 00:11:15.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.506 |      0.089 |                   43 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.611 |      0.298 |                   41 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.059 |      0.353 |                   20 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.145 |      0.184 |                   17 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3596082089552239
[2m[36m(func pid=18554)[0m top5: 0.8833955223880597
[2m[36m(func pid=18554)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=18554)[0m f1_macro: 0.3532977582402497
[2m[36m(func pid=18554)[0m f1_weighted: 0.3893176683646951
[2m[36m(func pid=18554)[0m f1_per_class: [0.428, 0.343, 0.647, 0.524, 0.121, 0.29, 0.379, 0.242, 0.179, 0.38]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.2905783582089552
[2m[36m(func pid=19406)[0m top5: 0.6231343283582089
[2m[36m(func pid=19406)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=19406)[0m f1_macro: 0.18508636217528385
[2m[36m(func pid=19406)[0m f1_weighted: 0.22749839851838952
[2m[36m(func pid=19406)[0m f1_per_class: [0.369, 0.094, 0.058, 0.074, 0.2, 0.038, 0.565, 0.05, 0.042, 0.361]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5028 | Steps: 2 | Val loss: 2.3430 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6401 | Steps: 2 | Val loss: 1.8242 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0413 | Steps: 2 | Val loss: 1.9372 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=12986)[0m top1: 0.11240671641791045
[2m[36m(func pid=12986)[0m top5: 0.5135261194029851
[2m[36m(func pid=12986)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=12986)[0m f1_macro: 0.09390717442167522
[2m[36m(func pid=12986)[0m f1_weighted: 0.12299551961825887
[2m[36m(func pid=12986)[0m f1_per_class: [0.101, 0.181, 0.11, 0.119, 0.0, 0.073, 0.131, 0.086, 0.086, 0.051]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3171 | Steps: 2 | Val loss: 20.1759 | Batch size: 32 | lr: 0.1 | Duration: 2.60s
[2m[36m(func pid=13733)[0m top1: 0.322294776119403
[2m[36m(func pid=13733)[0m top5: 0.8708022388059702
[2m[36m(func pid=13733)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=13733)[0m f1_macro: 0.30080513844744977
[2m[36m(func pid=13733)[0m f1_weighted: 0.3339496311820609
[2m[36m(func pid=13733)[0m f1_per_class: [0.383, 0.385, 0.55, 0.472, 0.077, 0.169, 0.268, 0.268, 0.211, 0.226]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:28 (running for 00:11:20.30)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.503 |      0.094 |                   44 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.64  |      0.301 |                   42 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.041 |      0.349 |                   21 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.072 |      0.185 |                   18 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.353544776119403
[2m[36m(func pid=18554)[0m top5: 0.8852611940298507
[2m[36m(func pid=18554)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=18554)[0m f1_macro: 0.34916668475027085
[2m[36m(func pid=18554)[0m f1_weighted: 0.37781405354777736
[2m[36m(func pid=18554)[0m f1_per_class: [0.421, 0.348, 0.647, 0.524, 0.135, 0.306, 0.332, 0.246, 0.176, 0.357]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.28638059701492535
[2m[36m(func pid=19406)[0m top5: 0.6282649253731343
[2m[36m(func pid=19406)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=19406)[0m f1_macro: 0.18089025167225065
[2m[36m(func pid=19406)[0m f1_weighted: 0.2377564978817669
[2m[36m(func pid=19406)[0m f1_per_class: [0.287, 0.101, 0.059, 0.095, 0.19, 0.043, 0.576, 0.067, 0.062, 0.329]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4749 | Steps: 2 | Val loss: 2.3444 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6178 | Steps: 2 | Val loss: 1.8099 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0876 | Steps: 2 | Val loss: 1.9296 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.0564 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=12986)[0m top1: 0.11240671641791045
[2m[36m(func pid=12986)[0m top5: 0.5125932835820896
[2m[36m(func pid=12986)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=12986)[0m f1_macro: 0.09479172721920007
[2m[36m(func pid=12986)[0m f1_weighted: 0.12320775311928692
[2m[36m(func pid=12986)[0m f1_per_class: [0.106, 0.18, 0.11, 0.125, 0.0, 0.077, 0.125, 0.088, 0.087, 0.05]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3306902985074627
[2m[36m(func pid=13733)[0m top5: 0.8745335820895522
[2m[36m(func pid=13733)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=13733)[0m f1_macro: 0.30423200316789784
[2m[36m(func pid=13733)[0m f1_weighted: 0.34521479004038363
[2m[36m(func pid=13733)[0m f1_per_class: [0.374, 0.389, 0.55, 0.476, 0.081, 0.173, 0.3, 0.26, 0.212, 0.227]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:33 (running for 00:11:25.46)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.475 |      0.095 |                   45 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.618 |      0.304 |                   43 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.088 |      0.348 |                   22 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.317 |      0.181 |                   19 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3521455223880597
[2m[36m(func pid=18554)[0m top5: 0.8899253731343284
[2m[36m(func pid=18554)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=18554)[0m f1_macro: 0.3483283182977783
[2m[36m(func pid=18554)[0m f1_weighted: 0.3729625381379534
[2m[36m(func pid=18554)[0m f1_per_class: [0.41, 0.357, 0.667, 0.524, 0.13, 0.302, 0.311, 0.256, 0.183, 0.345]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.24766791044776118
[2m[36m(func pid=19406)[0m top5: 0.6399253731343284
[2m[36m(func pid=19406)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=19406)[0m f1_macro: 0.19165973787983465
[2m[36m(func pid=19406)[0m f1_weighted: 0.2457124887648267
[2m[36m(func pid=19406)[0m f1_per_class: [0.232, 0.192, 0.074, 0.089, 0.136, 0.116, 0.512, 0.132, 0.146, 0.289]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4652 | Steps: 2 | Val loss: 2.3412 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5903 | Steps: 2 | Val loss: 1.7942 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0468 | Steps: 2 | Val loss: 1.9695 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.3289 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=12986)[0m top1: 0.1142723880597015
[2m[36m(func pid=12986)[0m top5: 0.5205223880597015
[2m[36m(func pid=12986)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=12986)[0m f1_macro: 0.09670969407845785
[2m[36m(func pid=12986)[0m f1_weighted: 0.12551856052227486
[2m[36m(func pid=12986)[0m f1_per_class: [0.106, 0.185, 0.109, 0.122, 0.0, 0.077, 0.13, 0.1, 0.089, 0.049]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3353544776119403
[2m[36m(func pid=13733)[0m top5: 0.8773320895522388
[2m[36m(func pid=13733)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=13733)[0m f1_macro: 0.3040805105667558
[2m[36m(func pid=13733)[0m f1_weighted: 0.355132931371483
[2m[36m(func pid=13733)[0m f1_per_class: [0.374, 0.386, 0.55, 0.482, 0.076, 0.169, 0.338, 0.233, 0.197, 0.236]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:38 (running for 00:11:30.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.465 |      0.097 |                   46 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.59  |      0.304 |                   44 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.047 |      0.347 |                   23 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.192 |                   20 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.34375
[2m[36m(func pid=18554)[0m top5: 0.8805970149253731
[2m[36m(func pid=18554)[0m f1_micro: 0.34375
[2m[36m(func pid=18554)[0m f1_macro: 0.34737100192150516
[2m[36m(func pid=18554)[0m f1_weighted: 0.3611689430722538
[2m[36m(func pid=18554)[0m f1_per_class: [0.431, 0.382, 0.688, 0.515, 0.094, 0.277, 0.269, 0.27, 0.192, 0.356]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.21082089552238806
[2m[36m(func pid=19406)[0m top5: 0.6455223880597015
[2m[36m(func pid=19406)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=19406)[0m f1_macro: 0.19046678781565035
[2m[36m(func pid=19406)[0m f1_weighted: 0.22181031151984165
[2m[36m(func pid=19406)[0m f1_per_class: [0.215, 0.228, 0.095, 0.089, 0.117, 0.147, 0.393, 0.16, 0.163, 0.298]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4483 | Steps: 2 | Val loss: 2.3393 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6547 | Steps: 2 | Val loss: 1.7779 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0258 | Steps: 2 | Val loss: 1.9715 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0000 | Steps: 2 | Val loss: 20.1900 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=12986)[0m top1: 0.11567164179104478
[2m[36m(func pid=12986)[0m top5: 0.5191231343283582
[2m[36m(func pid=12986)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=12986)[0m f1_macro: 0.0993437186783688
[2m[36m(func pid=12986)[0m f1_weighted: 0.12633347642782553
[2m[36m(func pid=12986)[0m f1_per_class: [0.106, 0.186, 0.126, 0.125, 0.0, 0.077, 0.128, 0.102, 0.094, 0.05]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.33955223880597013
[2m[36m(func pid=13733)[0m top5: 0.882929104477612
[2m[36m(func pid=13733)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=13733)[0m f1_macro: 0.3089406474355624
[2m[36m(func pid=13733)[0m f1_weighted: 0.3602254783469557
[2m[36m(func pid=13733)[0m f1_per_class: [0.386, 0.374, 0.564, 0.481, 0.074, 0.166, 0.362, 0.234, 0.201, 0.248]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:43 (running for 00:11:35.97)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.448 |      0.099 |                   47 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.655 |      0.309 |                   45 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.026 |      0.351 |                   24 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.19  |                   21 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.34841417910447764
[2m[36m(func pid=18554)[0m top5: 0.8726679104477612
[2m[36m(func pid=18554)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=18554)[0m f1_macro: 0.3507100316016614
[2m[36m(func pid=18554)[0m f1_weighted: 0.3631737243895922
[2m[36m(func pid=18554)[0m f1_per_class: [0.439, 0.418, 0.688, 0.518, 0.085, 0.257, 0.257, 0.273, 0.208, 0.364]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.18097014925373134
[2m[36m(func pid=19406)[0m top5: 0.6506529850746269
[2m[36m(func pid=19406)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=19406)[0m f1_macro: 0.17804424993811238
[2m[36m(func pid=19406)[0m f1_weighted: 0.18751799965894705
[2m[36m(func pid=19406)[0m f1_per_class: [0.202, 0.25, 0.116, 0.076, 0.102, 0.146, 0.279, 0.166, 0.145, 0.298]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3748 | Steps: 2 | Val loss: 2.3330 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5836 | Steps: 2 | Val loss: 1.7898 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0441 | Steps: 2 | Val loss: 1.9500 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1834 | Steps: 2 | Val loss: 20.0281 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=12986)[0m top1: 0.11893656716417911
[2m[36m(func pid=12986)[0m top5: 0.5261194029850746
[2m[36m(func pid=12986)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=12986)[0m f1_macro: 0.09957607090620658
[2m[36m(func pid=12986)[0m f1_weighted: 0.13029043644201269
[2m[36m(func pid=12986)[0m f1_per_class: [0.103, 0.194, 0.115, 0.129, 0.0, 0.076, 0.135, 0.099, 0.087, 0.058]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.33908582089552236
[2m[36m(func pid=13733)[0m top5: 0.8773320895522388
[2m[36m(func pid=13733)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=13733)[0m f1_macro: 0.3096937926226181
[2m[36m(func pid=13733)[0m f1_weighted: 0.3608770440167699
[2m[36m(func pid=13733)[0m f1_per_class: [0.4, 0.367, 0.55, 0.482, 0.078, 0.156, 0.368, 0.242, 0.204, 0.25]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:49 (running for 00:11:41.26)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.375 |      0.1   |                   48 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.584 |      0.31  |                   46 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.044 |      0.353 |                   25 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.178 |                   22 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.35401119402985076
[2m[36m(func pid=18554)[0m top5: 0.8759328358208955
[2m[36m(func pid=18554)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=18554)[0m f1_macro: 0.35334992832841294
[2m[36m(func pid=18554)[0m f1_weighted: 0.3699260129429378
[2m[36m(func pid=18554)[0m f1_per_class: [0.44, 0.435, 0.71, 0.523, 0.081, 0.239, 0.271, 0.279, 0.211, 0.345]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.16977611940298507
[2m[36m(func pid=19406)[0m top5: 0.6571828358208955
[2m[36m(func pid=19406)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=19406)[0m f1_macro: 0.1733256673743045
[2m[36m(func pid=19406)[0m f1_weighted: 0.16994202988061477
[2m[36m(func pid=19406)[0m f1_per_class: [0.19, 0.252, 0.144, 0.109, 0.092, 0.144, 0.187, 0.177, 0.146, 0.292]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3881 | Steps: 2 | Val loss: 2.3304 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4994 | Steps: 2 | Val loss: 1.7892 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0327 | Steps: 2 | Val loss: 18.8098 | Batch size: 32 | lr: 0.1 | Duration: 2.55s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0358 | Steps: 2 | Val loss: 1.9301 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=12986)[0m top1: 0.11940298507462686
[2m[36m(func pid=12986)[0m top5: 0.527518656716418
[2m[36m(func pid=12986)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=12986)[0m f1_macro: 0.10231876185305486
[2m[36m(func pid=12986)[0m f1_weighted: 0.1288642706654851
[2m[36m(func pid=12986)[0m f1_per_class: [0.109, 0.194, 0.131, 0.126, 0.0, 0.077, 0.13, 0.102, 0.088, 0.065]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3414179104477612
[2m[36m(func pid=13733)[0m top5: 0.8773320895522388
[2m[36m(func pid=13733)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=13733)[0m f1_macro: 0.3111598579674836
[2m[36m(func pid=13733)[0m f1_weighted: 0.36316457328743923
[2m[36m(func pid=13733)[0m f1_per_class: [0.412, 0.364, 0.55, 0.49, 0.068, 0.163, 0.365, 0.249, 0.207, 0.244]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:54 (running for 00:11:46.46)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.388 |      0.102 |                   49 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.499 |      0.311 |                   47 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.044 |      0.353 |                   25 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.033 |      0.184 |                   24 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3568097014925373
[2m[36m(func pid=18554)[0m top5: 0.8815298507462687
[2m[36m(func pid=18554)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=18554)[0m f1_macro: 0.35387584028394725
[2m[36m(func pid=18554)[0m f1_weighted: 0.3734496710605901
[2m[36m(func pid=18554)[0m f1_per_class: [0.433, 0.428, 0.71, 0.529, 0.073, 0.221, 0.288, 0.283, 0.204, 0.37]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.18097014925373134
[2m[36m(func pid=19406)[0m top5: 0.6702425373134329
[2m[36m(func pid=19406)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=19406)[0m f1_macro: 0.18370970961769836
[2m[36m(func pid=19406)[0m f1_weighted: 0.18361952452807195
[2m[36m(func pid=19406)[0m f1_per_class: [0.177, 0.295, 0.16, 0.189, 0.086, 0.166, 0.121, 0.185, 0.181, 0.277]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3762 | Steps: 2 | Val loss: 2.3256 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5114 | Steps: 2 | Val loss: 1.7916 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0330 | Steps: 2 | Val loss: 1.9009 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.6794 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=12986)[0m top1: 0.12033582089552239
[2m[36m(func pid=12986)[0m top5: 0.5359141791044776
[2m[36m(func pid=12986)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=12986)[0m f1_macro: 0.10435118224701531
[2m[36m(func pid=12986)[0m f1_weighted: 0.12975506324167446
[2m[36m(func pid=12986)[0m f1_per_class: [0.109, 0.193, 0.15, 0.131, 0.0, 0.076, 0.13, 0.101, 0.087, 0.066]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.34095149253731344
[2m[36m(func pid=13733)[0m top5: 0.8777985074626866
[2m[36m(func pid=13733)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=13733)[0m f1_macro: 0.3129828797474772
[2m[36m(func pid=13733)[0m f1_weighted: 0.3631296612409272
[2m[36m(func pid=13733)[0m f1_per_class: [0.425, 0.36, 0.55, 0.491, 0.076, 0.162, 0.363, 0.264, 0.201, 0.237]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:54:59 (running for 00:11:51.53)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.376 |      0.104 |                   50 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.511 |      0.313 |                   48 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.033 |      0.357 |                   27 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.033 |      0.184 |                   24 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.36380597014925375
[2m[36m(func pid=18554)[0m top5: 0.882929104477612
[2m[36m(func pid=18554)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=18554)[0m f1_macro: 0.3565653911840728
[2m[36m(func pid=18554)[0m f1_weighted: 0.3822090423023689
[2m[36m(func pid=18554)[0m f1_per_class: [0.438, 0.426, 0.71, 0.525, 0.069, 0.223, 0.321, 0.288, 0.201, 0.366]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.20009328358208955
[2m[36m(func pid=19406)[0m top5: 0.695429104477612
[2m[36m(func pid=19406)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=19406)[0m f1_macro: 0.19654446904896342
[2m[36m(func pid=19406)[0m f1_weighted: 0.2087926015602387
[2m[36m(func pid=19406)[0m f1_per_class: [0.177, 0.314, 0.164, 0.306, 0.081, 0.177, 0.078, 0.199, 0.173, 0.297]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3797 | Steps: 2 | Val loss: 2.3227 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5078 | Steps: 2 | Val loss: 1.8094 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0701 | Steps: 2 | Val loss: 16.2123 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0247 | Steps: 2 | Val loss: 1.8770 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=12986)[0m top1: 0.12126865671641791
[2m[36m(func pid=12986)[0m top5: 0.5387126865671642
[2m[36m(func pid=12986)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=12986)[0m f1_macro: 0.10728995444472275
[2m[36m(func pid=12986)[0m f1_weighted: 0.1299462341028151
[2m[36m(func pid=12986)[0m f1_per_class: [0.109, 0.195, 0.167, 0.132, 0.0, 0.077, 0.125, 0.106, 0.095, 0.067]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3358208955223881
[2m[36m(func pid=13733)[0m top5: 0.871268656716418
[2m[36m(func pid=13733)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=13733)[0m f1_macro: 0.31261254082081813
[2m[36m(func pid=13733)[0m f1_weighted: 0.3550256468087512
[2m[36m(func pid=13733)[0m f1_per_class: [0.434, 0.362, 0.55, 0.497, 0.074, 0.166, 0.325, 0.277, 0.202, 0.239]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.2224813432835821
[2m[36m(func pid=19406)[0m top5: 0.7318097014925373
[2m[36m(func pid=19406)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=19406)[0m f1_macro: 0.2049617301504499
[2m[36m(func pid=19406)[0m f1_weighted: 0.24558738011835554
[2m[36m(func pid=19406)[0m f1_per_class: [0.164, 0.338, 0.165, 0.378, 0.079, 0.176, 0.127, 0.204, 0.116, 0.303]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:55:04 (running for 00:11:56.56)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.38  |      0.107 |                   51 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.508 |      0.313 |                   49 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.025 |      0.357 |                   28 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.07  |      0.205 |                   26 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.37080223880597013
[2m[36m(func pid=18554)[0m top5: 0.8871268656716418
[2m[36m(func pid=18554)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=18554)[0m f1_macro: 0.35747857802288985
[2m[36m(func pid=18554)[0m f1_weighted: 0.3887384986049634
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.432, 0.71, 0.541, 0.068, 0.207, 0.334, 0.266, 0.202, 0.37]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4325 | Steps: 2 | Val loss: 2.3169 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4385 | Steps: 2 | Val loss: 1.8309 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0000 | Steps: 2 | Val loss: 14.6221 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0391 | Steps: 2 | Val loss: 1.8625 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=12986)[0m top1: 0.12546641791044777
[2m[36m(func pid=12986)[0m top5: 0.5433768656716418
[2m[36m(func pid=12986)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=12986)[0m f1_macro: 0.1097497770786767
[2m[36m(func pid=12986)[0m f1_weighted: 0.13262327809719612
[2m[36m(func pid=12986)[0m f1_per_class: [0.122, 0.199, 0.162, 0.134, 0.0, 0.076, 0.129, 0.113, 0.095, 0.068]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.32509328358208955
[2m[36m(func pid=13733)[0m top5: 0.8624067164179104
[2m[36m(func pid=13733)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=13733)[0m f1_macro: 0.30483563842965344
[2m[36m(func pid=13733)[0m f1_weighted: 0.34320081221942966
[2m[36m(func pid=13733)[0m f1_per_class: [0.42, 0.347, 0.537, 0.494, 0.071, 0.166, 0.299, 0.274, 0.206, 0.236]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:55:09 (running for 00:12:01.62)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.432 |      0.11  |                   52 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.438 |      0.305 |                   50 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.025 |      0.357 |                   28 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.22  |                   27 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.2555970149253731
[2m[36m(func pid=19406)[0m top5: 0.7588619402985075
[2m[36m(func pid=19406)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=19406)[0m f1_macro: 0.2196370362975471
[2m[36m(func pid=19406)[0m f1_weighted: 0.2952709135198181
[2m[36m(func pid=19406)[0m f1_per_class: [0.152, 0.355, 0.167, 0.443, 0.077, 0.197, 0.218, 0.206, 0.091, 0.291]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m top1: 0.37593283582089554
[2m[36m(func pid=18554)[0m top5: 0.8885261194029851
[2m[36m(func pid=18554)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=18554)[0m f1_macro: 0.3563577862583346
[2m[36m(func pid=18554)[0m f1_weighted: 0.3937943122248352
[2m[36m(func pid=18554)[0m f1_per_class: [0.44, 0.437, 0.71, 0.534, 0.068, 0.19, 0.362, 0.261, 0.203, 0.357]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3688 | Steps: 2 | Val loss: 2.3123 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4669 | Steps: 2 | Val loss: 1.8465 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0000 | Steps: 2 | Val loss: 13.7849 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0251 | Steps: 2 | Val loss: 1.8417 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=12986)[0m top1: 0.12639925373134328
[2m[36m(func pid=12986)[0m top5: 0.5494402985074627
[2m[36m(func pid=12986)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=12986)[0m f1_macro: 0.11053833821381576
[2m[36m(func pid=12986)[0m f1_weighted: 0.13310922524251864
[2m[36m(func pid=12986)[0m f1_per_class: [0.127, 0.198, 0.165, 0.134, 0.0, 0.076, 0.131, 0.111, 0.093, 0.069]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3204291044776119
[2m[36m(func pid=13733)[0m top5: 0.8605410447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=13733)[0m f1_macro: 0.3017422406734478
[2m[36m(func pid=13733)[0m f1_weighted: 0.3381129331588645
[2m[36m(func pid=13733)[0m f1_per_class: [0.405, 0.341, 0.537, 0.492, 0.072, 0.166, 0.287, 0.274, 0.209, 0.234]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:55:14 (running for 00:12:06.69)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.369 |      0.111 |                   53 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.467 |      0.302 |                   51 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.039 |      0.356 |                   29 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.224 |                   28 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.27658582089552236
[2m[36m(func pid=19406)[0m top5: 0.7882462686567164
[2m[36m(func pid=19406)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=19406)[0m f1_macro: 0.22448413522450447
[2m[36m(func pid=19406)[0m f1_weighted: 0.32067718843311194
[2m[36m(func pid=19406)[0m f1_per_class: [0.153, 0.34, 0.168, 0.488, 0.074, 0.19, 0.278, 0.19, 0.06, 0.303]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m top1: 0.38152985074626866
[2m[36m(func pid=18554)[0m top5: 0.8931902985074627
[2m[36m(func pid=18554)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=18554)[0m f1_macro: 0.35841381651767235
[2m[36m(func pid=18554)[0m f1_weighted: 0.4006187595903483
[2m[36m(func pid=18554)[0m f1_per_class: [0.454, 0.434, 0.71, 0.537, 0.068, 0.187, 0.385, 0.259, 0.211, 0.34]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3477 | Steps: 2 | Val loss: 2.3066 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5490 | Steps: 2 | Val loss: 1.8535 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0028 | Steps: 2 | Val loss: 13.3558 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0298 | Steps: 2 | Val loss: 1.8255 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=12986)[0m top1: 0.13013059701492538
[2m[36m(func pid=12986)[0m top5: 0.558768656716418
[2m[36m(func pid=12986)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=12986)[0m f1_macro: 0.11455170290489633
[2m[36m(func pid=12986)[0m f1_weighted: 0.1385219520761897
[2m[36m(func pid=12986)[0m f1_per_class: [0.13, 0.201, 0.185, 0.145, 0.0, 0.076, 0.137, 0.113, 0.09, 0.068]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:55:19 (running for 00:12:11.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.348 |      0.115 |                   54 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.467 |      0.302 |                   51 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.025 |      0.358 |                   30 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.003 |      0.234 |                   29 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.30130597014925375
[2m[36m(func pid=19406)[0m top5: 0.8064365671641791
[2m[36m(func pid=19406)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=19406)[0m f1_macro: 0.233615352748547
[2m[36m(func pid=19406)[0m f1_weighted: 0.3462917410134455
[2m[36m(func pid=19406)[0m f1_per_class: [0.158, 0.34, 0.179, 0.519, 0.075, 0.197, 0.334, 0.191, 0.043, 0.3]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31763059701492535
[2m[36m(func pid=13733)[0m top5: 0.8582089552238806
[2m[36m(func pid=13733)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=13733)[0m f1_macro: 0.3002206772784194
[2m[36m(func pid=13733)[0m f1_weighted: 0.3338285802654107
[2m[36m(func pid=13733)[0m f1_per_class: [0.385, 0.349, 0.537, 0.488, 0.069, 0.163, 0.272, 0.283, 0.218, 0.239]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.386660447761194
[2m[36m(func pid=18554)[0m top5: 0.8917910447761194
[2m[36m(func pid=18554)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.3613278750788874
[2m[36m(func pid=18554)[0m f1_weighted: 0.40616080032790963
[2m[36m(func pid=18554)[0m f1_per_class: [0.471, 0.429, 0.71, 0.539, 0.07, 0.18, 0.405, 0.26, 0.222, 0.327]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3379 | Steps: 2 | Val loss: 2.3015 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2431 | Steps: 2 | Val loss: 12.5458 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4301 | Steps: 2 | Val loss: 1.8600 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0304 | Steps: 2 | Val loss: 1.8424 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 00:55:24 (running for 00:12:16.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.348 |      0.115 |                   54 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.549 |      0.3   |                   52 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.03  |      0.361 |                   31 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.243 |      0.244 |                   30 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.3278917910447761
[2m[36m(func pid=19406)[0m top5: 0.8260261194029851
[2m[36m(func pid=19406)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=19406)[0m f1_macro: 0.2442977761382466
[2m[36m(func pid=19406)[0m f1_weighted: 0.36731834609008124
[2m[36m(func pid=19406)[0m f1_per_class: [0.206, 0.344, 0.177, 0.547, 0.075, 0.21, 0.371, 0.172, 0.058, 0.283]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.13246268656716417
[2m[36m(func pid=12986)[0m top5: 0.5643656716417911
[2m[36m(func pid=12986)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=12986)[0m f1_macro: 0.11616227168230334
[2m[36m(func pid=12986)[0m f1_weighted: 0.14099246070279836
[2m[36m(func pid=12986)[0m f1_per_class: [0.13, 0.201, 0.183, 0.152, 0.0, 0.076, 0.137, 0.121, 0.09, 0.071]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31763059701492535
[2m[36m(func pid=13733)[0m top5: 0.8530783582089553
[2m[36m(func pid=13733)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=13733)[0m f1_macro: 0.29716255241367834
[2m[36m(func pid=13733)[0m f1_weighted: 0.3329974773351807
[2m[36m(func pid=13733)[0m f1_per_class: [0.368, 0.355, 0.524, 0.495, 0.068, 0.165, 0.26, 0.286, 0.212, 0.239]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3833955223880597
[2m[36m(func pid=18554)[0m top5: 0.8903917910447762
[2m[36m(func pid=18554)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=18554)[0m f1_macro: 0.355750498655775
[2m[36m(func pid=18554)[0m f1_weighted: 0.4034539902326937
[2m[36m(func pid=18554)[0m f1_per_class: [0.46, 0.433, 0.71, 0.535, 0.07, 0.181, 0.401, 0.256, 0.215, 0.298]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0002 | Steps: 2 | Val loss: 11.7777 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3062 | Steps: 2 | Val loss: 2.3003 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4437 | Steps: 2 | Val loss: 1.8540 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0211 | Steps: 2 | Val loss: 1.8591 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:55:29 (running for 00:12:22.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.338 |      0.116 |                   55 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.43  |      0.297 |                   53 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.03  |      0.356 |                   32 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.26  |                   31 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.35074626865671643
[2m[36m(func pid=19406)[0m top5: 0.8446828358208955
[2m[36m(func pid=19406)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=19406)[0m f1_macro: 0.2602448775411691
[2m[36m(func pid=19406)[0m f1_weighted: 0.3805025941248641
[2m[36m(func pid=19406)[0m f1_per_class: [0.313, 0.333, 0.176, 0.566, 0.076, 0.216, 0.397, 0.134, 0.092, 0.3]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.13152985074626866
[2m[36m(func pid=12986)[0m top5: 0.5652985074626866
[2m[36m(func pid=12986)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=12986)[0m f1_macro: 0.11572753042150948
[2m[36m(func pid=12986)[0m f1_weighted: 0.14056834888259928
[2m[36m(func pid=12986)[0m f1_per_class: [0.129, 0.201, 0.183, 0.144, 0.0, 0.076, 0.143, 0.118, 0.093, 0.07]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.31902985074626866
[2m[36m(func pid=13733)[0m top5: 0.8521455223880597
[2m[36m(func pid=13733)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=13733)[0m f1_macro: 0.30039543034098043
[2m[36m(func pid=13733)[0m f1_weighted: 0.33401400548775406
[2m[36m(func pid=13733)[0m f1_per_class: [0.383, 0.35, 0.537, 0.494, 0.067, 0.166, 0.264, 0.293, 0.21, 0.241]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3810634328358209
[2m[36m(func pid=18554)[0m top5: 0.8880597014925373
[2m[36m(func pid=18554)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=18554)[0m f1_macro: 0.3540129618925383
[2m[36m(func pid=18554)[0m f1_weighted: 0.40134668092648773
[2m[36m(func pid=18554)[0m f1_per_class: [0.477, 0.436, 0.667, 0.531, 0.07, 0.172, 0.395, 0.274, 0.217, 0.301]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0007 | Steps: 2 | Val loss: 11.3416 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3328 | Steps: 2 | Val loss: 2.2976 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4641 | Steps: 2 | Val loss: 1.8493 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0258 | Steps: 2 | Val loss: 1.8657 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 00:55:34 (running for 00:12:27.15)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.306 |      0.116 |                   56 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.444 |      0.3   |                   54 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.021 |      0.354 |                   33 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.001 |      0.273 |                   32 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.3614738805970149
[2m[36m(func pid=19406)[0m top5: 0.8540111940298507
[2m[36m(func pid=19406)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=19406)[0m f1_macro: 0.27266286861925015
[2m[36m(func pid=19406)[0m f1_weighted: 0.38866242525383543
[2m[36m(func pid=19406)[0m f1_per_class: [0.389, 0.353, 0.182, 0.567, 0.077, 0.208, 0.412, 0.089, 0.15, 0.3]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.13432835820895522
[2m[36m(func pid=12986)[0m top5: 0.5690298507462687
[2m[36m(func pid=12986)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=12986)[0m f1_macro: 0.1170400317501455
[2m[36m(func pid=12986)[0m f1_weighted: 0.14444836792034635
[2m[36m(func pid=12986)[0m f1_per_class: [0.129, 0.201, 0.179, 0.159, 0.0, 0.076, 0.141, 0.122, 0.094, 0.07]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3199626865671642
[2m[36m(func pid=13733)[0m top5: 0.8572761194029851
[2m[36m(func pid=13733)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=13733)[0m f1_macro: 0.3004815015899496
[2m[36m(func pid=13733)[0m f1_weighted: 0.33666361149546614
[2m[36m(func pid=13733)[0m f1_per_class: [0.385, 0.349, 0.537, 0.5, 0.067, 0.162, 0.271, 0.282, 0.205, 0.246]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.37593283582089554
[2m[36m(func pid=18554)[0m top5: 0.8857276119402985
[2m[36m(func pid=18554)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=18554)[0m f1_macro: 0.35285369342542733
[2m[36m(func pid=18554)[0m f1_weighted: 0.39650911192520827
[2m[36m(func pid=18554)[0m f1_per_class: [0.468, 0.424, 0.688, 0.526, 0.075, 0.183, 0.388, 0.264, 0.218, 0.294]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0518 | Steps: 2 | Val loss: 11.3137 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3071 | Steps: 2 | Val loss: 2.2920 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4148 | Steps: 2 | Val loss: 1.8480 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0223 | Steps: 2 | Val loss: 1.8882 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:55:39 (running for 00:12:32.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.333 |      0.117 |                   57 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.464 |      0.3   |                   55 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.026 |      0.353 |                   34 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.052 |      0.284 |                   33 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=19406)[0m top1: 0.3614738805970149
[2m[36m(func pid=19406)[0m top5: 0.8535447761194029
[2m[36m(func pid=19406)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=19406)[0m f1_macro: 0.28426848103289576
[2m[36m(func pid=19406)[0m f1_weighted: 0.3893971117888236
[2m[36m(func pid=19406)[0m f1_per_class: [0.49, 0.333, 0.178, 0.569, 0.076, 0.202, 0.409, 0.141, 0.162, 0.283]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m top1: 0.14039179104477612
[2m[36m(func pid=12986)[0m top5: 0.5750932835820896
[2m[36m(func pid=12986)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=12986)[0m f1_macro: 0.12175878497810558
[2m[36m(func pid=12986)[0m f1_weighted: 0.15272168398883523
[2m[36m(func pid=12986)[0m f1_per_class: [0.131, 0.209, 0.189, 0.172, 0.0, 0.077, 0.153, 0.113, 0.098, 0.077]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3185634328358209
[2m[36m(func pid=13733)[0m top5: 0.8596082089552238
[2m[36m(func pid=13733)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=13733)[0m f1_macro: 0.3008713449731613
[2m[36m(func pid=13733)[0m f1_weighted: 0.33528072200276604
[2m[36m(func pid=13733)[0m f1_per_class: [0.387, 0.351, 0.55, 0.5, 0.064, 0.162, 0.266, 0.288, 0.196, 0.246]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3675373134328358
[2m[36m(func pid=18554)[0m top5: 0.8833955223880597
[2m[36m(func pid=18554)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=18554)[0m f1_macro: 0.35050714643809844
[2m[36m(func pid=18554)[0m f1_weighted: 0.3878762862974011
[2m[36m(func pid=18554)[0m f1_per_class: [0.457, 0.426, 0.71, 0.521, 0.069, 0.186, 0.362, 0.257, 0.225, 0.292]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0003 | Steps: 2 | Val loss: 11.5137 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.3275 | Steps: 2 | Val loss: 2.2918 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4420 | Steps: 2 | Val loss: 1.8439 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0214 | Steps: 2 | Val loss: 1.9080 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=19406)[0m top1: 0.3516791044776119
[2m[36m(func pid=19406)[0m top5: 0.8530783582089553
[2m[36m(func pid=19406)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=19406)[0m f1_macro: 0.2987559840483526
[2m[36m(func pid=19406)[0m f1_weighted: 0.37858158743347065
[2m[36m(func pid=19406)[0m f1_per_class: [0.574, 0.326, 0.178, 0.572, 0.073, 0.202, 0.349, 0.223, 0.189, 0.303]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:55:46 (running for 00:12:38.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.328 |      0.121 |                   59 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.415 |      0.301 |                   56 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.022 |      0.351 |                   35 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.299 |                   34 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.13899253731343283
[2m[36m(func pid=12986)[0m top5: 0.5816231343283582
[2m[36m(func pid=12986)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=12986)[0m f1_macro: 0.12136626506583059
[2m[36m(func pid=12986)[0m f1_weighted: 0.15056687978067523
[2m[36m(func pid=12986)[0m f1_per_class: [0.131, 0.205, 0.19, 0.175, 0.0, 0.076, 0.145, 0.114, 0.1, 0.078]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3185634328358209
[2m[36m(func pid=13733)[0m top5: 0.8628731343283582
[2m[36m(func pid=13733)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=13733)[0m f1_macro: 0.3015394725887163
[2m[36m(func pid=13733)[0m f1_weighted: 0.3358526778042795
[2m[36m(func pid=13733)[0m f1_per_class: [0.389, 0.345, 0.55, 0.496, 0.063, 0.17, 0.272, 0.282, 0.201, 0.248]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.363339552238806
[2m[36m(func pid=18554)[0m top5: 0.8810634328358209
[2m[36m(func pid=18554)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=18554)[0m f1_macro: 0.34705241746970883
[2m[36m(func pid=18554)[0m f1_weighted: 0.38474174895307717
[2m[36m(func pid=18554)[0m f1_per_class: [0.43, 0.425, 0.71, 0.511, 0.069, 0.192, 0.361, 0.263, 0.214, 0.294]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.7520 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3583 | Steps: 2 | Val loss: 2.2888 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3545 | Steps: 2 | Val loss: 1.8436 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0219 | Steps: 2 | Val loss: 1.9374 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=19406)[0m top1: 0.33722014925373134
[2m[36m(func pid=19406)[0m top5: 0.8521455223880597
[2m[36m(func pid=19406)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=19406)[0m f1_macro: 0.29784141132544434
[2m[36m(func pid=19406)[0m f1_weighted: 0.35908866008749446
[2m[36m(func pid=19406)[0m f1_per_class: [0.619, 0.332, 0.188, 0.583, 0.071, 0.169, 0.274, 0.241, 0.196, 0.306]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:55:51 (running for 00:12:43.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.358 |      0.123 |                   60 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.442 |      0.302 |                   57 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.021 |      0.347 |                   36 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.298 |                   35 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14132462686567165
[2m[36m(func pid=12986)[0m top5: 0.5816231343283582
[2m[36m(func pid=12986)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=12986)[0m f1_macro: 0.12281507910678066
[2m[36m(func pid=12986)[0m f1_weighted: 0.1532550982747794
[2m[36m(func pid=12986)[0m f1_per_class: [0.13, 0.208, 0.19, 0.18, 0.0, 0.077, 0.146, 0.117, 0.11, 0.07]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3162313432835821
[2m[36m(func pid=13733)[0m top5: 0.8610074626865671
[2m[36m(func pid=13733)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=13733)[0m f1_macro: 0.30073255665086157
[2m[36m(func pid=13733)[0m f1_weighted: 0.33106164833191637
[2m[36m(func pid=13733)[0m f1_per_class: [0.373, 0.339, 0.564, 0.5, 0.065, 0.179, 0.253, 0.284, 0.199, 0.252]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3558768656716418
[2m[36m(func pid=18554)[0m top5: 0.8745335820895522
[2m[36m(func pid=18554)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=18554)[0m f1_macro: 0.3421088537362099
[2m[36m(func pid=18554)[0m f1_weighted: 0.375764307382567
[2m[36m(func pid=18554)[0m f1_per_class: [0.425, 0.419, 0.688, 0.5, 0.073, 0.2, 0.341, 0.263, 0.223, 0.288]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 12.1347 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.2211 | Steps: 2 | Val loss: 2.2871 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3927 | Steps: 2 | Val loss: 1.8311 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0184 | Steps: 2 | Val loss: 1.9617 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=19406)[0m top1: 0.324160447761194
[2m[36m(func pid=19406)[0m top5: 0.8456156716417911
[2m[36m(func pid=19406)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=19406)[0m f1_macro: 0.28683281547207723
[2m[36m(func pid=19406)[0m f1_weighted: 0.34341131715062173
[2m[36m(func pid=19406)[0m f1_per_class: [0.558, 0.318, 0.195, 0.576, 0.071, 0.168, 0.242, 0.233, 0.202, 0.306]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:55:56 (running for 00:12:48.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.358 |      0.123 |                   60 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.393 |      0.305 |                   59 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.022 |      0.342 |                   37 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.287 |                   36 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14272388059701493
[2m[36m(func pid=12986)[0m top5: 0.5830223880597015
[2m[36m(func pid=12986)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=12986)[0m f1_macro: 0.12343390960507764
[2m[36m(func pid=12986)[0m f1_weighted: 0.1544165570409995
[2m[36m(func pid=12986)[0m f1_per_class: [0.125, 0.21, 0.192, 0.183, 0.0, 0.081, 0.144, 0.122, 0.104, 0.073]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.324160447761194
[2m[36m(func pid=13733)[0m top5: 0.8628731343283582
[2m[36m(func pid=13733)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=13733)[0m f1_macro: 0.3054361157132677
[2m[36m(func pid=13733)[0m f1_weighted: 0.3393388166571587
[2m[36m(func pid=13733)[0m f1_per_class: [0.362, 0.351, 0.564, 0.507, 0.066, 0.184, 0.265, 0.286, 0.198, 0.272]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3498134328358209
[2m[36m(func pid=18554)[0m top5: 0.8703358208955224
[2m[36m(func pid=18554)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=18554)[0m f1_macro: 0.3362824561269021
[2m[36m(func pid=18554)[0m f1_weighted: 0.37058877638164883
[2m[36m(func pid=18554)[0m f1_per_class: [0.416, 0.414, 0.688, 0.499, 0.074, 0.21, 0.328, 0.257, 0.216, 0.261]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1053 | Steps: 2 | Val loss: 12.3262 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2457 | Steps: 2 | Val loss: 2.2804 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4497 | Steps: 2 | Val loss: 1.8258 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0234 | Steps: 2 | Val loss: 1.9879 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=19406)[0m top1: 0.32136194029850745
[2m[36m(func pid=19406)[0m top5: 0.8423507462686567
[2m[36m(func pid=19406)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=19406)[0m f1_macro: 0.29081354992586367
[2m[36m(func pid=19406)[0m f1_weighted: 0.34067234302593274
[2m[36m(func pid=19406)[0m f1_per_class: [0.56, 0.383, 0.215, 0.575, 0.072, 0.16, 0.198, 0.229, 0.207, 0.309]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:56:01 (running for 00:12:54.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.246 |      0.128 |                   62 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.393 |      0.305 |                   59 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.018 |      0.336 |                   38 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.105 |      0.291 |                   37 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14598880597014927
[2m[36m(func pid=12986)[0m top5: 0.5904850746268657
[2m[36m(func pid=12986)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=12986)[0m f1_macro: 0.12769031774043052
[2m[36m(func pid=12986)[0m f1_weighted: 0.15662119676404707
[2m[36m(func pid=12986)[0m f1_per_class: [0.125, 0.212, 0.204, 0.187, 0.012, 0.08, 0.145, 0.126, 0.109, 0.077]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3278917910447761
[2m[36m(func pid=13733)[0m top5: 0.8638059701492538
[2m[36m(func pid=13733)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=13733)[0m f1_macro: 0.307052949411325
[2m[36m(func pid=13733)[0m f1_weighted: 0.3400462223000809
[2m[36m(func pid=13733)[0m f1_per_class: [0.362, 0.364, 0.564, 0.507, 0.072, 0.188, 0.257, 0.288, 0.205, 0.264]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34468283582089554
[2m[36m(func pid=18554)[0m top5: 0.8680037313432836
[2m[36m(func pid=18554)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=18554)[0m f1_macro: 0.33395725376586083
[2m[36m(func pid=18554)[0m f1_weighted: 0.36526254566645827
[2m[36m(func pid=18554)[0m f1_per_class: [0.409, 0.412, 0.688, 0.494, 0.073, 0.215, 0.314, 0.262, 0.219, 0.254]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0131 | Steps: 2 | Val loss: 12.7553 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2177 | Steps: 2 | Val loss: 2.2825 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3868 | Steps: 2 | Val loss: 1.8181 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=19406)[0m top1: 0.3069029850746269
[2m[36m(func pid=19406)[0m top5: 0.8236940298507462
[2m[36m(func pid=19406)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=19406)[0m f1_macro: 0.28404502870731574
[2m[36m(func pid=19406)[0m f1_weighted: 0.32280992389413093
[2m[36m(func pid=19406)[0m f1_per_class: [0.471, 0.434, 0.23, 0.519, 0.076, 0.167, 0.162, 0.236, 0.206, 0.34]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0310 | Steps: 2 | Val loss: 1.9958 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 00:56:06 (running for 00:12:59.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.218 |      0.126 |                   63 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.45  |      0.307 |                   60 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.334 |                   39 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.013 |      0.284 |                   38 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14458955223880596
[2m[36m(func pid=12986)[0m top5: 0.5890858208955224
[2m[36m(func pid=12986)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=12986)[0m f1_macro: 0.12603539600637398
[2m[36m(func pid=12986)[0m f1_weighted: 0.15456961709533848
[2m[36m(func pid=12986)[0m f1_per_class: [0.124, 0.215, 0.187, 0.174, 0.011, 0.082, 0.148, 0.124, 0.115, 0.081]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.32975746268656714
[2m[36m(func pid=13733)[0m top5: 0.8689365671641791
[2m[36m(func pid=13733)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=13733)[0m f1_macro: 0.31301547315440437
[2m[36m(func pid=13733)[0m f1_weighted: 0.3454464873169578
[2m[36m(func pid=13733)[0m f1_per_class: [0.379, 0.371, 0.579, 0.5, 0.069, 0.191, 0.274, 0.288, 0.216, 0.264]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34328358208955223
[2m[36m(func pid=18554)[0m top5: 0.867070895522388
[2m[36m(func pid=18554)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=18554)[0m f1_macro: 0.33429565237948483
[2m[36m(func pid=18554)[0m f1_weighted: 0.364018021471874
[2m[36m(func pid=18554)[0m f1_per_class: [0.418, 0.414, 0.688, 0.498, 0.073, 0.211, 0.306, 0.257, 0.222, 0.256]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 13.4615 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2723 | Steps: 2 | Val loss: 2.2786 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=19406)[0m top1: 0.29011194029850745
[2m[36m(func pid=19406)[0m top5: 0.8073694029850746
[2m[36m(func pid=19406)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=19406)[0m f1_macro: 0.278105694794914
[2m[36m(func pid=19406)[0m f1_weighted: 0.2972489635002419
[2m[36m(func pid=19406)[0m f1_per_class: [0.5, 0.443, 0.245, 0.451, 0.077, 0.163, 0.134, 0.236, 0.207, 0.326]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4387 | Steps: 2 | Val loss: 1.7907 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0197 | Steps: 2 | Val loss: 2.0152 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 00:56:12 (running for 00:13:04.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.272 |      0.126 |                   64 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.387 |      0.313 |                   61 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.031 |      0.334 |                   40 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.278 |                   39 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14412313432835822
[2m[36m(func pid=12986)[0m top5: 0.5909514925373134
[2m[36m(func pid=12986)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=12986)[0m f1_macro: 0.12623874320091946
[2m[36m(func pid=12986)[0m f1_weighted: 0.15368125946689837
[2m[36m(func pid=12986)[0m f1_per_class: [0.132, 0.21, 0.185, 0.175, 0.011, 0.081, 0.146, 0.123, 0.115, 0.083]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34328358208955223
[2m[36m(func pid=18554)[0m top5: 0.8666044776119403
[2m[36m(func pid=18554)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=18554)[0m f1_macro: 0.33484112496966495
[2m[36m(func pid=18554)[0m f1_weighted: 0.36555802709009705
[2m[36m(func pid=18554)[0m f1_per_class: [0.409, 0.415, 0.688, 0.501, 0.073, 0.218, 0.308, 0.251, 0.213, 0.274]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 14.3697 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=13733)[0m top1: 0.3376865671641791
[2m[36m(func pid=13733)[0m top5: 0.8805970149253731
[2m[36m(func pid=13733)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=13733)[0m f1_macro: 0.31741796743301043
[2m[36m(func pid=13733)[0m f1_weighted: 0.3572087576363832
[2m[36m(func pid=13733)[0m f1_per_class: [0.379, 0.362, 0.595, 0.501, 0.071, 0.201, 0.318, 0.266, 0.212, 0.269]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=19406)[0m top1: 0.279384328358209
[2m[36m(func pid=19406)[0m top5: 0.7961753731343284
[2m[36m(func pid=19406)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=19406)[0m f1_macro: 0.275454055370513
[2m[36m(func pid=19406)[0m f1_weighted: 0.27524502936657985
[2m[36m(func pid=19406)[0m f1_per_class: [0.529, 0.454, 0.263, 0.394, 0.059, 0.165, 0.103, 0.238, 0.208, 0.341]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2091 | Steps: 2 | Val loss: 2.2757 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0202 | Steps: 2 | Val loss: 2.0436 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3743 | Steps: 2 | Val loss: 1.7743 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 00:56:17 (running for 00:13:09.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.209 |      0.126 |                   65 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.439 |      0.317 |                   62 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.02  |      0.335 |                   41 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.275 |                   40 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14412313432835822
[2m[36m(func pid=12986)[0m top5: 0.5923507462686567
[2m[36m(func pid=12986)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=12986)[0m f1_macro: 0.12630865600751778
[2m[36m(func pid=12986)[0m f1_weighted: 0.15403267580050461
[2m[36m(func pid=12986)[0m f1_per_class: [0.134, 0.208, 0.185, 0.175, 0.011, 0.081, 0.148, 0.123, 0.115, 0.082]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 15.1031 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=13733)[0m top1: 0.34468283582089554
[2m[36m(func pid=13733)[0m top5: 0.8871268656716418
[2m[36m(func pid=13733)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=13733)[0m f1_macro: 0.3219418147826526
[2m[36m(func pid=13733)[0m f1_weighted: 0.36371600840244
[2m[36m(func pid=13733)[0m f1_per_class: [0.398, 0.377, 0.595, 0.507, 0.077, 0.201, 0.325, 0.26, 0.205, 0.275]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.33955223880597013
[2m[36m(func pid=18554)[0m top5: 0.8642723880597015
[2m[36m(func pid=18554)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=18554)[0m f1_macro: 0.3349438544451615
[2m[36m(func pid=18554)[0m f1_weighted: 0.36084222010588124
[2m[36m(func pid=18554)[0m f1_per_class: [0.402, 0.416, 0.688, 0.491, 0.074, 0.217, 0.299, 0.256, 0.219, 0.288]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.2733208955223881
[2m[36m(func pid=19406)[0m top5: 0.7905783582089553
[2m[36m(func pid=19406)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=19406)[0m f1_macro: 0.2721276346292
[2m[36m(func pid=19406)[0m f1_weighted: 0.2544157094129154
[2m[36m(func pid=19406)[0m f1_per_class: [0.507, 0.457, 0.283, 0.338, 0.062, 0.162, 0.082, 0.252, 0.214, 0.364]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2397 | Steps: 2 | Val loss: 2.2749 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3366 | Steps: 2 | Val loss: 1.7739 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0292 | Steps: 2 | Val loss: 2.0478 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.1355 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 00:56:22 (running for 00:13:15.16)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.24  |      0.128 |                   66 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.374 |      0.322 |                   63 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.02  |      0.335 |                   42 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.272 |                   41 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.1455223880597015
[2m[36m(func pid=12986)[0m top5: 0.5890858208955224
[2m[36m(func pid=12986)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=12986)[0m f1_macro: 0.12844772798803888
[2m[36m(func pid=12986)[0m f1_weighted: 0.15565822706607793
[2m[36m(func pid=12986)[0m f1_per_class: [0.135, 0.212, 0.196, 0.185, 0.011, 0.086, 0.14, 0.126, 0.116, 0.078]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.34701492537313433
[2m[36m(func pid=13733)[0m top5: 0.8852611940298507
[2m[36m(func pid=13733)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=13733)[0m f1_macro: 0.32249319295939005
[2m[36m(func pid=13733)[0m f1_weighted: 0.36576168455446856
[2m[36m(func pid=13733)[0m f1_per_class: [0.398, 0.385, 0.564, 0.502, 0.079, 0.212, 0.327, 0.265, 0.204, 0.289]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3400186567164179
[2m[36m(func pid=18554)[0m top5: 0.8642723880597015
[2m[36m(func pid=18554)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=18554)[0m f1_macro: 0.3357767069952372
[2m[36m(func pid=18554)[0m f1_weighted: 0.36504481985070747
[2m[36m(func pid=18554)[0m f1_per_class: [0.4, 0.405, 0.71, 0.494, 0.071, 0.212, 0.32, 0.254, 0.212, 0.281]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.26492537313432835
[2m[36m(func pid=19406)[0m top5: 0.7807835820895522
[2m[36m(func pid=19406)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=19406)[0m f1_macro: 0.2656449993260338
[2m[36m(func pid=19406)[0m f1_weighted: 0.23350036967015486
[2m[36m(func pid=19406)[0m f1_per_class: [0.462, 0.451, 0.306, 0.28, 0.063, 0.163, 0.071, 0.257, 0.211, 0.394]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2283 | Steps: 2 | Val loss: 2.2715 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3721 | Steps: 2 | Val loss: 1.7666 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0154 | Steps: 2 | Val loss: 2.0638 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0002 | Steps: 2 | Val loss: 16.9033 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 00:56:28 (running for 00:13:20.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.228 |      0.13  |                   67 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.337 |      0.322 |                   64 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.029 |      0.336 |                   43 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.266 |                   42 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.14692164179104478
[2m[36m(func pid=12986)[0m top5: 0.5932835820895522
[2m[36m(func pid=12986)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=12986)[0m f1_macro: 0.12970343830590805
[2m[36m(func pid=12986)[0m f1_weighted: 0.1577131701332748
[2m[36m(func pid=12986)[0m f1_per_class: [0.134, 0.211, 0.198, 0.188, 0.011, 0.09, 0.142, 0.128, 0.116, 0.079]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.34794776119402987
[2m[36m(func pid=13733)[0m top5: 0.886660447761194
[2m[36m(func pid=13733)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=13733)[0m f1_macro: 0.31950035615717276
[2m[36m(func pid=13733)[0m f1_weighted: 0.36722984581508644
[2m[36m(func pid=13733)[0m f1_per_class: [0.4, 0.387, 0.564, 0.508, 0.071, 0.201, 0.333, 0.256, 0.204, 0.272]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34048507462686567
[2m[36m(func pid=18554)[0m top5: 0.8647388059701493
[2m[36m(func pid=18554)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=18554)[0m f1_macro: 0.33617785145322454
[2m[36m(func pid=18554)[0m f1_weighted: 0.3644922336822395
[2m[36m(func pid=18554)[0m f1_per_class: [0.392, 0.404, 0.71, 0.506, 0.074, 0.221, 0.305, 0.261, 0.2, 0.291]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.25513059701492535
[2m[36m(func pid=19406)[0m top5: 0.7765858208955224
[2m[36m(func pid=19406)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=19406)[0m f1_macro: 0.25918547697648614
[2m[36m(func pid=19406)[0m f1_weighted: 0.2103475634298522
[2m[36m(func pid=19406)[0m f1_per_class: [0.438, 0.438, 0.329, 0.221, 0.064, 0.154, 0.057, 0.264, 0.221, 0.406]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1857 | Steps: 2 | Val loss: 2.2659 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4204 | Steps: 2 | Val loss: 1.7674 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0258 | Steps: 2 | Val loss: 2.0632 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0101 | Steps: 2 | Val loss: 17.4038 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=12986)[0m top1: 0.14738805970149255
[2m[36m(func pid=12986)[0m top5: 0.6012126865671642
[2m[36m(func pid=12986)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=12986)[0m f1_macro: 0.12944304086310499
[2m[36m(func pid=12986)[0m f1_weighted: 0.15921955722800243
[2m[36m(func pid=12986)[0m f1_per_class: [0.13, 0.213, 0.202, 0.19, 0.011, 0.091, 0.146, 0.125, 0.108, 0.079]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:56:33 (running for 00:13:25.64)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.186 |      0.129 |                   68 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.42  |      0.323 |                   66 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.015 |      0.336 |                   44 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.259 |                   43 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.35027985074626866
[2m[36m(func pid=13733)[0m top5: 0.8843283582089553
[2m[36m(func pid=13733)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=13733)[0m f1_macro: 0.32319562546822234
[2m[36m(func pid=13733)[0m f1_weighted: 0.36814374930687993
[2m[36m(func pid=13733)[0m f1_per_class: [0.429, 0.4, 0.564, 0.505, 0.07, 0.186, 0.333, 0.259, 0.22, 0.267]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34048507462686567
[2m[36m(func pid=18554)[0m top5: 0.8647388059701493
[2m[36m(func pid=18554)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=18554)[0m f1_macro: 0.33399824326659316
[2m[36m(func pid=18554)[0m f1_weighted: 0.36463049486855265
[2m[36m(func pid=18554)[0m f1_per_class: [0.387, 0.394, 0.688, 0.508, 0.076, 0.233, 0.305, 0.263, 0.193, 0.294]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.24953358208955223
[2m[36m(func pid=19406)[0m top5: 0.773320895522388
[2m[36m(func pid=19406)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=19406)[0m f1_macro: 0.24970098566140372
[2m[36m(func pid=19406)[0m f1_weighted: 0.19898737024205865
[2m[36m(func pid=19406)[0m f1_per_class: [0.413, 0.426, 0.321, 0.186, 0.068, 0.149, 0.062, 0.266, 0.223, 0.382]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1691 | Steps: 2 | Val loss: 2.2627 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3643 | Steps: 2 | Val loss: 1.7700 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0233 | Steps: 2 | Val loss: 2.0423 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0019 | Steps: 2 | Val loss: 17.7100 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=12986)[0m top1: 0.15065298507462688
[2m[36m(func pid=12986)[0m top5: 0.6044776119402985
[2m[36m(func pid=12986)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=12986)[0m f1_macro: 0.1315080528507572
[2m[36m(func pid=12986)[0m f1_weighted: 0.1640055595702692
[2m[36m(func pid=12986)[0m f1_per_class: [0.132, 0.213, 0.196, 0.193, 0.011, 0.091, 0.159, 0.122, 0.109, 0.089]
[2m[36m(func pid=12986)[0m 
== Status ==
Current time: 2024-01-07 00:56:38 (running for 00:13:30.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.169 |      0.132 |                   69 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.364 |      0.325 |                   67 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.026 |      0.334 |                   45 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.01  |      0.25  |                   44 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=13733)[0m top1: 0.3512126865671642

[2m[36m(func pid=13733)[0m top5: 0.8861940298507462
[2m[36m(func pid=13733)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=13733)[0m f1_macro: 0.3246487314719544
[2m[36m(func pid=13733)[0m f1_weighted: 0.3663713577655904
[2m[36m(func pid=13733)[0m f1_per_class: [0.431, 0.402, 0.564, 0.51, 0.072, 0.184, 0.317, 0.281, 0.219, 0.267]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3493470149253731
[2m[36m(func pid=18554)[0m top5: 0.867070895522388
[2m[36m(func pid=18554)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=18554)[0m f1_macro: 0.3375385242006869
[2m[36m(func pid=18554)[0m f1_weighted: 0.3765101024503034
[2m[36m(func pid=18554)[0m f1_per_class: [0.378, 0.396, 0.688, 0.513, 0.074, 0.236, 0.337, 0.267, 0.195, 0.291]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.2453358208955224
[2m[36m(func pid=19406)[0m top5: 0.7737873134328358
[2m[36m(func pid=19406)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=19406)[0m f1_macro: 0.2413824612702204
[2m[36m(func pid=19406)[0m f1_weighted: 0.19050384409189053
[2m[36m(func pid=19406)[0m f1_per_class: [0.4, 0.415, 0.31, 0.162, 0.07, 0.131, 0.07, 0.272, 0.22, 0.364]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1947 | Steps: 2 | Val loss: 2.2598 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3516 | Steps: 2 | Val loss: 1.7684 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0247 | Steps: 2 | Val loss: 2.0277 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0002 | Steps: 2 | Val loss: 17.7896 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:56:43 (running for 00:13:35.97)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.195 |      0.136 |                   70 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.364 |      0.325 |                   67 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.338 |                   46 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.002 |      0.241 |                   45 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.15438432835820895
[2m[36m(func pid=12986)[0m top5: 0.6063432835820896
[2m[36m(func pid=12986)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=12986)[0m f1_macro: 0.1356140808012883
[2m[36m(func pid=12986)[0m f1_weighted: 0.16791117058177019
[2m[36m(func pid=12986)[0m f1_per_class: [0.141, 0.21, 0.206, 0.209, 0.011, 0.091, 0.157, 0.123, 0.119, 0.09]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3493470149253731
[2m[36m(func pid=13733)[0m top5: 0.882929104477612
[2m[36m(func pid=13733)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=13733)[0m f1_macro: 0.32339297347392104
[2m[36m(func pid=13733)[0m f1_weighted: 0.3646533650554826
[2m[36m(func pid=13733)[0m f1_per_class: [0.436, 0.398, 0.55, 0.505, 0.084, 0.18, 0.321, 0.274, 0.213, 0.272]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.34888059701492535
[2m[36m(func pid=18554)[0m top5: 0.8689365671641791
[2m[36m(func pid=18554)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=18554)[0m f1_macro: 0.339304723390392
[2m[36m(func pid=18554)[0m f1_weighted: 0.37617604212387695
[2m[36m(func pid=18554)[0m f1_per_class: [0.384, 0.385, 0.71, 0.513, 0.074, 0.222, 0.347, 0.255, 0.203, 0.299]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.24720149253731344
[2m[36m(func pid=19406)[0m top5: 0.7742537313432836
[2m[36m(func pid=19406)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=19406)[0m f1_macro: 0.24465606763094594
[2m[36m(func pid=19406)[0m f1_weighted: 0.19333180375293751
[2m[36m(func pid=19406)[0m f1_per_class: [0.418, 0.41, 0.313, 0.149, 0.073, 0.14, 0.092, 0.263, 0.22, 0.369]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2123 | Steps: 2 | Val loss: 2.2539 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3383 | Steps: 2 | Val loss: 1.7752 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0284 | Steps: 2 | Val loss: 1.9795 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.7616 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 00:56:48 (running for 00:13:41.21)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.212 |      0.136 |                   71 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.352 |      0.323 |                   68 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.025 |      0.339 |                   47 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.245 |                   46 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.15578358208955223
[2m[36m(func pid=12986)[0m top5: 0.6128731343283582
[2m[36m(func pid=12986)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=12986)[0m f1_macro: 0.1356651988793975
[2m[36m(func pid=12986)[0m f1_weighted: 0.16902015373677032
[2m[36m(func pid=12986)[0m f1_per_class: [0.145, 0.216, 0.208, 0.208, 0.01, 0.089, 0.159, 0.129, 0.101, 0.091]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.34468283582089554
[2m[36m(func pid=13733)[0m top5: 0.8857276119402985
[2m[36m(func pid=13733)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=13733)[0m f1_macro: 0.3245288160674006
[2m[36m(func pid=13733)[0m f1_weighted: 0.35873942448659996
[2m[36m(func pid=13733)[0m f1_per_class: [0.44, 0.393, 0.564, 0.509, 0.081, 0.181, 0.297, 0.283, 0.214, 0.283]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3591417910447761
[2m[36m(func pid=18554)[0m top5: 0.8717350746268657
[2m[36m(func pid=18554)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=18554)[0m f1_macro: 0.3482418883005739
[2m[36m(func pid=18554)[0m f1_weighted: 0.3876595380732052
[2m[36m(func pid=18554)[0m f1_per_class: [0.398, 0.393, 0.733, 0.514, 0.074, 0.228, 0.379, 0.238, 0.211, 0.314]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.25093283582089554
[2m[36m(func pid=19406)[0m top5: 0.7765858208955224
[2m[36m(func pid=19406)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=19406)[0m f1_macro: 0.24653513627120266
[2m[36m(func pid=19406)[0m f1_weighted: 0.20226358152565874
[2m[36m(func pid=19406)[0m f1_per_class: [0.441, 0.408, 0.292, 0.141, 0.073, 0.141, 0.128, 0.267, 0.225, 0.349]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3258 | Steps: 2 | Val loss: 1.7763 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1999 | Steps: 2 | Val loss: 2.2440 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0360 | Steps: 2 | Val loss: 1.9352 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0004 | Steps: 2 | Val loss: 17.7636 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=13733)[0m top1: 0.3474813432835821
[2m[36m(func pid=13733)[0m top5: 0.8861940298507462
[2m[36m(func pid=13733)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=13733)[0m f1_macro: 0.32856573142452816
[2m[36m(func pid=13733)[0m f1_weighted: 0.3614277742910303
[2m[36m(func pid=13733)[0m f1_per_class: [0.449, 0.397, 0.564, 0.518, 0.084, 0.2, 0.287, 0.283, 0.221, 0.283]
[2m[36m(func pid=13733)[0m 
== Status ==
Current time: 2024-01-07 00:56:54 (running for 00:13:46.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.212 |      0.136 |                   71 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.326 |      0.329 |                   70 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.028 |      0.348 |                   48 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.247 |                   47 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.16044776119402984
[2m[36m(func pid=12986)[0m top5: 0.6268656716417911
[2m[36m(func pid=12986)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=12986)[0m f1_macro: 0.1388867292790685
[2m[36m(func pid=12986)[0m f1_weighted: 0.17539825144661325
[2m[36m(func pid=12986)[0m f1_per_class: [0.146, 0.221, 0.215, 0.221, 0.01, 0.09, 0.165, 0.125, 0.102, 0.093]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=18554)[0m top1: 0.37453358208955223
[2m[36m(func pid=18554)[0m top5: 0.8768656716417911
[2m[36m(func pid=18554)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=18554)[0m f1_macro: 0.35105301513279064
[2m[36m(func pid=18554)[0m f1_weighted: 0.40206966132498206
[2m[36m(func pid=18554)[0m f1_per_class: [0.4, 0.413, 0.688, 0.524, 0.076, 0.23, 0.404, 0.249, 0.21, 0.317]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.2513992537313433
[2m[36m(func pid=19406)[0m top5: 0.7793843283582089
[2m[36m(func pid=19406)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=19406)[0m f1_macro: 0.24381317569379482
[2m[36m(func pid=19406)[0m f1_weighted: 0.2044050510495608
[2m[36m(func pid=19406)[0m f1_per_class: [0.418, 0.407, 0.289, 0.141, 0.057, 0.145, 0.136, 0.272, 0.225, 0.349]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1271 | Steps: 2 | Val loss: 2.2415 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3243 | Steps: 2 | Val loss: 1.7792 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0237 | Steps: 2 | Val loss: 1.8968 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0003 | Steps: 2 | Val loss: 17.8894 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
== Status ==
Current time: 2024-01-07 00:56:59 (running for 00:13:51.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.2   |      0.139 |                   72 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.324 |      0.325 |                   71 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.036 |      0.351 |                   49 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.244 |                   48 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.16044776119402984
[2m[36m(func pid=12986)[0m top5: 0.6296641791044776
[2m[36m(func pid=12986)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=12986)[0m f1_macro: 0.13890519791959197
[2m[36m(func pid=12986)[0m f1_weighted: 0.1763407227168709
[2m[36m(func pid=12986)[0m f1_per_class: [0.144, 0.211, 0.22, 0.224, 0.011, 0.09, 0.172, 0.125, 0.1, 0.093]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3423507462686567
[2m[36m(func pid=13733)[0m top5: 0.8852611940298507
[2m[36m(func pid=13733)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=13733)[0m f1_macro: 0.3248480776751925
[2m[36m(func pid=13733)[0m f1_weighted: 0.3545760794776995
[2m[36m(func pid=13733)[0m f1_per_class: [0.436, 0.387, 0.564, 0.518, 0.086, 0.2, 0.271, 0.285, 0.22, 0.283]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.38152985074626866
[2m[36m(func pid=18554)[0m top5: 0.8857276119402985
[2m[36m(func pid=18554)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=18554)[0m f1_macro: 0.359155946057341
[2m[36m(func pid=18554)[0m f1_weighted: 0.4081290766655061
[2m[36m(func pid=18554)[0m f1_per_class: [0.423, 0.422, 0.71, 0.525, 0.074, 0.223, 0.419, 0.252, 0.209, 0.337]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.24953358208955223
[2m[36m(func pid=19406)[0m top5: 0.7779850746268657
[2m[36m(func pid=19406)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=19406)[0m f1_macro: 0.2413023303159001
[2m[36m(func pid=19406)[0m f1_weighted: 0.1997116782845579
[2m[36m(func pid=19406)[0m f1_per_class: [0.381, 0.404, 0.302, 0.121, 0.059, 0.149, 0.141, 0.272, 0.23, 0.355]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1716 | Steps: 2 | Val loss: 2.2412 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2937 | Steps: 2 | Val loss: 1.7786 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0184 | Steps: 2 | Val loss: 1.8666 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0039 | Steps: 2 | Val loss: 18.0028 | Batch size: 32 | lr: 0.1 | Duration: 2.55s
== Status ==
Current time: 2024-01-07 00:57:04 (running for 00:13:56.99)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3665
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00004 | RUNNING    | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.172 |      0.141 |                   74 |
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.324 |      0.325 |                   71 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.024 |      0.359 |                   50 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.241 |                   49 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.16184701492537312
[2m[36m(func pid=12986)[0m top5: 0.6301305970149254
[2m[36m(func pid=12986)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=12986)[0m f1_macro: 0.14060350912107653
[2m[36m(func pid=12986)[0m f1_weighted: 0.17722247536215793
[2m[36m(func pid=12986)[0m f1_per_class: [0.144, 0.214, 0.225, 0.222, 0.011, 0.09, 0.173, 0.128, 0.106, 0.092]
[2m[36m(func pid=12986)[0m 
[2m[36m(func pid=13733)[0m top1: 0.3460820895522388
[2m[36m(func pid=13733)[0m top5: 0.8833955223880597
[2m[36m(func pid=13733)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=13733)[0m f1_macro: 0.3271202030836791
[2m[36m(func pid=13733)[0m f1_weighted: 0.35873014120511837
[2m[36m(func pid=13733)[0m f1_per_class: [0.446, 0.395, 0.564, 0.519, 0.088, 0.198, 0.279, 0.284, 0.22, 0.278]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.38759328358208955
[2m[36m(func pid=18554)[0m top5: 0.8913246268656716
[2m[36m(func pid=18554)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=18554)[0m f1_macro: 0.3625449730085449
[2m[36m(func pid=18554)[0m f1_weighted: 0.4134730685605116
[2m[36m(func pid=18554)[0m f1_per_class: [0.43, 0.426, 0.688, 0.529, 0.076, 0.231, 0.425, 0.262, 0.199, 0.36]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.25093283582089554
[2m[36m(func pid=19406)[0m top5: 0.7817164179104478
[2m[36m(func pid=19406)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=19406)[0m f1_macro: 0.2449441664019953
[2m[36m(func pid=19406)[0m f1_weighted: 0.20051021379114123
[2m[36m(func pid=19406)[0m f1_per_class: [0.424, 0.402, 0.31, 0.123, 0.06, 0.142, 0.141, 0.277, 0.239, 0.333]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=12986)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1086 | Steps: 2 | Val loss: 2.2378 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3315 | Steps: 2 | Val loss: 1.7770 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0239 | Steps: 2 | Val loss: 1.8456 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4719 | Steps: 2 | Val loss: 17.1498 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 00:57:09 (running for 00:14:02.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.355
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.294 |      0.327 |                   72 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.018 |      0.363 |                   51 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.004 |      0.245 |                   50 |
| train_57e67_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12986)[0m top1: 0.16184701492537312
[2m[36m(func pid=12986)[0m top5: 0.6371268656716418
[2m[36m(func pid=12986)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=12986)[0m f1_macro: 0.14073036990618012
[2m[36m(func pid=12986)[0m f1_weighted: 0.17691575410543672
[2m[36m(func pid=12986)[0m f1_per_class: [0.14, 0.21, 0.227, 0.225, 0.011, 0.095, 0.171, 0.128, 0.107, 0.093]
[2m[36m(func pid=13733)[0m top1: 0.34375
[2m[36m(func pid=13733)[0m top5: 0.8824626865671642
[2m[36m(func pid=13733)[0m f1_micro: 0.34375
[2m[36m(func pid=13733)[0m f1_macro: 0.3239272463251858
[2m[36m(func pid=13733)[0m f1_weighted: 0.35875698129292666
[2m[36m(func pid=13733)[0m f1_per_class: [0.425, 0.383, 0.564, 0.52, 0.084, 0.2, 0.286, 0.281, 0.226, 0.27]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3908582089552239
[2m[36m(func pid=18554)[0m top5: 0.894589552238806
[2m[36m(func pid=18554)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=18554)[0m f1_macro: 0.3625155327980806
[2m[36m(func pid=18554)[0m f1_weighted: 0.41648203456962657
[2m[36m(func pid=18554)[0m f1_per_class: [0.424, 0.424, 0.688, 0.53, 0.078, 0.233, 0.436, 0.262, 0.192, 0.36]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.25886194029850745
[2m[36m(func pid=19406)[0m top5: 0.7821828358208955
[2m[36m(func pid=19406)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=19406)[0m f1_macro: 0.2588340413774102
[2m[36m(func pid=19406)[0m f1_weighted: 0.21739089692947702
[2m[36m(func pid=19406)[0m f1_per_class: [0.462, 0.412, 0.289, 0.151, 0.067, 0.15, 0.16, 0.261, 0.242, 0.394]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2853 | Steps: 2 | Val loss: 1.7866 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0201 | Steps: 2 | Val loss: 1.8545 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0001 | Steps: 2 | Val loss: 15.8259 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=13733)[0m top1: 0.34095149253731344
[2m[36m(func pid=13733)[0m top5: 0.8791977611940298
[2m[36m(func pid=13733)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=13733)[0m f1_macro: 0.3219873070454352
[2m[36m(func pid=13733)[0m f1_weighted: 0.35751715271067047
[2m[36m(func pid=13733)[0m f1_per_class: [0.435, 0.373, 0.564, 0.515, 0.079, 0.192, 0.294, 0.293, 0.219, 0.256]
[2m[36m(func pid=13733)[0m 
[2m[36m(func pid=18554)[0m top1: 0.3871268656716418
[2m[36m(func pid=18554)[0m top5: 0.8917910447761194
[2m[36m(func pid=18554)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=18554)[0m f1_macro: 0.3612777302604478
[2m[36m(func pid=18554)[0m f1_weighted: 0.4121855218815739
[2m[36m(func pid=18554)[0m f1_per_class: [0.43, 0.421, 0.688, 0.525, 0.08, 0.228, 0.43, 0.251, 0.201, 0.36]
[2m[36m(func pid=19406)[0m top1: 0.261660447761194
[2m[36m(func pid=19406)[0m top5: 0.7747201492537313
[2m[36m(func pid=19406)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=19406)[0m f1_macro: 0.2622804125824496
[2m[36m(func pid=19406)[0m f1_weighted: 0.23677671432160285
[2m[36m(func pid=19406)[0m f1_per_class: [0.516, 0.42, 0.283, 0.192, 0.063, 0.165, 0.183, 0.239, 0.208, 0.354]
[2m[36m(func pid=13733)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2647 | Steps: 2 | Val loss: 1.7972 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 00:57:15 (running for 00:14:07.56)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00005 | RUNNING    | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.285 |      0.322 |                   74 |
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.024 |      0.363 |                   52 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.472 |      0.259 |                   51 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=30734)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=30734)[0m Configuration completed!
[2m[36m(func pid=30734)[0m New optimizer parameters:
[2m[36m(func pid=30734)[0m SGD (
[2m[36m(func pid=30734)[0m Parameter Group 0
[2m[36m(func pid=30734)[0m     dampening: 0
[2m[36m(func pid=30734)[0m     differentiable: False
[2m[36m(func pid=30734)[0m     foreach: None
[2m[36m(func pid=30734)[0m     lr: 0.0001
[2m[36m(func pid=30734)[0m     maximize: False
[2m[36m(func pid=30734)[0m     momentum: 0.99
[2m[36m(func pid=30734)[0m     nesterov: False
[2m[36m(func pid=30734)[0m     weight_decay: 0.0001
[2m[36m(func pid=30734)[0m )
[2m[36m(func pid=30734)[0m 
== Status ==
Current time: 2024-01-07 00:57:20 (running for 00:14:12.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.02  |      0.361 |                   53 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.262 |                   52 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=13733)[0m top1: 0.33861940298507465
[2m[36m(func pid=13733)[0m top5: 0.8754664179104478
[2m[36m(func pid=13733)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=13733)[0m f1_macro: 0.31963797384838794
[2m[36m(func pid=13733)[0m f1_weighted: 0.3562842582369767
[2m[36m(func pid=13733)[0m f1_per_class: [0.427, 0.372, 0.564, 0.514, 0.076, 0.192, 0.292, 0.292, 0.22, 0.248]
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 15.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0162 | Steps: 2 | Val loss: 1.8661 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1916 | Steps: 2 | Val loss: 2.5168 | Batch size: 32 | lr: 0.0001 | Duration: 4.38s
[2m[36m(func pid=19406)[0m top1: 0.25652985074626866
[2m[36m(func pid=19406)[0m top5: 0.7588619402985075
[2m[36m(func pid=19406)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=19406)[0m f1_macro: 0.24791164884446096
[2m[36m(func pid=19406)[0m f1_weighted: 0.24361116084334977
[2m[36m(func pid=19406)[0m f1_per_class: [0.407, 0.423, 0.257, 0.218, 0.065, 0.162, 0.192, 0.23, 0.203, 0.322]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m top1: 0.38526119402985076
[2m[36m(func pid=18554)[0m top5: 0.8885261194029851
[2m[36m(func pid=18554)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=18554)[0m f1_macro: 0.36223576117506867
[2m[36m(func pid=18554)[0m f1_weighted: 0.41069563788660796
[2m[36m(func pid=18554)[0m f1_per_class: [0.426, 0.421, 0.71, 0.531, 0.079, 0.223, 0.422, 0.241, 0.214, 0.356]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m top1: 0.06669776119402986
[2m[36m(func pid=30734)[0m top5: 0.48507462686567165
[2m[36m(func pid=30734)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=30734)[0m f1_macro: 0.03980817400671628
[2m[36m(func pid=30734)[0m f1_weighted: 0.03808352694851962
[2m[36m(func pid=30734)[0m f1_per_class: [0.118, 0.01, 0.0, 0.088, 0.0, 0.019, 0.0, 0.104, 0.022, 0.037]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 15.2300 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0172 | Steps: 2 | Val loss: 1.8875 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1743 | Steps: 2 | Val loss: 2.5311 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=19406)[0m top1: 0.25
[2m[36m(func pid=19406)[0m top5: 0.7527985074626866
[2m[36m(func pid=19406)[0m f1_micro: 0.25
[2m[36m(func pid=19406)[0m f1_macro: 0.23567048393226617
[2m[36m(func pid=19406)[0m f1_weighted: 0.24876349125946792
[2m[36m(func pid=19406)[0m f1_per_class: [0.284, 0.419, 0.25, 0.252, 0.074, 0.149, 0.195, 0.226, 0.191, 0.316]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:57:28 (running for 00:14:20.62)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.016 |      0.362 |                   54 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.236 |                   54 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.192 |      0.04  |                    1 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=31420)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=31420)[0m Configuration completed!
[2m[36m(func pid=31420)[0m New optimizer parameters:
[2m[36m(func pid=31420)[0m SGD (
[2m[36m(func pid=31420)[0m Parameter Group 0
[2m[36m(func pid=31420)[0m     dampening: 0
[2m[36m(func pid=31420)[0m     differentiable: False
[2m[36m(func pid=31420)[0m     foreach: None
[2m[36m(func pid=31420)[0m     lr: 0.001
[2m[36m(func pid=31420)[0m     maximize: False
[2m[36m(func pid=31420)[0m     momentum: 0.99
[2m[36m(func pid=31420)[0m     nesterov: False
[2m[36m(func pid=31420)[0m     weight_decay: 0.0001
[2m[36m(func pid=31420)[0m )
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=18554)[0m top1: 0.37826492537313433
[2m[36m(func pid=18554)[0m top5: 0.8861940298507462
[2m[36m(func pid=18554)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=18554)[0m f1_macro: 0.356453991564345
[2m[36m(func pid=18554)[0m f1_weighted: 0.40342324337173474
[2m[36m(func pid=18554)[0m f1_per_class: [0.44, 0.423, 0.688, 0.53, 0.077, 0.215, 0.402, 0.246, 0.196, 0.348]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m top1: 0.06343283582089553
[2m[36m(func pid=30734)[0m top5: 0.4864738805970149
[2m[36m(func pid=30734)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=30734)[0m f1_macro: 0.03603181548163
[2m[36m(func pid=30734)[0m f1_weighted: 0.03455430623475303
[2m[36m(func pid=30734)[0m f1_per_class: [0.074, 0.01, 0.0, 0.076, 0.0, 0.025, 0.0, 0.102, 0.022, 0.05]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 15.4161 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0158 | Steps: 2 | Val loss: 1.9045 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0981 | Steps: 2 | Val loss: 2.4944 | Batch size: 32 | lr: 0.001 | Duration: 4.33s
[2m[36m(func pid=19406)[0m top1: 0.2453358208955224
[2m[36m(func pid=19406)[0m top5: 0.7411380597014925
[2m[36m(func pid=19406)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=19406)[0m f1_macro: 0.22989326621482958
[2m[36m(func pid=19406)[0m f1_weighted: 0.25532050223427843
[2m[36m(func pid=19406)[0m f1_per_class: [0.227, 0.412, 0.243, 0.274, 0.074, 0.157, 0.207, 0.213, 0.173, 0.319]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1285 | Steps: 2 | Val loss: 2.5444 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 00:57:34 (running for 00:14:26.57)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.016 |      0.353 |                   56 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.23  |                   55 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.174 |      0.036 |                    2 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.373134328358209
[2m[36m(func pid=18554)[0m top5: 0.8861940298507462
[2m[36m(func pid=18554)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=18554)[0m f1_macro: 0.35308285277210416
[2m[36m(func pid=18554)[0m f1_weighted: 0.3977096526803285
[2m[36m(func pid=18554)[0m f1_per_class: [0.443, 0.419, 0.688, 0.529, 0.079, 0.221, 0.386, 0.24, 0.19, 0.337]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.06669776119402986
[2m[36m(func pid=31420)[0m top5: 0.48134328358208955
[2m[36m(func pid=31420)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=31420)[0m f1_macro: 0.042417863715155754
[2m[36m(func pid=31420)[0m f1_weighted: 0.04142388314152964
[2m[36m(func pid=31420)[0m f1_per_class: [0.137, 0.01, 0.0, 0.098, 0.0, 0.023, 0.0, 0.102, 0.022, 0.033]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.06063432835820896
[2m[36m(func pid=30734)[0m top5: 0.47574626865671643
[2m[36m(func pid=30734)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=30734)[0m f1_macro: 0.03066927883021208
[2m[36m(func pid=30734)[0m f1_weighted: 0.03438820038149541
[2m[36m(func pid=30734)[0m f1_per_class: [0.054, 0.014, 0.0, 0.081, 0.0, 0.019, 0.0, 0.099, 0.0, 0.04]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 15.7112 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0345 | Steps: 2 | Val loss: 1.9272 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0529 | Steps: 2 | Val loss: 2.4703 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=19406)[0m top1: 0.23600746268656717
[2m[36m(func pid=19406)[0m top5: 0.738339552238806
[2m[36m(func pid=19406)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=19406)[0m f1_macro: 0.22220110764626394
[2m[36m(func pid=19406)[0m f1_weighted: 0.2535247723040002
[2m[36m(func pid=19406)[0m f1_per_class: [0.181, 0.407, 0.236, 0.278, 0.075, 0.15, 0.207, 0.207, 0.174, 0.306]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0935 | Steps: 2 | Val loss: 2.5445 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 00:57:39 (running for 00:14:31.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.034 |      0.355 |                   57 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.222 |                   56 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.128 |      0.031 |                    3 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  3.098 |      0.042 |                    1 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3675373134328358
[2m[36m(func pid=18554)[0m top5: 0.8857276119402985
[2m[36m(func pid=18554)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=18554)[0m f1_macro: 0.35540678331046627
[2m[36m(func pid=18554)[0m f1_weighted: 0.3913488083817735
[2m[36m(func pid=18554)[0m f1_per_class: [0.448, 0.422, 0.71, 0.527, 0.076, 0.202, 0.367, 0.249, 0.209, 0.344]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.06576492537313433
[2m[36m(func pid=31420)[0m top5: 0.46408582089552236
[2m[36m(func pid=31420)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=31420)[0m f1_macro: 0.046695400049501075
[2m[36m(func pid=31420)[0m f1_weighted: 0.04884243355006911
[2m[36m(func pid=31420)[0m f1_per_class: [0.085, 0.053, 0.0, 0.098, 0.0, 0.021, 0.0, 0.096, 0.056, 0.058]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.0625
[2m[36m(func pid=30734)[0m top5: 0.47527985074626866
[2m[36m(func pid=30734)[0m f1_micro: 0.0625
[2m[36m(func pid=30734)[0m f1_macro: 0.03589420629235972
[2m[36m(func pid=30734)[0m f1_weighted: 0.04064708962942546
[2m[36m(func pid=30734)[0m f1_per_class: [0.049, 0.032, 0.0, 0.09, 0.0, 0.019, 0.0, 0.096, 0.024, 0.048]
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.1442 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0149 | Steps: 2 | Val loss: 1.9554 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9477 | Steps: 2 | Val loss: 2.4326 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=19406)[0m top1: 0.228544776119403
[2m[36m(func pid=19406)[0m top5: 0.7327425373134329
[2m[36m(func pid=19406)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=19406)[0m f1_macro: 0.22053655004265144
[2m[36m(func pid=19406)[0m f1_weighted: 0.25183568980378557
[2m[36m(func pid=19406)[0m f1_per_class: [0.155, 0.391, 0.255, 0.283, 0.072, 0.154, 0.206, 0.207, 0.176, 0.306]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0465 | Steps: 2 | Val loss: 2.5438 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 00:57:44 (running for 00:14:37.10)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.015 |      0.349 |                   58 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.221 |                   57 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.093 |      0.036 |                    4 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  3.053 |      0.047 |                    2 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.363339552238806
[2m[36m(func pid=18554)[0m top5: 0.8805970149253731
[2m[36m(func pid=18554)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=18554)[0m f1_macro: 0.34879789769366465
[2m[36m(func pid=18554)[0m f1_weighted: 0.38629734424342604
[2m[36m(func pid=18554)[0m f1_per_class: [0.451, 0.414, 0.688, 0.526, 0.075, 0.203, 0.353, 0.264, 0.208, 0.305]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.06763059701492537
[2m[36m(func pid=31420)[0m top5: 0.45475746268656714
[2m[36m(func pid=31420)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=31420)[0m f1_macro: 0.05468603297004247
[2m[36m(func pid=31420)[0m f1_weighted: 0.0649162535054018
[2m[36m(func pid=31420)[0m f1_per_class: [0.079, 0.111, 0.0, 0.102, 0.0, 0.034, 0.012, 0.084, 0.084, 0.04]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.2360 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=30734)[0m top1: 0.06529850746268656
[2m[36m(func pid=30734)[0m top5: 0.470615671641791
[2m[36m(func pid=30734)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=30734)[0m f1_macro: 0.039374242176855874
[2m[36m(func pid=30734)[0m f1_weighted: 0.04896623832032499
[2m[36m(func pid=30734)[0m f1_per_class: [0.044, 0.067, 0.0, 0.099, 0.0, 0.018, 0.0, 0.094, 0.025, 0.047]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0385 | Steps: 2 | Val loss: 1.9941 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8361 | Steps: 2 | Val loss: 2.3948 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=19406)[0m top1: 0.22388059701492538
[2m[36m(func pid=19406)[0m top5: 0.7332089552238806
[2m[36m(func pid=19406)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=19406)[0m f1_macro: 0.2192485325798917
[2m[36m(func pid=19406)[0m f1_weighted: 0.2524037018524127
[2m[36m(func pid=19406)[0m f1_per_class: [0.139, 0.37, 0.277, 0.312, 0.073, 0.154, 0.196, 0.203, 0.161, 0.309]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0583 | Steps: 2 | Val loss: 2.5348 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 00:57:50 (running for 00:14:42.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.039 |      0.338 |                   59 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.219 |                   58 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.047 |      0.039 |                    5 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.948 |      0.055 |                    3 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3582089552238806
[2m[36m(func pid=18554)[0m top5: 0.8773320895522388
[2m[36m(func pid=18554)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=18554)[0m f1_macro: 0.33848982000892264
[2m[36m(func pid=18554)[0m f1_weighted: 0.3783019823832972
[2m[36m(func pid=18554)[0m f1_per_class: [0.448, 0.415, 0.647, 0.534, 0.072, 0.198, 0.323, 0.263, 0.208, 0.277]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.07369402985074627
[2m[36m(func pid=31420)[0m top5: 0.458955223880597
[2m[36m(func pid=31420)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=31420)[0m f1_macro: 0.07074692555964752
[2m[36m(func pid=31420)[0m f1_weighted: 0.07847589004429897
[2m[36m(func pid=31420)[0m f1_per_class: [0.097, 0.121, 0.071, 0.108, 0.0, 0.041, 0.038, 0.086, 0.104, 0.04]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.4899 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=30734)[0m top1: 0.06576492537313433
[2m[36m(func pid=30734)[0m top5: 0.46548507462686567
[2m[36m(func pid=30734)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=30734)[0m f1_macro: 0.04226860173005753
[2m[36m(func pid=30734)[0m f1_weighted: 0.05415615362818584
[2m[36m(func pid=30734)[0m f1_per_class: [0.055, 0.082, 0.0, 0.106, 0.0, 0.024, 0.0, 0.09, 0.025, 0.041]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0149 | Steps: 2 | Val loss: 1.9964 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6720 | Steps: 2 | Val loss: 2.3687 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=19406)[0m top1: 0.2196828358208955
[2m[36m(func pid=19406)[0m top5: 0.7308768656716418
[2m[36m(func pid=19406)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=19406)[0m f1_macro: 0.21822382720465897
[2m[36m(func pid=19406)[0m f1_weighted: 0.25136318764567017
[2m[36m(func pid=19406)[0m f1_per_class: [0.125, 0.359, 0.295, 0.318, 0.073, 0.148, 0.196, 0.208, 0.154, 0.306]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0074 | Steps: 2 | Val loss: 2.5177 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 00:57:55 (running for 00:14:47.32)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.015 |      0.34  |                   60 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.218 |                   59 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.058 |      0.042 |                    6 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.836 |      0.071 |                    4 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3591417910447761
[2m[36m(func pid=18554)[0m top5: 0.8773320895522388
[2m[36m(func pid=18554)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=18554)[0m f1_macro: 0.33977791330778306
[2m[36m(func pid=18554)[0m f1_weighted: 0.3792018721143129
[2m[36m(func pid=18554)[0m f1_per_class: [0.448, 0.399, 0.667, 0.549, 0.077, 0.197, 0.322, 0.26, 0.221, 0.259]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.09375
[2m[36m(func pid=31420)[0m top5: 0.49347014925373134
[2m[36m(func pid=31420)[0m f1_micro: 0.09375
[2m[36m(func pid=31420)[0m f1_macro: 0.08029139417667658
[2m[36m(func pid=31420)[0m f1_weighted: 0.10329301838360398
[2m[36m(func pid=31420)[0m f1_per_class: [0.094, 0.111, 0.085, 0.091, 0.016, 0.069, 0.14, 0.041, 0.106, 0.049]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.6868 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=30734)[0m top1: 0.07042910447761194
[2m[36m(func pid=30734)[0m top5: 0.45848880597014924
[2m[36m(func pid=30734)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.04651433947590865
[2m[36m(func pid=30734)[0m f1_weighted: 0.06183932513112162
[2m[36m(func pid=30734)[0m f1_per_class: [0.042, 0.093, 0.0, 0.12, 0.0, 0.028, 0.003, 0.096, 0.046, 0.038]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0149 | Steps: 2 | Val loss: 2.0111 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6032 | Steps: 2 | Val loss: 2.3416 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19406)[0m top1: 0.2126865671641791
[2m[36m(func pid=19406)[0m top5: 0.7276119402985075
[2m[36m(func pid=19406)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=19406)[0m f1_macro: 0.2211983483563699
[2m[36m(func pid=19406)[0m f1_weighted: 0.24708788924838637
[2m[36m(func pid=19406)[0m f1_per_class: [0.117, 0.33, 0.371, 0.323, 0.072, 0.132, 0.2, 0.202, 0.15, 0.316]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:58:00 (running for 00:14:52.38)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.015 |      0.338 |                   61 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.221 |                   60 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  3.007 |      0.047 |                    7 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.672 |      0.08  |                    5 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3591417910447761
[2m[36m(func pid=18554)[0m top5: 0.8740671641791045
[2m[36m(func pid=18554)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=18554)[0m f1_macro: 0.33788623781858307
[2m[36m(func pid=18554)[0m f1_weighted: 0.37730592418571174
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.377, 0.688, 0.568, 0.075, 0.195, 0.311, 0.26, 0.217, 0.242]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9448 | Steps: 2 | Val loss: 2.5015 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=31420)[0m top1: 0.12826492537313433
[2m[36m(func pid=31420)[0m top5: 0.5219216417910447
[2m[36m(func pid=31420)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=31420)[0m f1_macro: 0.08862397403824854
[2m[36m(func pid=31420)[0m f1_weighted: 0.12944523156254814
[2m[36m(func pid=31420)[0m f1_per_class: [0.097, 0.112, 0.089, 0.058, 0.022, 0.069, 0.266, 0.0, 0.105, 0.068]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9304 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=30734)[0m top1: 0.06949626865671642
[2m[36m(func pid=30734)[0m top5: 0.4454291044776119
[2m[36m(func pid=30734)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=30734)[0m f1_macro: 0.04674299807366764
[2m[36m(func pid=30734)[0m f1_weighted: 0.06484883173668252
[2m[36m(func pid=30734)[0m f1_per_class: [0.048, 0.1, 0.0, 0.13, 0.0, 0.021, 0.003, 0.094, 0.041, 0.03]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0141 | Steps: 2 | Val loss: 2.0312 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5144 | Steps: 2 | Val loss: 2.3256 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=19406)[0m top1: 0.20988805970149255
[2m[36m(func pid=19406)[0m top5: 0.7257462686567164
[2m[36m(func pid=19406)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=19406)[0m f1_macro: 0.22136891967373798
[2m[36m(func pid=19406)[0m f1_weighted: 0.24426480409316675
[2m[36m(func pid=19406)[0m f1_per_class: [0.113, 0.322, 0.371, 0.318, 0.074, 0.142, 0.195, 0.203, 0.152, 0.323]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:58:05 (running for 00:14:57.72)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.014 |      0.336 |                   62 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.221 |                   61 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.945 |      0.047 |                    8 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.603 |      0.089 |                    6 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3582089552238806
[2m[36m(func pid=18554)[0m top5: 0.8722014925373134
[2m[36m(func pid=18554)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=18554)[0m f1_macro: 0.3364238776121665
[2m[36m(func pid=18554)[0m f1_weighted: 0.3729270698566274
[2m[36m(func pid=18554)[0m f1_per_class: [0.462, 0.368, 0.688, 0.569, 0.083, 0.19, 0.301, 0.277, 0.197, 0.23]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9431 | Steps: 2 | Val loss: 2.4850 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=31420)[0m top1: 0.15625
[2m[36m(func pid=31420)[0m top5: 0.5452425373134329
[2m[36m(func pid=31420)[0m f1_micro: 0.15625
[2m[36m(func pid=31420)[0m f1_macro: 0.09431474460114905
[2m[36m(func pid=31420)[0m f1_weighted: 0.1434224687038826
[2m[36m(func pid=31420)[0m f1_per_class: [0.116, 0.101, 0.088, 0.038, 0.016, 0.087, 0.33, 0.0, 0.099, 0.067]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.2172 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=30734)[0m top1: 0.07369402985074627
[2m[36m(func pid=30734)[0m top5: 0.4375
[2m[36m(func pid=30734)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=30734)[0m f1_macro: 0.054342764784349386
[2m[36m(func pid=30734)[0m f1_weighted: 0.07340249045835458
[2m[36m(func pid=30734)[0m f1_per_class: [0.052, 0.113, 0.0, 0.135, 0.0, 0.047, 0.006, 0.092, 0.07, 0.028]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0269 | Steps: 2 | Val loss: 2.0413 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4069 | Steps: 2 | Val loss: 2.2719 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=19406)[0m top1: 0.20755597014925373
[2m[36m(func pid=19406)[0m top5: 0.7248134328358209
[2m[36m(func pid=19406)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=19406)[0m f1_macro: 0.21825545772842947
[2m[36m(func pid=19406)[0m f1_weighted: 0.242707459719258
[2m[36m(func pid=19406)[0m f1_per_class: [0.112, 0.321, 0.361, 0.319, 0.07, 0.143, 0.189, 0.211, 0.151, 0.304]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:58:10 (running for 00:15:02.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.332 |                   63 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.218 |                   62 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.943 |      0.054 |                    9 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.514 |      0.094 |                    7 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.355410447761194
[2m[36m(func pid=18554)[0m top5: 0.8703358208955224
[2m[36m(func pid=18554)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.3319442758557096
[2m[36m(func pid=18554)[0m f1_weighted: 0.36849236107317146
[2m[36m(func pid=18554)[0m f1_per_class: [0.451, 0.347, 0.667, 0.566, 0.087, 0.194, 0.299, 0.274, 0.217, 0.218]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9179 | Steps: 2 | Val loss: 2.4663 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=31420)[0m top1: 0.18796641791044777
[2m[36m(func pid=31420)[0m top5: 0.6007462686567164
[2m[36m(func pid=31420)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=31420)[0m f1_macro: 0.10486033069212801
[2m[36m(func pid=31420)[0m f1_weighted: 0.1561630194903724
[2m[36m(func pid=31420)[0m f1_per_class: [0.141, 0.09, 0.112, 0.035, 0.021, 0.081, 0.381, 0.0, 0.101, 0.087]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.2656 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=30734)[0m top1: 0.0732276119402985
[2m[36m(func pid=30734)[0m top5: 0.4351679104477612
[2m[36m(func pid=30734)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=30734)[0m f1_macro: 0.05468598746587452
[2m[36m(func pid=30734)[0m f1_weighted: 0.07699308981124117
[2m[36m(func pid=30734)[0m f1_per_class: [0.05, 0.124, 0.0, 0.144, 0.0, 0.049, 0.006, 0.078, 0.072, 0.025]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0229 | Steps: 2 | Val loss: 2.0501 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2512 | Steps: 2 | Val loss: 2.2074 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=19406)[0m top1: 0.20522388059701493
[2m[36m(func pid=19406)[0m top5: 0.7238805970149254
[2m[36m(func pid=19406)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=19406)[0m f1_macro: 0.21728863903876566
[2m[36m(func pid=19406)[0m f1_weighted: 0.24053486476996047
[2m[36m(func pid=19406)[0m f1_per_class: [0.11, 0.313, 0.361, 0.319, 0.071, 0.143, 0.187, 0.207, 0.15, 0.311]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:58:15 (running for 00:15:08.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.332 |                   64 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.217 |                   63 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.918 |      0.055 |                   10 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.407 |      0.105 |                    8 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.35634328358208955
[2m[36m(func pid=18554)[0m top5: 0.8698694029850746
[2m[36m(func pid=18554)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=18554)[0m f1_macro: 0.33236532308955014
[2m[36m(func pid=18554)[0m f1_weighted: 0.3677427120862289
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.341, 0.688, 0.568, 0.078, 0.186, 0.3, 0.283, 0.22, 0.215]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8730 | Steps: 2 | Val loss: 2.4517 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=31420)[0m top1: 0.21688432835820895
[2m[36m(func pid=31420)[0m top5: 0.6515858208955224
[2m[36m(func pid=31420)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=31420)[0m f1_macro: 0.12723567774673233
[2m[36m(func pid=31420)[0m f1_weighted: 0.18885461722515567
[2m[36m(func pid=31420)[0m f1_per_class: [0.148, 0.129, 0.159, 0.087, 0.034, 0.091, 0.411, 0.0, 0.12, 0.091]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0071 | Steps: 2 | Val loss: 17.4415 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=30734)[0m top1: 0.07136194029850747
[2m[36m(func pid=30734)[0m top5: 0.4319029850746269
[2m[36m(func pid=30734)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=30734)[0m f1_macro: 0.05437161408053104
[2m[36m(func pid=30734)[0m f1_weighted: 0.07699753886968691
[2m[36m(func pid=30734)[0m f1_per_class: [0.037, 0.12, 0.0, 0.138, 0.0, 0.047, 0.015, 0.071, 0.085, 0.031]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.0420 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0654 | Steps: 2 | Val loss: 2.1161 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=19406)[0m top1: 0.20569029850746268
[2m[36m(func pid=19406)[0m top5: 0.7201492537313433
[2m[36m(func pid=19406)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=19406)[0m f1_macro: 0.21627669847449277
[2m[36m(func pid=19406)[0m f1_weighted: 0.24218418028928002
[2m[36m(func pid=19406)[0m f1_per_class: [0.108, 0.313, 0.351, 0.318, 0.073, 0.142, 0.194, 0.213, 0.15, 0.301]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m top1: 0.35774253731343286
[2m[36m(func pid=18554)[0m top5: 0.8703358208955224
[2m[36m(func pid=18554)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=18554)[0m f1_macro: 0.33084486483140807
[2m[36m(func pid=18554)[0m f1_weighted: 0.371116493185768
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.342, 0.667, 0.568, 0.085, 0.192, 0.312, 0.279, 0.195, 0.223]
[2m[36m(func pid=18554)[0m 
== Status ==
Current time: 2024-01-07 00:58:21 (running for 00:15:13.26)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.022 |      0.331 |                   65 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0.007 |      0.216 |                   64 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.873 |      0.054 |                   11 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.251 |      0.127 |                    9 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8308 | Steps: 2 | Val loss: 2.4412 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=31420)[0m top1: 0.2355410447761194
[2m[36m(func pid=31420)[0m top5: 0.7388059701492538
[2m[36m(func pid=31420)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=31420)[0m f1_macro: 0.1620470004664289
[2m[36m(func pid=31420)[0m f1_weighted: 0.22763609093175213
[2m[36m(func pid=31420)[0m f1_per_class: [0.182, 0.193, 0.224, 0.186, 0.026, 0.101, 0.399, 0.0, 0.144, 0.164]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.0470 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=30734)[0m top1: 0.07276119402985075
[2m[36m(func pid=30734)[0m top5: 0.43097014925373134
[2m[36m(func pid=30734)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=30734)[0m f1_macro: 0.05882275089248079
[2m[36m(func pid=30734)[0m f1_weighted: 0.07977263065694765
[2m[36m(func pid=30734)[0m f1_per_class: [0.04, 0.132, 0.028, 0.117, 0.0, 0.059, 0.032, 0.071, 0.086, 0.024]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0196 | Steps: 2 | Val loss: 2.0227 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8834 | Steps: 2 | Val loss: 2.0348 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=19406)[0m top1: 0.20848880597014927
[2m[36m(func pid=19406)[0m top5: 0.7248134328358209
[2m[36m(func pid=19406)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=19406)[0m f1_macro: 0.21116837074871575
[2m[36m(func pid=19406)[0m f1_weighted: 0.24618779282593312
[2m[36m(func pid=19406)[0m f1_per_class: [0.112, 0.312, 0.292, 0.321, 0.071, 0.14, 0.207, 0.212, 0.14, 0.304]
[2m[36m(func pid=19406)[0m 
== Status ==
Current time: 2024-01-07 00:58:26 (running for 00:15:18.34)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.02  |      0.331 |                   66 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.211 |                   65 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.831 |      0.059 |                   12 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  2.065 |      0.162 |                   10 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3605410447761194
[2m[36m(func pid=18554)[0m top5: 0.8731343283582089
[2m[36m(func pid=18554)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.33094112590804864
[2m[36m(func pid=18554)[0m f1_weighted: 0.37568088649403825
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.345, 0.647, 0.57, 0.085, 0.186, 0.326, 0.274, 0.207, 0.225]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8215 | Steps: 2 | Val loss: 2.4268 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=31420)[0m top1: 0.26399253731343286
[2m[36m(func pid=31420)[0m top5: 0.7919776119402985
[2m[36m(func pid=31420)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=31420)[0m f1_macro: 0.20435804102872512
[2m[36m(func pid=31420)[0m f1_weighted: 0.27752376470656454
[2m[36m(func pid=31420)[0m f1_per_class: [0.239, 0.261, 0.361, 0.356, 0.03, 0.128, 0.35, 0.0, 0.151, 0.167]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1910 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=30734)[0m top1: 0.07182835820895522
[2m[36m(func pid=30734)[0m top5: 0.44029850746268656
[2m[36m(func pid=30734)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=30734)[0m f1_macro: 0.05886121997319292
[2m[36m(func pid=30734)[0m f1_weighted: 0.07910549620025528
[2m[36m(func pid=30734)[0m f1_per_class: [0.037, 0.132, 0.038, 0.085, 0.0, 0.066, 0.058, 0.069, 0.075, 0.028]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0233 | Steps: 2 | Val loss: 2.0037 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=19406)[0m top1: 0.20615671641791045
[2m[36m(func pid=19406)[0m top5: 0.7220149253731343
[2m[36m(func pid=19406)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=19406)[0m f1_macro: 0.20653242903831007
[2m[36m(func pid=19406)[0m f1_weighted: 0.2426360653999754
[2m[36m(func pid=19406)[0m f1_per_class: [0.112, 0.311, 0.268, 0.315, 0.071, 0.141, 0.203, 0.21, 0.143, 0.292]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.6739 | Steps: 2 | Val loss: 1.9750 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 00:58:31 (running for 00:15:23.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.336 |                   67 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.207 |                   66 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.822 |      0.059 |                   13 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  1.883 |      0.204 |                   11 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.36100746268656714
[2m[36m(func pid=18554)[0m top5: 0.8777985074626866
[2m[36m(func pid=18554)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=18554)[0m f1_macro: 0.33636759682928263
[2m[36m(func pid=18554)[0m f1_weighted: 0.38050743519792846
[2m[36m(func pid=18554)[0m f1_per_class: [0.43, 0.362, 0.688, 0.571, 0.074, 0.199, 0.329, 0.259, 0.205, 0.247]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7655 | Steps: 2 | Val loss: 2.4187 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=31420)[0m top1: 0.2868470149253731
[2m[36m(func pid=31420)[0m top5: 0.8292910447761194
[2m[36m(func pid=31420)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=31420)[0m f1_macro: 0.23591493122799748
[2m[36m(func pid=31420)[0m f1_weighted: 0.2927850483951023
[2m[36m(func pid=31420)[0m f1_per_class: [0.306, 0.33, 0.458, 0.44, 0.036, 0.157, 0.262, 0.0, 0.17, 0.2]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1020 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=30734)[0m top1: 0.07042910447761194
[2m[36m(func pid=30734)[0m top5: 0.43843283582089554
[2m[36m(func pid=30734)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.054464660509639974
[2m[36m(func pid=30734)[0m f1_weighted: 0.07752719911493956
[2m[36m(func pid=30734)[0m f1_per_class: [0.046, 0.135, 0.027, 0.051, 0.0, 0.078, 0.088, 0.019, 0.077, 0.025]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0181 | Steps: 2 | Val loss: 1.9834 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=19406)[0m top1: 0.21175373134328357
[2m[36m(func pid=19406)[0m top5: 0.7243470149253731
[2m[36m(func pid=19406)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=19406)[0m f1_macro: 0.21223864662258984
[2m[36m(func pid=19406)[0m f1_weighted: 0.2504875213161447
[2m[36m(func pid=19406)[0m f1_per_class: [0.114, 0.303, 0.265, 0.318, 0.072, 0.14, 0.229, 0.214, 0.149, 0.319]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.3966 | Steps: 2 | Val loss: 1.9438 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 00:58:36 (running for 00:15:28.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.018 |      0.339 |                   68 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.212 |                   67 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.766 |      0.054 |                   14 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  1.674 |      0.236 |                   12 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.36427238805970147
[2m[36m(func pid=18554)[0m top5: 0.8768656716417911
[2m[36m(func pid=18554)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=18554)[0m f1_macro: 0.33897806686598503
[2m[36m(func pid=18554)[0m f1_weighted: 0.3857625751753496
[2m[36m(func pid=18554)[0m f1_per_class: [0.443, 0.378, 0.667, 0.561, 0.078, 0.197, 0.347, 0.252, 0.209, 0.258]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7205 | Steps: 2 | Val loss: 2.4122 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=31420)[0m top1: 0.3003731343283582
[2m[36m(func pid=31420)[0m top5: 0.8325559701492538
[2m[36m(func pid=31420)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=31420)[0m f1_macro: 0.25357501275368055
[2m[36m(func pid=31420)[0m f1_weighted: 0.2906616605405618
[2m[36m(func pid=31420)[0m f1_per_class: [0.347, 0.358, 0.537, 0.486, 0.042, 0.185, 0.175, 0.029, 0.159, 0.217]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.0213 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=30734)[0m top1: 0.07555970149253731
[2m[36m(func pid=30734)[0m top5: 0.4542910447761194
[2m[36m(func pid=30734)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=30734)[0m f1_macro: 0.057484646574916164
[2m[36m(func pid=30734)[0m f1_weighted: 0.08234736169671811
[2m[36m(func pid=30734)[0m f1_per_class: [0.047, 0.128, 0.031, 0.035, 0.0, 0.081, 0.121, 0.011, 0.089, 0.03]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0109 | Steps: 2 | Val loss: 1.9808 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=19406)[0m top1: 0.2140858208955224
[2m[36m(func pid=19406)[0m top5: 0.7234141791044776
[2m[36m(func pid=19406)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=19406)[0m f1_macro: 0.21134980761656155
[2m[36m(func pid=19406)[0m f1_weighted: 0.2543920666748527
[2m[36m(func pid=19406)[0m f1_per_class: [0.117, 0.293, 0.252, 0.322, 0.07, 0.141, 0.243, 0.213, 0.15, 0.311]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.3045 | Steps: 2 | Val loss: 1.9327 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6880 | Steps: 2 | Val loss: 2.4076 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 00:58:41 (running for 00:15:33.94)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.011 |      0.338 |                   69 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.211 |                   68 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.72  |      0.057 |                   15 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  1.397 |      0.254 |                   13 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3605410447761194
[2m[36m(func pid=18554)[0m top5: 0.8777985074626866
[2m[36m(func pid=18554)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=18554)[0m f1_macro: 0.33788759373261146
[2m[36m(func pid=18554)[0m f1_weighted: 0.38308488832459703
[2m[36m(func pid=18554)[0m f1_per_class: [0.44, 0.389, 0.667, 0.545, 0.079, 0.21, 0.342, 0.253, 0.207, 0.247]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.30830223880597013
[2m[36m(func pid=31420)[0m top5: 0.8213619402985075
[2m[36m(func pid=31420)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=31420)[0m f1_macro: 0.27951239464979605
[2m[36m(func pid=31420)[0m f1_weighted: 0.28550462873622406
[2m[36m(func pid=31420)[0m f1_per_class: [0.364, 0.361, 0.611, 0.514, 0.044, 0.207, 0.078, 0.241, 0.153, 0.222]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8886 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=30734)[0m top1: 0.08722014925373134
[2m[36m(func pid=30734)[0m top5: 0.46781716417910446
[2m[36m(func pid=30734)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=30734)[0m f1_macro: 0.06393646047271659
[2m[36m(func pid=30734)[0m f1_weighted: 0.09546872940251483
[2m[36m(func pid=30734)[0m f1_per_class: [0.049, 0.129, 0.035, 0.033, 0.0, 0.095, 0.161, 0.012, 0.082, 0.042]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0183 | Steps: 2 | Val loss: 1.9663 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=19406)[0m top1: 0.21828358208955223
[2m[36m(func pid=19406)[0m top5: 0.7220149253731343
[2m[36m(func pid=19406)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=19406)[0m f1_macro: 0.21372330633304815
[2m[36m(func pid=19406)[0m f1_weighted: 0.25944141987565245
[2m[36m(func pid=19406)[0m f1_per_class: [0.117, 0.302, 0.25, 0.318, 0.072, 0.143, 0.258, 0.22, 0.147, 0.311]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.1586 | Steps: 2 | Val loss: 1.9335 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 00:58:46 (running for 00:15:39.04)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.018 |      0.342 |                   70 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.214 |                   69 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.688 |      0.064 |                   16 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  1.304 |      0.28  |                   14 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=18554)[0m top1: 0.3614738805970149

[2m[36m(func pid=18554)[0m top5: 0.8782649253731343
[2m[36m(func pid=18554)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=18554)[0m f1_macro: 0.3420193657382755
[2m[36m(func pid=18554)[0m f1_weighted: 0.3863017941182348
[2m[36m(func pid=18554)[0m f1_per_class: [0.435, 0.395, 0.688, 0.533, 0.081, 0.226, 0.356, 0.242, 0.215, 0.25]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6808 | Steps: 2 | Val loss: 2.4028 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.5295 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=31420)[0m top1: 0.31296641791044777
[2m[36m(func pid=31420)[0m top5: 0.8078358208955224
[2m[36m(func pid=31420)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=31420)[0m f1_macro: 0.3001551434327573
[2m[36m(func pid=31420)[0m f1_weighted: 0.28347005052244245
[2m[36m(func pid=31420)[0m f1_per_class: [0.382, 0.402, 0.71, 0.519, 0.053, 0.195, 0.03, 0.302, 0.161, 0.247]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.09095149253731344
[2m[36m(func pid=30734)[0m top5: 0.4724813432835821
[2m[36m(func pid=30734)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=30734)[0m f1_macro: 0.06265586609845644
[2m[36m(func pid=30734)[0m f1_weighted: 0.09810090401850052
[2m[36m(func pid=30734)[0m f1_per_class: [0.049, 0.118, 0.038, 0.019, 0.0, 0.102, 0.19, 0.0, 0.075, 0.034]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=19406)[0m top1: 0.22201492537313433
[2m[36m(func pid=19406)[0m top5: 0.7290111940298507
[2m[36m(func pid=19406)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=19406)[0m f1_macro: 0.21713783783093304
[2m[36m(func pid=19406)[0m f1_weighted: 0.26317554052290176
[2m[36m(func pid=19406)[0m f1_per_class: [0.119, 0.307, 0.263, 0.322, 0.074, 0.144, 0.263, 0.212, 0.149, 0.319]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0187 | Steps: 2 | Val loss: 1.9667 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.9151 | Steps: 2 | Val loss: 1.9298 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.4055 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6521 | Steps: 2 | Val loss: 2.3970 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 00:58:52 (running for 00:15:44.50)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.019 |      0.34  |                   71 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.217 |                   70 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.681 |      0.063 |                   17 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  1.159 |      0.3   |                   15 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3619402985074627
[2m[36m(func pid=18554)[0m top5: 0.878731343283582
[2m[36m(func pid=18554)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=18554)[0m f1_macro: 0.3404424349835216
[2m[36m(func pid=18554)[0m f1_weighted: 0.38668296097912425
[2m[36m(func pid=18554)[0m f1_per_class: [0.448, 0.408, 0.647, 0.523, 0.075, 0.236, 0.353, 0.26, 0.203, 0.252]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3101679104477612
[2m[36m(func pid=31420)[0m top5: 0.8129664179104478
[2m[36m(func pid=31420)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=31420)[0m f1_macro: 0.308344138635562
[2m[36m(func pid=31420)[0m f1_weighted: 0.2803026860147342
[2m[36m(func pid=31420)[0m f1_per_class: [0.407, 0.422, 0.733, 0.517, 0.066, 0.179, 0.015, 0.265, 0.189, 0.289]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m top1: 0.228544776119403
[2m[36m(func pid=19406)[0m top5: 0.730410447761194
[2m[36m(func pid=19406)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=19406)[0m f1_macro: 0.22262329631695743
[2m[36m(func pid=19406)[0m f1_weighted: 0.2722205961663276
[2m[36m(func pid=19406)[0m f1_per_class: [0.12, 0.306, 0.292, 0.332, 0.075, 0.144, 0.285, 0.207, 0.149, 0.316]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m top1: 0.10027985074626866
[2m[36m(func pid=30734)[0m top5: 0.478544776119403
[2m[36m(func pid=30734)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=30734)[0m f1_macro: 0.06644094402242397
[2m[36m(func pid=30734)[0m f1_weighted: 0.10438813392823859
[2m[36m(func pid=30734)[0m f1_per_class: [0.07, 0.12, 0.042, 0.01, 0.0, 0.107, 0.216, 0.0, 0.079, 0.022]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0109 | Steps: 2 | Val loss: 1.9680 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8497 | Steps: 2 | Val loss: 1.9479 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.3626 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6571 | Steps: 2 | Val loss: 2.3903 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 00:58:57 (running for 00:15:49.69)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.011 |      0.341 |                   72 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   71 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.652 |      0.066 |                   18 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.915 |      0.308 |                   16 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3614738805970149
[2m[36m(func pid=18554)[0m top5: 0.8796641791044776
[2m[36m(func pid=18554)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=18554)[0m f1_macro: 0.34138611767011523
[2m[36m(func pid=18554)[0m f1_weighted: 0.3865521188905246
[2m[36m(func pid=18554)[0m f1_per_class: [0.446, 0.416, 0.647, 0.514, 0.075, 0.239, 0.356, 0.248, 0.21, 0.262]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=31420)[0m top1: 0.29151119402985076
[2m[36m(func pid=31420)[0m top5: 0.8218283582089553
[2m[36m(func pid=31420)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=31420)[0m f1_macro: 0.303771194091628
[2m[36m(func pid=31420)[0m f1_weighted: 0.26876579228467046
[2m[36m(func pid=31420)[0m f1_per_class: [0.418, 0.419, 0.786, 0.505, 0.072, 0.166, 0.003, 0.22, 0.187, 0.263]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m top1: 0.23087686567164178
[2m[36m(func pid=19406)[0m top5: 0.7313432835820896
[2m[36m(func pid=19406)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=19406)[0m f1_macro: 0.22333422092739658
[2m[36m(func pid=19406)[0m f1_weighted: 0.27547312029373033
[2m[36m(func pid=19406)[0m f1_per_class: [0.118, 0.307, 0.283, 0.338, 0.075, 0.143, 0.289, 0.211, 0.16, 0.309]
[2m[36m(func pid=30734)[0m top1: 0.10960820895522388
[2m[36m(func pid=30734)[0m top5: 0.488339552238806
[2m[36m(func pid=30734)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=30734)[0m f1_macro: 0.06930612800852445
[2m[36m(func pid=30734)[0m f1_weighted: 0.1098976639898745
[2m[36m(func pid=30734)[0m f1_per_class: [0.074, 0.108, 0.063, 0.006, 0.0, 0.116, 0.24, 0.0, 0.086, 0.0]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0225 | Steps: 2 | Val loss: 1.9675 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7347 | Steps: 2 | Val loss: 1.9761 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.4711 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:59:02 (running for 00:15:54.85)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.341 |                   73 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   72 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.657 |      0.069 |                   19 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.85  |      0.304 |                   17 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3582089552238806
[2m[36m(func pid=18554)[0m top5: 0.8801305970149254
[2m[36m(func pid=18554)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=18554)[0m f1_macro: 0.3412631172568986
[2m[36m(func pid=18554)[0m f1_weighted: 0.3834429575265685
[2m[36m(func pid=18554)[0m f1_per_class: [0.439, 0.404, 0.647, 0.492, 0.08, 0.25, 0.37, 0.245, 0.202, 0.283]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6206 | Steps: 2 | Val loss: 2.3825 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=31420)[0m top1: 0.2658582089552239
[2m[36m(func pid=31420)[0m top5: 0.8274253731343284
[2m[36m(func pid=31420)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=31420)[0m f1_macro: 0.2963594434704938
[2m[36m(func pid=31420)[0m f1_weighted: 0.2510566379310425
[2m[36m(func pid=31420)[0m f1_per_class: [0.431, 0.427, 0.786, 0.454, 0.078, 0.129, 0.003, 0.199, 0.2, 0.257]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m top1: 0.23087686567164178
[2m[36m(func pid=19406)[0m top5: 0.7285447761194029
[2m[36m(func pid=19406)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=19406)[0m f1_macro: 0.22091215268509065
[2m[36m(func pid=19406)[0m f1_weighted: 0.2752950644719387
[2m[36m(func pid=19406)[0m f1_per_class: [0.121, 0.306, 0.265, 0.336, 0.074, 0.147, 0.291, 0.209, 0.156, 0.303]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m top1: 0.12033582089552239
[2m[36m(func pid=30734)[0m top5: 0.4939365671641791
[2m[36m(func pid=30734)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=30734)[0m f1_macro: 0.07179044979611243
[2m[36m(func pid=30734)[0m f1_weighted: 0.11823836226640162
[2m[36m(func pid=30734)[0m f1_per_class: [0.081, 0.098, 0.061, 0.007, 0.0, 0.12, 0.272, 0.0, 0.079, 0.0]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6532 | Steps: 2 | Val loss: 1.9829 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0158 | Steps: 2 | Val loss: 1.9641 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.5488 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5341 | Steps: 2 | Val loss: 2.3746 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 00:59:07 (running for 00:16:00.14)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.023 |      0.341 |                   73 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.221 |                   73 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.621 |      0.072 |                   20 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.653 |      0.298 |                   19 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.25093283582089554
[2m[36m(func pid=31420)[0m top5: 0.8390858208955224
[2m[36m(func pid=31420)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=31420)[0m f1_macro: 0.2979019364858502
[2m[36m(func pid=31420)[0m f1_weighted: 0.23863691550441174
[2m[36m(func pid=31420)[0m f1_per_class: [0.492, 0.445, 0.786, 0.395, 0.096, 0.117, 0.009, 0.19, 0.19, 0.259]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=18554)[0m top1: 0.35774253731343286
[2m[36m(func pid=18554)[0m top5: 0.8819962686567164
[2m[36m(func pid=18554)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=18554)[0m f1_macro: 0.3423621646260465
[2m[36m(func pid=18554)[0m f1_weighted: 0.38365858767819244
[2m[36m(func pid=18554)[0m f1_per_class: [0.45, 0.406, 0.647, 0.493, 0.073, 0.239, 0.372, 0.243, 0.205, 0.296]
[2m[36m(func pid=18554)[0m 
[2m[36m(func pid=19406)[0m top1: 0.22901119402985073
[2m[36m(func pid=19406)[0m top5: 0.7238805970149254
[2m[36m(func pid=19406)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=19406)[0m f1_macro: 0.21901479464507587
[2m[36m(func pid=19406)[0m f1_weighted: 0.2731346424758753
[2m[36m(func pid=19406)[0m f1_per_class: [0.121, 0.292, 0.26, 0.329, 0.075, 0.148, 0.298, 0.205, 0.156, 0.306]
[2m[36m(func pid=19406)[0m 
[2m[36m(func pid=30734)[0m top1: 0.13432835820895522
[2m[36m(func pid=30734)[0m top5: 0.503731343283582
[2m[36m(func pid=30734)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=30734)[0m f1_macro: 0.07589770664401192
[2m[36m(func pid=30734)[0m f1_weighted: 0.1266230141390749
[2m[36m(func pid=30734)[0m f1_per_class: [0.082, 0.095, 0.062, 0.003, 0.0, 0.125, 0.302, 0.0, 0.089, 0.0]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=18554)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0271 | Steps: 2 | Val loss: 1.9693 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5338 | Steps: 2 | Val loss: 1.9621 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=19406)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.4469 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4909 | Steps: 2 | Val loss: 2.3633 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 00:59:13 (running for 00:16:05.29)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.348
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00006 | RUNNING    | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.016 |      0.342 |                   74 |
| train_57e67_00007 | RUNNING    | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.219 |                   74 |
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.534 |      0.076 |                   21 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.534 |      0.295 |                   20 |
| train_57e67_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=18554)[0m top1: 0.3530783582089552
[2m[36m(func pid=18554)[0m top5: 0.8801305970149254
[2m[36m(func pid=18554)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=18554)[0m f1_macro: 0.3435171140165063
[2m[36m(func pid=18554)[0m f1_weighted: 0.38006969513714417
[2m[36m(func pid=18554)[0m f1_per_class: [0.437, 0.406, 0.688, 0.496, 0.07, 0.231, 0.362, 0.24, 0.191, 0.314]
[2m[36m(func pid=31420)[0m top1: 0.24953358208955223
[2m[36m(func pid=31420)[0m top5: 0.8530783582089553
[2m[36m(func pid=31420)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=31420)[0m f1_macro: 0.29542577147079885
[2m[36m(func pid=31420)[0m f1_weighted: 0.23610958982877317
[2m[36m(func pid=31420)[0m f1_per_class: [0.496, 0.437, 0.733, 0.384, 0.088, 0.105, 0.015, 0.191, 0.229, 0.275]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=19406)[0m top1: 0.23227611940298507
[2m[36m(func pid=19406)[0m top5: 0.7290111940298507
[2m[36m(func pid=19406)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=19406)[0m f1_macro: 0.22258415988797128
[2m[36m(func pid=19406)[0m f1_weighted: 0.27721696232333465
[2m[36m(func pid=19406)[0m f1_per_class: [0.122, 0.302, 0.28, 0.339, 0.074, 0.147, 0.297, 0.204, 0.154, 0.306]
[2m[36m(func pid=30734)[0m top1: 0.14412313432835822
[2m[36m(func pid=30734)[0m top5: 0.5237873134328358
[2m[36m(func pid=30734)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=30734)[0m f1_macro: 0.07673989197566036
[2m[36m(func pid=30734)[0m f1_weighted: 0.13265972769386453
[2m[36m(func pid=30734)[0m f1_per_class: [0.08, 0.098, 0.063, 0.003, 0.0, 0.11, 0.327, 0.0, 0.085, 0.0]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5680 | Steps: 2 | Val loss: 1.9189 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=31420)[0m top1: 0.26259328358208955
[2m[36m(func pid=31420)[0m top5: 0.8582089552238806
[2m[36m(func pid=31420)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=31420)[0m f1_macro: 0.30379187271372476
[2m[36m(func pid=31420)[0m f1_weighted: 0.2596180106966694
[2m[36m(func pid=31420)[0m f1_per_class: [0.484, 0.406, 0.733, 0.418, 0.088, 0.108, 0.078, 0.199, 0.23, 0.294]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5397 | Steps: 2 | Val loss: 2.3550 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=30734)[0m top1: 0.1501865671641791
[2m[36m(func pid=30734)[0m top5: 0.5289179104477612
[2m[36m(func pid=30734)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=30734)[0m f1_macro: 0.07782858502422448
[2m[36m(func pid=30734)[0m f1_weighted: 0.13646838107319562
[2m[36m(func pid=30734)[0m f1_per_class: [0.09, 0.096, 0.063, 0.003, 0.0, 0.103, 0.344, 0.0, 0.08, 0.0]
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4622 | Steps: 2 | Val loss: 1.8805 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36270)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36270)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=36270)[0m Configuration completed!
[2m[36m(func pid=36270)[0m New optimizer parameters:
[2m[36m(func pid=36270)[0m SGD (
[2m[36m(func pid=36270)[0m Parameter Group 0
[2m[36m(func pid=36270)[0m     dampening: 0
[2m[36m(func pid=36270)[0m     differentiable: False
[2m[36m(func pid=36270)[0m     foreach: None
[2m[36m(func pid=36270)[0m     lr: 0.01
[2m[36m(func pid=36270)[0m     maximize: False
[2m[36m(func pid=36270)[0m     momentum: 0.99
[2m[36m(func pid=36270)[0m     nesterov: False
[2m[36m(func pid=36270)[0m     weight_decay: 0.0001
[2m[36m(func pid=36270)[0m )
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.291044776119403
[2m[36m(func pid=31420)[0m top5: 0.8596082089552238
[2m[36m(func pid=31420)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=31420)[0m f1_macro: 0.317884429183771
[2m[36m(func pid=31420)[0m f1_weighted: 0.3071874955348309
[2m[36m(func pid=31420)[0m f1_per_class: [0.496, 0.396, 0.733, 0.432, 0.07, 0.139, 0.216, 0.216, 0.227, 0.252]
== Status ==
Current time: 2024-01-07 00:59:18 (running for 00:16:10.30)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.491 |      0.077 |                   22 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.568 |      0.304 |                   21 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36355)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=36355)[0m Configuration completed!
[2m[36m(func pid=36355)[0m New optimizer parameters:
[2m[36m(func pid=36355)[0m SGD (
[2m[36m(func pid=36355)[0m Parameter Group 0
[2m[36m(func pid=36355)[0m     dampening: 0
[2m[36m(func pid=36355)[0m     differentiable: False
[2m[36m(func pid=36355)[0m     foreach: None
[2m[36m(func pid=36355)[0m     lr: 0.1
[2m[36m(func pid=36355)[0m     maximize: False
[2m[36m(func pid=36355)[0m     momentum: 0.99
[2m[36m(func pid=36355)[0m     nesterov: False
[2m[36m(func pid=36355)[0m     weight_decay: 0.0001
[2m[36m(func pid=36355)[0m )
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 00:59:24 (running for 00:16:16.28)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.491 |      0.077 |                   22 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.462 |      0.318 |                   22 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1195 | Steps: 2 | Val loss: 2.3811 | Batch size: 32 | lr: 0.01 | Duration: 4.36s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3406 | Steps: 2 | Val loss: 1.8493 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5372 | Steps: 2 | Val loss: 2.3453 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.3536 | Steps: 2 | Val loss: 5.1940 | Batch size: 32 | lr: 0.1 | Duration: 4.48s
[2m[36m(func pid=36270)[0m top1: 0.06529850746268656
[2m[36m(func pid=36270)[0m top5: 0.4608208955223881
[2m[36m(func pid=36270)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=36270)[0m f1_macro: 0.07001370367077768
[2m[36m(func pid=36270)[0m f1_weighted: 0.06950022121366295
[2m[36m(func pid=36270)[0m f1_per_class: [0.109, 0.114, 0.049, 0.086, 0.005, 0.04, 0.031, 0.06, 0.161, 0.044]
[2m[36m(func pid=36270)[0m 
== Status ==
Current time: 2024-01-07 00:59:29 (running for 00:16:21.38)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.54  |      0.078 |                   23 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.341 |      0.331 |                   23 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  3.119 |      0.07  |                    1 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.3246268656716418
[2m[36m(func pid=31420)[0m top5: 0.8652052238805971
[2m[36m(func pid=31420)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=31420)[0m f1_macro: 0.33054060557492093
[2m[36m(func pid=31420)[0m f1_weighted: 0.355664198840102
[2m[36m(func pid=31420)[0m f1_per_class: [0.486, 0.377, 0.733, 0.447, 0.062, 0.153, 0.371, 0.229, 0.221, 0.227]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.16044776119402984
[2m[36m(func pid=30734)[0m top5: 0.5419776119402985
[2m[36m(func pid=30734)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=30734)[0m f1_macro: 0.08513288371183039
[2m[36m(func pid=30734)[0m f1_weighted: 0.14301200896207641
[2m[36m(func pid=30734)[0m f1_per_class: [0.088, 0.093, 0.07, 0.003, 0.0, 0.104, 0.365, 0.0, 0.085, 0.044]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.043843283582089554
[2m[36m(func pid=36355)[0m top5: 0.3773320895522388
[2m[36m(func pid=36355)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=36355)[0m f1_macro: 0.027338703929972046
[2m[36m(func pid=36355)[0m f1_weighted: 0.006496402126620534
[2m[36m(func pid=36355)[0m f1_per_class: [0.135, 0.0, 0.0, 0.0, 0.033, 0.0, 0.0, 0.0, 0.105, 0.0]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2859 | Steps: 2 | Val loss: 1.8099 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6852 | Steps: 2 | Val loss: 2.3614 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3286 | Steps: 2 | Val loss: 2.3292 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.8539 | Steps: 2 | Val loss: 5.1110 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=31420)[0m top1: 0.37453358208955223
[2m[36m(func pid=31420)[0m top5: 0.871268656716418
[2m[36m(func pid=31420)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=31420)[0m f1_macro: 0.3452395857749323
[2m[36m(func pid=31420)[0m f1_weighted: 0.40442623706545494
[2m[36m(func pid=31420)[0m f1_per_class: [0.473, 0.374, 0.71, 0.478, 0.08, 0.151, 0.512, 0.215, 0.211, 0.248]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.08722014925373134
[2m[36m(func pid=36270)[0m top5: 0.5382462686567164
[2m[36m(func pid=36270)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=36270)[0m f1_macro: 0.06456633958389837
[2m[36m(func pid=36270)[0m f1_weighted: 0.08648881254832208
[2m[36m(func pid=36270)[0m f1_per_class: [0.14, 0.053, 0.089, 0.007, 0.019, 0.051, 0.214, 0.0, 0.073, 0.0]
[2m[36m(func pid=36270)[0m 
== Status ==
Current time: 2024-01-07 00:59:34 (running for 00:16:26.43)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.537 |      0.085 |                   24 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.286 |      0.345 |                   24 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  2.685 |      0.065 |                    2 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  4.354 |      0.027 |                    1 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.16791044776119404
[2m[36m(func pid=30734)[0m top5: 0.5583022388059702
[2m[36m(func pid=30734)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=30734)[0m f1_macro: 0.08757791132373552
[2m[36m(func pid=30734)[0m f1_weighted: 0.14707725265210475
[2m[36m(func pid=30734)[0m f1_per_class: [0.098, 0.097, 0.074, 0.003, 0.0, 0.1, 0.377, 0.0, 0.084, 0.043]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3908582089552239
[2m[36m(func pid=36355)[0m top5: 0.8055037313432836
[2m[36m(func pid=36355)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=36355)[0m f1_macro: 0.12360580137433715
[2m[36m(func pid=36355)[0m f1_weighted: 0.2571589748658271
[2m[36m(func pid=36355)[0m f1_per_class: [0.2, 0.442, 0.0, 0.003, 0.0, 0.0, 0.591, 0.0, 0.0, 0.0]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2392 | Steps: 2 | Val loss: 1.7970 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3225 | Steps: 2 | Val loss: 2.1905 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5095 | Steps: 2 | Val loss: 2.3137 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=31420)[0m top1: 0.4048507462686567
[2m[36m(func pid=31420)[0m top5: 0.8684701492537313
[2m[36m(func pid=31420)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=31420)[0m f1_macro: 0.35064085803222816
[2m[36m(func pid=31420)[0m f1_weighted: 0.42362143174476125
[2m[36m(func pid=31420)[0m f1_per_class: [0.476, 0.359, 0.71, 0.486, 0.1, 0.157, 0.583, 0.172, 0.208, 0.255]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 10.0997 | Steps: 2 | Val loss: 10.8038 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 00:59:39 (running for 00:16:31.56)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.329 |      0.088 |                   25 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.239 |      0.351 |                   25 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  2.323 |      0.143 |                    3 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  7.854 |      0.124 |                    2 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.19776119402985073
[2m[36m(func pid=36270)[0m top5: 0.6809701492537313
[2m[36m(func pid=36270)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=36270)[0m f1_macro: 0.14291692133629047
[2m[36m(func pid=36270)[0m f1_weighted: 0.16948564595954416
[2m[36m(func pid=36270)[0m f1_per_class: [0.203, 0.106, 0.259, 0.066, 0.059, 0.111, 0.362, 0.0, 0.144, 0.12]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.17257462686567165
[2m[36m(func pid=30734)[0m top5: 0.5727611940298507
[2m[36m(func pid=30734)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=30734)[0m f1_macro: 0.08999974201326025
[2m[36m(func pid=30734)[0m f1_weighted: 0.1519266880394635
[2m[36m(func pid=30734)[0m f1_per_class: [0.103, 0.103, 0.078, 0.01, 0.0, 0.101, 0.383, 0.0, 0.079, 0.043]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.08582089552238806
[2m[36m(func pid=36355)[0m top5: 0.45475746268656714
[2m[36m(func pid=36355)[0m f1_micro: 0.08582089552238806
[2m[36m(func pid=36355)[0m f1_macro: 0.12702548845358627
[2m[36m(func pid=36355)[0m f1_weighted: 0.028671859708576757
[2m[36m(func pid=36355)[0m f1_per_class: [0.464, 0.021, 0.4, 0.01, 0.2, 0.0, 0.0, 0.122, 0.053, 0.0]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2405 | Steps: 2 | Val loss: 1.7951 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.8815 | Steps: 2 | Val loss: 1.9922 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.2796 | Steps: 2 | Val loss: 2.2899 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=31420)[0m top1: 0.42350746268656714
[2m[36m(func pid=31420)[0m top5: 0.8708022388059702
[2m[36m(func pid=31420)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=31420)[0m f1_macro: 0.3539064724689045
[2m[36m(func pid=31420)[0m f1_weighted: 0.43425702906707103
[2m[36m(func pid=31420)[0m f1_per_class: [0.473, 0.36, 0.71, 0.493, 0.102, 0.163, 0.616, 0.132, 0.205, 0.286]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 8.6975 | Steps: 2 | Val loss: 14.0404 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36270)[0m top1: 0.26119402985074625
[2m[36m(func pid=36270)[0m top5: 0.8423507462686567
[2m[36m(func pid=36270)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=36270)[0m f1_macro: 0.2326464257019932
[2m[36m(func pid=36270)[0m f1_weighted: 0.24024796604379395
[2m[36m(func pid=36270)[0m f1_per_class: [0.231, 0.172, 0.595, 0.458, 0.111, 0.246, 0.126, 0.0, 0.155, 0.232]
[2m[36m(func pid=36270)[0m 
== Status ==
Current time: 2024-01-07 00:59:44 (running for 00:16:36.92)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.51  |      0.09  |                   26 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.24  |      0.354 |                   26 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  1.881 |      0.233 |                    4 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 | 10.1   |      0.127 |                    3 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.18330223880597016
[2m[36m(func pid=30734)[0m top5: 0.5853544776119403
[2m[36m(func pid=30734)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=30734)[0m f1_macro: 0.09645367332672125
[2m[36m(func pid=30734)[0m f1_weighted: 0.15922392997631452
[2m[36m(func pid=30734)[0m f1_per_class: [0.117, 0.117, 0.093, 0.016, 0.0, 0.107, 0.39, 0.0, 0.087, 0.038]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2341 | Steps: 2 | Val loss: 1.8140 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=36355)[0m top1: 0.04291044776119403
[2m[36m(func pid=36355)[0m top5: 0.4300373134328358
[2m[36m(func pid=36355)[0m f1_micro: 0.04291044776119403
[2m[36m(func pid=36355)[0m f1_macro: 0.05081271575602031
[2m[36m(func pid=36355)[0m f1_weighted: 0.017458938009247024
[2m[36m(func pid=36355)[0m f1_per_class: [0.044, 0.056, 0.24, 0.007, 0.092, 0.008, 0.0, 0.0, 0.061, 0.0]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.1937 | Steps: 2 | Val loss: 1.9973 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2230 | Steps: 2 | Val loss: 2.2691 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=31420)[0m top1: 0.4197761194029851
[2m[36m(func pid=31420)[0m top5: 0.8745335820895522
[2m[36m(func pid=31420)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=31420)[0m f1_macro: 0.34177423524456263
[2m[36m(func pid=31420)[0m f1_weighted: 0.42939476754297135
[2m[36m(func pid=31420)[0m f1_per_class: [0.464, 0.346, 0.647, 0.488, 0.101, 0.169, 0.618, 0.11, 0.205, 0.271]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 4.2827 | Steps: 2 | Val loss: 15.2665 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 00:59:49 (running for 00:16:42.10)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.28  |      0.096 |                   27 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.234 |      0.342 |                   27 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  1.194 |      0.312 |                    5 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  8.698 |      0.051 |                    4 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.33115671641791045
[2m[36m(func pid=36270)[0m top5: 0.7947761194029851
[2m[36m(func pid=36270)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=36270)[0m f1_macro: 0.3120576094578998
[2m[36m(func pid=36270)[0m f1_weighted: 0.26299661392804474
[2m[36m(func pid=36270)[0m f1_per_class: [0.505, 0.219, 0.647, 0.547, 0.135, 0.242, 0.009, 0.299, 0.175, 0.343]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.1884328358208955
[2m[36m(func pid=30734)[0m top5: 0.6077425373134329
[2m[36m(func pid=30734)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=30734)[0m f1_macro: 0.1037815547237686
[2m[36m(func pid=30734)[0m f1_weighted: 0.16509295276248134
[2m[36m(func pid=30734)[0m f1_per_class: [0.128, 0.134, 0.101, 0.019, 0.0, 0.106, 0.395, 0.0, 0.087, 0.068]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2064 | Steps: 2 | Val loss: 1.8368 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=36355)[0m top1: 0.13759328358208955
[2m[36m(func pid=36355)[0m top5: 0.7821828358208955
[2m[36m(func pid=36355)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=36355)[0m f1_macro: 0.09757815041243365
[2m[36m(func pid=36355)[0m f1_weighted: 0.040941659263562476
[2m[36m(func pid=36355)[0m f1_per_class: [0.258, 0.016, 0.375, 0.01, 0.083, 0.23, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7299 | Steps: 2 | Val loss: 2.0976 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.2248 | Steps: 2 | Val loss: 2.2438 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=31420)[0m top1: 0.4197761194029851
[2m[36m(func pid=31420)[0m top5: 0.8759328358208955
[2m[36m(func pid=31420)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=31420)[0m f1_macro: 0.3418060776851452
[2m[36m(func pid=31420)[0m f1_weighted: 0.4296555458575402
[2m[36m(func pid=31420)[0m f1_per_class: [0.443, 0.342, 0.667, 0.491, 0.1, 0.175, 0.617, 0.106, 0.206, 0.271]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 5.1902 | Steps: 2 | Val loss: 11.8046 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 00:59:55 (running for 00:16:47.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.223 |      0.104 |                   28 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.206 |      0.342 |                   28 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.73  |      0.31  |                    6 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  4.283 |      0.098 |                    5 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.26865671641791045
[2m[36m(func pid=36270)[0m top5: 0.8236940298507462
[2m[36m(func pid=36270)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=36270)[0m f1_macro: 0.31025083101431494
[2m[36m(func pid=36270)[0m f1_weighted: 0.24468323851774001
[2m[36m(func pid=36270)[0m f1_per_class: [0.634, 0.251, 0.667, 0.52, 0.131, 0.146, 0.0, 0.204, 0.168, 0.381]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.19449626865671643
[2m[36m(func pid=30734)[0m top5: 0.6259328358208955
[2m[36m(func pid=30734)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=30734)[0m f1_macro: 0.11119792916137654
[2m[36m(func pid=30734)[0m f1_weighted: 0.17298628068585714
[2m[36m(func pid=30734)[0m f1_per_class: [0.139, 0.148, 0.109, 0.032, 0.0, 0.106, 0.399, 0.0, 0.088, 0.091]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1582 | Steps: 2 | Val loss: 1.8531 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=36355)[0m top1: 0.251865671641791
[2m[36m(func pid=36355)[0m top5: 0.7453358208955224
[2m[36m(func pid=36355)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=36355)[0m f1_macro: 0.21415509496614465
[2m[36m(func pid=36355)[0m f1_weighted: 0.26733377901292465
[2m[36m(func pid=36355)[0m f1_per_class: [0.309, 0.005, 0.533, 0.468, 0.156, 0.315, 0.297, 0.0, 0.0, 0.058]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4815 | Steps: 2 | Val loss: 1.9923 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1580 | Steps: 2 | Val loss: 2.2227 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=31420)[0m top1: 0.4146455223880597
[2m[36m(func pid=31420)[0m top5: 0.8745335820895522
[2m[36m(func pid=31420)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=31420)[0m f1_macro: 0.34348518069263745
[2m[36m(func pid=31420)[0m f1_weighted: 0.4292547832587692
[2m[36m(func pid=31420)[0m f1_per_class: [0.429, 0.335, 0.667, 0.499, 0.099, 0.195, 0.602, 0.118, 0.211, 0.279]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.7530 | Steps: 2 | Val loss: 11.2604 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:00:00 (running for 00:16:52.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.225 |      0.111 |                   29 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.158 |      0.343 |                   29 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.481 |      0.321 |                    7 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  5.19  |      0.214 |                    6 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.2555970149253731
[2m[36m(func pid=36270)[0m top5: 0.8656716417910447
[2m[36m(func pid=36270)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=36270)[0m f1_macro: 0.3213165446228908
[2m[36m(func pid=36270)[0m f1_weighted: 0.26662742162125314
[2m[36m(func pid=36270)[0m f1_per_class: [0.617, 0.336, 0.786, 0.443, 0.074, 0.151, 0.092, 0.223, 0.18, 0.31]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.20242537313432835
[2m[36m(func pid=30734)[0m top5: 0.6492537313432836
[2m[36m(func pid=30734)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=30734)[0m f1_macro: 0.11973389382814706
[2m[36m(func pid=30734)[0m f1_weighted: 0.18580298661284414
[2m[36m(func pid=30734)[0m f1_per_class: [0.14, 0.173, 0.118, 0.056, 0.0, 0.113, 0.401, 0.0, 0.096, 0.1]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1866 | Steps: 2 | Val loss: 1.8562 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=36355)[0m top1: 0.33955223880597013
[2m[36m(func pid=36355)[0m top5: 0.804570895522388
[2m[36m(func pid=36355)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=36355)[0m f1_macro: 0.27486889924966806
[2m[36m(func pid=36355)[0m f1_weighted: 0.30618971037976417
[2m[36m(func pid=36355)[0m f1_per_class: [0.415, 0.027, 0.8, 0.527, 0.053, 0.023, 0.42, 0.136, 0.051, 0.298]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.2673 | Steps: 2 | Val loss: 1.8871 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.1249 | Steps: 2 | Val loss: 2.2017 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=31420)[0m top1: 0.40951492537313433
[2m[36m(func pid=31420)[0m top5: 0.8745335820895522
[2m[36m(func pid=31420)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=31420)[0m f1_macro: 0.34323853370350627
[2m[36m(func pid=31420)[0m f1_weighted: 0.4278328399032374
[2m[36m(func pid=31420)[0m f1_per_class: [0.44, 0.319, 0.647, 0.52, 0.101, 0.193, 0.584, 0.14, 0.207, 0.281]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7208 | Steps: 2 | Val loss: 16.2225 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=36270)[0m top1: 0.35867537313432835
[2m[36m(func pid=36270)[0m top5: 0.8791977611940298
[2m[36m(func pid=36270)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=36270)[0m f1_macro: 0.34998107848293314
[2m[36m(func pid=36270)[0m f1_weighted: 0.3886397768320226
[2m[36m(func pid=36270)[0m f1_per_class: [0.583, 0.356, 0.786, 0.391, 0.065, 0.161, 0.555, 0.124, 0.202, 0.276]
[2m[36m(func pid=36270)[0m 
== Status ==
Current time: 2024-01-07 01:00:05 (running for 00:16:57.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.158 |      0.12  |                   30 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.187 |      0.343 |                   30 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.267 |      0.35  |                    8 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  3.753 |      0.275 |                    7 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.208955223880597
[2m[36m(func pid=30734)[0m top5: 0.6693097014925373
[2m[36m(func pid=30734)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=30734)[0m f1_macro: 0.13296260273510094
[2m[36m(func pid=30734)[0m f1_weighted: 0.19596436369082224
[2m[36m(func pid=30734)[0m f1_per_class: [0.153, 0.181, 0.139, 0.082, 0.037, 0.117, 0.402, 0.0, 0.098, 0.121]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1229 | Steps: 2 | Val loss: 1.8743 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=36355)[0m top1: 0.283115671641791
[2m[36m(func pid=36355)[0m top5: 0.7807835820895522
[2m[36m(func pid=36355)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=36355)[0m f1_macro: 0.25460824579264957
[2m[36m(func pid=36355)[0m f1_weighted: 0.24983270799038546
[2m[36m(func pid=36355)[0m f1_per_class: [0.437, 0.042, 0.815, 0.519, 0.045, 0.0, 0.207, 0.301, 0.108, 0.071]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.1855 | Steps: 2 | Val loss: 2.0280 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0642 | Steps: 2 | Val loss: 2.1783 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=31420)[0m top1: 0.40158582089552236
[2m[36m(func pid=31420)[0m top5: 0.8782649253731343
[2m[36m(func pid=31420)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=31420)[0m f1_macro: 0.3391179502689622
[2m[36m(func pid=31420)[0m f1_weighted: 0.422828560113685
[2m[36m(func pid=31420)[0m f1_per_class: [0.42, 0.32, 0.647, 0.527, 0.104, 0.189, 0.564, 0.133, 0.206, 0.281]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.8691 | Steps: 2 | Val loss: 19.3848 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:00:10 (running for 00:17:03.03)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.125 |      0.133 |                   31 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.123 |      0.339 |                   31 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.185 |      0.34  |                    9 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.721 |      0.255 |                    8 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.3666044776119403
[2m[36m(func pid=36270)[0m top5: 0.8680037313432836
[2m[36m(func pid=36270)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=36270)[0m f1_macro: 0.33998697479855494
[2m[36m(func pid=36270)[0m f1_weighted: 0.35930063875540663
[2m[36m(func pid=36270)[0m f1_per_class: [0.574, 0.35, 0.815, 0.279, 0.07, 0.184, 0.576, 0.015, 0.196, 0.341]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.21222014925373134
[2m[36m(func pid=30734)[0m top5: 0.6833022388059702
[2m[36m(func pid=30734)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=30734)[0m f1_macro: 0.14187985961221802
[2m[36m(func pid=30734)[0m f1_weighted: 0.20546496966282612
[2m[36m(func pid=30734)[0m f1_per_class: [0.15, 0.201, 0.163, 0.11, 0.036, 0.109, 0.396, 0.0, 0.103, 0.15]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1304 | Steps: 2 | Val loss: 1.9077 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=36355)[0m top1: 0.26725746268656714
[2m[36m(func pid=36355)[0m top5: 0.7719216417910447
[2m[36m(func pid=36355)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=36355)[0m f1_macro: 0.2663023574926248
[2m[36m(func pid=36355)[0m f1_weighted: 0.24584205739322817
[2m[36m(func pid=36355)[0m f1_per_class: [0.424, 0.182, 0.833, 0.516, 0.061, 0.0, 0.115, 0.245, 0.212, 0.074]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.1570 | Steps: 2 | Val loss: 2.1606 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.0188 | Steps: 2 | Val loss: 2.1557 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=31420)[0m top1: 0.38992537313432835
[2m[36m(func pid=31420)[0m top5: 0.8805970149253731
[2m[36m(func pid=31420)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=31420)[0m f1_macro: 0.3352389318491993
[2m[36m(func pid=31420)[0m f1_weighted: 0.41525188169425586
[2m[36m(func pid=31420)[0m f1_per_class: [0.396, 0.325, 0.611, 0.528, 0.104, 0.2, 0.525, 0.17, 0.209, 0.283]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4481 | Steps: 2 | Val loss: 18.7429 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:00:15 (running for 00:17:08.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.064 |      0.142 |                   32 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.13  |      0.335 |                   32 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.157 |      0.33  |                   10 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  3.869 |      0.266 |                    9 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.34841417910447764
[2m[36m(func pid=36270)[0m top5: 0.8568097014925373
[2m[36m(func pid=36270)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=36270)[0m f1_macro: 0.32977355357880744
[2m[36m(func pid=36270)[0m f1_weighted: 0.3429673159251672
[2m[36m(func pid=36270)[0m f1_per_class: [0.486, 0.372, 0.815, 0.224, 0.079, 0.211, 0.548, 0.056, 0.202, 0.304]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.21361940298507462
[2m[36m(func pid=30734)[0m top5: 0.6977611940298507
[2m[36m(func pid=30734)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=30734)[0m f1_macro: 0.1480703475623315
[2m[36m(func pid=30734)[0m f1_weighted: 0.21356904913155628
[2m[36m(func pid=30734)[0m f1_per_class: [0.161, 0.219, 0.182, 0.138, 0.034, 0.106, 0.386, 0.0, 0.119, 0.135]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1094 | Steps: 2 | Val loss: 1.9536 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36355)[0m top1: 0.30177238805970147
[2m[36m(func pid=36355)[0m top5: 0.7723880597014925
[2m[36m(func pid=36355)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=36355)[0m f1_macro: 0.27006648436702924
[2m[36m(func pid=36355)[0m f1_weighted: 0.2956516104699816
[2m[36m(func pid=36355)[0m f1_per_class: [0.338, 0.375, 0.667, 0.497, 0.122, 0.008, 0.197, 0.249, 0.172, 0.077]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.0696 | Steps: 2 | Val loss: 2.3625 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=31420)[0m top1: 0.3773320895522388
[2m[36m(func pid=31420)[0m top5: 0.8763992537313433
[2m[36m(func pid=31420)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=31420)[0m f1_macro: 0.33494413638470766
[2m[36m(func pid=31420)[0m f1_weighted: 0.4071686502714775
[2m[36m(func pid=31420)[0m f1_per_class: [0.387, 0.325, 0.629, 0.525, 0.098, 0.202, 0.492, 0.219, 0.207, 0.266]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8791 | Steps: 2 | Val loss: 2.1381 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4463 | Steps: 2 | Val loss: 18.1833 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:00:21 (running for 00:17:13.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  2.019 |      0.148 |                   33 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.109 |      0.335 |                   33 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.07  |      0.333 |                   11 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  2.448 |      0.27  |                   10 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.3208955223880597
[2m[36m(func pid=36270)[0m top5: 0.84375
[2m[36m(func pid=36270)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=36270)[0m f1_macro: 0.3326828018773795
[2m[36m(func pid=36270)[0m f1_weighted: 0.3278931061471817
[2m[36m(func pid=36270)[0m f1_per_class: [0.416, 0.387, 0.815, 0.214, 0.089, 0.236, 0.476, 0.145, 0.175, 0.374]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.2103544776119403
[2m[36m(func pid=30734)[0m top5: 0.7122201492537313
[2m[36m(func pid=30734)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=30734)[0m f1_macro: 0.1508613230725313
[2m[36m(func pid=30734)[0m f1_weighted: 0.21741330005468826
[2m[36m(func pid=30734)[0m f1_per_class: [0.157, 0.223, 0.208, 0.176, 0.033, 0.1, 0.363, 0.0, 0.117, 0.133]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1135 | Steps: 2 | Val loss: 1.9797 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=36355)[0m top1: 0.3162313432835821
[2m[36m(func pid=36355)[0m top5: 0.8302238805970149
[2m[36m(func pid=36355)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=36355)[0m f1_macro: 0.26990246866559037
[2m[36m(func pid=36355)[0m f1_weighted: 0.326088013826057
[2m[36m(func pid=36355)[0m f1_per_class: [0.302, 0.369, 0.471, 0.458, 0.207, 0.054, 0.322, 0.268, 0.172, 0.077]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0860 | Steps: 2 | Val loss: 2.6368 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31420)[0m top1: 0.365205223880597
[2m[36m(func pid=31420)[0m top5: 0.875
[2m[36m(func pid=31420)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=31420)[0m f1_macro: 0.3349878922103897
[2m[36m(func pid=31420)[0m f1_weighted: 0.39700232684504566
[2m[36m(func pid=31420)[0m f1_per_class: [0.381, 0.333, 0.667, 0.529, 0.103, 0.203, 0.448, 0.224, 0.201, 0.26]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9029 | Steps: 2 | Val loss: 2.1192 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5524 | Steps: 2 | Val loss: 18.4243 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:00:26 (running for 00:17:18.62)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.879 |      0.151 |                   34 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.114 |      0.335 |                   34 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.086 |      0.331 |                   12 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.446 |      0.27  |                   11 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.2873134328358209
[2m[36m(func pid=36270)[0m top5: 0.8325559701492538
[2m[36m(func pid=36270)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=36270)[0m f1_macro: 0.33066983435512215
[2m[36m(func pid=36270)[0m f1_weighted: 0.2864768754433565
[2m[36m(func pid=36270)[0m f1_per_class: [0.394, 0.397, 0.815, 0.189, 0.098, 0.253, 0.33, 0.232, 0.185, 0.415]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.21735074626865672
[2m[36m(func pid=30734)[0m top5: 0.7322761194029851
[2m[36m(func pid=30734)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=30734)[0m f1_macro: 0.15956547177374336
[2m[36m(func pid=30734)[0m f1_weighted: 0.23057339981850788
[2m[36m(func pid=30734)[0m f1_per_class: [0.171, 0.233, 0.239, 0.228, 0.032, 0.106, 0.349, 0.0, 0.123, 0.113]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1019 | Steps: 2 | Val loss: 2.0522 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=36355)[0m top1: 0.31763059701492535
[2m[36m(func pid=36355)[0m top5: 0.8889925373134329
[2m[36m(func pid=36355)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=36355)[0m f1_macro: 0.27297409056766364
[2m[36m(func pid=36355)[0m f1_weighted: 0.3259867450775479
[2m[36m(func pid=36355)[0m f1_per_class: [0.333, 0.376, 0.444, 0.431, 0.125, 0.235, 0.277, 0.248, 0.184, 0.077]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3516791044776119
[2m[36m(func pid=31420)[0m top5: 0.8661380597014925
[2m[36m(func pid=31420)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=31420)[0m f1_macro: 0.3286496992951605
[2m[36m(func pid=31420)[0m f1_weighted: 0.38440177114679497
[2m[36m(func pid=31420)[0m f1_per_class: [0.363, 0.322, 0.647, 0.529, 0.1, 0.205, 0.407, 0.262, 0.196, 0.256]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0276 | Steps: 2 | Val loss: 2.9746 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8130 | Steps: 2 | Val loss: 2.1056 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0031 | Steps: 2 | Val loss: 20.0565 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:00:31 (running for 00:17:23.83)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.903 |      0.16  |                   35 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.102 |      0.329 |                   35 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.028 |      0.323 |                   13 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  2.552 |      0.273 |                   12 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.269589552238806
[2m[36m(func pid=36270)[0m top5: 0.8246268656716418
[2m[36m(func pid=36270)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=36270)[0m f1_macro: 0.3229704301695167
[2m[36m(func pid=36270)[0m f1_weighted: 0.2521335388281109
[2m[36m(func pid=36270)[0m f1_per_class: [0.413, 0.407, 0.815, 0.194, 0.084, 0.269, 0.194, 0.24, 0.203, 0.41]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0604 | Steps: 2 | Val loss: 2.0891 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=30734)[0m top1: 0.2234141791044776
[2m[36m(func pid=30734)[0m top5: 0.7476679104477612
[2m[36m(func pid=30734)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=30734)[0m f1_macro: 0.16938527090681438
[2m[36m(func pid=30734)[0m f1_weighted: 0.2411474557884105
[2m[36m(func pid=30734)[0m f1_per_class: [0.185, 0.235, 0.293, 0.271, 0.03, 0.109, 0.34, 0.0, 0.141, 0.091]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3451492537313433
[2m[36m(func pid=36355)[0m top5: 0.902518656716418
[2m[36m(func pid=36355)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=36355)[0m f1_macro: 0.33140535976889046
[2m[36m(func pid=36355)[0m f1_weighted: 0.32540562941534457
[2m[36m(func pid=36355)[0m f1_per_class: [0.532, 0.44, 0.783, 0.428, 0.1, 0.326, 0.185, 0.241, 0.203, 0.077]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.341884328358209
[2m[36m(func pid=31420)[0m top5: 0.8684701492537313
[2m[36m(func pid=31420)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=31420)[0m f1_macro: 0.3296384389586494
[2m[36m(func pid=31420)[0m f1_weighted: 0.3717743359909778
[2m[36m(func pid=31420)[0m f1_per_class: [0.368, 0.328, 0.667, 0.526, 0.104, 0.22, 0.357, 0.263, 0.193, 0.271]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0143 | Steps: 2 | Val loss: 3.3297 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7349 | Steps: 2 | Val loss: 2.0904 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0001 | Steps: 2 | Val loss: 24.7781 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0489 | Steps: 2 | Val loss: 2.1338 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=36270)[0m top1: 0.26119402985074625
[2m[36m(func pid=36270)[0m top5: 0.8106343283582089
[2m[36m(func pid=36270)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=36270)[0m f1_macro: 0.3220983354641616
[2m[36m(func pid=36270)[0m f1_weighted: 0.23136314943277822
[2m[36m(func pid=36270)[0m f1_per_class: [0.45, 0.409, 0.815, 0.196, 0.095, 0.277, 0.113, 0.243, 0.229, 0.395]
[2m[36m(func pid=36270)[0m 
== Status ==
Current time: 2024-01-07 01:00:36 (running for 00:17:29.23)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.813 |      0.169 |                   36 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.06  |      0.33  |                   36 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.014 |      0.322 |                   14 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.003 |      0.331 |                   13 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.22994402985074627
[2m[36m(func pid=30734)[0m top5: 0.7611940298507462
[2m[36m(func pid=30734)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=30734)[0m f1_macro: 0.17976890863701328
[2m[36m(func pid=30734)[0m f1_weighted: 0.24921500905572797
[2m[36m(func pid=30734)[0m f1_per_class: [0.207, 0.242, 0.344, 0.317, 0.027, 0.101, 0.319, 0.0, 0.159, 0.082]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.341884328358209
[2m[36m(func pid=36355)[0m top5: 0.8973880597014925
[2m[36m(func pid=36355)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=36355)[0m f1_macro: 0.3303806110538542
[2m[36m(func pid=36355)[0m f1_weighted: 0.29878275374392327
[2m[36m(func pid=36355)[0m f1_per_class: [0.541, 0.465, 0.87, 0.403, 0.057, 0.34, 0.099, 0.22, 0.233, 0.077]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3381529850746269
[2m[36m(func pid=31420)[0m top5: 0.8642723880597015
[2m[36m(func pid=31420)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=31420)[0m f1_macro: 0.32993413292853724
[2m[36m(func pid=31420)[0m f1_weighted: 0.3632674868153916
[2m[36m(func pid=31420)[0m f1_per_class: [0.379, 0.333, 0.647, 0.53, 0.11, 0.228, 0.316, 0.271, 0.195, 0.291]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0301 | Steps: 2 | Val loss: 3.6085 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7300 | Steps: 2 | Val loss: 2.0805 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0357 | Steps: 2 | Val loss: 2.1690 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6701 | Steps: 2 | Val loss: 28.5126 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:00:42 (running for 00:17:34.36)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.735 |      0.18  |                   37 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.049 |      0.33  |                   37 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.03  |      0.324 |                   15 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.33  |                   14 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.26026119402985076
[2m[36m(func pid=36270)[0m top5: 0.8138992537313433
[2m[36m(func pid=36270)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=36270)[0m f1_macro: 0.3242570710010226
[2m[36m(func pid=36270)[0m f1_weighted: 0.22644296420863239
[2m[36m(func pid=36270)[0m f1_per_class: [0.471, 0.411, 0.815, 0.221, 0.11, 0.281, 0.069, 0.246, 0.22, 0.4]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.23367537313432835
[2m[36m(func pid=30734)[0m top5: 0.7602611940298507
[2m[36m(func pid=30734)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=30734)[0m f1_macro: 0.1843364106151802
[2m[36m(func pid=30734)[0m f1_weighted: 0.2515524866267259
[2m[36m(func pid=30734)[0m f1_per_class: [0.237, 0.247, 0.355, 0.353, 0.026, 0.112, 0.284, 0.0, 0.157, 0.073]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.33302238805970147
[2m[36m(func pid=31420)[0m top5: 0.8610074626865671
[2m[36m(func pid=31420)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=31420)[0m f1_macro: 0.32971599683705044
[2m[36m(func pid=31420)[0m f1_weighted: 0.35336276093533303
[2m[36m(func pid=31420)[0m f1_per_class: [0.385, 0.347, 0.667, 0.526, 0.114, 0.228, 0.276, 0.281, 0.197, 0.277]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.34888059701492535
[2m[36m(func pid=36355)[0m top5: 0.8875932835820896
[2m[36m(func pid=36355)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=36355)[0m f1_macro: 0.3540543852971842
[2m[36m(func pid=36355)[0m f1_weighted: 0.29337238730301013
[2m[36m(func pid=36355)[0m f1_per_class: [0.532, 0.502, 0.87, 0.398, 0.158, 0.337, 0.056, 0.218, 0.257, 0.214]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0392 | Steps: 2 | Val loss: 3.8609 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.5929 | Steps: 2 | Val loss: 2.0685 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0466 | Steps: 2 | Val loss: 2.2167 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6122 | Steps: 2 | Val loss: 30.2000 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 01:00:47 (running for 00:17:39.54)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.73  |      0.184 |                   38 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.036 |      0.33  |                   38 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.039 |      0.327 |                   16 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  1.67  |      0.354 |                   15 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.26119402985074625
[2m[36m(func pid=36270)[0m top5: 0.8138992537313433
[2m[36m(func pid=36270)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=36270)[0m f1_macro: 0.32745739032635746
[2m[36m(func pid=36270)[0m f1_weighted: 0.2291666057561984
[2m[36m(func pid=36270)[0m f1_per_class: [0.49, 0.413, 0.815, 0.249, 0.119, 0.275, 0.051, 0.242, 0.22, 0.4]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.23880597014925373
[2m[36m(func pid=30734)[0m top5: 0.7658582089552238
[2m[36m(func pid=30734)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=30734)[0m f1_macro: 0.20068149593204893
[2m[36m(func pid=30734)[0m f1_weighted: 0.2545376952366117
[2m[36m(func pid=30734)[0m f1_per_class: [0.26, 0.259, 0.431, 0.378, 0.027, 0.105, 0.249, 0.076, 0.154, 0.067]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.36007462686567165
[2m[36m(func pid=36355)[0m top5: 0.8768656716417911
[2m[36m(func pid=36355)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=36355)[0m f1_macro: 0.3681246532425472
[2m[36m(func pid=36355)[0m f1_weighted: 0.306835174080082
[2m[36m(func pid=36355)[0m f1_per_class: [0.537, 0.524, 0.818, 0.438, 0.154, 0.334, 0.053, 0.217, 0.183, 0.424]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3292910447761194
[2m[36m(func pid=31420)[0m top5: 0.8600746268656716
[2m[36m(func pid=31420)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=31420)[0m f1_macro: 0.3309561187248964
[2m[36m(func pid=31420)[0m f1_weighted: 0.3421449909544138
[2m[36m(func pid=31420)[0m f1_per_class: [0.394, 0.346, 0.667, 0.533, 0.119, 0.238, 0.226, 0.279, 0.195, 0.313]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0076 | Steps: 2 | Val loss: 4.0553 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6171 | Steps: 2 | Val loss: 2.0570 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0342 | Steps: 2 | Val loss: 2.2708 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:00:52 (running for 00:17:44.77)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.593 |      0.201 |                   39 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.047 |      0.331 |                   39 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.008 |      0.332 |                   17 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.612 |      0.368 |                   16 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.26632462686567165
[2m[36m(func pid=36270)[0m top5: 0.8148320895522388
[2m[36m(func pid=36270)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=36270)[0m f1_macro: 0.3321970826721888
[2m[36m(func pid=36270)[0m f1_weighted: 0.23627365376971418
[2m[36m(func pid=36270)[0m f1_per_class: [0.487, 0.425, 0.815, 0.273, 0.123, 0.277, 0.045, 0.239, 0.218, 0.419]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.4063 | Steps: 2 | Val loss: 31.6719 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=30734)[0m top1: 0.24720149253731344
[2m[36m(func pid=30734)[0m top5: 0.7691231343283582
[2m[36m(func pid=30734)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=30734)[0m f1_macro: 0.20943984028488966
[2m[36m(func pid=30734)[0m f1_weighted: 0.26013811091219796
[2m[36m(func pid=30734)[0m f1_per_class: [0.259, 0.277, 0.449, 0.403, 0.028, 0.102, 0.223, 0.138, 0.143, 0.072]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3255597014925373
[2m[36m(func pid=31420)[0m top5: 0.855410447761194
[2m[36m(func pid=31420)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=31420)[0m f1_macro: 0.33044562193408267
[2m[36m(func pid=31420)[0m f1_weighted: 0.33372493750289867
[2m[36m(func pid=31420)[0m f1_per_class: [0.396, 0.354, 0.667, 0.531, 0.123, 0.24, 0.193, 0.285, 0.191, 0.325]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.36613805970149255
[2m[36m(func pid=36355)[0m top5: 0.8535447761194029
[2m[36m(func pid=36355)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=36355)[0m f1_macro: 0.3713790916963284
[2m[36m(func pid=36355)[0m f1_weighted: 0.31584074732637085
[2m[36m(func pid=36355)[0m f1_per_class: [0.565, 0.536, 0.833, 0.485, 0.102, 0.324, 0.039, 0.215, 0.137, 0.478]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0107 | Steps: 2 | Val loss: 4.1519 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5680 | Steps: 2 | Val loss: 2.0488 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0325 | Steps: 2 | Val loss: 2.3453 | Batch size: 32 | lr: 0.001 | Duration: 2.57s
== Status ==
Current time: 2024-01-07 01:00:57 (running for 00:17:50.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.617 |      0.209 |                   40 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.034 |      0.33  |                   40 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.011 |      0.339 |                   18 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  1.406 |      0.371 |                   17 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.27472014925373134
[2m[36m(func pid=36270)[0m top5: 0.820429104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=36270)[0m f1_macro: 0.33920953003349474
[2m[36m(func pid=36270)[0m f1_weighted: 0.2510974918287818
[2m[36m(func pid=36270)[0m f1_per_class: [0.484, 0.419, 0.815, 0.324, 0.133, 0.282, 0.048, 0.242, 0.212, 0.433]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.24860074626865672
[2m[36m(func pid=30734)[0m top5: 0.7691231343283582
[2m[36m(func pid=30734)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=30734)[0m f1_macro: 0.21457796413273109
[2m[36m(func pid=30734)[0m f1_weighted: 0.25696795361853153
[2m[36m(func pid=30734)[0m f1_per_class: [0.273, 0.289, 0.468, 0.408, 0.015, 0.101, 0.193, 0.179, 0.138, 0.082]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7176 | Steps: 2 | Val loss: 32.2371 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=31420)[0m top1: 0.3204291044776119
[2m[36m(func pid=31420)[0m top5: 0.8488805970149254
[2m[36m(func pid=31420)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=31420)[0m f1_macro: 0.3256852064910381
[2m[36m(func pid=31420)[0m f1_weighted: 0.32405428369990164
[2m[36m(func pid=31420)[0m f1_per_class: [0.387, 0.358, 0.667, 0.528, 0.126, 0.241, 0.162, 0.284, 0.191, 0.313]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.36380597014925375
[2m[36m(func pid=36355)[0m top5: 0.8372201492537313
[2m[36m(func pid=36355)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=36355)[0m f1_macro: 0.3621237397877365
[2m[36m(func pid=36355)[0m f1_weighted: 0.32253696618112804
[2m[36m(func pid=36355)[0m f1_per_class: [0.569, 0.522, 0.88, 0.522, 0.075, 0.311, 0.044, 0.234, 0.085, 0.378]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0141 | Steps: 2 | Val loss: 4.2674 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5050 | Steps: 2 | Val loss: 2.0412 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0272 | Steps: 2 | Val loss: 2.3871 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
== Status ==
Current time: 2024-01-07 01:01:02 (running for 00:17:55.14)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.568 |      0.215 |                   41 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.032 |      0.326 |                   41 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.014 |      0.342 |                   19 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  2.718 |      0.362 |                   18 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.28031716417910446
[2m[36m(func pid=36270)[0m top5: 0.8246268656716418
[2m[36m(func pid=36270)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=36270)[0m f1_macro: 0.34201263195119236
[2m[36m(func pid=36270)[0m f1_weighted: 0.25844875824506497
[2m[36m(func pid=36270)[0m f1_per_class: [0.51, 0.413, 0.815, 0.35, 0.121, 0.29, 0.048, 0.245, 0.209, 0.419]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0001 | Steps: 2 | Val loss: 33.0431 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=30734)[0m top1: 0.2574626865671642
[2m[36m(func pid=30734)[0m top5: 0.7681902985074627
[2m[36m(func pid=30734)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=30734)[0m f1_macro: 0.22215656271465134
[2m[36m(func pid=30734)[0m f1_weighted: 0.2573279030417595
[2m[36m(func pid=30734)[0m f1_per_class: [0.286, 0.298, 0.478, 0.439, 0.03, 0.1, 0.152, 0.212, 0.144, 0.084]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.31902985074626866
[2m[36m(func pid=31420)[0m top5: 0.851679104477612
[2m[36m(func pid=31420)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=31420)[0m f1_macro: 0.3255426531968616
[2m[36m(func pid=31420)[0m f1_weighted: 0.3190567315765905
[2m[36m(func pid=31420)[0m f1_per_class: [0.4, 0.36, 0.667, 0.532, 0.127, 0.235, 0.142, 0.281, 0.19, 0.321]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3516791044776119
[2m[36m(func pid=36355)[0m top5: 0.8157649253731343
[2m[36m(func pid=36355)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=36355)[0m f1_macro: 0.3497042529334967
[2m[36m(func pid=36355)[0m f1_weighted: 0.3183209257344217
[2m[36m(func pid=36355)[0m f1_per_class: [0.559, 0.506, 0.833, 0.531, 0.087, 0.284, 0.039, 0.276, 0.069, 0.313]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0117 | Steps: 2 | Val loss: 4.3524 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.5484 | Steps: 2 | Val loss: 2.0452 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0533 | Steps: 2 | Val loss: 2.4493 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:01:08 (running for 00:18:00.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.505 |      0.222 |                   42 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.027 |      0.326 |                   42 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.012 |      0.346 |                   20 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.35  |                   19 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.29011194029850745
[2m[36m(func pid=36270)[0m top5: 0.8344216417910447
[2m[36m(func pid=36270)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=36270)[0m f1_macro: 0.34605035274013335
[2m[36m(func pid=36270)[0m f1_weighted: 0.27153380830457036
[2m[36m(func pid=36270)[0m f1_per_class: [0.521, 0.416, 0.815, 0.385, 0.119, 0.289, 0.057, 0.244, 0.214, 0.4]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.26259328358208955
[2m[36m(func pid=30734)[0m top5: 0.7523320895522388
[2m[36m(func pid=30734)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=30734)[0m f1_macro: 0.22639438048022287
[2m[36m(func pid=30734)[0m f1_weighted: 0.2514607511309323
[2m[36m(func pid=30734)[0m f1_per_class: [0.287, 0.305, 0.5, 0.46, 0.046, 0.11, 0.101, 0.23, 0.138, 0.088]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0034 | Steps: 2 | Val loss: 35.3782 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=31420)[0m top1: 0.3138992537313433
[2m[36m(func pid=31420)[0m top5: 0.8488805970149254
[2m[36m(func pid=31420)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=31420)[0m f1_macro: 0.3225757046016061
[2m[36m(func pid=31420)[0m f1_weighted: 0.30730343413895406
[2m[36m(func pid=31420)[0m f1_per_class: [0.411, 0.353, 0.647, 0.538, 0.119, 0.241, 0.099, 0.275, 0.186, 0.356]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0089 | Steps: 2 | Val loss: 4.4132 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=36355)[0m top1: 0.33255597014925375
[2m[36m(func pid=36355)[0m top5: 0.7952425373134329
[2m[36m(func pid=36355)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=36355)[0m f1_macro: 0.3316455983274694
[2m[36m(func pid=36355)[0m f1_weighted: 0.3086121798785255
[2m[36m(func pid=36355)[0m f1_per_class: [0.559, 0.474, 0.8, 0.527, 0.076, 0.274, 0.036, 0.289, 0.051, 0.23]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4137 | Steps: 2 | Val loss: 2.0483 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0265 | Steps: 2 | Val loss: 2.4806 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=36270)[0m top1: 0.30177238805970147
[2m[36m(func pid=36270)[0m top5: 0.8395522388059702
[2m[36m(func pid=36270)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=36270)[0m f1_macro: 0.3527711541309397
[2m[36m(func pid=36270)[0m f1_weighted: 0.28719454578887826
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.421, 0.815, 0.425, 0.111, 0.288, 0.069, 0.249, 0.216, 0.406]
== Status ==
Current time: 2024-01-07 01:01:13 (running for 00:18:05.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.548 |      0.226 |                   43 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.053 |      0.323 |                   43 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.009 |      0.353 |                   21 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.003 |      0.332 |                   20 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.261660447761194
[2m[36m(func pid=30734)[0m top5: 0.7472014925373134
[2m[36m(func pid=30734)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.22842933209289615
[2m[36m(func pid=30734)[0m f1_weighted: 0.24789362053058728
[2m[36m(func pid=30734)[0m f1_per_class: [0.293, 0.293, 0.524, 0.467, 0.044, 0.103, 0.089, 0.239, 0.143, 0.091]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0002 | Steps: 2 | Val loss: 38.7842 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=31420)[0m top1: 0.3162313432835821
[2m[36m(func pid=31420)[0m top5: 0.8484141791044776
[2m[36m(func pid=31420)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=31420)[0m f1_macro: 0.32329282006081245
[2m[36m(func pid=31420)[0m f1_weighted: 0.30773124983758215
[2m[36m(func pid=31420)[0m f1_per_class: [0.407, 0.366, 0.647, 0.54, 0.122, 0.24, 0.091, 0.278, 0.189, 0.353]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0028 | Steps: 2 | Val loss: 4.4175 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=36355)[0m top1: 0.30783582089552236
[2m[36m(func pid=36355)[0m top5: 0.7714552238805971
[2m[36m(func pid=36355)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=36355)[0m f1_macro: 0.31528177836961174
[2m[36m(func pid=36355)[0m f1_weighted: 0.2922142383336918
[2m[36m(func pid=36355)[0m f1_per_class: [0.574, 0.413, 0.769, 0.529, 0.066, 0.236, 0.024, 0.334, 0.027, 0.179]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3723 | Steps: 2 | Val loss: 2.0496 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0251 | Steps: 2 | Val loss: 2.5006 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:01:18 (running for 00:18:10.81)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.414 |      0.228 |                   44 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.027 |      0.323 |                   44 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.003 |      0.358 |                   22 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   21 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.3111007462686567
[2m[36m(func pid=36270)[0m top5: 0.8451492537313433
[2m[36m(func pid=36270)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.3576281313095332
[2m[36m(func pid=36270)[0m f1_weighted: 0.30001357275150753
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.422, 0.815, 0.452, 0.111, 0.287, 0.086, 0.253, 0.21, 0.413]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.26072761194029853
[2m[36m(func pid=30734)[0m top5: 0.7434701492537313
[2m[36m(func pid=30734)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=30734)[0m f1_macro: 0.2355525077387078
[2m[36m(func pid=30734)[0m f1_weighted: 0.24353629506697713
[2m[36m(func pid=30734)[0m f1_per_class: [0.3, 0.293, 0.595, 0.466, 0.043, 0.102, 0.071, 0.25, 0.145, 0.091]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.3517 | Steps: 2 | Val loss: 42.6758 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=31420)[0m top1: 0.31669776119402987
[2m[36m(func pid=31420)[0m top5: 0.8498134328358209
[2m[36m(func pid=31420)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=31420)[0m f1_macro: 0.32665977789649797
[2m[36m(func pid=31420)[0m f1_weighted: 0.30971608953120977
[2m[36m(func pid=31420)[0m f1_per_class: [0.42, 0.375, 0.647, 0.54, 0.129, 0.24, 0.093, 0.269, 0.185, 0.369]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.28777985074626866
[2m[36m(func pid=36355)[0m top5: 0.7630597014925373
[2m[36m(func pid=36355)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=36355)[0m f1_macro: 0.3024367958987466
[2m[36m(func pid=36355)[0m f1_weighted: 0.28045778381809666
[2m[36m(func pid=36355)[0m f1_per_class: [0.584, 0.395, 0.769, 0.53, 0.058, 0.202, 0.015, 0.298, 0.027, 0.145]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0016 | Steps: 2 | Val loss: 4.4587 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3608 | Steps: 2 | Val loss: 2.0488 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0359 | Steps: 2 | Val loss: 2.5208 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:01:23 (running for 00:18:16.13)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.372 |      0.236 |                   45 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.025 |      0.327 |                   45 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.002 |      0.357 |                   23 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  2.352 |      0.302 |                   22 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.314365671641791
[2m[36m(func pid=36270)[0m top5: 0.8502798507462687
[2m[36m(func pid=36270)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=36270)[0m f1_macro: 0.3568152179456784
[2m[36m(func pid=36270)[0m f1_weighted: 0.3083680259856132
[2m[36m(func pid=36270)[0m f1_per_class: [0.517, 0.417, 0.815, 0.465, 0.108, 0.289, 0.107, 0.247, 0.203, 0.4]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.2644589552238806
[2m[36m(func pid=30734)[0m top5: 0.7388059701492538
[2m[36m(func pid=30734)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=30734)[0m f1_macro: 0.24138142133902143
[2m[36m(func pid=30734)[0m f1_weighted: 0.24341848388320922
[2m[36m(func pid=30734)[0m f1_per_class: [0.316, 0.31, 0.611, 0.471, 0.046, 0.117, 0.048, 0.25, 0.151, 0.093]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4291 | Steps: 2 | Val loss: 44.9130 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=31420)[0m top1: 0.31902985074626866
[2m[36m(func pid=31420)[0m top5: 0.8526119402985075
[2m[36m(func pid=31420)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=31420)[0m f1_macro: 0.33181645893507944
[2m[36m(func pid=31420)[0m f1_weighted: 0.3112060058682338
[2m[36m(func pid=31420)[0m f1_per_class: [0.418, 0.394, 0.647, 0.537, 0.116, 0.24, 0.088, 0.267, 0.192, 0.419]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.28031716417910446
[2m[36m(func pid=36355)[0m top5: 0.7630597014925373
[2m[36m(func pid=36355)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=36355)[0m f1_macro: 0.29521173692340463
[2m[36m(func pid=36355)[0m f1_weighted: 0.2793967729751659
[2m[36m(func pid=36355)[0m f1_per_class: [0.564, 0.397, 0.769, 0.535, 0.058, 0.17, 0.025, 0.277, 0.028, 0.13]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.3024 | Steps: 2 | Val loss: 2.0507 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0025 | Steps: 2 | Val loss: 4.4668 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0405 | Steps: 2 | Val loss: 2.5302 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:01:29 (running for 00:18:21.53)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.361 |      0.241 |                   46 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.036 |      0.332 |                   46 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.003 |      0.363 |                   24 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.429 |      0.295 |                   23 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.324160447761194
[2m[36m(func pid=36270)[0m top5: 0.8568097014925373
[2m[36m(func pid=36270)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=36270)[0m f1_macro: 0.363211384297147
[2m[36m(func pid=36270)[0m f1_weighted: 0.3208868048874186
[2m[36m(func pid=36270)[0m f1_per_class: [0.525, 0.422, 0.815, 0.48, 0.112, 0.282, 0.133, 0.252, 0.206, 0.406]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8306 | Steps: 2 | Val loss: 46.4357 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=30734)[0m top1: 0.26725746268656714
[2m[36m(func pid=30734)[0m top5: 0.7313432835820896
[2m[36m(func pid=30734)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=30734)[0m f1_macro: 0.24443575966622758
[2m[36m(func pid=30734)[0m f1_weighted: 0.24447889649381593
[2m[36m(func pid=30734)[0m f1_per_class: [0.333, 0.318, 0.611, 0.475, 0.044, 0.119, 0.039, 0.258, 0.154, 0.092]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.32276119402985076
[2m[36m(func pid=31420)[0m top5: 0.8498134328358209
[2m[36m(func pid=31420)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=31420)[0m f1_macro: 0.3316610461792289
[2m[36m(func pid=31420)[0m f1_weighted: 0.31515176650074395
[2m[36m(func pid=31420)[0m f1_per_class: [0.416, 0.401, 0.647, 0.538, 0.12, 0.24, 0.096, 0.273, 0.193, 0.393]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.2756529850746269
[2m[36m(func pid=36355)[0m top5: 0.773320895522388
[2m[36m(func pid=36355)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=36355)[0m f1_macro: 0.2950585472880404
[2m[36m(func pid=36355)[0m f1_weighted: 0.2762255844059957
[2m[36m(func pid=36355)[0m f1_per_class: [0.564, 0.38, 0.815, 0.537, 0.059, 0.15, 0.03, 0.269, 0.027, 0.12]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0006 | Steps: 2 | Val loss: 4.5224 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.3281 | Steps: 2 | Val loss: 2.0500 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0130 | Steps: 2 | Val loss: 2.5459 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 01:01:34 (running for 00:18:26.61)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.302 |      0.244 |                   47 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.04  |      0.332 |                   47 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.367 |                   25 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.831 |      0.295 |                   24 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.3283582089552239
[2m[36m(func pid=36270)[0m top5: 0.8628731343283582
[2m[36m(func pid=36270)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=36270)[0m f1_macro: 0.3673822097156613
[2m[36m(func pid=36270)[0m f1_weighted: 0.32688395645414237
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.419, 0.815, 0.488, 0.11, 0.283, 0.145, 0.25, 0.208, 0.426]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.32322761194029853
[2m[36m(func pid=31420)[0m top5: 0.8512126865671642
[2m[36m(func pid=31420)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=31420)[0m f1_macro: 0.3347845495741631
[2m[36m(func pid=31420)[0m f1_weighted: 0.3158198400277678
[2m[36m(func pid=31420)[0m f1_per_class: [0.42, 0.413, 0.647, 0.532, 0.106, 0.237, 0.096, 0.274, 0.197, 0.426]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.9288 | Steps: 2 | Val loss: 45.6355 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=30734)[0m top1: 0.26865671641791045
[2m[36m(func pid=30734)[0m top5: 0.7276119402985075
[2m[36m(func pid=30734)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=30734)[0m f1_macro: 0.2445232573828457
[2m[36m(func pid=30734)[0m f1_weighted: 0.24442183716022603
[2m[36m(func pid=30734)[0m f1_per_class: [0.344, 0.324, 0.595, 0.474, 0.045, 0.127, 0.033, 0.251, 0.155, 0.096]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.279384328358209
[2m[36m(func pid=36355)[0m top5: 0.7947761194029851
[2m[36m(func pid=36355)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=36355)[0m f1_macro: 0.2972063730848463
[2m[36m(func pid=36355)[0m f1_weighted: 0.2829863274859695
[2m[36m(func pid=36355)[0m f1_per_class: [0.561, 0.375, 0.815, 0.537, 0.063, 0.138, 0.06, 0.261, 0.051, 0.111]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0171 | Steps: 2 | Val loss: 2.5616 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0009 | Steps: 2 | Val loss: 4.5653 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2656 | Steps: 2 | Val loss: 2.0516 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:01:39 (running for 00:18:31.84)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.328 |      0.245 |                   48 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.017 |      0.332 |                   49 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.367 |                   25 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.929 |      0.297 |                   25 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.324160447761194
[2m[36m(func pid=31420)[0m top5: 0.8535447761194029
[2m[36m(func pid=31420)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=31420)[0m f1_macro: 0.33241077601579466
[2m[36m(func pid=31420)[0m f1_weighted: 0.318193097013418
[2m[36m(func pid=31420)[0m f1_per_class: [0.416, 0.418, 0.647, 0.53, 0.105, 0.236, 0.104, 0.275, 0.2, 0.393]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.33861940298507465
[2m[36m(func pid=36270)[0m top5: 0.8666044776119403
[2m[36m(func pid=36270)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=36270)[0m f1_macro: 0.3745405640226296
[2m[36m(func pid=36270)[0m f1_weighted: 0.3387265577750384
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.426, 0.815, 0.498, 0.11, 0.293, 0.166, 0.258, 0.211, 0.441]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.2667910447761194
[2m[36m(func pid=30734)[0m top5: 0.7252798507462687
[2m[36m(func pid=30734)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.244025064590847
[2m[36m(func pid=30734)[0m f1_weighted: 0.24152954572116675
[2m[36m(func pid=30734)[0m f1_per_class: [0.346, 0.335, 0.595, 0.467, 0.046, 0.121, 0.027, 0.244, 0.157, 0.103]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0003 | Steps: 2 | Val loss: 44.9129 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=36355)[0m top1: 0.28404850746268656
[2m[36m(func pid=36355)[0m top5: 0.8125
[2m[36m(func pid=36355)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=36355)[0m f1_macro: 0.2992754212105043
[2m[36m(func pid=36355)[0m f1_weighted: 0.29760959864849035
[2m[36m(func pid=36355)[0m f1_per_class: [0.549, 0.37, 0.815, 0.541, 0.067, 0.131, 0.114, 0.252, 0.049, 0.106]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0161 | Steps: 2 | Val loss: 2.5529 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0006 | Steps: 2 | Val loss: 4.5527 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1746 | Steps: 2 | Val loss: 2.0522 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=31420)[0m top1: 0.3278917910447761
[2m[36m(func pid=31420)[0m top5: 0.8572761194029851
[2m[36m(func pid=31420)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=31420)[0m f1_macro: 0.33570634673685645
[2m[36m(func pid=31420)[0m f1_weighted: 0.324484457213926
[2m[36m(func pid=31420)[0m f1_per_class: [0.416, 0.417, 0.647, 0.531, 0.107, 0.239, 0.123, 0.276, 0.201, 0.4]
== Status ==
Current time: 2024-01-07 01:01:44 (running for 00:18:36.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.266 |      0.244 |                   49 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.016 |      0.336 |                   50 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.375 |                   26 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.299 |                   26 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.34281716417910446
[2m[36m(func pid=36270)[0m top5: 0.8726679104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=36270)[0m f1_macro: 0.375997964133224
[2m[36m(func pid=36270)[0m f1_weighted: 0.34454098060121474
[2m[36m(func pid=36270)[0m f1_per_class: [0.525, 0.422, 0.815, 0.509, 0.111, 0.288, 0.179, 0.255, 0.216, 0.441]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0000 | Steps: 2 | Val loss: 45.6225 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=30734)[0m top1: 0.2681902985074627
[2m[36m(func pid=30734)[0m top5: 0.7220149253731343
[2m[36m(func pid=30734)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=30734)[0m f1_macro: 0.2481583364325212
[2m[36m(func pid=30734)[0m f1_weighted: 0.24159999385513697
[2m[36m(func pid=30734)[0m f1_per_class: [0.337, 0.347, 0.629, 0.468, 0.044, 0.113, 0.022, 0.24, 0.168, 0.113]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0131 | Steps: 2 | Val loss: 2.5590 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=36355)[0m top1: 0.28591417910447764
[2m[36m(func pid=36355)[0m top5: 0.8222947761194029
[2m[36m(func pid=36355)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=36355)[0m f1_macro: 0.2969402034810289
[2m[36m(func pid=36355)[0m f1_weighted: 0.30652847437724884
[2m[36m(func pid=36355)[0m f1_per_class: [0.527, 0.356, 0.815, 0.543, 0.072, 0.119, 0.161, 0.231, 0.045, 0.1]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0004 | Steps: 2 | Val loss: 4.5678 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1334 | Steps: 2 | Val loss: 2.0535 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:01:49 (running for 00:18:42.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.175 |      0.248 |                   50 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.013 |      0.339 |                   51 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.376 |                   27 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.297 |                   27 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.3278917910447761
[2m[36m(func pid=31420)[0m top5: 0.8582089552238806
[2m[36m(func pid=31420)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=31420)[0m f1_macro: 0.3385203692390977
[2m[36m(func pid=31420)[0m f1_weighted: 0.3253935835751375
[2m[36m(func pid=31420)[0m f1_per_class: [0.416, 0.413, 0.647, 0.524, 0.107, 0.239, 0.134, 0.279, 0.201, 0.426]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.26492537313432835
[2m[36m(func pid=30734)[0m top5: 0.7229477611940298
[2m[36m(func pid=30734)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=30734)[0m f1_macro: 0.24658481110895614
[2m[36m(func pid=30734)[0m f1_weighted: 0.23820943719487916
[2m[36m(func pid=30734)[0m f1_per_class: [0.342, 0.354, 0.629, 0.469, 0.056, 0.104, 0.012, 0.229, 0.148, 0.123]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.3493470149253731
[2m[36m(func pid=36270)[0m top5: 0.8759328358208955
[2m[36m(func pid=36270)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=36270)[0m f1_macro: 0.3803656538356759
[2m[36m(func pid=36270)[0m f1_weighted: 0.3526773468191569
[2m[36m(func pid=36270)[0m f1_per_class: [0.532, 0.421, 0.815, 0.525, 0.117, 0.278, 0.195, 0.259, 0.214, 0.448]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0846 | Steps: 2 | Val loss: 44.6776 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=36355)[0m top1: 0.29757462686567165
[2m[36m(func pid=36355)[0m top5: 0.8269589552238806
[2m[36m(func pid=36355)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=36355)[0m f1_macro: 0.3028930944764038
[2m[36m(func pid=36355)[0m f1_weighted: 0.3239292388182319
[2m[36m(func pid=36355)[0m f1_per_class: [0.532, 0.363, 0.815, 0.546, 0.07, 0.12, 0.212, 0.23, 0.043, 0.098]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0161 | Steps: 2 | Val loss: 2.5434 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0935 | Steps: 2 | Val loss: 2.0562 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0003 | Steps: 2 | Val loss: 4.5661 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 01:01:54 (running for 00:18:47.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.133 |      0.247 |                   51 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.016 |      0.341 |                   52 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.38  |                   28 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.085 |      0.303 |                   28 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.33302238805970147
[2m[36m(func pid=31420)[0m top5: 0.8586753731343284
[2m[36m(func pid=31420)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=31420)[0m f1_macro: 0.34120080669178804
[2m[36m(func pid=31420)[0m f1_weighted: 0.33240762804656104
[2m[36m(func pid=31420)[0m f1_per_class: [0.425, 0.416, 0.647, 0.531, 0.097, 0.238, 0.149, 0.279, 0.204, 0.426]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0000 | Steps: 2 | Val loss: 44.1432 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=30734)[0m top1: 0.261660447761194
[2m[36m(func pid=30734)[0m top5: 0.7262126865671642
[2m[36m(func pid=30734)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.24571633913465032
[2m[36m(func pid=30734)[0m f1_weighted: 0.23628916680744866
[2m[36m(func pid=30734)[0m f1_per_class: [0.332, 0.354, 0.629, 0.469, 0.052, 0.094, 0.009, 0.225, 0.161, 0.132]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.35494402985074625
[2m[36m(func pid=36270)[0m top5: 0.8815298507462687
[2m[36m(func pid=36270)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=36270)[0m f1_macro: 0.38197958494834283
[2m[36m(func pid=36270)[0m f1_weighted: 0.3610207671398699
[2m[36m(func pid=36270)[0m f1_per_class: [0.525, 0.42, 0.815, 0.534, 0.11, 0.285, 0.214, 0.261, 0.201, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0099 | Steps: 2 | Val loss: 2.5504 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=36355)[0m top1: 0.31763059701492535
[2m[36m(func pid=36355)[0m top5: 0.8316231343283582
[2m[36m(func pid=36355)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=36355)[0m f1_macro: 0.3151090697902637
[2m[36m(func pid=36355)[0m f1_weighted: 0.3530396195612287
[2m[36m(func pid=36355)[0m f1_per_class: [0.542, 0.361, 0.815, 0.552, 0.072, 0.108, 0.307, 0.233, 0.061, 0.101]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.1125 | Steps: 2 | Val loss: 2.0594 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.6052 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:02:00 (running for 00:18:52.28)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.094 |      0.246 |                   52 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.01  |      0.343 |                   53 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.382 |                   29 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   29 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.33722014925373134
[2m[36m(func pid=31420)[0m top5: 0.8628731343283582
[2m[36m(func pid=31420)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=31420)[0m f1_macro: 0.3427716428458373
[2m[36m(func pid=31420)[0m f1_weighted: 0.3405976330831696
[2m[36m(func pid=31420)[0m f1_per_class: [0.435, 0.418, 0.611, 0.522, 0.103, 0.24, 0.181, 0.283, 0.208, 0.426]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0000 | Steps: 2 | Val loss: 43.6266 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=30734)[0m top1: 0.2630597014925373
[2m[36m(func pid=30734)[0m top5: 0.7285447761194029
[2m[36m(func pid=30734)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=30734)[0m f1_macro: 0.2510419743192508
[2m[36m(func pid=30734)[0m f1_weighted: 0.23920258094317975
[2m[36m(func pid=30734)[0m f1_per_class: [0.332, 0.363, 0.667, 0.471, 0.051, 0.095, 0.012, 0.222, 0.154, 0.144]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.36100746268656714
[2m[36m(func pid=36270)[0m top5: 0.8819962686567164
[2m[36m(func pid=36270)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=36270)[0m f1_macro: 0.38558009688941247
[2m[36m(func pid=36270)[0m f1_weighted: 0.369890225967428
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.426, 0.815, 0.538, 0.112, 0.277, 0.24, 0.251, 0.212, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0094 | Steps: 2 | Val loss: 2.5491 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=36355)[0m top1: 0.332089552238806
[2m[36m(func pid=36355)[0m top5: 0.8376865671641791
[2m[36m(func pid=36355)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=36355)[0m f1_macro: 0.316891663075847
[2m[36m(func pid=36355)[0m f1_weighted: 0.3691169051042698
[2m[36m(func pid=36355)[0m f1_per_class: [0.528, 0.36, 0.815, 0.559, 0.078, 0.082, 0.368, 0.216, 0.059, 0.102]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0632 | Steps: 2 | Val loss: 2.0598 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.6493 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31420)[0m top1: 0.3423507462686567
[2m[36m(func pid=31420)[0m top5: 0.863339552238806
[2m[36m(func pid=31420)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=31420)[0m f1_macro: 0.3485912329925732
[2m[36m(func pid=31420)[0m f1_weighted: 0.34668545663675293
[2m[36m(func pid=31420)[0m f1_per_class: [0.428, 0.416, 0.649, 0.536, 0.107, 0.235, 0.191, 0.287, 0.205, 0.433]
[2m[36m(func pid=31420)[0m 
== Status ==
Current time: 2024-01-07 01:02:06 (running for 00:18:58.37)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.063 |      0.262 |                   54 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.009 |      0.349 |                   54 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.386 |                   30 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.317 |                   30 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.2593283582089552
[2m[36m(func pid=30734)[0m top5: 0.7346082089552238
[2m[36m(func pid=30734)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=30734)[0m f1_macro: 0.2618892128059578
[2m[36m(func pid=30734)[0m f1_weighted: 0.23755176171135006
[2m[36m(func pid=30734)[0m f1_per_class: [0.351, 0.355, 0.733, 0.465, 0.046, 0.099, 0.009, 0.219, 0.193, 0.15]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0000 | Steps: 2 | Val loss: 42.9230 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=36270)[0m top1: 0.36847014925373134
[2m[36m(func pid=36270)[0m top5: 0.8824626865671642
[2m[36m(func pid=36270)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=36270)[0m f1_macro: 0.38987829144343605
[2m[36m(func pid=36270)[0m f1_weighted: 0.3780776738495797
[2m[36m(func pid=36270)[0m f1_per_class: [0.533, 0.431, 0.815, 0.541, 0.112, 0.283, 0.257, 0.255, 0.215, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0109 | Steps: 2 | Val loss: 2.5474 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=36355)[0m top1: 0.35494402985074625
[2m[36m(func pid=36355)[0m top5: 0.8404850746268657
[2m[36m(func pid=36355)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=36355)[0m f1_macro: 0.3271072176079547
[2m[36m(func pid=36355)[0m f1_weighted: 0.3900738747543714
[2m[36m(func pid=36355)[0m f1_per_class: [0.514, 0.346, 0.815, 0.562, 0.085, 0.074, 0.443, 0.202, 0.127, 0.102]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.0398 | Steps: 2 | Val loss: 2.0559 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.7260 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=31420)[0m top1: 0.34095149253731344
[2m[36m(func pid=31420)[0m top5: 0.8666044776119403
[2m[36m(func pid=31420)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=31420)[0m f1_macro: 0.3431307483543115
[2m[36m(func pid=31420)[0m f1_weighted: 0.34649180172102134
[2m[36m(func pid=31420)[0m f1_per_class: [0.428, 0.416, 0.6, 0.53, 0.108, 0.235, 0.198, 0.284, 0.199, 0.433]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0000 | Steps: 2 | Val loss: 42.6965 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:02:11 (running for 00:19:03.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  1.04  |      0.26  |                   55 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.011 |      0.343 |                   55 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.39  |                   31 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.327 |                   31 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.2560634328358209
[2m[36m(func pid=30734)[0m top5: 0.7472014925373134
[2m[36m(func pid=30734)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=30734)[0m f1_macro: 0.25986795897703063
[2m[36m(func pid=30734)[0m f1_weighted: 0.23720290832516527
[2m[36m(func pid=30734)[0m f1_per_class: [0.344, 0.353, 0.71, 0.463, 0.055, 0.099, 0.012, 0.215, 0.191, 0.157]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.37173507462686567
[2m[36m(func pid=36270)[0m top5: 0.886660447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.39223532714651543
[2m[36m(func pid=36270)[0m f1_weighted: 0.3810089690441843
[2m[36m(func pid=36270)[0m f1_per_class: [0.533, 0.428, 0.815, 0.546, 0.117, 0.285, 0.263, 0.257, 0.214, 0.464]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0074 | Steps: 2 | Val loss: 2.5555 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=36355)[0m top1: 0.3726679104477612
[2m[36m(func pid=36355)[0m top5: 0.8362873134328358
[2m[36m(func pid=36355)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=36355)[0m f1_macro: 0.331983274700779
[2m[36m(func pid=36355)[0m f1_weighted: 0.4023513223942836
[2m[36m(func pid=36355)[0m f1_per_class: [0.529, 0.348, 0.786, 0.562, 0.094, 0.075, 0.482, 0.196, 0.137, 0.111]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8881 | Steps: 2 | Val loss: 2.0492 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0001 | Steps: 2 | Val loss: 4.7930 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=31420)[0m top1: 0.34328358208955223
[2m[36m(func pid=31420)[0m top5: 0.8647388059701493
[2m[36m(func pid=31420)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=31420)[0m f1_macro: 0.34221115700673
[2m[36m(func pid=31420)[0m f1_weighted: 0.34965341326745225
[2m[36m(func pid=31420)[0m f1_per_class: [0.433, 0.421, 0.571, 0.525, 0.115, 0.228, 0.212, 0.287, 0.204, 0.426]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0000 | Steps: 2 | Val loss: 43.8098 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:02:16 (running for 00:19:08.89)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.888 |      0.264 |                   56 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.007 |      0.342 |                   56 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.392 |                   32 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.332 |                   32 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.2593283582089552
[2m[36m(func pid=30734)[0m top5: 0.7611940298507462
[2m[36m(func pid=30734)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=30734)[0m f1_macro: 0.2644734580926141
[2m[36m(func pid=30734)[0m f1_weighted: 0.24011349322289452
[2m[36m(func pid=30734)[0m f1_per_class: [0.346, 0.36, 0.71, 0.465, 0.066, 0.104, 0.012, 0.216, 0.197, 0.169]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.37593283582089554
[2m[36m(func pid=36270)[0m top5: 0.8908582089552238
[2m[36m(func pid=36270)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=36270)[0m f1_macro: 0.3923038249119809
[2m[36m(func pid=36270)[0m f1_weighted: 0.3861848277616713
[2m[36m(func pid=36270)[0m f1_per_class: [0.522, 0.432, 0.815, 0.554, 0.111, 0.273, 0.276, 0.26, 0.208, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0084 | Steps: 2 | Val loss: 2.5471 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=36355)[0m top1: 0.3903917910447761
[2m[36m(func pid=36355)[0m top5: 0.8348880597014925
[2m[36m(func pid=36355)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=36355)[0m f1_macro: 0.3361764279823325
[2m[36m(func pid=36355)[0m f1_weighted: 0.41241251939953977
[2m[36m(func pid=36355)[0m f1_per_class: [0.551, 0.328, 0.786, 0.557, 0.103, 0.077, 0.534, 0.167, 0.145, 0.114]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9279 | Steps: 2 | Val loss: 2.0415 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.8470 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=31420)[0m top1: 0.34841417910447764
[2m[36m(func pid=31420)[0m top5: 0.8656716417910447
[2m[36m(func pid=31420)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=31420)[0m f1_macro: 0.3451104733949052
[2m[36m(func pid=31420)[0m f1_weighted: 0.3569992846187457
[2m[36m(func pid=31420)[0m f1_per_class: [0.435, 0.431, 0.558, 0.533, 0.099, 0.228, 0.223, 0.285, 0.202, 0.456]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 45.3918 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:02:21 (running for 00:19:14.10)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.928 |      0.262 |                   57 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.008 |      0.345 |                   57 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.392 |                   33 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.336 |                   33 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.25886194029850745
[2m[36m(func pid=30734)[0m top5: 0.7686567164179104
[2m[36m(func pid=30734)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=30734)[0m f1_macro: 0.26168404335196127
[2m[36m(func pid=30734)[0m f1_weighted: 0.2397761355014867
[2m[36m(func pid=30734)[0m f1_per_class: [0.36, 0.361, 0.667, 0.461, 0.064, 0.101, 0.015, 0.216, 0.197, 0.175]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.37779850746268656
[2m[36m(func pid=36270)[0m top5: 0.8941231343283582
[2m[36m(func pid=36270)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=36270)[0m f1_macro: 0.39396950868834274
[2m[36m(func pid=36270)[0m f1_weighted: 0.38949389064352236
[2m[36m(func pid=36270)[0m f1_per_class: [0.526, 0.44, 0.815, 0.553, 0.108, 0.271, 0.287, 0.242, 0.208, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0050 | Steps: 2 | Val loss: 2.5440 | Batch size: 32 | lr: 0.001 | Duration: 2.61s
[2m[36m(func pid=36355)[0m top1: 0.39738805970149255
[2m[36m(func pid=36355)[0m top5: 0.8311567164179104
[2m[36m(func pid=36355)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=36355)[0m f1_macro: 0.33221136269710616
[2m[36m(func pid=36355)[0m f1_weighted: 0.40779040408322415
[2m[36m(func pid=36355)[0m f1_per_class: [0.547, 0.292, 0.786, 0.554, 0.116, 0.065, 0.551, 0.142, 0.15, 0.119]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8984 | Steps: 2 | Val loss: 2.0341 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0018 | Steps: 2 | Val loss: 4.9028 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=31420)[0m top1: 0.34794776119402987
[2m[36m(func pid=31420)[0m top5: 0.8703358208955224
[2m[36m(func pid=31420)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=31420)[0m f1_macro: 0.34719033017192286
[2m[36m(func pid=31420)[0m f1_weighted: 0.35779544627760423
[2m[36m(func pid=31420)[0m f1_per_class: [0.43, 0.43, 0.585, 0.535, 0.099, 0.221, 0.228, 0.276, 0.203, 0.464]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1650 | Steps: 2 | Val loss: 46.5414 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 01:02:27 (running for 00:19:19.50)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.898 |      0.269 |                   58 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.005 |      0.347 |                   58 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.394 |                   34 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.332 |                   34 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.26026119402985076
[2m[36m(func pid=30734)[0m top5: 0.7779850746268657
[2m[36m(func pid=30734)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=30734)[0m f1_macro: 0.26884424250422095
[2m[36m(func pid=30734)[0m f1_weighted: 0.2420961125393919
[2m[36m(func pid=30734)[0m f1_per_class: [0.368, 0.362, 0.71, 0.464, 0.061, 0.108, 0.015, 0.217, 0.187, 0.196]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.38572761194029853
[2m[36m(func pid=36270)[0m top5: 0.8964552238805971
[2m[36m(func pid=36270)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=36270)[0m f1_macro: 0.3971971612343327
[2m[36m(func pid=36270)[0m f1_weighted: 0.39806132438030584
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.445, 0.815, 0.574, 0.108, 0.265, 0.296, 0.233, 0.206, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0056 | Steps: 2 | Val loss: 2.5551 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=36355)[0m top1: 0.4025186567164179
[2m[36m(func pid=36355)[0m top5: 0.8302238805970149
[2m[36m(func pid=36355)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=36355)[0m f1_macro: 0.32938734329887376
[2m[36m(func pid=36355)[0m f1_weighted: 0.4031975846929547
[2m[36m(func pid=36355)[0m f1_per_class: [0.556, 0.283, 0.786, 0.549, 0.131, 0.052, 0.556, 0.102, 0.159, 0.12]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.8794 | Steps: 2 | Val loss: 2.0201 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=31420)[0m top1: 0.3498134328358209
[2m[36m(func pid=31420)[0m top5: 0.8708022388059702
[2m[36m(func pid=31420)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=31420)[0m f1_macro: 0.3460566201328276
[2m[36m(func pid=31420)[0m f1_weighted: 0.361388418532923
[2m[36m(func pid=31420)[0m f1_per_class: [0.43, 0.435, 0.571, 0.528, 0.1, 0.221, 0.245, 0.275, 0.207, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 4.9674 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:02:32 (running for 00:19:24.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.879 |      0.275 |                   59 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.346 |                   59 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.002 |      0.397 |                   35 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.165 |      0.329 |                   35 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.2574626865671642
[2m[36m(func pid=30734)[0m top5: 0.789179104477612
[2m[36m(func pid=30734)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=30734)[0m f1_macro: 0.27482787977664835
[2m[36m(func pid=30734)[0m f1_weighted: 0.24176244912732292
[2m[36m(func pid=30734)[0m f1_per_class: [0.372, 0.369, 0.759, 0.457, 0.068, 0.116, 0.015, 0.212, 0.174, 0.207]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1055 | Steps: 2 | Val loss: 48.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=36270)[0m top1: 0.3885261194029851
[2m[36m(func pid=36270)[0m top5: 0.8969216417910447
[2m[36m(func pid=36270)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=36270)[0m f1_macro: 0.39877327847549887
[2m[36m(func pid=36270)[0m f1_weighted: 0.4011976751446679
[2m[36m(func pid=36270)[0m f1_per_class: [0.529, 0.443, 0.815, 0.577, 0.104, 0.264, 0.304, 0.245, 0.206, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0275 | Steps: 2 | Val loss: 2.5522 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=36355)[0m top1: 0.40205223880597013
[2m[36m(func pid=36355)[0m top5: 0.8269589552238806
[2m[36m(func pid=36355)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=36355)[0m f1_macro: 0.32990982341642405
[2m[36m(func pid=36355)[0m f1_weighted: 0.40026277694769796
[2m[36m(func pid=36355)[0m f1_per_class: [0.556, 0.274, 0.786, 0.543, 0.133, 0.052, 0.554, 0.109, 0.167, 0.126]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8381 | Steps: 2 | Val loss: 2.0102 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=31420)[0m top1: 0.3516791044776119
[2m[36m(func pid=31420)[0m top5: 0.8722014925373134
[2m[36m(func pid=31420)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=31420)[0m f1_macro: 0.3457971746788934
[2m[36m(func pid=31420)[0m f1_weighted: 0.36375847914387155
[2m[36m(func pid=31420)[0m f1_per_class: [0.433, 0.434, 0.558, 0.529, 0.103, 0.223, 0.252, 0.277, 0.201, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.0173 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:02:37 (running for 00:19:29.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.838 |      0.273 |                   60 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.028 |      0.346 |                   60 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.399 |                   36 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.105 |      0.33  |                   36 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.25326492537313433
[2m[36m(func pid=30734)[0m top5: 0.7915111940298507
[2m[36m(func pid=30734)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=30734)[0m f1_macro: 0.2731498946694126
[2m[36m(func pid=30734)[0m f1_weighted: 0.24048750654568735
[2m[36m(func pid=30734)[0m f1_per_class: [0.37, 0.373, 0.759, 0.442, 0.059, 0.112, 0.025, 0.21, 0.174, 0.209]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0000 | Steps: 2 | Val loss: 49.8242 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=36270)[0m top1: 0.3880597014925373
[2m[36m(func pid=36270)[0m top5: 0.8973880597014925
[2m[36m(func pid=36270)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=36270)[0m f1_macro: 0.39732430516358697
[2m[36m(func pid=36270)[0m f1_weighted: 0.40195259406333955
[2m[36m(func pid=36270)[0m f1_per_class: [0.533, 0.441, 0.815, 0.573, 0.104, 0.259, 0.314, 0.237, 0.206, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0060 | Steps: 2 | Val loss: 2.5441 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=36355)[0m top1: 0.4006529850746269
[2m[36m(func pid=36355)[0m top5: 0.8185634328358209
[2m[36m(func pid=36355)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=36355)[0m f1_macro: 0.3349862376539554
[2m[36m(func pid=36355)[0m f1_weighted: 0.3987030927246537
[2m[36m(func pid=36355)[0m f1_per_class: [0.562, 0.286, 0.815, 0.529, 0.145, 0.052, 0.553, 0.115, 0.162, 0.13]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8588 | Steps: 2 | Val loss: 2.0018 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=31420)[0m top1: 0.3596082089552239
[2m[36m(func pid=31420)[0m top5: 0.8773320895522388
[2m[36m(func pid=31420)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=31420)[0m f1_macro: 0.34788670330099325
[2m[36m(func pid=31420)[0m f1_weighted: 0.3742414797845172
[2m[36m(func pid=31420)[0m f1_per_class: [0.44, 0.436, 0.533, 0.538, 0.103, 0.226, 0.276, 0.287, 0.191, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.0282 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 51.9613 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:02:43 (running for 00:19:35.31)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.859 |      0.282 |                   61 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.348 |                   61 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.397 |                   37 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.335 |                   37 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.25652985074626866
[2m[36m(func pid=30734)[0m top5: 0.7999067164179104
[2m[36m(func pid=30734)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=30734)[0m f1_macro: 0.2816939858510683
[2m[36m(func pid=30734)[0m f1_weighted: 0.24635107355587196
[2m[36m(func pid=30734)[0m f1_per_class: [0.378, 0.373, 0.759, 0.439, 0.055, 0.123, 0.036, 0.215, 0.216, 0.224]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0023 | Steps: 2 | Val loss: 2.5577 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36270)[0m top1: 0.3927238805970149
[2m[36m(func pid=36270)[0m top5: 0.9011194029850746
[2m[36m(func pid=36270)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=36270)[0m f1_macro: 0.39968066766309607
[2m[36m(func pid=36270)[0m f1_weighted: 0.4057896202242122
[2m[36m(func pid=36270)[0m f1_per_class: [0.533, 0.441, 0.815, 0.577, 0.112, 0.266, 0.321, 0.234, 0.207, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.39365671641791045
[2m[36m(func pid=36355)[0m top5: 0.8097014925373134
[2m[36m(func pid=36355)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=36355)[0m f1_macro: 0.32946108956637843
[2m[36m(func pid=36355)[0m f1_weighted: 0.3882998378201833
[2m[36m(func pid=36355)[0m f1_per_class: [0.549, 0.276, 0.815, 0.507, 0.156, 0.031, 0.553, 0.117, 0.157, 0.134]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.36100746268656714
[2m[36m(func pid=31420)[0m top5: 0.8763992537313433
[2m[36m(func pid=31420)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=31420)[0m f1_macro: 0.34938193307783844
[2m[36m(func pid=31420)[0m f1_weighted: 0.37535763355779794
[2m[36m(func pid=31420)[0m f1_per_class: [0.446, 0.437, 0.533, 0.538, 0.103, 0.222, 0.28, 0.281, 0.198, 0.456]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7324 | Steps: 2 | Val loss: 1.9920 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0002 | Steps: 2 | Val loss: 5.0938 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 54.8864 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0072 | Steps: 2 | Val loss: 2.5632 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:02:48 (running for 00:19:40.95)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.732 |      0.283 |                   62 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.349 |                   62 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.4   |                   38 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.329 |                   38 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m top1: 0.2593283582089552
[2m[36m(func pid=30734)[0m top5: 0.8022388059701493
[2m[36m(func pid=30734)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=30734)[0m f1_macro: 0.2828571420621887
[2m[36m(func pid=30734)[0m f1_weighted: 0.25087886038592605
[2m[36m(func pid=30734)[0m f1_per_class: [0.379, 0.377, 0.71, 0.433, 0.062, 0.128, 0.051, 0.221, 0.22, 0.248]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m top1: 0.3931902985074627
[2m[36m(func pid=36270)[0m top5: 0.9011194029850746
[2m[36m(func pid=36270)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=36270)[0m f1_macro: 0.3989662287847242
[2m[36m(func pid=36270)[0m f1_weighted: 0.4056673729682322
[2m[36m(func pid=36270)[0m f1_per_class: [0.537, 0.434, 0.815, 0.58, 0.114, 0.262, 0.325, 0.224, 0.208, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3824626865671642
[2m[36m(func pid=36355)[0m top5: 0.8022388059701493
[2m[36m(func pid=36355)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=36355)[0m f1_macro: 0.32067277657920273
[2m[36m(func pid=36355)[0m f1_weighted: 0.3767555695900287
[2m[36m(func pid=36355)[0m f1_per_class: [0.532, 0.273, 0.786, 0.474, 0.158, 0.031, 0.549, 0.116, 0.156, 0.132]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3628731343283582
[2m[36m(func pid=31420)[0m top5: 0.8801305970149254
[2m[36m(func pid=31420)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=31420)[0m f1_macro: 0.35308773383963526
[2m[36m(func pid=31420)[0m f1_weighted: 0.3795808613725402
[2m[36m(func pid=31420)[0m f1_per_class: [0.44, 0.438, 0.545, 0.534, 0.1, 0.217, 0.298, 0.277, 0.208, 0.473]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.1800 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8017 | Steps: 2 | Val loss: 1.9786 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0039 | Steps: 2 | Val loss: 2.5587 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 57.2098 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:02:54 (running for 00:19:46.48)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.732 |      0.283 |                   62 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.007 |      0.353 |                   63 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.401 |                   40 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.321 |                   39 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.396455223880597
[2m[36m(func pid=36270)[0m top5: 0.9001865671641791
[2m[36m(func pid=36270)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=36270)[0m f1_macro: 0.4014305128184443
[2m[36m(func pid=36270)[0m f1_weighted: 0.40879629217141883
[2m[36m(func pid=36270)[0m f1_per_class: [0.533, 0.437, 0.815, 0.576, 0.114, 0.278, 0.33, 0.234, 0.206, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.26026119402985076
[2m[36m(func pid=30734)[0m top5: 0.8106343283582089
[2m[36m(func pid=30734)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=30734)[0m f1_macro: 0.27647315325482485
[2m[36m(func pid=30734)[0m f1_weighted: 0.2538110538833099
[2m[36m(func pid=30734)[0m f1_per_class: [0.372, 0.375, 0.647, 0.435, 0.061, 0.128, 0.062, 0.228, 0.203, 0.254]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3675373134328358
[2m[36m(func pid=31420)[0m top5: 0.8810634328358209
[2m[36m(func pid=31420)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=31420)[0m f1_macro: 0.35891519325469934
[2m[36m(func pid=31420)[0m f1_weighted: 0.3870685279948006
[2m[36m(func pid=31420)[0m f1_per_class: [0.446, 0.44, 0.545, 0.53, 0.105, 0.221, 0.324, 0.28, 0.198, 0.5]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.376865671641791
[2m[36m(func pid=36355)[0m top5: 0.7905783582089553
[2m[36m(func pid=36355)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=36355)[0m f1_macro: 0.31493641671795203
[2m[36m(func pid=36355)[0m f1_weighted: 0.3690986087764062
[2m[36m(func pid=36355)[0m f1_per_class: [0.527, 0.263, 0.759, 0.451, 0.167, 0.031, 0.552, 0.115, 0.15, 0.135]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.2655 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8054 | Steps: 2 | Val loss: 1.9518 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0087 | Steps: 2 | Val loss: 2.5623 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 59.0614 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:02:59 (running for 00:19:51.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.802 |      0.276 |                   63 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.359 |                   64 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.402 |                   41 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   40 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.3941231343283582
[2m[36m(func pid=36270)[0m top5: 0.9006529850746269
[2m[36m(func pid=36270)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=36270)[0m f1_macro: 0.4021640656357948
[2m[36m(func pid=36270)[0m f1_weighted: 0.4054080075360629
[2m[36m(func pid=36270)[0m f1_per_class: [0.562, 0.433, 0.815, 0.58, 0.115, 0.27, 0.319, 0.224, 0.212, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.2677238805970149
[2m[36m(func pid=30734)[0m top5: 0.8264925373134329
[2m[36m(func pid=30734)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=30734)[0m f1_macro: 0.28963017860945883
[2m[36m(func pid=30734)[0m f1_weighted: 0.26720211176048764
[2m[36m(func pid=30734)[0m f1_per_class: [0.376, 0.358, 0.688, 0.438, 0.066, 0.155, 0.101, 0.223, 0.222, 0.27]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.36473880597014924
[2m[36m(func pid=31420)[0m top5: 0.8810634328358209
[2m[36m(func pid=31420)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=31420)[0m f1_macro: 0.3544867634207384
[2m[36m(func pid=31420)[0m f1_weighted: 0.3860786454868721
[2m[36m(func pid=31420)[0m f1_per_class: [0.44, 0.442, 0.533, 0.525, 0.103, 0.216, 0.331, 0.262, 0.193, 0.5]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3810634328358209
[2m[36m(func pid=36355)[0m top5: 0.7821828358208955
[2m[36m(func pid=36355)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=36355)[0m f1_macro: 0.3250456502023453
[2m[36m(func pid=36355)[0m f1_weighted: 0.37055703930631584
[2m[36m(func pid=36355)[0m f1_per_class: [0.531, 0.277, 0.815, 0.439, 0.176, 0.031, 0.557, 0.111, 0.167, 0.147]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0006 | Steps: 2 | Val loss: 5.3414 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7037 | Steps: 2 | Val loss: 1.9368 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0041 | Steps: 2 | Val loss: 2.5346 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0000 | Steps: 2 | Val loss: 61.8491 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:03:04 (running for 00:19:57.11)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.805 |      0.29  |                   64 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.009 |      0.354 |                   65 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.406 |                   42 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.325 |                   41 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.40111940298507465
[2m[36m(func pid=36270)[0m top5: 0.9020522388059702
[2m[36m(func pid=36270)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=36270)[0m f1_macro: 0.40575338634661795
[2m[36m(func pid=36270)[0m f1_weighted: 0.4135750089833563
[2m[36m(func pid=36270)[0m f1_per_class: [0.569, 0.436, 0.815, 0.585, 0.114, 0.268, 0.339, 0.233, 0.207, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.27472014925373134
[2m[36m(func pid=30734)[0m top5: 0.8344216417910447
[2m[36m(func pid=30734)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=30734)[0m f1_macro: 0.298745131732278
[2m[36m(func pid=30734)[0m f1_weighted: 0.276592965003685
[2m[36m(func pid=30734)[0m f1_per_class: [0.387, 0.363, 0.733, 0.443, 0.064, 0.165, 0.117, 0.234, 0.218, 0.262]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3670708955223881
[2m[36m(func pid=31420)[0m top5: 0.8833955223880597
[2m[36m(func pid=31420)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=31420)[0m f1_macro: 0.35111303512492553
[2m[36m(func pid=31420)[0m f1_weighted: 0.3884516449695026
[2m[36m(func pid=31420)[0m f1_per_class: [0.433, 0.441, 0.522, 0.526, 0.101, 0.215, 0.34, 0.264, 0.189, 0.481]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.37220149253731344
[2m[36m(func pid=36355)[0m top5: 0.7742537313432836
[2m[36m(func pid=36355)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=36355)[0m f1_macro: 0.31486840109505926
[2m[36m(func pid=36355)[0m f1_weighted: 0.35910788453206116
[2m[36m(func pid=36355)[0m f1_per_class: [0.517, 0.266, 0.786, 0.416, 0.169, 0.023, 0.554, 0.105, 0.163, 0.15]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0002 | Steps: 2 | Val loss: 5.3421 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6739 | Steps: 2 | Val loss: 1.9195 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0320 | Steps: 2 | Val loss: 2.5463 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0000 | Steps: 2 | Val loss: 64.3315 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:03:10 (running for 00:20:02.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.704 |      0.299 |                   65 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.351 |                   66 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                   43 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.315 |                   42 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.4039179104477612
[2m[36m(func pid=36270)[0m top5: 0.9043843283582089
[2m[36m(func pid=36270)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=36270)[0m f1_macro: 0.4073681018459828
[2m[36m(func pid=36270)[0m f1_weighted: 0.4176191711055291
[2m[36m(func pid=36270)[0m f1_per_class: [0.565, 0.429, 0.815, 0.59, 0.127, 0.262, 0.355, 0.231, 0.209, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=30734)[0m top1: 0.279384328358209
[2m[36m(func pid=30734)[0m top5: 0.8456156716417911
[2m[36m(func pid=30734)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=30734)[0m f1_macro: 0.2981659888645706
[2m[36m(func pid=30734)[0m f1_weighted: 0.28469061950295327
[2m[36m(func pid=30734)[0m f1_per_class: [0.4, 0.375, 0.667, 0.444, 0.063, 0.175, 0.134, 0.232, 0.216, 0.277]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3712686567164179
[2m[36m(func pid=31420)[0m top5: 0.8819962686567164
[2m[36m(func pid=31420)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=31420)[0m f1_macro: 0.35116545528602733
[2m[36m(func pid=31420)[0m f1_weighted: 0.39067309938120026
[2m[36m(func pid=31420)[0m f1_per_class: [0.424, 0.444, 0.511, 0.523, 0.103, 0.22, 0.347, 0.264, 0.195, 0.481]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3666044776119403
[2m[36m(func pid=36355)[0m top5: 0.7705223880597015
[2m[36m(func pid=36355)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=36355)[0m f1_macro: 0.3121574524403908
[2m[36m(func pid=36355)[0m f1_weighted: 0.35164555832811295
[2m[36m(func pid=36355)[0m f1_per_class: [0.517, 0.26, 0.786, 0.394, 0.177, 0.023, 0.553, 0.101, 0.16, 0.151]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.4543 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.5170 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7638 | Steps: 2 | Val loss: 1.9090 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 67.0730 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:03:15 (running for 00:20:07.68)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.674 |      0.298 |                   66 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.032 |      0.351 |                   67 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.409 |                   44 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.312 |                   43 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.40345149253731344
[2m[36m(func pid=36270)[0m top5: 0.9039179104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=36270)[0m f1_macro: 0.4087726959986755
[2m[36m(func pid=36270)[0m f1_weighted: 0.4156326918200607
[2m[36m(func pid=36270)[0m f1_per_class: [0.574, 0.422, 0.815, 0.589, 0.124, 0.264, 0.35, 0.236, 0.214, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3736007462686567
[2m[36m(func pid=31420)[0m top5: 0.8838619402985075
[2m[36m(func pid=31420)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=31420)[0m f1_macro: 0.3528053750314041
[2m[36m(func pid=31420)[0m f1_weighted: 0.3953407576574796
[2m[36m(func pid=31420)[0m f1_per_class: [0.43, 0.439, 0.522, 0.532, 0.104, 0.214, 0.361, 0.248, 0.196, 0.481]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.28591417910447764
[2m[36m(func pid=30734)[0m top5: 0.8484141791044776
[2m[36m(func pid=30734)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=30734)[0m f1_macro: 0.3051312892658523
[2m[36m(func pid=30734)[0m f1_weighted: 0.29078249202274936
[2m[36m(func pid=30734)[0m f1_per_class: [0.4, 0.379, 0.688, 0.446, 0.063, 0.18, 0.144, 0.248, 0.215, 0.289]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.36380597014925375
[2m[36m(func pid=36355)[0m top5: 0.7621268656716418
[2m[36m(func pid=36355)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=36355)[0m f1_macro: 0.3110021617652773
[2m[36m(func pid=36355)[0m f1_weighted: 0.34622043659508717
[2m[36m(func pid=36355)[0m f1_per_class: [0.492, 0.264, 0.786, 0.375, 0.189, 0.023, 0.549, 0.116, 0.156, 0.16]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.5118 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0033 | Steps: 2 | Val loss: 2.5296 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6864 | Steps: 2 | Val loss: 1.8922 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0008 | Steps: 2 | Val loss: 69.9815 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:03:20 (running for 00:20:12.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.764 |      0.305 |                   67 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.353 |                   68 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.408 |                   45 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.311 |                   44 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.40625
[2m[36m(func pid=36270)[0m top5: 0.90625
[2m[36m(func pid=36270)[0m f1_micro: 0.40625
[2m[36m(func pid=36270)[0m f1_macro: 0.40805762415245284
[2m[36m(func pid=36270)[0m f1_weighted: 0.4183196346408531
[2m[36m(func pid=36270)[0m f1_per_class: [0.567, 0.417, 0.815, 0.588, 0.123, 0.264, 0.363, 0.238, 0.215, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.37453358208955223
[2m[36m(func pid=31420)[0m top5: 0.8843283582089553
[2m[36m(func pid=31420)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=31420)[0m f1_macro: 0.35011112741196326
[2m[36m(func pid=31420)[0m f1_weighted: 0.3950061928504036
[2m[36m(func pid=31420)[0m f1_per_class: [0.438, 0.44, 0.511, 0.526, 0.108, 0.217, 0.364, 0.253, 0.197, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.292910447761194
[2m[36m(func pid=30734)[0m top5: 0.8568097014925373
[2m[36m(func pid=30734)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=30734)[0m f1_macro: 0.31064539023705706
[2m[36m(func pid=30734)[0m f1_weighted: 0.3028516048791575
[2m[36m(func pid=30734)[0m f1_per_class: [0.417, 0.382, 0.667, 0.457, 0.062, 0.181, 0.171, 0.244, 0.218, 0.308]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3558768656716418
[2m[36m(func pid=36355)[0m top5: 0.7551305970149254
[2m[36m(func pid=36355)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=36355)[0m f1_macro: 0.3083659141071941
[2m[36m(func pid=36355)[0m f1_weighted: 0.3378585869942181
[2m[36m(func pid=36355)[0m f1_per_class: [0.488, 0.255, 0.786, 0.36, 0.2, 0.023, 0.54, 0.114, 0.155, 0.163]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.5235 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0023 | Steps: 2 | Val loss: 2.5082 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7198 | Steps: 2 | Val loss: 1.8746 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0000 | Steps: 2 | Val loss: 72.0890 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:03:26 (running for 00:20:18.29)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.686 |      0.311 |                   68 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.35  |                   69 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                   46 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.001 |      0.308 |                   45 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.40904850746268656
[2m[36m(func pid=36270)[0m top5: 0.9090485074626866
[2m[36m(func pid=36270)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=36270)[0m f1_macro: 0.40671323523065983
[2m[36m(func pid=36270)[0m f1_weighted: 0.4209567602488094
[2m[36m(func pid=36270)[0m f1_per_class: [0.558, 0.425, 0.815, 0.588, 0.11, 0.262, 0.369, 0.241, 0.208, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.3824626865671642
[2m[36m(func pid=31420)[0m top5: 0.8894589552238806
[2m[36m(func pid=31420)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=31420)[0m f1_macro: 0.35498512231870266
[2m[36m(func pid=31420)[0m f1_weighted: 0.40381935414229597
[2m[36m(func pid=31420)[0m f1_per_class: [0.434, 0.44, 0.511, 0.528, 0.112, 0.239, 0.384, 0.247, 0.199, 0.456]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m top1: 0.29757462686567165
[2m[36m(func pid=30734)[0m top5: 0.8624067164179104
[2m[36m(func pid=30734)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=30734)[0m f1_macro: 0.3144293154594725
[2m[36m(func pid=30734)[0m f1_weighted: 0.30959057434200443
[2m[36m(func pid=30734)[0m f1_per_class: [0.419, 0.377, 0.667, 0.456, 0.063, 0.18, 0.196, 0.248, 0.224, 0.315]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m top1: 0.35074626865671643
[2m[36m(func pid=36355)[0m top5: 0.7472014925373134
[2m[36m(func pid=36355)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=36355)[0m f1_macro: 0.3043403362620381
[2m[36m(func pid=36355)[0m f1_weighted: 0.33128298203367273
[2m[36m(func pid=36355)[0m f1_per_class: [0.492, 0.248, 0.786, 0.343, 0.187, 0.023, 0.539, 0.106, 0.152, 0.165]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0056 | Steps: 2 | Val loss: 2.5354 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.5479 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 75.0321 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5992 | Steps: 2 | Val loss: 1.8514 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 01:03:31 (running for 00:20:23.52)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.72  |      0.314 |                   69 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.356 |                   71 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                   46 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.304 |                   46 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.38386194029850745
[2m[36m(func pid=31420)[0m top5: 0.8917910447761194
[2m[36m(func pid=31420)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=31420)[0m f1_macro: 0.35560940571249644
[2m[36m(func pid=31420)[0m f1_weighted: 0.4066573642104859
[2m[36m(func pid=31420)[0m f1_per_class: [0.443, 0.439, 0.5, 0.528, 0.109, 0.242, 0.392, 0.25, 0.196, 0.456]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4085820895522388
[2m[36m(func pid=36270)[0m top5: 0.9095149253731343
[2m[36m(func pid=36270)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=36270)[0m f1_macro: 0.4060967921116352
[2m[36m(func pid=36270)[0m f1_weighted: 0.4200820227603083
[2m[36m(func pid=36270)[0m f1_per_class: [0.558, 0.42, 0.815, 0.587, 0.111, 0.257, 0.372, 0.241, 0.209, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.34421641791044777
[2m[36m(func pid=36355)[0m top5: 0.738339552238806
[2m[36m(func pid=36355)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=36355)[0m f1_macro: 0.298045558506912
[2m[36m(func pid=36355)[0m f1_weighted: 0.320745470424047
[2m[36m(func pid=36355)[0m f1_per_class: [0.471, 0.237, 0.786, 0.315, 0.188, 0.023, 0.539, 0.103, 0.151, 0.167]
[2m[36m(func pid=30734)[0m top1: 0.30223880597014924
[2m[36m(func pid=30734)[0m top5: 0.8731343283582089
[2m[36m(func pid=30734)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=30734)[0m f1_macro: 0.3207477040321182
[2m[36m(func pid=30734)[0m f1_weighted: 0.3169238144091076
[2m[36m(func pid=30734)[0m f1_per_class: [0.44, 0.368, 0.667, 0.456, 0.065, 0.181, 0.223, 0.253, 0.212, 0.341]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.5066 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.5605 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 77.8280 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6115 | Steps: 2 | Val loss: 1.8326 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 01:03:36 (running for 00:20:28.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.599 |      0.321 |                   70 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.359 |                   72 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.406 |                   47 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.298 |                   47 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.3908582089552239
[2m[36m(func pid=31420)[0m top5: 0.8969216417910447
[2m[36m(func pid=31420)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=31420)[0m f1_macro: 0.3592674304715576
[2m[36m(func pid=31420)[0m f1_weighted: 0.41475436051153175
[2m[36m(func pid=31420)[0m f1_per_class: [0.456, 0.434, 0.49, 0.542, 0.112, 0.244, 0.408, 0.253, 0.191, 0.464]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4137126865671642
[2m[36m(func pid=36270)[0m top5: 0.9095149253731343
[2m[36m(func pid=36270)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=36270)[0m f1_macro: 0.41195380842664264
[2m[36m(func pid=36270)[0m f1_weighted: 0.4263783370836166
[2m[36m(func pid=36270)[0m f1_per_class: [0.562, 0.422, 0.815, 0.587, 0.112, 0.258, 0.389, 0.248, 0.208, 0.519]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3381529850746269
[2m[36m(func pid=36355)[0m top5: 0.7290111940298507
[2m[36m(func pid=36355)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=36355)[0m f1_macro: 0.29265422658446083
[2m[36m(func pid=36355)[0m f1_weighted: 0.3096101224735244
[2m[36m(func pid=36355)[0m f1_per_class: [0.46, 0.218, 0.786, 0.295, 0.188, 0.016, 0.535, 0.1, 0.149, 0.179]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m top1: 0.30970149253731344
[2m[36m(func pid=30734)[0m top5: 0.8777985074626866
[2m[36m(func pid=30734)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=30734)[0m f1_macro: 0.32579958428390093
[2m[36m(func pid=30734)[0m f1_weighted: 0.33150437811789074
[2m[36m(func pid=30734)[0m f1_per_class: [0.447, 0.372, 0.667, 0.453, 0.068, 0.193, 0.272, 0.238, 0.204, 0.346]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.4844 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.6265 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2104 | Steps: 2 | Val loss: 81.1536 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6101 | Steps: 2 | Val loss: 1.8220 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:03:41 (running for 00:20:33.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.612 |      0.326 |                   71 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.365 |                   73 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   48 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.293 |                   48 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=31420)[0m top1: 0.3969216417910448
[2m[36m(func pid=31420)[0m top5: 0.898320895522388
[2m[36m(func pid=31420)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=31420)[0m f1_macro: 0.36515280772667325
[2m[36m(func pid=31420)[0m f1_weighted: 0.42062371182488517
[2m[36m(func pid=31420)[0m f1_per_class: [0.468, 0.437, 0.5, 0.548, 0.114, 0.238, 0.42, 0.256, 0.199, 0.473]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4155783582089552
[2m[36m(func pid=36270)[0m top5: 0.9104477611940298
[2m[36m(func pid=36270)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=36270)[0m f1_macro: 0.4124306587721864
[2m[36m(func pid=36270)[0m f1_weighted: 0.42599783896400123
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.429, 0.815, 0.589, 0.111, 0.243, 0.388, 0.238, 0.212, 0.528]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3278917910447761
[2m[36m(func pid=36355)[0m top5: 0.7248134328358209
[2m[36m(func pid=36355)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=36355)[0m f1_macro: 0.28668551758460953
[2m[36m(func pid=36355)[0m f1_weighted: 0.2959448047069101
[2m[36m(func pid=36355)[0m f1_per_class: [0.423, 0.2, 0.786, 0.262, 0.212, 0.016, 0.532, 0.102, 0.15, 0.184]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m top1: 0.3138992537313433
[2m[36m(func pid=30734)[0m top5: 0.8791977611940298
[2m[36m(func pid=30734)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=30734)[0m f1_macro: 0.32618313786518727
[2m[36m(func pid=30734)[0m f1_weighted: 0.3412072382565992
[2m[36m(func pid=30734)[0m f1_per_class: [0.423, 0.372, 0.667, 0.452, 0.067, 0.187, 0.312, 0.219, 0.199, 0.364]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.5320 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.6849 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 82.9182 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:03:46 (running for 00:20:38.82)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.61  |      0.326 |                   72 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.365 |                   74 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   49 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.21  |      0.287 |                   49 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4794 | Steps: 2 | Val loss: 1.8136 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=31420)[0m top1: 0.3917910447761194
[2m[36m(func pid=31420)[0m top5: 0.8997201492537313
[2m[36m(func pid=31420)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=31420)[0m f1_macro: 0.3650373524333142
[2m[36m(func pid=31420)[0m f1_weighted: 0.4159363383770263
[2m[36m(func pid=31420)[0m f1_per_class: [0.477, 0.438, 0.511, 0.533, 0.11, 0.229, 0.421, 0.249, 0.201, 0.481]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.41511194029850745
[2m[36m(func pid=36270)[0m top5: 0.9104477611940298
[2m[36m(func pid=36270)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=36270)[0m f1_macro: 0.4116999673631027
[2m[36m(func pid=36270)[0m f1_weighted: 0.4250607841800008
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.429, 0.815, 0.586, 0.109, 0.234, 0.391, 0.239, 0.214, 0.528]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3246268656716418
[2m[36m(func pid=36355)[0m top5: 0.7210820895522388
[2m[36m(func pid=36355)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=36355)[0m f1_macro: 0.28659320355183215
[2m[36m(func pid=36355)[0m f1_weighted: 0.28974403569919305
[2m[36m(func pid=36355)[0m f1_per_class: [0.412, 0.2, 0.815, 0.24, 0.212, 0.016, 0.532, 0.102, 0.15, 0.188]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=30734)[0m top1: 0.32276119402985076
[2m[36m(func pid=30734)[0m top5: 0.8815298507462687
[2m[36m(func pid=30734)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=30734)[0m f1_macro: 0.33124130755145675
[2m[36m(func pid=30734)[0m f1_weighted: 0.34984365532221023
[2m[36m(func pid=30734)[0m f1_per_class: [0.42, 0.369, 0.667, 0.453, 0.071, 0.197, 0.334, 0.238, 0.199, 0.364]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.5207 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.7176 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 84.1485 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=31420)[0m top1: 0.39738805970149255
[2m[36m(func pid=31420)[0m top5: 0.8992537313432836
[2m[36m(func pid=31420)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=31420)[0m f1_macro: 0.36760955215698893
[2m[36m(func pid=31420)[0m f1_weighted: 0.4217639055441488
[2m[36m(func pid=31420)[0m f1_per_class: [0.468, 0.444, 0.522, 0.538, 0.108, 0.243, 0.426, 0.258, 0.197, 0.473]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5292 | Steps: 2 | Val loss: 1.7964 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:03:52 (running for 00:20:45.21)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.479 |      0.331 |                   73 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.368 |                   75 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   51 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.287 |                   50 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.4183768656716418
[2m[36m(func pid=36270)[0m top5: 0.9127798507462687
[2m[36m(func pid=36270)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=36270)[0m f1_macro: 0.4140044677928184
[2m[36m(func pid=36270)[0m f1_weighted: 0.42941554128316706
[2m[36m(func pid=36270)[0m f1_per_class: [0.56, 0.42, 0.815, 0.592, 0.123, 0.241, 0.403, 0.24, 0.208, 0.538]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.3292910447761194
[2m[36m(func pid=36355)[0m top5: 0.715018656716418
[2m[36m(func pid=36355)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=36355)[0m f1_macro: 0.2884744185698422
[2m[36m(func pid=36355)[0m f1_weighted: 0.29155236911707827
[2m[36m(func pid=36355)[0m f1_per_class: [0.395, 0.208, 0.815, 0.233, 0.222, 0.024, 0.537, 0.109, 0.147, 0.196]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0095 | Steps: 2 | Val loss: 2.5213 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=30734)[0m top1: 0.33255597014925375
[2m[36m(func pid=30734)[0m top5: 0.8871268656716418
[2m[36m(func pid=30734)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=30734)[0m f1_macro: 0.3346045566493815
[2m[36m(func pid=30734)[0m f1_weighted: 0.3621411736319851
[2m[36m(func pid=30734)[0m f1_per_class: [0.41, 0.367, 0.667, 0.464, 0.071, 0.202, 0.365, 0.243, 0.194, 0.364]
[2m[36m(func pid=30734)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0001 | Steps: 2 | Val loss: 5.7776 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=31420)[0m top1: 0.39738805970149255
[2m[36m(func pid=31420)[0m top5: 0.8987873134328358
[2m[36m(func pid=31420)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=31420)[0m f1_macro: 0.36446699881106037
[2m[36m(func pid=31420)[0m f1_weighted: 0.42233836704690786
[2m[36m(func pid=31420)[0m f1_per_class: [0.48, 0.437, 0.522, 0.54, 0.105, 0.212, 0.442, 0.251, 0.207, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 86.8318 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=30734)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5394 | Steps: 2 | Val loss: 1.7889 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:03:58 (running for 00:20:50.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00008 | RUNNING    | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.529 |      0.335 |                   74 |
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.01  |      0.364 |                   76 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.417 |                   52 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.288 |                   51 |
| train_57e67_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=36270)[0m top1: 0.4197761194029851
[2m[36m(func pid=36270)[0m top5: 0.9132462686567164
[2m[36m(func pid=36270)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=36270)[0m f1_macro: 0.41667620530194494
[2m[36m(func pid=36270)[0m f1_weighted: 0.4310589536710455
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.423, 0.815, 0.593, 0.12, 0.234, 0.406, 0.239, 0.216, 0.549]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.322294776119403
[2m[36m(func pid=36355)[0m top5: 0.7061567164179104
[2m[36m(func pid=36355)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=36355)[0m f1_macro: 0.2839362487699948
[2m[36m(func pid=36355)[0m f1_weighted: 0.27967195345626317
[2m[36m(func pid=36355)[0m f1_per_class: [0.373, 0.196, 0.815, 0.2, 0.222, 0.023, 0.535, 0.103, 0.165, 0.207]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0063 | Steps: 2 | Val loss: 2.5248 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=30734)[0m top1: 0.3400186567164179
[2m[36m(func pid=30734)[0m top5: 0.8885261194029851
[2m[36m(func pid=30734)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=30734)[0m f1_macro: 0.3343813378905097
[2m[36m(func pid=30734)[0m f1_weighted: 0.37099972703615186
[2m[36m(func pid=30734)[0m f1_per_class: [0.408, 0.368, 0.667, 0.461, 0.074, 0.204, 0.4, 0.229, 0.192, 0.342]
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0004 | Steps: 2 | Val loss: 5.8038 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=31420)[0m top1: 0.3987873134328358
[2m[36m(func pid=31420)[0m top5: 0.8997201492537313
[2m[36m(func pid=31420)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=31420)[0m f1_macro: 0.36420983639121723
[2m[36m(func pid=31420)[0m f1_weighted: 0.423792504448471
[2m[36m(func pid=31420)[0m f1_per_class: [0.477, 0.435, 0.511, 0.54, 0.105, 0.209, 0.451, 0.245, 0.203, 0.467]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 88.7649 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36270)[0m top1: 0.4207089552238806
[2m[36m(func pid=36270)[0m top5: 0.9165111940298507
[2m[36m(func pid=36270)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=36270)[0m f1_macro: 0.4159969879620979
[2m[36m(func pid=36270)[0m f1_weighted: 0.4322199139953575
[2m[36m(func pid=36270)[0m f1_per_class: [0.56, 0.417, 0.815, 0.591, 0.125, 0.258, 0.409, 0.24, 0.208, 0.538]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.4982 | Batch size: 32 | lr: 0.001 | Duration: 2.60s
[2m[36m(func pid=36355)[0m top1: 0.31763059701492535
[2m[36m(func pid=36355)[0m top5: 0.7024253731343284
[2m[36m(func pid=36355)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=36355)[0m f1_macro: 0.2825223781615624
[2m[36m(func pid=36355)[0m f1_weighted: 0.2748759038185889
[2m[36m(func pid=36355)[0m f1_per_class: [0.361, 0.191, 0.815, 0.189, 0.222, 0.023, 0.532, 0.103, 0.164, 0.225]
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.9152 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=31420)[0m top1: 0.4039179104477612
[2m[36m(func pid=31420)[0m top5: 0.9011194029850746
[2m[36m(func pid=31420)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=31420)[0m f1_macro: 0.3639199898640484
[2m[36m(func pid=31420)[0m f1_weighted: 0.42839693675682566
[2m[36m(func pid=31420)[0m f1_per_class: [0.483, 0.435, 0.5, 0.548, 0.107, 0.215, 0.458, 0.245, 0.201, 0.448]
== Status ==
Current time: 2024-01-07 01:04:03 (running for 00:20:55.50)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.006 |      0.364 |                   77 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   53 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   52 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=48412)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=48412)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=48412)[0m Configuration completed!
[2m[36m(func pid=48412)[0m New optimizer parameters:
[2m[36m(func pid=48412)[0m SGD (
[2m[36m(func pid=48412)[0m Parameter Group 0
[2m[36m(func pid=48412)[0m     dampening: 0
[2m[36m(func pid=48412)[0m     differentiable: False
[2m[36m(func pid=48412)[0m     foreach: None
[2m[36m(func pid=48412)[0m     lr: 0.0001
[2m[36m(func pid=48412)[0m     maximize: False
[2m[36m(func pid=48412)[0m     momentum: 0.9
[2m[36m(func pid=48412)[0m     nesterov: False
[2m[36m(func pid=48412)[0m     weight_decay: 0.0001
[2m[36m(func pid=48412)[0m )
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:04:08 (running for 00:21:00.88)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.364 |                   78 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   54 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.283 |                   53 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.417910447761194
[2m[36m(func pid=36270)[0m top5: 0.9146455223880597
[2m[36m(func pid=36270)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=36270)[0m f1_macro: 0.4142044190368589
[2m[36m(func pid=36270)[0m f1_weighted: 0.4289573171775338
[2m[36m(func pid=36270)[0m f1_per_class: [0.569, 0.414, 0.815, 0.588, 0.125, 0.255, 0.403, 0.241, 0.204, 0.528]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0019 | Steps: 2 | Val loss: 2.5002 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 90.7180 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 5.9571 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1587 | Steps: 2 | Val loss: 2.5144 | Batch size: 32 | lr: 0.0001 | Duration: 4.69s
[2m[36m(func pid=31420)[0m top1: 0.4085820895522388
[2m[36m(func pid=31420)[0m top5: 0.8992537313432836
[2m[36m(func pid=31420)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=31420)[0m f1_macro: 0.36532004581116473
[2m[36m(func pid=31420)[0m f1_weighted: 0.43256967765013327
[2m[36m(func pid=31420)[0m f1_per_class: [0.48, 0.44, 0.5, 0.548, 0.11, 0.212, 0.47, 0.245, 0.199, 0.448]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.31203358208955223
[2m[36m(func pid=36355)[0m top5: 0.6902985074626866
[2m[36m(func pid=36355)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=36355)[0m f1_macro: 0.27757613311206103
[2m[36m(func pid=36355)[0m f1_weighted: 0.2707217764649201
[2m[36m(func pid=36355)[0m f1_per_class: [0.351, 0.189, 0.786, 0.182, 0.219, 0.023, 0.527, 0.104, 0.161, 0.234]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.41744402985074625
[2m[36m(func pid=36270)[0m top5: 0.914179104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=36270)[0m f1_macro: 0.4122902182528597
[2m[36m(func pid=36270)[0m f1_weighted: 0.4283561657842032
[2m[36m(func pid=36270)[0m f1_per_class: [0.56, 0.409, 0.815, 0.59, 0.125, 0.245, 0.407, 0.239, 0.204, 0.528]
== Status ==
Current time: 2024-01-07 01:04:13 (running for 00:21:06.24)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.365 |                   79 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   55 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.278 |                   54 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.06716417910447761
[2m[36m(func pid=48412)[0m top5: 0.4846082089552239
[2m[36m(func pid=48412)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=48412)[0m f1_macro: 0.03981860526366128
[2m[36m(func pid=48412)[0m f1_weighted: 0.03886391180633626
[2m[36m(func pid=48412)[0m f1_per_class: [0.117, 0.01, 0.0, 0.091, 0.0, 0.019, 0.0, 0.105, 0.022, 0.035]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.4894 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 92.2040 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.0173 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1073 | Steps: 2 | Val loss: 2.5350 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=31420)[0m top1: 0.4146455223880597
[2m[36m(func pid=31420)[0m top5: 0.902518656716418
[2m[36m(func pid=31420)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=31420)[0m f1_macro: 0.3744391865714873
[2m[36m(func pid=31420)[0m f1_weighted: 0.4374665916660544
[2m[36m(func pid=31420)[0m f1_per_class: [0.5, 0.44, 0.533, 0.547, 0.107, 0.226, 0.478, 0.244, 0.209, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.31156716417910446
[2m[36m(func pid=36355)[0m top5: 0.6856343283582089
[2m[36m(func pid=36355)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=36355)[0m f1_macro: 0.2772801725658269
[2m[36m(func pid=36355)[0m f1_weighted: 0.2659470684547927
[2m[36m(func pid=36355)[0m f1_per_class: [0.34, 0.181, 0.786, 0.171, 0.232, 0.023, 0.526, 0.099, 0.158, 0.255]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:19 (running for 00:21:11.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.374 |                   80 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.41  |                   56 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.277 |                   55 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.159 |      0.04  |                    1 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.41884328358208955
[2m[36m(func pid=36270)[0m top5: 0.914179104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=36270)[0m f1_macro: 0.40986359706292985
[2m[36m(func pid=36270)[0m f1_weighted: 0.4295546226572361
[2m[36m(func pid=36270)[0m f1_per_class: [0.567, 0.411, 0.815, 0.588, 0.109, 0.258, 0.407, 0.243, 0.21, 0.491]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.06343283582089553
[2m[36m(func pid=48412)[0m top5: 0.47994402985074625
[2m[36m(func pid=48412)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=48412)[0m f1_macro: 0.034624671513626956
[2m[36m(func pid=48412)[0m f1_weighted: 0.03533331498013381
[2m[36m(func pid=48412)[0m f1_per_class: [0.078, 0.01, 0.0, 0.082, 0.0, 0.019, 0.0, 0.102, 0.023, 0.033]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.5124 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 94.2983 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.1281 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1754 | Steps: 2 | Val loss: 2.5528 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=31420)[0m top1: 0.41511194029850745
[2m[36m(func pid=31420)[0m top5: 0.9001865671641791
[2m[36m(func pid=31420)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=31420)[0m f1_macro: 0.3719122451790215
[2m[36m(func pid=31420)[0m f1_weighted: 0.43769129080134767
[2m[36m(func pid=31420)[0m f1_per_class: [0.49, 0.439, 0.511, 0.548, 0.114, 0.206, 0.487, 0.253, 0.207, 0.467]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m top1: 0.31203358208955223
[2m[36m(func pid=36355)[0m top5: 0.6875
[2m[36m(func pid=36355)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=36355)[0m f1_macro: 0.28090603185040597
[2m[36m(func pid=36355)[0m f1_weighted: 0.2625233331408015
[2m[36m(func pid=36355)[0m f1_per_class: [0.338, 0.165, 0.815, 0.163, 0.239, 0.023, 0.53, 0.102, 0.157, 0.277]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:24 (running for 00:21:16.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.372 |                   81 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.41  |                   57 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.281 |                   56 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.107 |      0.035 |                    2 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4146455223880597
[2m[36m(func pid=36270)[0m top5: 0.9113805970149254
[2m[36m(func pid=36270)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=36270)[0m f1_macro: 0.4097414536129305
[2m[36m(func pid=36270)[0m f1_weighted: 0.42575433119168543
[2m[36m(func pid=36270)[0m f1_per_class: [0.576, 0.418, 0.815, 0.584, 0.107, 0.243, 0.398, 0.248, 0.208, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.06203358208955224
[2m[36m(func pid=48412)[0m top5: 0.4762126865671642
[2m[36m(func pid=48412)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=48412)[0m f1_macro: 0.031342498672470055
[2m[36m(func pid=48412)[0m f1_weighted: 0.03534683915238284
[2m[36m(func pid=48412)[0m f1_per_class: [0.055, 0.014, 0.0, 0.083, 0.0, 0.02, 0.0, 0.1, 0.0, 0.042]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.5150 | Batch size: 32 | lr: 0.001 | Duration: 2.64s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 96.7851 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.1175 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=31420)[0m top1: 0.4216417910447761
[2m[36m(func pid=31420)[0m top5: 0.9011194029850746
[2m[36m(func pid=31420)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=31420)[0m f1_macro: 0.37169801676465
[2m[36m(func pid=31420)[0m f1_weighted: 0.4452597477486757
[2m[36m(func pid=31420)[0m f1_per_class: [0.49, 0.438, 0.511, 0.558, 0.112, 0.213, 0.501, 0.257, 0.207, 0.431]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1318 | Steps: 2 | Val loss: 2.5623 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=36355)[0m top1: 0.30597014925373134
[2m[36m(func pid=36355)[0m top5: 0.6800373134328358
[2m[36m(func pid=36355)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=36355)[0m f1_macro: 0.27535017727839295
[2m[36m(func pid=36355)[0m f1_weighted: 0.2560940440494421
[2m[36m(func pid=36355)[0m f1_per_class: [0.337, 0.164, 0.786, 0.147, 0.239, 0.023, 0.525, 0.105, 0.154, 0.274]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:29 (running for 00:21:22.11)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.372 |                   82 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   58 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.275 |                   57 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.175 |      0.031 |                    3 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4207089552238806
[2m[36m(func pid=36270)[0m top5: 0.9132462686567164
[2m[36m(func pid=36270)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=36270)[0m f1_macro: 0.412605112435492
[2m[36m(func pid=36270)[0m f1_weighted: 0.432673421518251
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.417, 0.815, 0.588, 0.107, 0.247, 0.418, 0.235, 0.209, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.5245 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=48412)[0m top1: 0.06063432835820896
[2m[36m(func pid=48412)[0m top5: 0.4748134328358209
[2m[36m(func pid=48412)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=48412)[0m f1_macro: 0.032844949448985444
[2m[36m(func pid=48412)[0m f1_weighted: 0.03784071375473582
[2m[36m(func pid=48412)[0m f1_per_class: [0.051, 0.031, 0.0, 0.083, 0.0, 0.02, 0.0, 0.095, 0.0, 0.049]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.8714 | Steps: 2 | Val loss: 97.3560 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.1759 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31420)[0m top1: 0.42630597014925375
[2m[36m(func pid=31420)[0m top5: 0.902518656716418
[2m[36m(func pid=31420)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=31420)[0m f1_macro: 0.3759080533135889
[2m[36m(func pid=31420)[0m f1_weighted: 0.4501332295996838
[2m[36m(func pid=31420)[0m f1_per_class: [0.493, 0.442, 0.511, 0.552, 0.107, 0.213, 0.519, 0.258, 0.206, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0978 | Steps: 2 | Val loss: 2.5680 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=36355)[0m top1: 0.3003731343283582
[2m[36m(func pid=36355)[0m top5: 0.6800373134328358
[2m[36m(func pid=36355)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=36355)[0m f1_macro: 0.27830042325581394
[2m[36m(func pid=36355)[0m f1_weighted: 0.253822800342452
[2m[36m(func pid=36355)[0m f1_per_class: [0.337, 0.164, 0.815, 0.141, 0.246, 0.023, 0.522, 0.104, 0.152, 0.278]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:35 (running for 00:21:27.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.003 |      0.376 |                   83 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.41  |                   59 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0.871 |      0.278 |                   58 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.132 |      0.033 |                    4 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4207089552238806
[2m[36m(func pid=36270)[0m top5: 0.9151119402985075
[2m[36m(func pid=36270)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=36270)[0m f1_macro: 0.4103829684639616
[2m[36m(func pid=36270)[0m f1_weighted: 0.4317876911607641
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.416, 0.815, 0.586, 0.104, 0.231, 0.423, 0.238, 0.21, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0018 | Steps: 2 | Val loss: 2.5299 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=48412)[0m top1: 0.06203358208955224
[2m[36m(func pid=48412)[0m top5: 0.4724813432835821
[2m[36m(func pid=48412)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=48412)[0m f1_macro: 0.037742513246620944
[2m[36m(func pid=48412)[0m f1_weighted: 0.04354663831855559
[2m[36m(func pid=48412)[0m f1_per_class: [0.047, 0.055, 0.0, 0.087, 0.0, 0.019, 0.0, 0.092, 0.025, 0.053]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 97.9633 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.2095 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=31420)[0m top1: 0.425839552238806
[2m[36m(func pid=31420)[0m top5: 0.9039179104477612
[2m[36m(func pid=31420)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=31420)[0m f1_macro: 0.37539353837421363
[2m[36m(func pid=31420)[0m f1_weighted: 0.4497429732948332
[2m[36m(func pid=31420)[0m f1_per_class: [0.5, 0.437, 0.49, 0.55, 0.107, 0.225, 0.517, 0.262, 0.199, 0.467]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0488 | Steps: 2 | Val loss: 2.5664 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=36355)[0m top1: 0.29990671641791045
[2m[36m(func pid=36355)[0m top5: 0.6800373134328358
[2m[36m(func pid=36355)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=36355)[0m f1_macro: 0.2759184461863846
[2m[36m(func pid=36355)[0m f1_weighted: 0.2587479548726169
[2m[36m(func pid=36355)[0m f1_per_class: [0.32, 0.19, 0.759, 0.138, 0.25, 0.023, 0.526, 0.119, 0.145, 0.288]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:40 (running for 00:21:32.53)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.375 |                   84 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.409 |                   60 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.276 |                   59 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.098 |      0.038 |                    5 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.42350746268656714
[2m[36m(func pid=36270)[0m top5: 0.9151119402985075
[2m[36m(func pid=36270)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=36270)[0m f1_macro: 0.4092980817652948
[2m[36m(func pid=36270)[0m f1_weighted: 0.43550970568645236
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.404, 0.815, 0.583, 0.108, 0.24, 0.443, 0.238, 0.206, 0.475]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.5441 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=48412)[0m top1: 0.06436567164179105
[2m[36m(func pid=48412)[0m top5: 0.46455223880597013
[2m[36m(func pid=48412)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=48412)[0m f1_macro: 0.03834558294927372
[2m[36m(func pid=48412)[0m f1_weighted: 0.04806588568351247
[2m[36m(func pid=48412)[0m f1_per_class: [0.045, 0.071, 0.0, 0.095, 0.0, 0.013, 0.0, 0.095, 0.025, 0.039]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 99.5211 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0010 | Steps: 2 | Val loss: 6.3069 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=31420)[0m top1: 0.427705223880597
[2m[36m(func pid=31420)[0m top5: 0.9039179104477612
[2m[36m(func pid=31420)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=31420)[0m f1_macro: 0.3772975958398415
[2m[36m(func pid=31420)[0m f1_weighted: 0.4514900063470778
[2m[36m(func pid=31420)[0m f1_per_class: [0.522, 0.439, 0.5, 0.548, 0.102, 0.214, 0.527, 0.251, 0.209, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0253 | Steps: 2 | Val loss: 2.5590 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=36355)[0m top1: 0.29197761194029853
[2m[36m(func pid=36355)[0m top5: 0.6777052238805971
[2m[36m(func pid=36355)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=36355)[0m f1_macro: 0.27233614469383893
[2m[36m(func pid=36355)[0m f1_weighted: 0.2551778329554508
[2m[36m(func pid=36355)[0m f1_per_class: [0.3, 0.197, 0.759, 0.127, 0.242, 0.023, 0.522, 0.117, 0.159, 0.278]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:04:45 (running for 00:21:37.70)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.377 |                   85 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.409 |                   61 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.272 |                   60 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.049 |      0.038 |                    6 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4230410447761194
[2m[36m(func pid=36270)[0m top5: 0.914179104477612
[2m[36m(func pid=36270)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=36270)[0m f1_macro: 0.40859041928546536
[2m[36m(func pid=36270)[0m f1_weighted: 0.433882395205192
[2m[36m(func pid=36270)[0m f1_per_class: [0.567, 0.406, 0.815, 0.588, 0.117, 0.232, 0.435, 0.241, 0.203, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.5406 | Batch size: 32 | lr: 0.001 | Duration: 2.62s
[2m[36m(func pid=48412)[0m top1: 0.06576492537313433
[2m[36m(func pid=48412)[0m top5: 0.4603544776119403
[2m[36m(func pid=48412)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=48412)[0m f1_macro: 0.0439715720611169
[2m[36m(func pid=48412)[0m f1_weighted: 0.05390941399473307
[2m[36m(func pid=48412)[0m f1_per_class: [0.057, 0.091, 0.0, 0.099, 0.0, 0.019, 0.0, 0.091, 0.049, 0.033]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 100.5710 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3505 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=31420)[0m top1: 0.43097014925373134
[2m[36m(func pid=31420)[0m top5: 0.9053171641791045
[2m[36m(func pid=31420)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=31420)[0m f1_macro: 0.3799019077763744
[2m[36m(func pid=31420)[0m f1_weighted: 0.4539449103481384
[2m[36m(func pid=31420)[0m f1_per_class: [0.53, 0.436, 0.511, 0.56, 0.105, 0.217, 0.524, 0.255, 0.209, 0.452]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0273 | Steps: 2 | Val loss: 2.5480 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:04:50 (running for 00:21:42.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.38  |                   86 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0.001 |      0.409 |                   61 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.274 |                   61 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.025 |      0.044 |                    7 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.2905783582089552
[2m[36m(func pid=36355)[0m top5: 0.6767723880597015
[2m[36m(func pid=36355)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=36355)[0m f1_macro: 0.27443525160718835
[2m[36m(func pid=36355)[0m f1_weighted: 0.2603562403460411
[2m[36m(func pid=36355)[0m f1_per_class: [0.31, 0.225, 0.733, 0.121, 0.23, 0.031, 0.525, 0.117, 0.156, 0.297]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4230410447761194
[2m[36m(func pid=36270)[0m top5: 0.9160447761194029
[2m[36m(func pid=36270)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=36270)[0m f1_macro: 0.4054643769601415
[2m[36m(func pid=36270)[0m f1_weighted: 0.4326184810736198
[2m[36m(func pid=36270)[0m f1_per_class: [0.553, 0.396, 0.815, 0.589, 0.115, 0.226, 0.439, 0.242, 0.207, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0037 | Steps: 2 | Val loss: 2.5299 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=48412)[0m top1: 0.06576492537313433
[2m[36m(func pid=48412)[0m top5: 0.4603544776119403
[2m[36m(func pid=48412)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=48412)[0m f1_macro: 0.04277380591191661
[2m[36m(func pid=48412)[0m f1_weighted: 0.057479768702579585
[2m[36m(func pid=48412)[0m f1_per_class: [0.048, 0.091, 0.0, 0.116, 0.0, 0.012, 0.0, 0.089, 0.045, 0.026]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 101.0822 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=31420)[0m top1: 0.43423507462686567
[2m[36m(func pid=31420)[0m top5: 0.90625
[2m[36m(func pid=31420)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=31420)[0m f1_macro: 0.382709062887734
[2m[36m(func pid=31420)[0m f1_weighted: 0.4548319192196332
[2m[36m(func pid=31420)[0m f1_per_class: [0.534, 0.436, 0.533, 0.559, 0.116, 0.211, 0.53, 0.252, 0.211, 0.444]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3642 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.0108 | Steps: 2 | Val loss: 2.5406 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=36355)[0m top1: 0.28777985074626866
[2m[36m(func pid=36355)[0m top5: 0.6777052238805971
[2m[36m(func pid=36355)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=36355)[0m f1_macro: 0.2750552037949048
[2m[36m(func pid=36355)[0m f1_weighted: 0.2599738599493366
[2m[36m(func pid=36355)[0m f1_per_class: [0.305, 0.241, 0.733, 0.115, 0.23, 0.031, 0.52, 0.118, 0.159, 0.3]
== Status ==
Current time: 2024-01-07 01:04:55 (running for 00:21:48.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.383 |                   87 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.405 |                   62 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.275 |                   62 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.027 |      0.043 |                    8 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4244402985074627
[2m[36m(func pid=36270)[0m top5: 0.9169776119402985
[2m[36m(func pid=36270)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=36270)[0m f1_macro: 0.4131171973989091
[2m[36m(func pid=36270)[0m f1_weighted: 0.43550286517747167
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.399, 0.815, 0.589, 0.122, 0.229, 0.443, 0.243, 0.201, 0.519]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.5462 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=48412)[0m top1: 0.06763059701492537
[2m[36m(func pid=48412)[0m top5: 0.45242537313432835
[2m[36m(func pid=48412)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=48412)[0m f1_macro: 0.04422647066037553
[2m[36m(func pid=48412)[0m f1_weighted: 0.06151953613901772
[2m[36m(func pid=48412)[0m f1_per_class: [0.041, 0.097, 0.0, 0.126, 0.0, 0.018, 0.0, 0.088, 0.043, 0.03]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m top1: 0.43236940298507465
[2m[36m(func pid=31420)[0m top5: 0.9048507462686567
[2m[36m(func pid=31420)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=31420)[0m f1_macro: 0.3814223767664167
[2m[36m(func pid=31420)[0m f1_weighted: 0.45330864924454906
[2m[36m(func pid=31420)[0m f1_per_class: [0.53, 0.444, 0.533, 0.552, 0.112, 0.212, 0.53, 0.235, 0.214, 0.452]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 102.1758 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.3954 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9561 | Steps: 2 | Val loss: 2.5309 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:05:01 (running for 00:21:53.52)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.381 |                   88 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   63 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.28  |                   63 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  3.011 |      0.044 |                    9 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.5512 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=36355)[0m top1: 0.28544776119402987
[2m[36m(func pid=36355)[0m top5: 0.6721082089552238
[2m[36m(func pid=36355)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=36355)[0m f1_macro: 0.279713196016358
[2m[36m(func pid=36355)[0m f1_weighted: 0.26011713090086974
[2m[36m(func pid=36355)[0m f1_per_class: [0.299, 0.258, 0.759, 0.104, 0.23, 0.031, 0.52, 0.117, 0.165, 0.316]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4281716417910448
[2m[36m(func pid=36270)[0m top5: 0.9165111940298507
[2m[36m(func pid=36270)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=36270)[0m f1_macro: 0.4131761061429118
[2m[36m(func pid=36270)[0m f1_weighted: 0.43714162637209264
[2m[36m(func pid=36270)[0m f1_per_class: [0.562, 0.403, 0.815, 0.595, 0.12, 0.233, 0.439, 0.246, 0.211, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.06902985074626866
[2m[36m(func pid=48412)[0m top5: 0.44449626865671643
[2m[36m(func pid=48412)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=48412)[0m f1_macro: 0.04624714546447227
[2m[36m(func pid=48412)[0m f1_weighted: 0.06577818485169729
[2m[36m(func pid=48412)[0m f1_per_class: [0.048, 0.104, 0.0, 0.132, 0.0, 0.023, 0.003, 0.084, 0.041, 0.028]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m top1: 0.43097014925373134
[2m[36m(func pid=31420)[0m top5: 0.9053171641791045
[2m[36m(func pid=31420)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=31420)[0m f1_macro: 0.3805963413151546
[2m[36m(func pid=31420)[0m f1_weighted: 0.45143749007021466
[2m[36m(func pid=31420)[0m f1_per_class: [0.519, 0.44, 0.545, 0.542, 0.112, 0.205, 0.537, 0.24, 0.214, 0.452]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 2 | Val loss: 104.7386 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3971 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9724 | Steps: 2 | Val loss: 2.5266 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.5795 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:05:06 (running for 00:21:58.89)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                   89 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   64 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.275 |                   64 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.956 |      0.046 |                   10 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43236940298507465
[2m[36m(func pid=36270)[0m top5: 0.9183768656716418
[2m[36m(func pid=36270)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=36270)[0m f1_macro: 0.4159256447369204
[2m[36m(func pid=36270)[0m f1_weighted: 0.4411705931637604
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.392, 0.815, 0.593, 0.14, 0.244, 0.456, 0.24, 0.207, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.279384328358209
[2m[36m(func pid=36355)[0m top5: 0.664179104477612
[2m[36m(func pid=36355)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=36355)[0m f1_macro: 0.27538937642704553
[2m[36m(func pid=36355)[0m f1_weighted: 0.2545951189582191
[2m[36m(func pid=36355)[0m f1_per_class: [0.302, 0.259, 0.733, 0.086, 0.226, 0.031, 0.518, 0.114, 0.162, 0.323]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=48412)[0m top1: 0.06996268656716417
[2m[36m(func pid=48412)[0m top5: 0.4351679104477612
[2m[36m(func pid=48412)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=48412)[0m f1_macro: 0.048918415601798805
[2m[36m(func pid=48412)[0m f1_weighted: 0.06737063580588003
[2m[36m(func pid=48412)[0m f1_per_class: [0.045, 0.104, 0.0, 0.134, 0.0, 0.033, 0.0, 0.086, 0.059, 0.027]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=31420)[0m top1: 0.425839552238806
[2m[36m(func pid=31420)[0m top5: 0.9043843283582089
[2m[36m(func pid=31420)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=31420)[0m f1_macro: 0.3760139478729506
[2m[36m(func pid=31420)[0m f1_weighted: 0.4476682885520272
[2m[36m(func pid=31420)[0m f1_per_class: [0.535, 0.442, 0.511, 0.54, 0.097, 0.203, 0.527, 0.242, 0.205, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3620 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 106.8515 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9396 | Steps: 2 | Val loss: 2.5163 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0020 | Steps: 2 | Val loss: 2.5889 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 01:05:11 (running for 00:22:04.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.376 |                   90 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   65 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.274 |                   65 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.972 |      0.049 |                   11 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43703358208955223
[2m[36m(func pid=36270)[0m top5: 0.917910447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=36270)[0m f1_macro: 0.41933983098041094
[2m[36m(func pid=36270)[0m f1_weighted: 0.44669748498908385
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.406, 0.815, 0.595, 0.133, 0.243, 0.466, 0.232, 0.213, 0.519]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=36355)[0m top1: 0.2733208955223881
[2m[36m(func pid=36355)[0m top5: 0.6623134328358209
[2m[36m(func pid=36355)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=36355)[0m f1_macro: 0.27400882380724545
[2m[36m(func pid=36355)[0m f1_weighted: 0.25267506988925037
[2m[36m(func pid=36355)[0m f1_per_class: [0.291, 0.273, 0.733, 0.077, 0.233, 0.031, 0.513, 0.115, 0.158, 0.315]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4253731343283582
[2m[36m(func pid=31420)[0m top5: 0.9039179104477612
[2m[36m(func pid=31420)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=31420)[0m f1_macro: 0.37700907402427497
[2m[36m(func pid=31420)[0m f1_weighted: 0.44658386120858123
[2m[36m(func pid=31420)[0m f1_per_class: [0.548, 0.448, 0.5, 0.531, 0.098, 0.2, 0.528, 0.237, 0.205, 0.475]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07602611940298508
[2m[36m(func pid=48412)[0m top5: 0.4351679104477612
[2m[36m(func pid=48412)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=48412)[0m f1_macro: 0.05213008305681699
[2m[36m(func pid=48412)[0m f1_weighted: 0.07637932765257043
[2m[36m(func pid=48412)[0m f1_per_class: [0.04, 0.123, 0.0, 0.149, 0.0, 0.044, 0.003, 0.085, 0.054, 0.024]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.4173 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 107.6103 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.6102 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9244 | Steps: 2 | Val loss: 2.5043 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:05:17 (running for 00:22:09.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.377 |                   91 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.419 |                   66 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.273 |                   66 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.94  |      0.052 |                   12 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.27052238805970147
[2m[36m(func pid=36355)[0m top5: 0.6609141791044776
[2m[36m(func pid=36355)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=36355)[0m f1_macro: 0.27313263633508533
[2m[36m(func pid=36355)[0m f1_weighted: 0.25248390737495535
[2m[36m(func pid=36355)[0m f1_per_class: [0.284, 0.286, 0.733, 0.071, 0.241, 0.031, 0.513, 0.109, 0.158, 0.306]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.4361007462686567
[2m[36m(func pid=36270)[0m top5: 0.9169776119402985
[2m[36m(func pid=36270)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.42010049949817113
[2m[36m(func pid=36270)[0m f1_weighted: 0.44605094268310713
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.403, 0.815, 0.596, 0.136, 0.269, 0.456, 0.23, 0.207, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4239738805970149
[2m[36m(func pid=31420)[0m top5: 0.9020522388059702
[2m[36m(func pid=31420)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=31420)[0m f1_macro: 0.37653019186801395
[2m[36m(func pid=31420)[0m f1_weighted: 0.4447635625711348
[2m[36m(func pid=31420)[0m f1_per_class: [0.54, 0.446, 0.511, 0.523, 0.098, 0.203, 0.53, 0.233, 0.206, 0.475]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07602611940298508
[2m[36m(func pid=48412)[0m top5: 0.4351679104477612
[2m[36m(func pid=48412)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=48412)[0m f1_macro: 0.053634967504283246
[2m[36m(func pid=48412)[0m f1_weighted: 0.07878355450324179
[2m[36m(func pid=48412)[0m f1_per_class: [0.044, 0.127, 0.0, 0.153, 0.0, 0.043, 0.006, 0.072, 0.067, 0.025]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 109.3893 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.4842 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0065 | Steps: 2 | Val loss: 2.6276 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8700 | Steps: 2 | Val loss: 2.4979 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:05:22 (running for 00:22:14.59)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.377 |                   92 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.42  |                   67 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.273 |                   67 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.924 |      0.054 |                   13 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.2653917910447761
[2m[36m(func pid=36355)[0m top5: 0.6613805970149254
[2m[36m(func pid=36355)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=36355)[0m f1_macro: 0.2733572627588183
[2m[36m(func pid=36355)[0m f1_weighted: 0.24979018944809916
[2m[36m(func pid=36355)[0m f1_per_class: [0.275, 0.29, 0.759, 0.061, 0.25, 0.031, 0.51, 0.112, 0.156, 0.289]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43283582089552236
[2m[36m(func pid=36270)[0m top5: 0.9174440298507462
[2m[36m(func pid=36270)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=36270)[0m f1_macro: 0.4140614691671483
[2m[36m(func pid=36270)[0m f1_weighted: 0.4422420542032384
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.397, 0.815, 0.592, 0.14, 0.255, 0.457, 0.227, 0.203, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4221082089552239
[2m[36m(func pid=31420)[0m top5: 0.9006529850746269
[2m[36m(func pid=31420)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=31420)[0m f1_macro: 0.3701014074173786
[2m[36m(func pid=31420)[0m f1_weighted: 0.442524253897645
[2m[36m(func pid=31420)[0m f1_per_class: [0.527, 0.438, 0.511, 0.523, 0.095, 0.195, 0.537, 0.207, 0.208, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07789179104477612
[2m[36m(func pid=48412)[0m top5: 0.43236940298507465
[2m[36m(func pid=48412)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=48412)[0m f1_macro: 0.0554705691224904
[2m[36m(func pid=48412)[0m f1_weighted: 0.0804715465474186
[2m[36m(func pid=48412)[0m f1_per_class: [0.041, 0.127, 0.0, 0.158, 0.0, 0.042, 0.006, 0.074, 0.075, 0.031]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 110.2913 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.5234 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.6241 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:05:27 (running for 00:22:19.67)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.007 |      0.37  |                   93 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   68 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.27  |                   68 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.87  |      0.055 |                   14 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.2583955223880597
[2m[36m(func pid=36355)[0m top5: 0.6609141791044776
[2m[36m(func pid=36355)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=36355)[0m f1_macro: 0.26979767626383455
[2m[36m(func pid=36355)[0m f1_weighted: 0.244850869693285
[2m[36m(func pid=36355)[0m f1_per_class: [0.279, 0.293, 0.786, 0.058, 0.185, 0.031, 0.495, 0.113, 0.158, 0.3]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8582 | Steps: 2 | Val loss: 2.4875 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=36270)[0m top1: 0.4375
[2m[36m(func pid=36270)[0m top5: 0.9183768656716418
[2m[36m(func pid=36270)[0m f1_micro: 0.4375
[2m[36m(func pid=36270)[0m f1_macro: 0.4176954479533738
[2m[36m(func pid=36270)[0m f1_weighted: 0.4476397648493949
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.405, 0.815, 0.597, 0.136, 0.258, 0.463, 0.23, 0.21, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4244402985074627
[2m[36m(func pid=31420)[0m top5: 0.9011194029850746
[2m[36m(func pid=31420)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=31420)[0m f1_macro: 0.3720950794931441
[2m[36m(func pid=31420)[0m f1_weighted: 0.44455767213238734
[2m[36m(func pid=31420)[0m f1_per_class: [0.531, 0.44, 0.522, 0.524, 0.096, 0.207, 0.538, 0.2, 0.211, 0.452]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07649253731343283
[2m[36m(func pid=48412)[0m top5: 0.4314365671641791
[2m[36m(func pid=48412)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=48412)[0m f1_macro: 0.05483989205614016
[2m[36m(func pid=48412)[0m f1_weighted: 0.07773750492537973
[2m[36m(func pid=48412)[0m f1_per_class: [0.047, 0.122, 0.0, 0.15, 0.0, 0.042, 0.006, 0.083, 0.068, 0.031]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 111.0739 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6133 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0017 | Steps: 2 | Val loss: 2.6405 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:05:32 (running for 00:22:24.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.372 |                   94 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.418 |                   69 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.271 |                   69 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.858 |      0.055 |                   15 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.25466417910447764
[2m[36m(func pid=36355)[0m top5: 0.6618470149253731
[2m[36m(func pid=36355)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=36355)[0m f1_macro: 0.27112704987279385
[2m[36m(func pid=36355)[0m f1_weighted: 0.24262938039846466
[2m[36m(func pid=36355)[0m f1_per_class: [0.273, 0.302, 0.759, 0.055, 0.218, 0.031, 0.485, 0.114, 0.154, 0.32]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8585 | Steps: 2 | Val loss: 2.4816 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=36270)[0m top1: 0.4300373134328358
[2m[36m(func pid=36270)[0m top5: 0.9207089552238806
[2m[36m(func pid=36270)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=36270)[0m f1_macro: 0.41197129384371534
[2m[36m(func pid=36270)[0m f1_weighted: 0.439139906697468
[2m[36m(func pid=36270)[0m f1_per_class: [0.571, 0.392, 0.815, 0.591, 0.132, 0.254, 0.452, 0.226, 0.205, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4230410447761194
[2m[36m(func pid=31420)[0m top5: 0.9029850746268657
[2m[36m(func pid=31420)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=31420)[0m f1_macro: 0.3750575037233701
[2m[36m(func pid=31420)[0m f1_weighted: 0.4437428916293939
[2m[36m(func pid=31420)[0m f1_per_class: [0.538, 0.437, 0.522, 0.527, 0.095, 0.207, 0.528, 0.229, 0.216, 0.452]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 112.1238 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=48412)[0m top1: 0.07695895522388059
[2m[36m(func pid=48412)[0m top5: 0.4291044776119403
[2m[36m(func pid=48412)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=48412)[0m f1_macro: 0.05664313335418382
[2m[36m(func pid=48412)[0m f1_weighted: 0.0787877338269374
[2m[36m(func pid=48412)[0m f1_per_class: [0.045, 0.125, 0.0, 0.148, 0.0, 0.047, 0.006, 0.082, 0.087, 0.027]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6296 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.6628 | Batch size: 32 | lr: 0.001 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:05:37 (running for 00:22:29.93)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.375 |                   95 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   70 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.269 |                   70 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.858 |      0.057 |                   16 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.25279850746268656
[2m[36m(func pid=36355)[0m top5: 0.6609141791044776
[2m[36m(func pid=36355)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=36355)[0m f1_macro: 0.26940580792978924
[2m[36m(func pid=36355)[0m f1_weighted: 0.24189907720085796
[2m[36m(func pid=36355)[0m f1_per_class: [0.28, 0.317, 0.759, 0.049, 0.192, 0.031, 0.48, 0.113, 0.154, 0.32]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43283582089552236
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=36270)[0m f1_macro: 0.41343901176884196
[2m[36m(func pid=36270)[0m f1_weighted: 0.4419711268627451
[2m[36m(func pid=36270)[0m f1_per_class: [0.586, 0.395, 0.815, 0.591, 0.132, 0.258, 0.458, 0.226, 0.2, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8763 | Steps: 2 | Val loss: 2.4750 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=31420)[0m top1: 0.42350746268656714
[2m[36m(func pid=31420)[0m top5: 0.9034514925373134
[2m[36m(func pid=31420)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=31420)[0m f1_macro: 0.3732498260824557
[2m[36m(func pid=31420)[0m f1_weighted: 0.4444340374883239
[2m[36m(func pid=31420)[0m f1_per_class: [0.534, 0.437, 0.511, 0.534, 0.095, 0.218, 0.524, 0.207, 0.213, 0.459]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 112.8842 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=48412)[0m top1: 0.07975746268656717
[2m[36m(func pid=48412)[0m top5: 0.42723880597014924
[2m[36m(func pid=48412)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=48412)[0m f1_macro: 0.057793070964517954
[2m[36m(func pid=48412)[0m f1_weighted: 0.08188853850659072
[2m[36m(func pid=48412)[0m f1_per_class: [0.05, 0.133, 0.0, 0.154, 0.0, 0.047, 0.006, 0.085, 0.081, 0.022]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6880 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0040 | Steps: 2 | Val loss: 2.6760 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=36355)[0m top1: 0.25279850746268656
[2m[36m(func pid=36355)[0m top5: 0.6567164179104478
[2m[36m(func pid=36355)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=36355)[0m f1_macro: 0.27454173332799753
[2m[36m(func pid=36355)[0m f1_weighted: 0.2432408698827678
[2m[36m(func pid=36355)[0m f1_per_class: [0.278, 0.321, 0.786, 0.049, 0.192, 0.03, 0.481, 0.117, 0.154, 0.338]
[2m[36m(func pid=36355)[0m 
== Status ==
Current time: 2024-01-07 01:05:42 (running for 00:22:35.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.001 |      0.373 |                   96 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   71 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.275 |                   71 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.876 |      0.058 |                   17 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43330223880597013
[2m[36m(func pid=36270)[0m top5: 0.9197761194029851
[2m[36m(func pid=36270)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36270)[0m f1_macro: 0.41426775968848933
[2m[36m(func pid=36270)[0m f1_weighted: 0.44436254830575994
[2m[36m(func pid=36270)[0m f1_per_class: [0.586, 0.404, 0.815, 0.593, 0.127, 0.245, 0.464, 0.226, 0.201, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7927 | Steps: 2 | Val loss: 2.4707 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=31420)[0m top1: 0.4221082089552239
[2m[36m(func pid=31420)[0m top5: 0.9043843283582089
[2m[36m(func pid=31420)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=31420)[0m f1_macro: 0.37231354877976236
[2m[36m(func pid=31420)[0m f1_weighted: 0.44287311528811396
[2m[36m(func pid=31420)[0m f1_per_class: [0.526, 0.435, 0.511, 0.535, 0.099, 0.231, 0.518, 0.192, 0.21, 0.467]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1264 | Steps: 2 | Val loss: 113.6613 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=48412)[0m top1: 0.07695895522388059
[2m[36m(func pid=48412)[0m top5: 0.42257462686567165
[2m[36m(func pid=48412)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=48412)[0m f1_macro: 0.05536481270077641
[2m[36m(func pid=48412)[0m f1_weighted: 0.08165730482938861
[2m[36m(func pid=48412)[0m f1_per_class: [0.049, 0.129, 0.0, 0.156, 0.0, 0.04, 0.012, 0.068, 0.078, 0.022]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0004 | Steps: 2 | Val loss: 6.7841 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0017 | Steps: 2 | Val loss: 2.7078 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:05:48 (running for 00:22:40.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.004 |      0.372 |                   97 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   72 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  2.126 |      0.271 |                   72 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.793 |      0.055 |                   18 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.2523320895522388
[2m[36m(func pid=36355)[0m top5: 0.65625
[2m[36m(func pid=36355)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=36355)[0m f1_macro: 0.2713882776047877
[2m[36m(func pid=36355)[0m f1_weighted: 0.24463613899575165
[2m[36m(func pid=36355)[0m f1_per_class: [0.279, 0.339, 0.786, 0.049, 0.157, 0.03, 0.477, 0.114, 0.156, 0.328]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43330223880597013
[2m[36m(func pid=36270)[0m top5: 0.9183768656716418
[2m[36m(func pid=36270)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36270)[0m f1_macro: 0.41686158591682726
[2m[36m(func pid=36270)[0m f1_weighted: 0.44404838785761525
[2m[36m(func pid=36270)[0m f1_per_class: [0.591, 0.406, 0.815, 0.594, 0.127, 0.239, 0.462, 0.223, 0.203, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4197761194029851
[2m[36m(func pid=31420)[0m top5: 0.9057835820895522
[2m[36m(func pid=31420)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=31420)[0m f1_macro: 0.37228045361421647
[2m[36m(func pid=31420)[0m f1_weighted: 0.44029349743404694
[2m[36m(func pid=31420)[0m f1_per_class: [0.533, 0.441, 0.5, 0.541, 0.099, 0.227, 0.498, 0.212, 0.205, 0.467]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7985 | Steps: 2 | Val loss: 2.4638 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4890 | Steps: 2 | Val loss: 111.9889 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0025 | Steps: 2 | Val loss: 2.6918 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.7619 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=48412)[0m top1: 0.07742537313432836
[2m[36m(func pid=48412)[0m top5: 0.42490671641791045
[2m[36m(func pid=48412)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=48412)[0m f1_macro: 0.05627859399732678
[2m[36m(func pid=48412)[0m f1_weighted: 0.08260145504340938
[2m[36m(func pid=48412)[0m f1_per_class: [0.048, 0.132, 0.0, 0.151, 0.0, 0.049, 0.015, 0.07, 0.075, 0.023]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:05:53 (running for 00:22:45.71)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.372 |                   98 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.417 |                   73 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  1.489 |      0.271 |                   73 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.799 |      0.056 |                   19 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.25699626865671643
[2m[36m(func pid=36355)[0m top5: 0.6613805970149254
[2m[36m(func pid=36355)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=36355)[0m f1_macro: 0.27053169147925316
[2m[36m(func pid=36355)[0m f1_weighted: 0.2478071141852998
[2m[36m(func pid=36355)[0m f1_per_class: [0.268, 0.349, 0.759, 0.055, 0.157, 0.03, 0.475, 0.12, 0.159, 0.333]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4281716417910448
[2m[36m(func pid=31420)[0m top5: 0.9057835820895522
[2m[36m(func pid=31420)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=31420)[0m f1_macro: 0.3752346575119024
[2m[36m(func pid=31420)[0m f1_weighted: 0.44912096543755037
[2m[36m(func pid=31420)[0m f1_per_class: [0.543, 0.444, 0.522, 0.551, 0.105, 0.217, 0.523, 0.194, 0.21, 0.444]
[2m[36m(func pid=31420)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43330223880597013
[2m[36m(func pid=36270)[0m top5: 0.9211753731343284
[2m[36m(func pid=36270)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36270)[0m f1_macro: 0.41290711191408985
[2m[36m(func pid=36270)[0m f1_weighted: 0.4427770076369339
[2m[36m(func pid=36270)[0m f1_per_class: [0.586, 0.399, 0.815, 0.595, 0.126, 0.235, 0.464, 0.219, 0.209, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7910 | Steps: 2 | Val loss: 2.4626 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 106.8534 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=31420)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.6658 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.7864 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=48412)[0m top1: 0.07649253731343283
[2m[36m(func pid=48412)[0m top5: 0.42630597014925375
[2m[36m(func pid=48412)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=48412)[0m f1_macro: 0.05783502042845553
[2m[36m(func pid=48412)[0m f1_weighted: 0.0809805771931293
[2m[36m(func pid=48412)[0m f1_per_class: [0.047, 0.134, 0.024, 0.142, 0.0, 0.053, 0.015, 0.068, 0.07, 0.024]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:05:58 (running for 00:22:51.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00009 | RUNNING    | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.375 |                   99 |
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   74 |
| train_57e67_00011 | RUNNING    | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.279 |                   74 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.791 |      0.058 |                   20 |
| train_57e67_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=36355)[0m top1: 0.2751865671641791
[2m[36m(func pid=36355)[0m top5: 0.6702425373134329
[2m[36m(func pid=36355)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=36355)[0m f1_macro: 0.2791923737148726
[2m[36m(func pid=36355)[0m f1_weighted: 0.26523043673301
[2m[36m(func pid=36355)[0m f1_per_class: [0.255, 0.355, 0.759, 0.083, 0.192, 0.03, 0.503, 0.125, 0.161, 0.329]
[2m[36m(func pid=36355)[0m 
[2m[36m(func pid=36270)[0m top1: 0.435634328358209
[2m[36m(func pid=36270)[0m top5: 0.9188432835820896
[2m[36m(func pid=36270)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=36270)[0m f1_macro: 0.41629599787918103
[2m[36m(func pid=36270)[0m f1_weighted: 0.44640797469097515
[2m[36m(func pid=36270)[0m f1_per_class: [0.591, 0.402, 0.815, 0.595, 0.127, 0.229, 0.477, 0.212, 0.206, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=31420)[0m top1: 0.4361007462686567
[2m[36m(func pid=31420)[0m top5: 0.9048507462686567
[2m[36m(func pid=31420)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=31420)[0m f1_macro: 0.38078335455459955
[2m[36m(func pid=31420)[0m f1_weighted: 0.4558053957331932
[2m[36m(func pid=31420)[0m f1_per_class: [0.551, 0.441, 0.545, 0.556, 0.11, 0.221, 0.539, 0.193, 0.22, 0.431]
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8299 | Steps: 2 | Val loss: 2.4577 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=36355)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 103.0359 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8080 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=48412)[0m top1: 0.07649253731343283
[2m[36m(func pid=48412)[0m top5: 0.4253731343283582
[2m[36m(func pid=48412)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=48412)[0m f1_macro: 0.05813565318673895
[2m[36m(func pid=48412)[0m f1_weighted: 0.0807088776928025
[2m[36m(func pid=48412)[0m f1_per_class: [0.047, 0.137, 0.022, 0.137, 0.0, 0.048, 0.018, 0.073, 0.075, 0.024]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36355)[0m top1: 0.2905783582089552
[2m[36m(func pid=36355)[0m top5: 0.6805037313432836
[2m[36m(func pid=36355)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=36355)[0m f1_macro: 0.2844021772044719
[2m[36m(func pid=36355)[0m f1_weighted: 0.280835098670822
[2m[36m(func pid=36355)[0m f1_per_class: [0.241, 0.381, 0.759, 0.101, 0.192, 0.03, 0.524, 0.13, 0.157, 0.328]
[2m[36m(func pid=36270)[0m top1: 0.4361007462686567
[2m[36m(func pid=36270)[0m top5: 0.9174440298507462
[2m[36m(func pid=36270)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.4148495726976832
[2m[36m(func pid=36270)[0m f1_weighted: 0.44817783780515774
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.412, 0.815, 0.597, 0.125, 0.222, 0.481, 0.196, 0.21, 0.509]
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7741 | Steps: 2 | Val loss: 2.4487 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:06:04 (running for 00:22:56.34)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 3 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   75 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.83  |      0.058 |                   21 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 2 TERMINATED)


[2m[36m(func pid=53465)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53465)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=53465)[0m Configuration completed!
[2m[36m(func pid=53465)[0m New optimizer parameters:
[2m[36m(func pid=53465)[0m SGD (
[2m[36m(func pid=53465)[0m Parameter Group 0
[2m[36m(func pid=53465)[0m     dampening: 0
[2m[36m(func pid=53465)[0m     differentiable: False
[2m[36m(func pid=53465)[0m     foreach: None
[2m[36m(func pid=53465)[0m     lr: 0.001
[2m[36m(func pid=53465)[0m     maximize: False
[2m[36m(func pid=53465)[0m     momentum: 0.9
[2m[36m(func pid=53465)[0m     nesterov: False
[2m[36m(func pid=53465)[0m     weight_decay: 0.0001
[2m[36m(func pid=53465)[0m )
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07555970149253731
[2m[36m(func pid=48412)[0m top5: 0.43097014925373134
[2m[36m(func pid=48412)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=48412)[0m f1_macro: 0.05775452034866409
[2m[36m(func pid=48412)[0m f1_weighted: 0.08075875270404459
[2m[36m(func pid=48412)[0m f1_per_class: [0.044, 0.134, 0.021, 0.13, 0.0, 0.058, 0.024, 0.068, 0.072, 0.026]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.9056 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7334 | Steps: 2 | Val loss: 2.4422 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1252 | Steps: 2 | Val loss: 2.4957 | Batch size: 32 | lr: 0.001 | Duration: 4.45s
[2m[36m(func pid=36270)[0m top1: 0.4361007462686567
[2m[36m(func pid=36270)[0m top5: 0.9155783582089553
[2m[36m(func pid=36270)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.41592870201074134
[2m[36m(func pid=36270)[0m f1_weighted: 0.44704122499421695
[2m[36m(func pid=36270)[0m f1_per_class: [0.591, 0.409, 0.815, 0.599, 0.125, 0.213, 0.477, 0.21, 0.212, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m top1: 0.08022388059701492
[2m[36m(func pid=48412)[0m top5: 0.4337686567164179
[2m[36m(func pid=48412)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=48412)[0m f1_macro: 0.06261228770190551
[2m[36m(func pid=48412)[0m f1_weighted: 0.08342879618173588
[2m[36m(func pid=48412)[0m f1_per_class: [0.046, 0.144, 0.039, 0.125, 0.0, 0.056, 0.029, 0.079, 0.08, 0.029]
[2m[36m(func pid=53465)[0m top1: 0.06669776119402986
[2m[36m(func pid=53465)[0m top5: 0.4818097014925373
[2m[36m(func pid=53465)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=53465)[0m f1_macro: 0.04236456389314704
[2m[36m(func pid=53465)[0m f1_weighted: 0.04150846995618565
[2m[36m(func pid=53465)[0m f1_per_class: [0.139, 0.01, 0.0, 0.1, 0.0, 0.017, 0.0, 0.101, 0.021, 0.034]
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8773 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:06:13 (running for 00:23:05.40)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   77 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.774 |      0.058 |                   22 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53984)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=53984)[0m Configuration completed!
[2m[36m(func pid=53984)[0m New optimizer parameters:
[2m[36m(func pid=53984)[0m SGD (
[2m[36m(func pid=53984)[0m Parameter Group 0
[2m[36m(func pid=53984)[0m     dampening: 0
[2m[36m(func pid=53984)[0m     differentiable: False
[2m[36m(func pid=53984)[0m     foreach: None
[2m[36m(func pid=53984)[0m     lr: 0.01
[2m[36m(func pid=53984)[0m     maximize: False
[2m[36m(func pid=53984)[0m     momentum: 0.9
[2m[36m(func pid=53984)[0m     nesterov: False
[2m[36m(func pid=53984)[0m     weight_decay: 0.0001
[2m[36m(func pid=53984)[0m )
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:06:18 (running for 00:23:10.75)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   78 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.733 |      0.063 |                   23 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  3.125 |      0.042 |                    1 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4375
[2m[36m(func pid=36270)[0m top5: 0.9174440298507462
[2m[36m(func pid=36270)[0m f1_micro: 0.4375
[2m[36m(func pid=36270)[0m f1_macro: 0.4163423391658882
[2m[36m(func pid=36270)[0m f1_weighted: 0.4490184199200352
[2m[36m(func pid=36270)[0m f1_per_class: [0.596, 0.404, 0.815, 0.598, 0.127, 0.216, 0.487, 0.205, 0.206, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0126 | Steps: 2 | Val loss: 2.4861 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7386 | Steps: 2 | Val loss: 2.4350 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1145 | Steps: 2 | Val loss: 2.3839 | Batch size: 32 | lr: 0.01 | Duration: 4.69s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8810 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=53465)[0m top1: 0.06576492537313433
[2m[36m(func pid=53465)[0m top5: 0.46222014925373134
[2m[36m(func pid=53465)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=53465)[0m f1_macro: 0.04775700569914702
[2m[36m(func pid=53465)[0m f1_weighted: 0.04928691622650177
[2m[36m(func pid=53465)[0m f1_per_class: [0.096, 0.045, 0.0, 0.105, 0.0, 0.02, 0.0, 0.095, 0.058, 0.059]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.07975746268656717
[2m[36m(func pid=48412)[0m top5: 0.44216417910447764
[2m[36m(func pid=48412)[0m f1_micro: 0.07975746268656717
[2m[36m(func pid=48412)[0m f1_macro: 0.06383632607329798
[2m[36m(func pid=48412)[0m f1_weighted: 0.08345778103159952
[2m[36m(func pid=48412)[0m f1_per_class: [0.046, 0.143, 0.056, 0.122, 0.0, 0.06, 0.032, 0.07, 0.08, 0.029]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:23 (running for 00:23:16.02)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   78 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.739 |      0.064 |                   24 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  3.013 |      0.048 |                    2 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  3.114 |      0.069 |                    1 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.06389925373134328
[2m[36m(func pid=53984)[0m top5: 0.4533582089552239
[2m[36m(func pid=53984)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=53984)[0m f1_macro: 0.06881021231307403
[2m[36m(func pid=53984)[0m f1_weighted: 0.06664680383362363
[2m[36m(func pid=53984)[0m f1_per_class: [0.107, 0.118, 0.057, 0.078, 0.006, 0.035, 0.029, 0.061, 0.161, 0.035]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=36270)[0m top1: 0.435634328358209
[2m[36m(func pid=36270)[0m top5: 0.9174440298507462
[2m[36m(func pid=36270)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=36270)[0m f1_macro: 0.41195693865679883
[2m[36m(func pid=36270)[0m f1_weighted: 0.4465386196539567
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.402, 0.815, 0.598, 0.117, 0.209, 0.486, 0.197, 0.206, 0.509]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9766 | Steps: 2 | Val loss: 2.4747 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7402 | Steps: 2 | Val loss: 2.4262 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8486 | Steps: 2 | Val loss: 2.3127 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8849 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=53465)[0m top1: 0.05830223880597015
[2m[36m(func pid=53465)[0m top5: 0.42677238805970147
[2m[36m(func pid=53465)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=53465)[0m f1_macro: 0.04686673788347984
[2m[36m(func pid=53465)[0m f1_weighted: 0.05453565875181072
[2m[36m(func pid=53465)[0m f1_per_class: [0.087, 0.091, 0.0, 0.09, 0.0, 0.034, 0.006, 0.079, 0.035, 0.046]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.08488805970149253
[2m[36m(func pid=48412)[0m top5: 0.4430970149253731
[2m[36m(func pid=48412)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=48412)[0m f1_macro: 0.06807835719234101
[2m[36m(func pid=48412)[0m f1_weighted: 0.08982680664014744
[2m[36m(func pid=48412)[0m f1_per_class: [0.047, 0.148, 0.074, 0.129, 0.0, 0.06, 0.044, 0.07, 0.08, 0.029]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m top1: 0.12173507462686567
== Status ==
Current time: 2024-01-07 01:06:28 (running for 00:23:21.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   79 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.74  |      0.068 |                   25 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.977 |      0.047 |                    3 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  2.849 |      0.122 |                    2 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)

[2m[36m(func pid=53984)[0m top5: 0.5802238805970149

[2m[36m(func pid=53984)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=53984)[0m f1_macro: 0.12229861702812186
[2m[36m(func pid=53984)[0m f1_weighted: 0.11413793979615511
[2m[36m(func pid=53984)[0m f1_per_class: [0.134, 0.149, 0.348, 0.02, 0.016, 0.108, 0.199, 0.0, 0.165, 0.085]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43796641791044777
[2m[36m(func pid=36270)[0m top5: 0.9188432835820896
[2m[36m(func pid=36270)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=36270)[0m f1_macro: 0.4138102921839888
[2m[36m(func pid=36270)[0m f1_weighted: 0.4494833427942479
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.403, 0.815, 0.597, 0.115, 0.224, 0.488, 0.207, 0.207, 0.5]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9138 | Steps: 2 | Val loss: 2.4631 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6564 | Steps: 2 | Val loss: 2.4181 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.9603 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3882 | Steps: 2 | Val loss: 2.1888 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=53465)[0m top1: 0.05317164179104478
[2m[36m(func pid=53465)[0m top5: 0.4048507462686567
[2m[36m(func pid=53465)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=53465)[0m f1_macro: 0.053753064954826804
[2m[36m(func pid=53465)[0m f1_weighted: 0.05540628314193404
[2m[36m(func pid=53465)[0m f1_per_class: [0.062, 0.104, 0.043, 0.073, 0.0, 0.034, 0.012, 0.081, 0.098, 0.032]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.08675373134328358
[2m[36m(func pid=48412)[0m top5: 0.45475746268656714
[2m[36m(func pid=48412)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=48412)[0m f1_macro: 0.069519713360091
[2m[36m(func pid=48412)[0m f1_weighted: 0.09193011083914271
[2m[36m(func pid=48412)[0m f1_per_class: [0.052, 0.149, 0.075, 0.131, 0.0, 0.065, 0.046, 0.069, 0.079, 0.029]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:34 (running for 00:23:26.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.413 |                   81 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.656 |      0.07  |                   26 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.914 |      0.054 |                    4 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  2.849 |      0.122 |                    2 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4375
[2m[36m(func pid=36270)[0m top5: 0.9216417910447762
[2m[36m(func pid=36270)[0m f1_micro: 0.4375
[2m[36m(func pid=36270)[0m f1_macro: 0.41258088066943727
[2m[36m(func pid=36270)[0m f1_weighted: 0.44710210492048813
[2m[36m(func pid=36270)[0m f1_per_class: [0.576, 0.395, 0.815, 0.594, 0.123, 0.255, 0.478, 0.201, 0.207, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.14925373134328357
[2m[36m(func pid=53984)[0m top5: 0.7084888059701493
[2m[36m(func pid=53984)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=53984)[0m f1_macro: 0.14782945785070387
[2m[36m(func pid=53984)[0m f1_weighted: 0.15959327129740028
[2m[36m(func pid=53984)[0m f1_per_class: [0.21, 0.191, 0.308, 0.217, 0.038, 0.147, 0.125, 0.0, 0.137, 0.105]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7873 | Steps: 2 | Val loss: 2.4318 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7068 | Steps: 2 | Val loss: 2.4123 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.0185 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=53465)[0m top1: 0.06063432835820896
[2m[36m(func pid=53465)[0m top5: 0.4314365671641791
[2m[36m(func pid=53465)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=53465)[0m f1_macro: 0.05871087924582454
[2m[36m(func pid=53465)[0m f1_weighted: 0.06535925958830026
[2m[36m(func pid=53465)[0m f1_per_class: [0.058, 0.136, 0.028, 0.051, 0.009, 0.063, 0.038, 0.069, 0.098, 0.036]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.9094 | Steps: 2 | Val loss: 2.0882 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=48412)[0m top1: 0.08908582089552239
[2m[36m(func pid=48412)[0m top5: 0.4556902985074627
[2m[36m(func pid=48412)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=48412)[0m f1_macro: 0.07215858075126624
[2m[36m(func pid=48412)[0m f1_weighted: 0.09520285364517356
[2m[36m(func pid=48412)[0m f1_per_class: [0.052, 0.155, 0.088, 0.127, 0.0, 0.064, 0.057, 0.07, 0.078, 0.029]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:39 (running for 00:23:31.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.411 |                   82 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.707 |      0.072 |                   27 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.787 |      0.059 |                    5 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  2.388 |      0.148 |                    3 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4375
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.4375
[2m[36m(func pid=36270)[0m f1_macro: 0.4111646653426603
[2m[36m(func pid=36270)[0m f1_weighted: 0.4449069379245638
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.395, 0.815, 0.593, 0.112, 0.252, 0.471, 0.211, 0.208, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.25279850746268656
[2m[36m(func pid=53984)[0m top5: 0.7555970149253731
[2m[36m(func pid=53984)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=53984)[0m f1_macro: 0.19775360307693718
[2m[36m(func pid=53984)[0m f1_weighted: 0.2095441209938846
[2m[36m(func pid=53984)[0m f1_per_class: [0.314, 0.096, 0.245, 0.51, 0.073, 0.112, 0.033, 0.213, 0.131, 0.25]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7247 | Steps: 2 | Val loss: 2.4078 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6886 | Steps: 2 | Val loss: 2.4073 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=53465)[0m top1: 0.07602611940298508
[2m[36m(func pid=53465)[0m top5: 0.4612873134328358
[2m[36m(func pid=53465)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=53465)[0m f1_macro: 0.06680969854676592
[2m[36m(func pid=53465)[0m f1_weighted: 0.08196576841356037
[2m[36m(func pid=53465)[0m f1_per_class: [0.08, 0.131, 0.065, 0.037, 0.01, 0.075, 0.11, 0.03, 0.099, 0.03]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1029 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.4683 | Steps: 2 | Val loss: 2.0438 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=48412)[0m top1: 0.08908582089552239
[2m[36m(func pid=48412)[0m top5: 0.4594216417910448
[2m[36m(func pid=48412)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=48412)[0m f1_macro: 0.07080598798062404
[2m[36m(func pid=48412)[0m f1_weighted: 0.09377886719418708
[2m[36m(func pid=48412)[0m f1_per_class: [0.053, 0.164, 0.071, 0.12, 0.0, 0.061, 0.054, 0.077, 0.079, 0.029]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:45 (running for 00:23:37.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.411 |                   82 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.689 |      0.071 |                   28 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.725 |      0.067 |                    6 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  1.468 |      0.197 |                    5 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.20289179104477612
[2m[36m(func pid=53984)[0m top5: 0.8283582089552238
[2m[36m(func pid=53984)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=53984)[0m f1_macro: 0.19652009233875126
[2m[36m(func pid=53984)[0m f1_weighted: 0.18531534857950382
[2m[36m(func pid=53984)[0m f1_per_class: [0.302, 0.24, 0.353, 0.408, 0.152, 0.031, 0.006, 0.168, 0.098, 0.207]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=36270)[0m top1: 0.43330223880597013
[2m[36m(func pid=36270)[0m top5: 0.9211753731343284
[2m[36m(func pid=36270)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36270)[0m f1_macro: 0.4080852155439308
[2m[36m(func pid=36270)[0m f1_weighted: 0.4406204197232547
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.389, 0.815, 0.592, 0.12, 0.218, 0.475, 0.203, 0.206, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6555 | Steps: 2 | Val loss: 2.3730 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6933 | Steps: 2 | Val loss: 2.4043 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=53465)[0m top1: 0.10774253731343283
[2m[36m(func pid=53465)[0m top5: 0.5023320895522388
[2m[36m(func pid=53465)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=53465)[0m f1_macro: 0.08340594610000583
[2m[36m(func pid=53465)[0m f1_weighted: 0.11066133063260745
[2m[36m(func pid=53465)[0m f1_per_class: [0.094, 0.133, 0.101, 0.019, 0.012, 0.098, 0.211, 0.035, 0.102, 0.029]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9263 | Steps: 2 | Val loss: 1.8134 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0005 | Steps: 2 | Val loss: 7.1870 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=48412)[0m top1: 0.08628731343283583
[2m[36m(func pid=48412)[0m top5: 0.46361940298507465
[2m[36m(func pid=48412)[0m f1_micro: 0.08628731343283583
[2m[36m(func pid=48412)[0m f1_macro: 0.07024972171600441
[2m[36m(func pid=48412)[0m f1_weighted: 0.09113293753527613
[2m[36m(func pid=48412)[0m f1_per_class: [0.052, 0.157, 0.086, 0.114, 0.0, 0.061, 0.057, 0.068, 0.078, 0.028]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:50 (running for 00:23:42.73)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.409 |                   84 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.693 |      0.07  |                   29 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.655 |      0.083 |                    7 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  1.468 |      0.197 |                    5 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43330223880597013
[2m[36m(func pid=36270)[0m top5: 0.9216417910447762
[2m[36m(func pid=36270)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=36270)[0m f1_macro: 0.4085812470982271
[2m[36m(func pid=36270)[0m f1_weighted: 0.4403244555875702
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.38, 0.815, 0.592, 0.119, 0.218, 0.477, 0.219, 0.204, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.31716417910447764
[2m[36m(func pid=53984)[0m top5: 0.9127798507462687
[2m[36m(func pid=53984)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=53984)[0m f1_macro: 0.3221677658834894
[2m[36m(func pid=53984)[0m f1_weighted: 0.33456862421955713
[2m[36m(func pid=53984)[0m f1_per_class: [0.473, 0.401, 0.611, 0.499, 0.123, 0.207, 0.231, 0.169, 0.174, 0.333]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5007 | Steps: 2 | Val loss: 2.3280 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6243 | Steps: 2 | Val loss: 2.3996 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1901 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=53465)[0m top1: 0.14878731343283583
[2m[36m(func pid=53465)[0m top5: 0.5457089552238806
[2m[36m(func pid=53465)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=53465)[0m f1_macro: 0.10918016832841326
[2m[36m(func pid=53465)[0m f1_weighted: 0.1369232370095859
[2m[36m(func pid=53465)[0m f1_per_class: [0.124, 0.165, 0.19, 0.007, 0.008, 0.101, 0.287, 0.013, 0.125, 0.071]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.6268 | Steps: 2 | Val loss: 1.7850 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=48412)[0m top1: 0.08815298507462686
[2m[36m(func pid=48412)[0m top5: 0.4710820895522388
[2m[36m(func pid=48412)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=48412)[0m f1_macro: 0.07174789339147396
[2m[36m(func pid=48412)[0m f1_weighted: 0.09290812602805208
[2m[36m(func pid=48412)[0m f1_per_class: [0.053, 0.164, 0.085, 0.114, 0.0, 0.061, 0.057, 0.075, 0.081, 0.028]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:06:55 (running for 00:23:48.08)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.409 |                   85 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.624 |      0.072 |                   30 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.501 |      0.109 |                    8 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.926 |      0.322 |                    6 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4351679104477612
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=36270)[0m f1_macro: 0.40881473864686113
[2m[36m(func pid=36270)[0m f1_weighted: 0.4422469805738766
[2m[36m(func pid=36270)[0m f1_per_class: [0.576, 0.384, 0.815, 0.593, 0.124, 0.238, 0.475, 0.211, 0.199, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3292910447761194
[2m[36m(func pid=53984)[0m top5: 0.9071828358208955
[2m[36m(func pid=53984)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=53984)[0m f1_macro: 0.32756470170575236
[2m[36m(func pid=53984)[0m f1_weighted: 0.3530533282730842
[2m[36m(func pid=53984)[0m f1_per_class: [0.492, 0.396, 0.71, 0.465, 0.087, 0.243, 0.336, 0.045, 0.174, 0.328]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4753 | Steps: 2 | Val loss: 2.2790 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6159 | Steps: 2 | Val loss: 2.3964 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1496 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=53465)[0m top1: 0.17350746268656717
[2m[36m(func pid=53465)[0m top5: 0.6026119402985075
[2m[36m(func pid=53465)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=53465)[0m f1_macro: 0.1233344816741578
[2m[36m(func pid=53465)[0m f1_weighted: 0.15436765324069407
[2m[36m(func pid=53465)[0m f1_per_class: [0.141, 0.207, 0.225, 0.007, 0.011, 0.099, 0.318, 0.025, 0.123, 0.078]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5770 | Steps: 2 | Val loss: 1.8693 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=48412)[0m top1: 0.08675373134328358
[2m[36m(func pid=48412)[0m top5: 0.47294776119402987
[2m[36m(func pid=48412)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=48412)[0m f1_macro: 0.07054386945449458
[2m[36m(func pid=48412)[0m f1_weighted: 0.09276843370908686
[2m[36m(func pid=48412)[0m f1_per_class: [0.052, 0.157, 0.085, 0.109, 0.0, 0.065, 0.065, 0.071, 0.073, 0.028]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:07:01 (running for 00:23:53.43)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   86 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.616 |      0.071 |                   31 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.475 |      0.123 |                    9 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.627 |      0.328 |                    7 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4375
[2m[36m(func pid=36270)[0m top5: 0.9225746268656716
[2m[36m(func pid=36270)[0m f1_micro: 0.4375
[2m[36m(func pid=36270)[0m f1_macro: 0.41209738646075394
[2m[36m(func pid=36270)[0m f1_weighted: 0.4455257864218389
[2m[36m(func pid=36270)[0m f1_per_class: [0.591, 0.393, 0.815, 0.592, 0.128, 0.236, 0.481, 0.208, 0.203, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3045708955223881
[2m[36m(func pid=53984)[0m top5: 0.8861940298507462
[2m[36m(func pid=53984)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=53984)[0m f1_macro: 0.31472495325930333
[2m[36m(func pid=53984)[0m f1_weighted: 0.3220502563937335
[2m[36m(func pid=53984)[0m f1_per_class: [0.433, 0.302, 0.759, 0.517, 0.073, 0.229, 0.233, 0.107, 0.2, 0.295]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3074 | Steps: 2 | Val loss: 2.2430 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6520 | Steps: 2 | Val loss: 2.3924 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1419 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=53465)[0m top1: 0.18470149253731344
[2m[36m(func pid=53465)[0m top5: 0.6319962686567164
[2m[36m(func pid=53465)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=53465)[0m f1_macro: 0.1398401942208474
[2m[36m(func pid=53465)[0m f1_weighted: 0.17218910546168595
[2m[36m(func pid=53465)[0m f1_per_class: [0.18, 0.224, 0.241, 0.038, 0.01, 0.108, 0.328, 0.045, 0.121, 0.103]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3982 | Steps: 2 | Val loss: 1.9379 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=48412)[0m top1: 0.08908582089552239
[2m[36m(func pid=48412)[0m top5: 0.4724813432835821
[2m[36m(func pid=48412)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=48412)[0m f1_macro: 0.07267133534696499
[2m[36m(func pid=48412)[0m f1_weighted: 0.09402063007799327
[2m[36m(func pid=48412)[0m f1_per_class: [0.057, 0.163, 0.08, 0.107, 0.0, 0.065, 0.067, 0.073, 0.072, 0.042]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:07:06 (running for 00:23:58.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   87 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.652 |      0.073 |                   32 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.307 |      0.14  |                   10 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.577 |      0.315 |                    8 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.44029850746268656
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=36270)[0m f1_macro: 0.4124996706578328
[2m[36m(func pid=36270)[0m f1_weighted: 0.44767206021666517
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.398, 0.815, 0.594, 0.128, 0.231, 0.485, 0.212, 0.208, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53984)[0m top1: 0.302705223880597
[2m[36m(func pid=53984)[0m top5: 0.855410447761194
[2m[36m(func pid=53984)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=53984)[0m f1_macro: 0.32482670142982045
[2m[36m(func pid=53984)[0m f1_weighted: 0.309926249682671
[2m[36m(func pid=53984)[0m f1_per_class: [0.444, 0.274, 0.815, 0.551, 0.059, 0.203, 0.153, 0.277, 0.202, 0.271]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2003 | Steps: 2 | Val loss: 2.2125 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6127 | Steps: 2 | Val loss: 2.3897 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.0966 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=53465)[0m top1: 0.18236940298507462
[2m[36m(func pid=53465)[0m top5: 0.6553171641791045
[2m[36m(func pid=53465)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=53465)[0m f1_macro: 0.14898460963982565
[2m[36m(func pid=53465)[0m f1_weighted: 0.19000439122408821
[2m[36m(func pid=53465)[0m f1_per_class: [0.165, 0.225, 0.263, 0.121, 0.01, 0.113, 0.305, 0.054, 0.131, 0.102]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.0914179104477612
[2m[36m(func pid=48412)[0m top5: 0.4748134328358209
[2m[36m(func pid=48412)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=48412)[0m f1_macro: 0.07551084810425787
[2m[36m(func pid=48412)[0m f1_weighted: 0.0970232624386179
[2m[36m(func pid=48412)[0m f1_per_class: [0.074, 0.164, 0.08, 0.109, 0.0, 0.068, 0.072, 0.074, 0.073, 0.042]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2989 | Steps: 2 | Val loss: 1.9239 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 01:07:11 (running for 00:24:04.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   88 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.613 |      0.076 |                   33 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.2   |      0.149 |                   11 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.398 |      0.325 |                    9 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4430970149253731
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.4430970149253731
[2m[36m(func pid=36270)[0m f1_macro: 0.4137550240712666
[2m[36m(func pid=36270)[0m f1_weighted: 0.45030265167055417
[2m[36m(func pid=36270)[0m f1_per_class: [0.586, 0.39, 0.815, 0.593, 0.131, 0.23, 0.499, 0.215, 0.205, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1330 | Steps: 2 | Val loss: 2.1931 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=53984)[0m top1: 0.31343283582089554
[2m[36m(func pid=53984)[0m top5: 0.8558768656716418
[2m[36m(func pid=53984)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=53984)[0m f1_macro: 0.32965967204479607
[2m[36m(func pid=53984)[0m f1_weighted: 0.30390033969576485
[2m[36m(func pid=53984)[0m f1_per_class: [0.474, 0.332, 0.759, 0.556, 0.064, 0.16, 0.101, 0.3, 0.221, 0.33]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6126 | Steps: 2 | Val loss: 2.3870 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=53465)[0m top1: 0.18236940298507462
[2m[36m(func pid=53465)[0m top5: 0.6711753731343284
[2m[36m(func pid=53465)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=53465)[0m f1_macro: 0.16707231845013315
[2m[36m(func pid=53465)[0m f1_weighted: 0.20285273375707952
[2m[36m(func pid=53465)[0m f1_per_class: [0.164, 0.228, 0.349, 0.237, 0.015, 0.1, 0.229, 0.124, 0.116, 0.107]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.09375
[2m[36m(func pid=48412)[0m top5: 0.47994402985074625
[2m[36m(func pid=48412)[0m f1_micro: 0.09375
[2m[36m(func pid=48412)[0m f1_macro: 0.07881998496010152
[2m[36m(func pid=48412)[0m f1_weighted: 0.09940502959316935
[2m[36m(func pid=48412)[0m f1_per_class: [0.074, 0.163, 0.094, 0.116, 0.0, 0.068, 0.072, 0.08, 0.074, 0.048]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1250 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.2097 | Steps: 2 | Val loss: 1.8203 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 01:07:17 (running for 00:24:09.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.416 |                   89 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.613 |      0.079 |                   34 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  2.133 |      0.167 |                   12 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.299 |      0.33  |                   10 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.447294776119403
[2m[36m(func pid=36270)[0m top5: 0.9253731343283582
[2m[36m(func pid=36270)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=36270)[0m f1_macro: 0.4155733580582816
[2m[36m(func pid=36270)[0m f1_weighted: 0.453315156212061
[2m[36m(func pid=36270)[0m f1_per_class: [0.602, 0.395, 0.815, 0.597, 0.119, 0.242, 0.499, 0.206, 0.209, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9933 | Steps: 2 | Val loss: 2.1926 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5885 | Steps: 2 | Val loss: 2.3821 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=53984)[0m top1: 0.3353544776119403
[2m[36m(func pid=53984)[0m top5: 0.8880597014925373
[2m[36m(func pid=53984)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=53984)[0m f1_macro: 0.34865155461080954
[2m[36m(func pid=53984)[0m f1_weighted: 0.33878694810642307
[2m[36m(func pid=53984)[0m f1_per_class: [0.444, 0.364, 0.815, 0.561, 0.074, 0.174, 0.195, 0.277, 0.207, 0.377]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.18330223880597016
[2m[36m(func pid=53465)[0m top5: 0.6777052238805971
[2m[36m(func pid=53465)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=53465)[0m f1_macro: 0.1736786802245021
[2m[36m(func pid=53465)[0m f1_weighted: 0.1933124012844747
[2m[36m(func pid=53465)[0m f1_per_class: [0.19, 0.237, 0.393, 0.29, 0.016, 0.092, 0.137, 0.142, 0.129, 0.11]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.09561567164179105
[2m[36m(func pid=48412)[0m top5: 0.4832089552238806
[2m[36m(func pid=48412)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=48412)[0m f1_macro: 0.07962567369039637
[2m[36m(func pid=48412)[0m f1_weighted: 0.10234055468406453
[2m[36m(func pid=48412)[0m f1_per_class: [0.075, 0.161, 0.088, 0.121, 0.0, 0.068, 0.078, 0.084, 0.074, 0.048]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.1651 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1868 | Steps: 2 | Val loss: 1.6988 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:07:22 (running for 00:24:14.82)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.415 |                   90 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.589 |      0.08  |                   35 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.993 |      0.174 |                   13 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.21  |      0.349 |                   11 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4435634328358209
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=36270)[0m f1_macro: 0.4153840370215164
[2m[36m(func pid=36270)[0m f1_weighted: 0.45088339231113794
[2m[36m(func pid=36270)[0m f1_per_class: [0.602, 0.397, 0.815, 0.594, 0.128, 0.222, 0.498, 0.213, 0.212, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9404 | Steps: 2 | Val loss: 2.1912 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5770 | Steps: 2 | Val loss: 2.3793 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=53984)[0m top1: 0.37406716417910446
[2m[36m(func pid=53984)[0m top5: 0.9127798507462687
[2m[36m(func pid=53984)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=53984)[0m f1_macro: 0.367855329332186
[2m[36m(func pid=53984)[0m f1_weighted: 0.3929126136946307
[2m[36m(func pid=53984)[0m f1_per_class: [0.444, 0.408, 0.759, 0.562, 0.079, 0.218, 0.336, 0.272, 0.186, 0.415]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.18983208955223882
[2m[36m(func pid=53465)[0m top5: 0.679570895522388
[2m[36m(func pid=53465)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=53465)[0m f1_macro: 0.18639201248419257
[2m[36m(func pid=53465)[0m f1_weighted: 0.1876545943878477
[2m[36m(func pid=53465)[0m f1_per_class: [0.206, 0.241, 0.458, 0.313, 0.028, 0.087, 0.082, 0.195, 0.137, 0.116]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.3124 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=48412)[0m top1: 0.09421641791044776
[2m[36m(func pid=48412)[0m top5: 0.48507462686567165
[2m[36m(func pid=48412)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=48412)[0m f1_macro: 0.07934039597810141
[2m[36m(func pid=48412)[0m f1_weighted: 0.09973857085958501
[2m[36m(func pid=48412)[0m f1_per_class: [0.075, 0.16, 0.088, 0.114, 0.0, 0.069, 0.075, 0.084, 0.082, 0.047]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1487 | Steps: 2 | Val loss: 1.6150 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:07:27 (running for 00:24:20.13)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.414 |                   91 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.577 |      0.079 |                   36 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.94  |      0.186 |                   14 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.187 |      0.368 |                   12 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43656716417910446
[2m[36m(func pid=36270)[0m top5: 0.9249067164179104
[2m[36m(func pid=36270)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=36270)[0m f1_macro: 0.413972083260189
[2m[36m(func pid=36270)[0m f1_weighted: 0.44201643980359534
[2m[36m(func pid=36270)[0m f1_per_class: [0.606, 0.381, 0.815, 0.593, 0.132, 0.241, 0.471, 0.213, 0.207, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7437 | Steps: 2 | Val loss: 2.1875 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5152 | Steps: 2 | Val loss: 2.3753 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=53984)[0m top1: 0.4039179104477612
[2m[36m(func pid=53984)[0m top5: 0.9253731343283582
[2m[36m(func pid=53984)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=53984)[0m f1_macro: 0.3796156200510777
[2m[36m(func pid=53984)[0m f1_weighted: 0.424588305861812
[2m[36m(func pid=53984)[0m f1_per_class: [0.47, 0.442, 0.733, 0.55, 0.089, 0.252, 0.425, 0.242, 0.184, 0.409]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.19682835820895522
[2m[36m(func pid=53465)[0m top5: 0.6753731343283582
[2m[36m(func pid=53465)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=53465)[0m f1_macro: 0.19815797260054419
[2m[36m(func pid=53465)[0m f1_weighted: 0.18536438237070096
[2m[36m(func pid=53465)[0m f1_per_class: [0.222, 0.251, 0.55, 0.334, 0.031, 0.083, 0.044, 0.21, 0.146, 0.11]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.09748134328358209
[2m[36m(func pid=48412)[0m top5: 0.488339552238806
[2m[36m(func pid=48412)[0m f1_micro: 0.09748134328358209
[2m[36m(func pid=48412)[0m f1_macro: 0.07912628298821286
[2m[36m(func pid=48412)[0m f1_weighted: 0.10795834128155811
[2m[36m(func pid=48412)[0m f1_per_class: [0.058, 0.17, 0.079, 0.13, 0.0, 0.068, 0.085, 0.078, 0.078, 0.044]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.3319 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1345 | Steps: 2 | Val loss: 1.6134 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.8229 | Steps: 2 | Val loss: 2.1830 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:07:33 (running for 00:24:25.39)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.412 |                   92 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.515 |      0.079 |                   37 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.744 |      0.198 |                   15 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.149 |      0.38  |                   13 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43796641791044777
[2m[36m(func pid=36270)[0m top5: 0.9249067164179104
[2m[36m(func pid=36270)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=36270)[0m f1_macro: 0.4122867083731725
[2m[36m(func pid=36270)[0m f1_weighted: 0.44350725549523706
[2m[36m(func pid=36270)[0m f1_per_class: [0.6, 0.398, 0.815, 0.589, 0.115, 0.237, 0.472, 0.214, 0.21, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5708 | Steps: 2 | Val loss: 2.3681 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=53984)[0m top1: 0.4123134328358209
[2m[36m(func pid=53984)[0m top5: 0.9263059701492538
[2m[36m(func pid=53984)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=53984)[0m f1_macro: 0.3855347739394158
[2m[36m(func pid=53984)[0m f1_weighted: 0.43452032542399627
[2m[36m(func pid=53984)[0m f1_per_class: [0.47, 0.446, 0.733, 0.55, 0.097, 0.265, 0.449, 0.246, 0.19, 0.409]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.19682835820895522
[2m[36m(func pid=53465)[0m top5: 0.6781716417910447
[2m[36m(func pid=53465)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=53465)[0m f1_macro: 0.20171092141186472
[2m[36m(func pid=53465)[0m f1_weighted: 0.1890474028692109
[2m[36m(func pid=53465)[0m f1_per_class: [0.234, 0.242, 0.579, 0.344, 0.031, 0.091, 0.048, 0.214, 0.15, 0.085]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.3009 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=48412)[0m top1: 0.09981343283582089
[2m[36m(func pid=48412)[0m top5: 0.49720149253731344
[2m[36m(func pid=48412)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=48412)[0m f1_macro: 0.08364452527333673
[2m[36m(func pid=48412)[0m f1_weighted: 0.10862926772583299
[2m[36m(func pid=48412)[0m f1_per_class: [0.073, 0.174, 0.098, 0.121, 0.0, 0.072, 0.089, 0.087, 0.079, 0.044]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1100 | Steps: 2 | Val loss: 1.6463 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6581 | Steps: 2 | Val loss: 2.1807 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:07:38 (running for 00:24:30.55)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.415 |                   93 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.571 |      0.084 |                   38 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.823 |      0.202 |                   16 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.135 |      0.386 |                   14 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.44076492537313433
[2m[36m(func pid=36270)[0m top5: 0.9253731343283582
[2m[36m(func pid=36270)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=36270)[0m f1_macro: 0.41502514225910314
[2m[36m(func pid=36270)[0m f1_weighted: 0.44648928824495
[2m[36m(func pid=36270)[0m f1_per_class: [0.606, 0.387, 0.815, 0.591, 0.137, 0.231, 0.488, 0.209, 0.213, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5155 | Steps: 2 | Val loss: 2.3643 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=53984)[0m top1: 0.40205223880597013
[2m[36m(func pid=53984)[0m top5: 0.9197761194029851
[2m[36m(func pid=53984)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=53984)[0m f1_macro: 0.3830161862916099
[2m[36m(func pid=53984)[0m f1_weighted: 0.4233900066806206
[2m[36m(func pid=53984)[0m f1_per_class: [0.452, 0.451, 0.733, 0.546, 0.085, 0.251, 0.421, 0.227, 0.193, 0.471]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.19682835820895522
[2m[36m(func pid=53465)[0m top5: 0.6781716417910447
[2m[36m(func pid=53465)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=53465)[0m f1_macro: 0.20116428648748008
[2m[36m(func pid=53465)[0m f1_weighted: 0.18978292306108885
[2m[36m(func pid=53465)[0m f1_per_class: [0.246, 0.251, 0.55, 0.336, 0.024, 0.109, 0.045, 0.209, 0.164, 0.078]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.09934701492537314
[2m[36m(func pid=48412)[0m top5: 0.49673507462686567
[2m[36m(func pid=48412)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=48412)[0m f1_macro: 0.08389348007891809
[2m[36m(func pid=48412)[0m f1_weighted: 0.1082956180312861
[2m[36m(func pid=48412)[0m f1_per_class: [0.079, 0.17, 0.099, 0.123, 0.0, 0.072, 0.088, 0.084, 0.08, 0.044]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.3923 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.1010 | Steps: 2 | Val loss: 1.7527 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.5812 | Steps: 2 | Val loss: 2.1664 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:07:43 (running for 00:24:35.85)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.415 |                   94 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.516 |      0.084 |                   39 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.658 |      0.201 |                   17 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.11  |      0.383 |                   15 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43843283582089554
[2m[36m(func pid=36270)[0m top5: 0.9253731343283582
[2m[36m(func pid=36270)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=36270)[0m f1_macro: 0.4152097020076121
[2m[36m(func pid=36270)[0m f1_weighted: 0.4439446050761135
[2m[36m(func pid=36270)[0m f1_per_class: [0.611, 0.389, 0.815, 0.589, 0.136, 0.231, 0.48, 0.211, 0.209, 0.481]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5084 | Steps: 2 | Val loss: 2.3592 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=53984)[0m top1: 0.37406716417910446
[2m[36m(func pid=53984)[0m top5: 0.9043843283582089
[2m[36m(func pid=53984)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=53984)[0m f1_macro: 0.36881830104920255
[2m[36m(func pid=53984)[0m f1_weighted: 0.39133259192443337
[2m[36m(func pid=53984)[0m f1_per_class: [0.454, 0.433, 0.759, 0.531, 0.084, 0.234, 0.339, 0.262, 0.21, 0.384]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.19869402985074627
[2m[36m(func pid=53465)[0m top5: 0.6819029850746269
[2m[36m(func pid=53465)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=53465)[0m f1_macro: 0.19849940478162362
[2m[36m(func pid=53465)[0m f1_weighted: 0.1918426978194387
[2m[36m(func pid=53465)[0m f1_per_class: [0.239, 0.261, 0.5, 0.327, 0.025, 0.107, 0.053, 0.223, 0.169, 0.081]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.10307835820895522
[2m[36m(func pid=48412)[0m top5: 0.5046641791044776
[2m[36m(func pid=48412)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=48412)[0m f1_macro: 0.08739311280151078
[2m[36m(func pid=48412)[0m f1_weighted: 0.11236504974800966
[2m[36m(func pid=48412)[0m f1_per_class: [0.085, 0.172, 0.106, 0.131, 0.0, 0.074, 0.09, 0.088, 0.082, 0.045]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.4812 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0517 | Steps: 2 | Val loss: 1.8766 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:07:48 (running for 00:24:41.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.41  |                   95 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.508 |      0.087 |                   40 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.581 |      0.198 |                   18 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.101 |      0.369 |                   16 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43703358208955223
[2m[36m(func pid=36270)[0m top5: 0.9230410447761194
[2m[36m(func pid=36270)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=36270)[0m f1_macro: 0.4095859303360333
[2m[36m(func pid=36270)[0m f1_weighted: 0.4430842595825051
[2m[36m(func pid=36270)[0m f1_per_class: [0.607, 0.402, 0.815, 0.589, 0.11, 0.221, 0.477, 0.205, 0.206, 0.464]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5294 | Steps: 2 | Val loss: 2.1481 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5196 | Steps: 2 | Val loss: 2.3538 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=53984)[0m top1: 0.341884328358209
[2m[36m(func pid=53984)[0m top5: 0.8796641791044776
[2m[36m(func pid=53984)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=53984)[0m f1_macro: 0.3429324681576073
[2m[36m(func pid=53984)[0m f1_weighted: 0.35114363354847494
[2m[36m(func pid=53984)[0m f1_per_class: [0.438, 0.419, 0.71, 0.509, 0.085, 0.211, 0.243, 0.261, 0.231, 0.321]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.20662313432835822
[2m[36m(func pid=53465)[0m top5: 0.7052238805970149
[2m[36m(func pid=53465)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=53465)[0m f1_macro: 0.20384667669039405
[2m[36m(func pid=53465)[0m f1_weighted: 0.20074116359931468
[2m[36m(func pid=53465)[0m f1_per_class: [0.265, 0.282, 0.449, 0.319, 0.032, 0.111, 0.073, 0.229, 0.182, 0.096]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.5295 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=48412)[0m top1: 0.10634328358208955
[2m[36m(func pid=48412)[0m top5: 0.507929104477612
[2m[36m(func pid=48412)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=48412)[0m f1_macro: 0.09151076420750118
[2m[36m(func pid=48412)[0m f1_weighted: 0.11543409545665723
[2m[36m(func pid=48412)[0m f1_per_class: [0.091, 0.177, 0.125, 0.132, 0.0, 0.078, 0.096, 0.086, 0.08, 0.051]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0627 | Steps: 2 | Val loss: 1.9465 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4211 | Steps: 2 | Val loss: 2.1256 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:07:54 (running for 00:24:46.54)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                   96 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.52  |      0.092 |                   41 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.529 |      0.204 |                   19 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.052 |      0.343 |                   17 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4337686567164179
[2m[36m(func pid=36270)[0m top5: 0.9235074626865671
[2m[36m(func pid=36270)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=36270)[0m f1_macro: 0.40665212387698685
[2m[36m(func pid=36270)[0m f1_weighted: 0.43886960738716485
[2m[36m(func pid=36270)[0m f1_per_class: [0.602, 0.406, 0.815, 0.587, 0.111, 0.215, 0.465, 0.206, 0.203, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4971 | Steps: 2 | Val loss: 2.3533 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=53984)[0m top1: 0.3353544776119403
[2m[36m(func pid=53984)[0m top5: 0.8689365671641791
[2m[36m(func pid=53984)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=53984)[0m f1_macro: 0.3337267761833701
[2m[36m(func pid=53984)[0m f1_weighted: 0.3435601543530743
[2m[36m(func pid=53984)[0m f1_per_class: [0.442, 0.427, 0.667, 0.509, 0.1, 0.212, 0.217, 0.248, 0.237, 0.279]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.21128731343283583
[2m[36m(func pid=53465)[0m top5: 0.7257462686567164
[2m[36m(func pid=53465)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=53465)[0m f1_macro: 0.20920315859740746
[2m[36m(func pid=53465)[0m f1_weighted: 0.21121760345541832
[2m[36m(func pid=53465)[0m f1_per_class: [0.266, 0.298, 0.44, 0.299, 0.037, 0.125, 0.112, 0.228, 0.179, 0.107]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.10960820895522388
[2m[36m(func pid=48412)[0m top5: 0.5069962686567164
[2m[36m(func pid=48412)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=48412)[0m f1_macro: 0.0954491003677015
[2m[36m(func pid=48412)[0m f1_weighted: 0.1198386505501414
[2m[36m(func pid=48412)[0m f1_per_class: [0.104, 0.176, 0.132, 0.145, 0.0, 0.074, 0.098, 0.085, 0.091, 0.05]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.5163 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0518 | Steps: 2 | Val loss: 1.9959 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3335 | Steps: 2 | Val loss: 2.0993 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:07:59 (running for 00:24:51.83)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                   97 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.497 |      0.095 |                   42 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.421 |      0.209 |                   20 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.063 |      0.334 |                   18 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4361007462686567
[2m[36m(func pid=36270)[0m top5: 0.9239738805970149
[2m[36m(func pid=36270)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=36270)[0m f1_macro: 0.4065653390424829
[2m[36m(func pid=36270)[0m f1_weighted: 0.44287051192678123
[2m[36m(func pid=36270)[0m f1_per_class: [0.596, 0.4, 0.815, 0.592, 0.107, 0.205, 0.481, 0.206, 0.207, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4833 | Steps: 2 | Val loss: 2.3497 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=53984)[0m top1: 0.3306902985074627
[2m[36m(func pid=53984)[0m top5: 0.8582089552238806
[2m[36m(func pid=53984)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=53984)[0m f1_macro: 0.32540323332525645
[2m[36m(func pid=53984)[0m f1_weighted: 0.33808247628663524
[2m[36m(func pid=53984)[0m f1_per_class: [0.446, 0.408, 0.667, 0.518, 0.097, 0.212, 0.202, 0.253, 0.236, 0.216]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.2178171641791045
[2m[36m(func pid=53465)[0m top5: 0.7490671641791045
[2m[36m(func pid=53465)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=53465)[0m f1_macro: 0.2181557763002287
[2m[36m(func pid=53465)[0m f1_weighted: 0.22333008895444723
[2m[36m(func pid=53465)[0m f1_per_class: [0.307, 0.311, 0.431, 0.294, 0.041, 0.129, 0.148, 0.215, 0.175, 0.13]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.6075 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=48412)[0m top1: 0.11007462686567164
[2m[36m(func pid=48412)[0m top5: 0.5111940298507462
[2m[36m(func pid=48412)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=48412)[0m f1_macro: 0.09565168379639735
[2m[36m(func pid=48412)[0m f1_weighted: 0.12009454300306498
[2m[36m(func pid=48412)[0m f1_per_class: [0.109, 0.174, 0.132, 0.148, 0.0, 0.073, 0.098, 0.08, 0.093, 0.049]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0581 | Steps: 2 | Val loss: 2.0033 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3385 | Steps: 2 | Val loss: 2.0695 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:08:05 (running for 00:24:57.31)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.402 |                   98 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.483 |      0.096 |                   43 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.333 |      0.218 |                   21 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.052 |      0.325 |                   19 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43050373134328357
[2m[36m(func pid=36270)[0m top5: 0.9239738805970149
[2m[36m(func pid=36270)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=36270)[0m f1_macro: 0.40238341653973897
[2m[36m(func pid=36270)[0m f1_weighted: 0.43559034740922087
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.388, 0.815, 0.586, 0.109, 0.206, 0.469, 0.217, 0.197, 0.456]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4929 | Steps: 2 | Val loss: 2.3482 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=53984)[0m top1: 0.3344216417910448
[2m[36m(func pid=53984)[0m top5: 0.8600746268656716
[2m[36m(func pid=53984)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=53984)[0m f1_macro: 0.32169216790669297
[2m[36m(func pid=53984)[0m f1_weighted: 0.34825930274452754
[2m[36m(func pid=53984)[0m f1_per_class: [0.446, 0.395, 0.629, 0.527, 0.08, 0.206, 0.24, 0.256, 0.231, 0.208]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.21921641791044777
[2m[36m(func pid=53465)[0m top5: 0.7756529850746269
[2m[36m(func pid=53465)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=53465)[0m f1_macro: 0.22050134526623827
[2m[36m(func pid=53465)[0m f1_weighted: 0.2356136706496749
[2m[36m(func pid=53465)[0m f1_per_class: [0.294, 0.298, 0.44, 0.275, 0.037, 0.135, 0.217, 0.192, 0.174, 0.143]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.11147388059701492
[2m[36m(func pid=48412)[0m top5: 0.511660447761194
[2m[36m(func pid=48412)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=48412)[0m f1_macro: 0.09780638787688968
[2m[36m(func pid=48412)[0m f1_weighted: 0.12121350937932732
[2m[36m(func pid=48412)[0m f1_per_class: [0.12, 0.175, 0.137, 0.139, 0.0, 0.074, 0.108, 0.085, 0.086, 0.054]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.5583 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0512 | Steps: 2 | Val loss: 1.9872 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2177 | Steps: 2 | Val loss: 2.0307 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:08:10 (running for 00:25:02.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00010 | RUNNING    | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.409 |                   99 |
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.493 |      0.098 |                   44 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.339 |      0.221 |                   22 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.058 |      0.322 |                   20 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.43843283582089554
[2m[36m(func pid=36270)[0m top5: 0.9239738805970149
[2m[36m(func pid=36270)[0m f1_micro: 0.43843283582089554
[2m[36m(func pid=36270)[0m f1_macro: 0.4094552448206169
[2m[36m(func pid=36270)[0m f1_weighted: 0.443987122591341
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.397, 0.815, 0.59, 0.117, 0.22, 0.481, 0.219, 0.203, 0.473]
[2m[36m(func pid=36270)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4588 | Steps: 2 | Val loss: 2.3419 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=53984)[0m top1: 0.341884328358209
[2m[36m(func pid=53984)[0m top5: 0.8600746268656716
[2m[36m(func pid=53984)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=53984)[0m f1_macro: 0.32535618303736646
[2m[36m(func pid=53984)[0m f1_weighted: 0.36355294429889395
[2m[36m(func pid=53984)[0m f1_per_class: [0.44, 0.377, 0.647, 0.534, 0.073, 0.19, 0.299, 0.265, 0.237, 0.191]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.24113805970149255
[2m[36m(func pid=53465)[0m top5: 0.7985074626865671
[2m[36m(func pid=53465)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=53465)[0m f1_macro: 0.23771074303953718
[2m[36m(func pid=53465)[0m f1_weighted: 0.2632487616880483
[2m[36m(func pid=53465)[0m f1_per_class: [0.309, 0.321, 0.468, 0.296, 0.044, 0.134, 0.276, 0.181, 0.18, 0.168]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=36270)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.6014 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=48412)[0m top1: 0.11054104477611941
[2m[36m(func pid=48412)[0m top5: 0.519589552238806
[2m[36m(func pid=48412)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=48412)[0m f1_macro: 0.09643570483542072
[2m[36m(func pid=48412)[0m f1_weighted: 0.11981529764372151
[2m[36m(func pid=48412)[0m f1_per_class: [0.119, 0.175, 0.14, 0.132, 0.0, 0.071, 0.112, 0.084, 0.076, 0.055]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0589 | Steps: 2 | Val loss: 1.9995 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.1312 | Steps: 2 | Val loss: 2.0000 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:08:15 (running for 00:25:07.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 3 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.459 |      0.096 |                   45 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.218 |      0.238 |                   23 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.051 |      0.325 |                   21 |
| train_57e67_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 TERMINATED)


[2m[36m(func pid=36270)[0m top1: 0.4351679104477612
[2m[36m(func pid=36270)[0m top5: 0.9249067164179104
[2m[36m(func pid=36270)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=36270)[0m f1_macro: 0.4072223921651079
[2m[36m(func pid=36270)[0m f1_weighted: 0.43981211254016683
[2m[36m(func pid=36270)[0m f1_per_class: [0.581, 0.396, 0.815, 0.587, 0.121, 0.225, 0.47, 0.209, 0.203, 0.464]
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.5085 | Steps: 2 | Val loss: 2.3377 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=53984)[0m top1: 0.33908582089552236
[2m[36m(func pid=53984)[0m top5: 0.8568097014925373
[2m[36m(func pid=53984)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=53984)[0m f1_macro: 0.3179054464402099
[2m[36m(func pid=53984)[0m f1_weighted: 0.36454065615077974
[2m[36m(func pid=53984)[0m f1_per_class: [0.402, 0.358, 0.629, 0.53, 0.075, 0.181, 0.327, 0.255, 0.215, 0.207]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.2537313432835821
[2m[36m(func pid=53465)[0m top5: 0.8171641791044776
[2m[36m(func pid=53465)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=53465)[0m f1_macro: 0.2462758975350007
[2m[36m(func pid=53465)[0m f1_weighted: 0.27857260803669176
[2m[36m(func pid=53465)[0m f1_per_class: [0.337, 0.308, 0.468, 0.327, 0.047, 0.148, 0.298, 0.179, 0.181, 0.168]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1166044776119403
[2m[36m(func pid=48412)[0m top5: 0.5237873134328358
[2m[36m(func pid=48412)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=48412)[0m f1_macro: 0.10080343398973923
[2m[36m(func pid=48412)[0m f1_weighted: 0.12580143892418266
[2m[36m(func pid=48412)[0m f1_per_class: [0.119, 0.186, 0.136, 0.142, 0.0, 0.082, 0.11, 0.086, 0.089, 0.058]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0386 | Steps: 2 | Val loss: 1.9803 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1358 | Steps: 2 | Val loss: 1.9651 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4579 | Steps: 2 | Val loss: 2.3315 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=53984)[0m top1: 0.35074626865671643
[2m[36m(func pid=53984)[0m top5: 0.8600746268656716
[2m[36m(func pid=53984)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=53984)[0m f1_macro: 0.32494603403474887
[2m[36m(func pid=53984)[0m f1_weighted: 0.3795650120352507
[2m[36m(func pid=53984)[0m f1_per_class: [0.398, 0.36, 0.629, 0.527, 0.073, 0.186, 0.377, 0.25, 0.22, 0.23]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.271455223880597
[2m[36m(func pid=53465)[0m top5: 0.8386194029850746
[2m[36m(func pid=53465)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=53465)[0m f1_macro: 0.25313439787648745
[2m[36m(func pid=53465)[0m f1_weighted: 0.29796325169435706
[2m[36m(func pid=53465)[0m f1_per_class: [0.346, 0.322, 0.458, 0.35, 0.052, 0.168, 0.329, 0.163, 0.193, 0.151]
[2m[36m(func pid=48412)[0m top1: 0.11847014925373134
[2m[36m(func pid=48412)[0m top5: 0.5298507462686567
[2m[36m(func pid=48412)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=48412)[0m f1_macro: 0.10205390344737264
[2m[36m(func pid=48412)[0m f1_weighted: 0.12836677391546364
[2m[36m(func pid=48412)[0m f1_per_class: [0.117, 0.191, 0.152, 0.15, 0.0, 0.085, 0.11, 0.08, 0.078, 0.058]
== Status ==
Current time: 2024-01-07 01:08:22 (running for 00:25:14.54)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.508 |      0.101 |                   46 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.131 |      0.246 |                   24 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.039 |      0.325 |                   23 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=59563)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=59563)[0m Configuration completed!
[2m[36m(func pid=59563)[0m New optimizer parameters:
[2m[36m(func pid=59563)[0m SGD (
[2m[36m(func pid=59563)[0m Parameter Group 0
[2m[36m(func pid=59563)[0m     dampening: 0
[2m[36m(func pid=59563)[0m     differentiable: False
[2m[36m(func pid=59563)[0m     foreach: None
[2m[36m(func pid=59563)[0m     lr: 0.1
[2m[36m(func pid=59563)[0m     maximize: False
[2m[36m(func pid=59563)[0m     momentum: 0.9
[2m[36m(func pid=59563)[0m     nesterov: False
[2m[36m(func pid=59563)[0m     weight_decay: 0.0001
[2m[36m(func pid=59563)[0m )
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0373 | Steps: 2 | Val loss: 1.9650 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4561 | Steps: 2 | Val loss: 2.3244 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1121 | Steps: 2 | Val loss: 1.9450 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:08:27 (running for 00:25:20.16)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.458 |      0.102 |                   47 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.136 |      0.253 |                   25 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.037 |      0.33  |                   24 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.35774253731343286
[2m[36m(func pid=53984)[0m top5: 0.8610074626865671
[2m[36m(func pid=53984)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=53984)[0m f1_macro: 0.3303299358945293
[2m[36m(func pid=53984)[0m f1_weighted: 0.3887741593194267
[2m[36m(func pid=53984)[0m f1_per_class: [0.383, 0.368, 0.647, 0.524, 0.081, 0.194, 0.404, 0.254, 0.214, 0.234]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.0751 | Steps: 2 | Val loss: 4.6608 | Batch size: 32 | lr: 0.1 | Duration: 4.38s
[2m[36m(func pid=48412)[0m top1: 0.12126865671641791
[2m[36m(func pid=48412)[0m top5: 0.5387126865671642
[2m[36m(func pid=48412)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=48412)[0m f1_macro: 0.10514155228106388
[2m[36m(func pid=48412)[0m f1_weighted: 0.13082964499602578
[2m[36m(func pid=48412)[0m f1_per_class: [0.11, 0.194, 0.161, 0.153, 0.0, 0.081, 0.112, 0.092, 0.09, 0.059]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m top1: 0.27425373134328357
[2m[36m(func pid=53465)[0m top5: 0.8414179104477612
[2m[36m(func pid=53465)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=53465)[0m f1_macro: 0.2555868408836778
[2m[36m(func pid=53465)[0m f1_weighted: 0.2997027289891498
[2m[36m(func pid=53465)[0m f1_per_class: [0.348, 0.318, 0.468, 0.394, 0.054, 0.162, 0.296, 0.172, 0.185, 0.159]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.05643656716417911
[2m[36m(func pid=59563)[0m top5: 0.4337686567164179
[2m[36m(func pid=59563)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=59563)[0m f1_macro: 0.039809749899632915
[2m[36m(func pid=59563)[0m f1_weighted: 0.029142425292352957
[2m[36m(func pid=59563)[0m f1_per_class: [0.07, 0.0, 0.018, 0.0, 0.0, 0.234, 0.0, 0.0, 0.0, 0.077]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0559 | Steps: 2 | Val loss: 1.9397 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4721 | Steps: 2 | Val loss: 2.3238 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0359 | Steps: 2 | Val loss: 1.9174 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:08:33 (running for 00:25:25.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.456 |      0.105 |                   48 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.112 |      0.256 |                   26 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.056 |      0.337 |                   25 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  4.075 |      0.04  |                    1 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.3675373134328358
[2m[36m(func pid=53984)[0m top5: 0.8675373134328358
[2m[36m(func pid=53984)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=53984)[0m f1_macro: 0.33697644589304104
[2m[36m(func pid=53984)[0m f1_weighted: 0.39775636379933815
[2m[36m(func pid=53984)[0m f1_per_class: [0.387, 0.367, 0.647, 0.532, 0.081, 0.209, 0.421, 0.245, 0.214, 0.267]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.2068 | Steps: 2 | Val loss: 6.1473 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=48412)[0m top1: 0.12033582089552239
[2m[36m(func pid=48412)[0m top5: 0.5415111940298507
[2m[36m(func pid=48412)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=48412)[0m f1_macro: 0.1068074635980566
[2m[36m(func pid=48412)[0m f1_weighted: 0.12720296822157184
[2m[36m(func pid=48412)[0m f1_per_class: [0.12, 0.194, 0.167, 0.144, 0.0, 0.082, 0.105, 0.095, 0.102, 0.059]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m top1: 0.2891791044776119
[2m[36m(func pid=53465)[0m top5: 0.851679104477612
[2m[36m(func pid=53465)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=53465)[0m f1_macro: 0.2652688668688601
[2m[36m(func pid=53465)[0m f1_weighted: 0.31273838516307
[2m[36m(func pid=53465)[0m f1_per_class: [0.339, 0.316, 0.5, 0.439, 0.061, 0.156, 0.298, 0.178, 0.199, 0.167]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2933768656716418
[2m[36m(func pid=59563)[0m top5: 0.5965485074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=59563)[0m f1_macro: 0.10253585079756537
[2m[36m(func pid=59563)[0m f1_weighted: 0.16310682554804087
[2m[36m(func pid=59563)[0m f1_per_class: [0.351, 0.087, 0.0, 0.448, 0.0, 0.139, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0263 | Steps: 2 | Val loss: 1.9271 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9742 | Steps: 2 | Val loss: 1.8960 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.4234 | Steps: 2 | Val loss: 2.3190 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 01:08:38 (running for 00:25:31.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.472 |      0.107 |                   49 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  1.036 |      0.265 |                   27 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.026 |      0.334 |                   26 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  7.207 |      0.103 |                    2 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.37033582089552236
[2m[36m(func pid=53984)[0m top5: 0.871268656716418
[2m[36m(func pid=53984)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=53984)[0m f1_macro: 0.33431059171097244
[2m[36m(func pid=53984)[0m f1_weighted: 0.400102561386728
[2m[36m(func pid=53984)[0m f1_per_class: [0.394, 0.371, 0.579, 0.532, 0.08, 0.206, 0.428, 0.24, 0.215, 0.298]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 10.8070 | Steps: 2 | Val loss: 10.7519 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=53465)[0m top1: 0.3041044776119403
[2m[36m(func pid=53465)[0m top5: 0.8619402985074627
[2m[36m(func pid=53465)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=53465)[0m f1_macro: 0.2776237981577691
[2m[36m(func pid=53465)[0m f1_weighted: 0.3253327090392014
[2m[36m(func pid=53465)[0m f1_per_class: [0.344, 0.311, 0.512, 0.456, 0.075, 0.157, 0.321, 0.202, 0.186, 0.212]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.12453358208955224
[2m[36m(func pid=48412)[0m top5: 0.5485074626865671
[2m[36m(func pid=48412)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=48412)[0m f1_macro: 0.1103453150035935
[2m[36m(func pid=48412)[0m f1_weighted: 0.13327608235399105
[2m[36m(func pid=48412)[0m f1_per_class: [0.116, 0.193, 0.171, 0.144, 0.0, 0.082, 0.124, 0.101, 0.114, 0.06]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.13805970149253732
[2m[36m(func pid=59563)[0m top5: 0.6175373134328358
[2m[36m(func pid=59563)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=59563)[0m f1_macro: 0.12264024496056403
[2m[36m(func pid=59563)[0m f1_weighted: 0.08701405117451662
[2m[36m(func pid=59563)[0m f1_per_class: [0.542, 0.353, 0.0, 0.0, 0.05, 0.0, 0.003, 0.189, 0.088, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0366 | Steps: 2 | Val loss: 1.8990 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9787 | Steps: 2 | Val loss: 1.8851 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3739 | Steps: 2 | Val loss: 2.3110 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:08:44 (running for 00:25:36.39)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.423 |      0.11  |                   50 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.974 |      0.278 |                   28 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.037 |      0.336 |                   27 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 | 10.807 |      0.123 |                    3 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.376865671641791
[2m[36m(func pid=53984)[0m top5: 0.8791977611940298
[2m[36m(func pid=53984)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=53984)[0m f1_macro: 0.3356160902303563
[2m[36m(func pid=53984)[0m f1_weighted: 0.4060717008744107
[2m[36m(func pid=53984)[0m f1_per_class: [0.409, 0.397, 0.537, 0.535, 0.076, 0.2, 0.431, 0.25, 0.196, 0.324]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 15.0018 | Steps: 2 | Val loss: 16.5901 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=53465)[0m top1: 0.3069029850746269
[2m[36m(func pid=53465)[0m top5: 0.8628731343283582
[2m[36m(func pid=53465)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=53465)[0m f1_macro: 0.28076811260474605
[2m[36m(func pid=53465)[0m f1_weighted: 0.3264599925934539
[2m[36m(func pid=53465)[0m f1_per_class: [0.348, 0.304, 0.524, 0.458, 0.071, 0.162, 0.323, 0.198, 0.198, 0.22]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1287313432835821
[2m[36m(func pid=48412)[0m top5: 0.5564365671641791
[2m[36m(func pid=48412)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=48412)[0m f1_macro: 0.11291612183559388
[2m[36m(func pid=48412)[0m f1_weighted: 0.13717209436973363
[2m[36m(func pid=48412)[0m f1_per_class: [0.118, 0.2, 0.175, 0.158, 0.0, 0.081, 0.119, 0.106, 0.111, 0.061]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.06716417910447761
[2m[36m(func pid=59563)[0m top5: 0.4976679104477612
[2m[36m(func pid=59563)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=59563)[0m f1_macro: 0.05234183798919112
[2m[36m(func pid=59563)[0m f1_weighted: 0.03455720494200014
[2m[36m(func pid=59563)[0m f1_per_class: [0.043, 0.1, 0.0, 0.0, 0.069, 0.0, 0.0, 0.229, 0.082, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0322 | Steps: 2 | Val loss: 1.9130 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9620 | Steps: 2 | Val loss: 1.8689 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3702 | Steps: 2 | Val loss: 2.3033 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 8.8523 | Steps: 2 | Val loss: 12.8445 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:08:49 (running for 00:25:41.69)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.374 |      0.113 |                   51 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.979 |      0.281 |                   29 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.032 |      0.333 |                   28 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 | 15.002 |      0.052 |                    4 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.3656716417910448
[2m[36m(func pid=53984)[0m top5: 0.8754664179104478
[2m[36m(func pid=53984)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=53984)[0m f1_macro: 0.3328912075748589
[2m[36m(func pid=53984)[0m f1_weighted: 0.3936623675516501
[2m[36m(func pid=53984)[0m f1_per_class: [0.409, 0.396, 0.571, 0.533, 0.079, 0.195, 0.398, 0.232, 0.197, 0.319]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.3138992537313433
[2m[36m(func pid=53465)[0m top5: 0.8666044776119403
[2m[36m(func pid=53465)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=53465)[0m f1_macro: 0.286704742505841
[2m[36m(func pid=53465)[0m f1_weighted: 0.33095350454587275
[2m[36m(func pid=53465)[0m f1_per_class: [0.349, 0.316, 0.524, 0.467, 0.073, 0.165, 0.315, 0.235, 0.194, 0.23]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.13292910447761194
[2m[36m(func pid=48412)[0m top5: 0.5643656716417911
[2m[36m(func pid=48412)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=48412)[0m f1_macro: 0.1157049165346522
[2m[36m(func pid=48412)[0m f1_weighted: 0.14328669868180963
[2m[36m(func pid=48412)[0m f1_per_class: [0.114, 0.2, 0.179, 0.171, 0.0, 0.085, 0.126, 0.106, 0.114, 0.062]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.10634328358208955
[2m[36m(func pid=59563)[0m top5: 0.40671641791044777
[2m[36m(func pid=59563)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=59563)[0m f1_macro: 0.15639006544325534
[2m[36m(func pid=59563)[0m f1_weighted: 0.07869833613535863
[2m[36m(func pid=59563)[0m f1_per_class: [0.098, 0.197, 0.71, 0.0, 0.047, 0.151, 0.0, 0.361, 0.0, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0260 | Steps: 2 | Val loss: 1.9413 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8878 | Steps: 2 | Val loss: 1.8594 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4337 | Steps: 2 | Val loss: 2.2995 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.2463 | Steps: 2 | Val loss: 10.0645 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:08:54 (running for 00:25:47.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.37  |      0.116 |                   52 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.962 |      0.287 |                   30 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.026 |      0.328 |                   29 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  8.852 |      0.156 |                    5 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m top1: 0.3568097014925373
[2m[36m(func pid=53984)[0m top5: 0.8731343283582089
[2m[36m(func pid=53984)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=53984)[0m f1_macro: 0.32818747990260044
[2m[36m(func pid=53984)[0m f1_weighted: 0.3845104838560736
[2m[36m(func pid=53984)[0m f1_per_class: [0.404, 0.404, 0.545, 0.529, 0.082, 0.197, 0.366, 0.232, 0.192, 0.329]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m top1: 0.3101679104477612
[2m[36m(func pid=53465)[0m top5: 0.8684701492537313
[2m[36m(func pid=53465)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=53465)[0m f1_macro: 0.28706354375581167
[2m[36m(func pid=53465)[0m f1_weighted: 0.32597944839480847
[2m[36m(func pid=53465)[0m f1_per_class: [0.354, 0.321, 0.55, 0.466, 0.082, 0.164, 0.299, 0.217, 0.197, 0.22]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m top1: 0.13246268656716417
[2m[36m(func pid=48412)[0m top5: 0.5694962686567164
[2m[36m(func pid=48412)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=48412)[0m f1_macro: 0.11686443018615708
[2m[36m(func pid=48412)[0m f1_weighted: 0.14293511761226002
[2m[36m(func pid=48412)[0m f1_per_class: [0.121, 0.198, 0.192, 0.164, 0.0, 0.085, 0.131, 0.103, 0.114, 0.06]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.24580223880597016
[2m[36m(func pid=59563)[0m top5: 0.7583955223880597
[2m[36m(func pid=59563)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=59563)[0m f1_macro: 0.1492065436719116
[2m[36m(func pid=59563)[0m f1_weighted: 0.2188280000942205
[2m[36m(func pid=59563)[0m f1_per_class: [0.0, 0.104, 0.05, 0.504, 0.2, 0.368, 0.024, 0.138, 0.0, 0.103]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0302 | Steps: 2 | Val loss: 1.9411 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9841 | Steps: 2 | Val loss: 1.8587 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3731 | Steps: 2 | Val loss: 2.2913 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 7.8563 | Steps: 2 | Val loss: 15.6667 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:09:00 (running for 00:25:52.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.434 |      0.117 |                   53 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.984 |      0.291 |                   32 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.026 |      0.328 |                   29 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  3.246 |      0.149 |                    6 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.31203358208955223
[2m[36m(func pid=53465)[0m top5: 0.8689365671641791
[2m[36m(func pid=53465)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=53465)[0m f1_macro: 0.29057660729748236
[2m[36m(func pid=53465)[0m f1_weighted: 0.32636286498323214
[2m[36m(func pid=53465)[0m f1_per_class: [0.37, 0.338, 0.55, 0.476, 0.071, 0.162, 0.278, 0.229, 0.2, 0.231]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3582089552238806
[2m[36m(func pid=53984)[0m top5: 0.8796641791044776
[2m[36m(func pid=53984)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=53984)[0m f1_macro: 0.33301508715825723
[2m[36m(func pid=53984)[0m f1_weighted: 0.38280085426325416
[2m[36m(func pid=53984)[0m f1_per_class: [0.409, 0.418, 0.545, 0.53, 0.086, 0.192, 0.347, 0.249, 0.198, 0.354]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.13759328358208955
[2m[36m(func pid=48412)[0m top5: 0.5797574626865671
[2m[36m(func pid=48412)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=48412)[0m f1_macro: 0.1187066628119063
[2m[36m(func pid=48412)[0m f1_weighted: 0.149257503508424
[2m[36m(func pid=48412)[0m f1_per_class: [0.123, 0.203, 0.187, 0.184, 0.0, 0.074, 0.136, 0.103, 0.115, 0.063]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.28451492537313433
[2m[36m(func pid=59563)[0m top5: 0.9053171641791045
[2m[36m(func pid=59563)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=59563)[0m f1_macro: 0.10044648864885411
[2m[36m(func pid=59563)[0m f1_weighted: 0.14956845937612487
[2m[36m(func pid=59563)[0m f1_per_class: [0.0, 0.011, 0.16, 0.449, 0.0, 0.084, 0.027, 0.016, 0.0, 0.258]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8064 | Steps: 2 | Val loss: 1.8641 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0253 | Steps: 2 | Val loss: 1.9765 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3570 | Steps: 2 | Val loss: 2.2884 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 5.1774 | Steps: 2 | Val loss: 6.4758 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 01:09:05 (running for 00:25:57.93)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.373 |      0.119 |                   54 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.806 |      0.287 |                   33 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.03  |      0.333 |                   30 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  7.856 |      0.1   |                    7 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3064365671641791
[2m[36m(func pid=53465)[0m top5: 0.8656716417910447
[2m[36m(func pid=53465)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=53465)[0m f1_macro: 0.28696462461272326
[2m[36m(func pid=53465)[0m f1_weighted: 0.3210272379127885
[2m[36m(func pid=53465)[0m f1_per_class: [0.378, 0.346, 0.537, 0.464, 0.074, 0.162, 0.265, 0.231, 0.209, 0.203]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34794776119402987
[2m[36m(func pid=53984)[0m top5: 0.8708022388059702
[2m[36m(func pid=53984)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=53984)[0m f1_macro: 0.3267858678059434
[2m[36m(func pid=53984)[0m f1_weighted: 0.3711518112827718
[2m[36m(func pid=53984)[0m f1_per_class: [0.4, 0.409, 0.545, 0.523, 0.083, 0.191, 0.322, 0.248, 0.201, 0.346]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1357276119402985
[2m[36m(func pid=48412)[0m top5: 0.5788246268656716
[2m[36m(func pid=48412)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=48412)[0m f1_macro: 0.11894152675824818
[2m[36m(func pid=48412)[0m f1_weighted: 0.14708598543771162
[2m[36m(func pid=48412)[0m f1_per_class: [0.124, 0.195, 0.194, 0.181, 0.0, 0.078, 0.133, 0.108, 0.113, 0.063]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.3572761194029851
[2m[36m(func pid=59563)[0m top5: 0.855410447761194
[2m[36m(func pid=59563)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=59563)[0m f1_macro: 0.2491613296605562
[2m[36m(func pid=59563)[0m f1_weighted: 0.35654615916200394
[2m[36m(func pid=59563)[0m f1_per_class: [0.25, 0.49, 0.214, 0.467, 0.116, 0.289, 0.278, 0.231, 0.155, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8505 | Steps: 2 | Val loss: 1.8803 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0226 | Steps: 2 | Val loss: 2.0052 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3328 | Steps: 2 | Val loss: 2.2825 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5548 | Steps: 2 | Val loss: 17.6126 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:09:10 (running for 00:26:02.95)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.357 |      0.119 |                   55 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.851 |      0.289 |                   34 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.025 |      0.327 |                   31 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  5.177 |      0.249 |                    8 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3045708955223881
[2m[36m(func pid=53465)[0m top5: 0.8586753731343284
[2m[36m(func pid=53465)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=53465)[0m f1_macro: 0.288628947620114
[2m[36m(func pid=53465)[0m f1_weighted: 0.32050687448661874
[2m[36m(func pid=53465)[0m f1_per_class: [0.398, 0.35, 0.537, 0.464, 0.069, 0.165, 0.26, 0.234, 0.204, 0.205]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34328358208955223
[2m[36m(func pid=53984)[0m top5: 0.8698694029850746
[2m[36m(func pid=53984)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=53984)[0m f1_macro: 0.3228312932187902
[2m[36m(func pid=53984)[0m f1_weighted: 0.3647407973029461
[2m[36m(func pid=53984)[0m f1_per_class: [0.394, 0.41, 0.533, 0.524, 0.083, 0.2, 0.296, 0.246, 0.2, 0.341]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.14085820895522388
[2m[36m(func pid=48412)[0m top5: 0.5881529850746269
[2m[36m(func pid=48412)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=48412)[0m f1_macro: 0.12068798518439483
[2m[36m(func pid=48412)[0m f1_weighted: 0.15460101178436744
[2m[36m(func pid=48412)[0m f1_per_class: [0.119, 0.202, 0.187, 0.19, 0.0, 0.078, 0.146, 0.108, 0.114, 0.063]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m top1: 0.18190298507462688
[2m[36m(func pid=59563)[0m top5: 0.6506529850746269
[2m[36m(func pid=59563)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=59563)[0m f1_macro: 0.23082420426565253
[2m[36m(func pid=59563)[0m f1_weighted: 0.1709965321700887
[2m[36m(func pid=59563)[0m f1_per_class: [0.593, 0.318, 0.632, 0.0, 0.079, 0.115, 0.238, 0.182, 0.151, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7788 | Steps: 2 | Val loss: 1.9055 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0472 | Steps: 2 | Val loss: 2.0078 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.5252 | Steps: 2 | Val loss: 27.3388 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3289 | Steps: 2 | Val loss: 2.2766 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:09:15 (running for 00:26:08.23)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.333 |      0.121 |                   56 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.779 |      0.28  |                   35 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.023 |      0.323 |                   32 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  1.555 |      0.231 |                    9 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2905783582089552
[2m[36m(func pid=53465)[0m top5: 0.8470149253731343
[2m[36m(func pid=53465)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=53465)[0m f1_macro: 0.2802958006055173
[2m[36m(func pid=53465)[0m f1_weighted: 0.3070788547333677
[2m[36m(func pid=53465)[0m f1_per_class: [0.382, 0.333, 0.537, 0.456, 0.07, 0.151, 0.238, 0.238, 0.201, 0.198]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3414179104477612
[2m[36m(func pid=53984)[0m top5: 0.8708022388059702
[2m[36m(func pid=53984)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=53984)[0m f1_macro: 0.3220997322940812
[2m[36m(func pid=53984)[0m f1_weighted: 0.3613414458199823
[2m[36m(func pid=53984)[0m f1_per_class: [0.392, 0.417, 0.545, 0.524, 0.084, 0.197, 0.283, 0.25, 0.183, 0.346]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m top1: 0.1357276119402985
[2m[36m(func pid=59563)[0m top5: 0.6072761194029851
[2m[36m(func pid=59563)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=59563)[0m f1_macro: 0.18584461462666788
[2m[36m(func pid=59563)[0m f1_weighted: 0.14485131087979722
[2m[36m(func pid=59563)[0m f1_per_class: [0.306, 0.073, 0.741, 0.0, 0.051, 0.035, 0.341, 0.209, 0.102, 0.0]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m top1: 0.14225746268656717
[2m[36m(func pid=48412)[0m top5: 0.5942164179104478
[2m[36m(func pid=48412)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=48412)[0m f1_macro: 0.12258602918971273
[2m[36m(func pid=48412)[0m f1_weighted: 0.15594266222504
[2m[36m(func pid=48412)[0m f1_per_class: [0.122, 0.204, 0.194, 0.19, 0.0, 0.077, 0.15, 0.112, 0.114, 0.064]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7695 | Steps: 2 | Val loss: 1.9142 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0265 | Steps: 2 | Val loss: 1.9980 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.1277 | Steps: 2 | Val loss: 23.6246 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2949 | Steps: 2 | Val loss: 2.2705 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:09:21 (running for 00:26:13.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.329 |      0.123 |                   57 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.769 |      0.281 |                   36 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.047 |      0.322 |                   33 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  3.525 |      0.186 |                   10 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2896455223880597
[2m[36m(func pid=53465)[0m top5: 0.8465485074626866
[2m[36m(func pid=53465)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=53465)[0m f1_macro: 0.2812393316238181
[2m[36m(func pid=53465)[0m f1_weighted: 0.3057963118309151
[2m[36m(func pid=53465)[0m f1_per_class: [0.389, 0.343, 0.537, 0.446, 0.069, 0.151, 0.235, 0.248, 0.211, 0.185]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34281716417910446
[2m[36m(func pid=53984)[0m top5: 0.8736007462686567
[2m[36m(func pid=53984)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=53984)[0m f1_macro: 0.3261267162904923
[2m[36m(func pid=53984)[0m f1_weighted: 0.36020473744233444
[2m[36m(func pid=53984)[0m f1_per_class: [0.42, 0.426, 0.533, 0.515, 0.077, 0.2, 0.278, 0.251, 0.185, 0.375]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m top1: 0.16884328358208955
[2m[36m(func pid=59563)[0m top5: 0.6277985074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=59563)[0m f1_macro: 0.2196031593028839
[2m[36m(func pid=59563)[0m f1_weighted: 0.1892369802065954
[2m[36m(func pid=59563)[0m f1_per_class: [0.183, 0.251, 0.815, 0.0, 0.031, 0.054, 0.372, 0.221, 0.195, 0.074]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1455223880597015
[2m[36m(func pid=48412)[0m top5: 0.6063432835820896
[2m[36m(func pid=48412)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=48412)[0m f1_macro: 0.12897062648360685
[2m[36m(func pid=48412)[0m f1_weighted: 0.15900160245098593
[2m[36m(func pid=48412)[0m f1_per_class: [0.134, 0.199, 0.226, 0.195, 0.0, 0.077, 0.154, 0.111, 0.127, 0.066]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7419 | Steps: 2 | Val loss: 1.9274 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0333 | Steps: 2 | Val loss: 1.9777 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8140 | Steps: 2 | Val loss: 17.6613 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2787 | Steps: 2 | Val loss: 2.2667 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:09:26 (running for 00:26:18.51)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.295 |      0.129 |                   58 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.742 |      0.275 |                   37 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.027 |      0.326 |                   34 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  4.128 |      0.22  |                   11 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.27798507462686567
[2m[36m(func pid=53465)[0m top5: 0.8404850746268657
[2m[36m(func pid=53465)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=53465)[0m f1_macro: 0.27522566215717387
[2m[36m(func pid=53465)[0m f1_weighted: 0.29293501133800537
[2m[36m(func pid=53465)[0m f1_per_class: [0.374, 0.332, 0.55, 0.427, 0.065, 0.145, 0.222, 0.237, 0.202, 0.2]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34888059701492535
[2m[36m(func pid=53984)[0m top5: 0.8726679104477612
[2m[36m(func pid=53984)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=53984)[0m f1_macro: 0.3313306979590894
[2m[36m(func pid=53984)[0m f1_weighted: 0.364213136404586
[2m[36m(func pid=53984)[0m f1_per_class: [0.43, 0.427, 0.533, 0.502, 0.082, 0.218, 0.292, 0.267, 0.196, 0.366]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m top1: 0.18563432835820895
[2m[36m(func pid=59563)[0m top5: 0.6791044776119403
[2m[36m(func pid=59563)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=59563)[0m f1_macro: 0.2585433699549585
[2m[36m(func pid=59563)[0m f1_weighted: 0.20366973970121907
[2m[36m(func pid=59563)[0m f1_per_class: [0.148, 0.311, 0.846, 0.045, 0.042, 0.088, 0.322, 0.22, 0.171, 0.392]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m top1: 0.146455223880597
[2m[36m(func pid=48412)[0m top5: 0.6082089552238806
[2m[36m(func pid=48412)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=48412)[0m f1_macro: 0.12806043081574964
[2m[36m(func pid=48412)[0m f1_weighted: 0.15928061928121215
[2m[36m(func pid=48412)[0m f1_per_class: [0.132, 0.208, 0.212, 0.201, 0.0, 0.078, 0.145, 0.113, 0.124, 0.067]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6940 | Steps: 2 | Val loss: 1.9260 | Batch size: 32 | lr: 0.001 | Duration: 2.59s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0357 | Steps: 2 | Val loss: 1.9820 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0255 | Steps: 2 | Val loss: 12.7216 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:09:31 (running for 00:26:23.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.279 |      0.128 |                   59 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.694 |      0.275 |                   38 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.033 |      0.331 |                   35 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.814 |      0.259 |                   12 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2775186567164179
[2m[36m(func pid=53465)[0m top5: 0.8414179104477612
[2m[36m(func pid=53465)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=53465)[0m f1_macro: 0.274656420580637
[2m[36m(func pid=53465)[0m f1_weighted: 0.2926740858759492
[2m[36m(func pid=53465)[0m f1_per_class: [0.368, 0.346, 0.55, 0.428, 0.065, 0.133, 0.219, 0.225, 0.192, 0.219]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2832 | Steps: 2 | Val loss: 2.2666 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=59563)[0m top1: 0.24953358208955223
[2m[36m(func pid=59563)[0m top5: 0.7476679104477612
[2m[36m(func pid=59563)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=59563)[0m f1_macro: 0.2691081988854398
[2m[36m(func pid=59563)[0m f1_weighted: 0.28739058905205694
[2m[36m(func pid=59563)[0m f1_per_class: [0.212, 0.363, 0.846, 0.34, 0.046, 0.131, 0.297, 0.227, 0.048, 0.181]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3474813432835821
[2m[36m(func pid=53984)[0m top5: 0.8684701492537313
[2m[36m(func pid=53984)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=53984)[0m f1_macro: 0.3304309629298357
[2m[36m(func pid=53984)[0m f1_weighted: 0.36171790412187027
[2m[36m(func pid=53984)[0m f1_per_class: [0.436, 0.426, 0.522, 0.483, 0.081, 0.216, 0.301, 0.267, 0.211, 0.361]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.14925373134328357
[2m[36m(func pid=48412)[0m top5: 0.6086753731343284
[2m[36m(func pid=48412)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=48412)[0m f1_macro: 0.13379797130254473
[2m[36m(func pid=48412)[0m f1_weighted: 0.16244348616921367
[2m[36m(func pid=48412)[0m f1_per_class: [0.134, 0.205, 0.265, 0.204, 0.0, 0.078, 0.154, 0.114, 0.116, 0.069]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6957 | Steps: 2 | Val loss: 1.9280 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0205 | Steps: 2 | Val loss: 1.9558 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3130 | Steps: 2 | Val loss: 11.9422 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:09:36 (running for 00:26:28.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.283 |      0.134 |                   60 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.696 |      0.277 |                   39 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.036 |      0.33  |                   36 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.026 |      0.269 |                   13 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2775186567164179
[2m[36m(func pid=53465)[0m top5: 0.840018656716418
[2m[36m(func pid=53465)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=53465)[0m f1_macro: 0.2766201697230973
[2m[36m(func pid=53465)[0m f1_weighted: 0.2918568217709228
[2m[36m(func pid=53465)[0m f1_per_class: [0.361, 0.346, 0.564, 0.437, 0.072, 0.135, 0.207, 0.221, 0.198, 0.225]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.2809 | Steps: 2 | Val loss: 2.2608 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=59563)[0m top1: 0.2947761194029851
[2m[36m(func pid=59563)[0m top5: 0.7994402985074627
[2m[36m(func pid=59563)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=59563)[0m f1_macro: 0.29562936381232496
[2m[36m(func pid=59563)[0m f1_weighted: 0.3305948976347673
[2m[36m(func pid=59563)[0m f1_per_class: [0.41, 0.32, 0.815, 0.529, 0.072, 0.161, 0.265, 0.235, 0.069, 0.081]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35447761194029853
[2m[36m(func pid=53984)[0m top5: 0.8722014925373134
[2m[36m(func pid=53984)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=53984)[0m f1_macro: 0.33295565193937693
[2m[36m(func pid=53984)[0m f1_weighted: 0.372337584715058
[2m[36m(func pid=53984)[0m f1_per_class: [0.436, 0.424, 0.522, 0.482, 0.071, 0.217, 0.339, 0.264, 0.209, 0.366]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.15205223880597016
[2m[36m(func pid=48412)[0m top5: 0.6138059701492538
[2m[36m(func pid=48412)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=48412)[0m f1_macro: 0.13791991288376487
[2m[36m(func pid=48412)[0m f1_weighted: 0.16449784677108678
[2m[36m(func pid=48412)[0m f1_per_class: [0.138, 0.21, 0.277, 0.204, 0.012, 0.078, 0.156, 0.12, 0.113, 0.071]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7212 | Steps: 2 | Val loss: 1.9256 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0165 | Steps: 2 | Val loss: 14.9380 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0326 | Steps: 2 | Val loss: 1.9335 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:09:41 (running for 00:26:34.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.281 |      0.138 |                   61 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.721 |      0.279 |                   40 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.02  |      0.333 |                   37 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.313 |      0.296 |                   14 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2798507462686567
[2m[36m(func pid=53465)[0m top5: 0.8404850746268657
[2m[36m(func pid=53465)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=53465)[0m f1_macro: 0.2790100058845776
[2m[36m(func pid=53465)[0m f1_weighted: 0.2924191588064578
[2m[36m(func pid=53465)[0m f1_per_class: [0.357, 0.352, 0.579, 0.432, 0.063, 0.141, 0.204, 0.236, 0.21, 0.216]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2303 | Steps: 2 | Val loss: 2.2589 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=59563)[0m top1: 0.27611940298507465
[2m[36m(func pid=59563)[0m top5: 0.8227611940298507
[2m[36m(func pid=59563)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=59563)[0m f1_macro: 0.2790169139779349
[2m[36m(func pid=59563)[0m f1_weighted: 0.2797385218679332
[2m[36m(func pid=59563)[0m f1_per_class: [0.5, 0.178, 0.786, 0.546, 0.091, 0.136, 0.158, 0.265, 0.07, 0.06]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.36100746268656714
[2m[36m(func pid=53984)[0m top5: 0.8740671641791045
[2m[36m(func pid=53984)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=53984)[0m f1_macro: 0.3364865500476715
[2m[36m(func pid=53984)[0m f1_weighted: 0.3796698304397744
[2m[36m(func pid=53984)[0m f1_per_class: [0.45, 0.426, 0.533, 0.491, 0.073, 0.219, 0.354, 0.253, 0.213, 0.353]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.15111940298507462
[2m[36m(func pid=48412)[0m top5: 0.617070895522388
[2m[36m(func pid=48412)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=48412)[0m f1_macro: 0.13681395321740283
[2m[36m(func pid=48412)[0m f1_weighted: 0.16319486540778597
[2m[36m(func pid=48412)[0m f1_per_class: [0.14, 0.208, 0.269, 0.201, 0.012, 0.081, 0.154, 0.12, 0.111, 0.072]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7349 | Steps: 2 | Val loss: 1.9026 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5258 | Steps: 2 | Val loss: 15.5990 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0252 | Steps: 2 | Val loss: 1.9413 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:09:47 (running for 00:26:39.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.23  |      0.137 |                   62 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.735 |      0.287 |                   41 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.033 |      0.336 |                   38 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.017 |      0.279 |                   15 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.291044776119403
[2m[36m(func pid=53465)[0m top5: 0.8460820895522388
[2m[36m(func pid=53465)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=53465)[0m f1_macro: 0.2871671930712764
[2m[36m(func pid=53465)[0m f1_weighted: 0.30673412930037797
[2m[36m(func pid=53465)[0m f1_per_class: [0.378, 0.348, 0.579, 0.452, 0.058, 0.148, 0.229, 0.255, 0.206, 0.219]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2541 | Steps: 2 | Val loss: 2.2530 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=59563)[0m top1: 0.2947761194029851
[2m[36m(func pid=59563)[0m top5: 0.8404850746268657
[2m[36m(func pid=59563)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=59563)[0m f1_macro: 0.2701546585946169
[2m[36m(func pid=59563)[0m f1_weighted: 0.26151686174158306
[2m[36m(func pid=59563)[0m f1_per_class: [0.379, 0.197, 0.786, 0.517, 0.129, 0.127, 0.12, 0.262, 0.104, 0.08]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35867537313432835
[2m[36m(func pid=53984)[0m top5: 0.8754664179104478
[2m[36m(func pid=53984)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=53984)[0m f1_macro: 0.3342794274104655
[2m[36m(func pid=53984)[0m f1_weighted: 0.37702839995817533
[2m[36m(func pid=53984)[0m f1_per_class: [0.45, 0.421, 0.533, 0.491, 0.075, 0.213, 0.351, 0.248, 0.224, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1525186567164179
[2m[36m(func pid=48412)[0m top5: 0.6231343283582089
[2m[36m(func pid=48412)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=48412)[0m f1_macro: 0.1366400492199142
[2m[36m(func pid=48412)[0m f1_weighted: 0.1637086240972643
[2m[36m(func pid=48412)[0m f1_per_class: [0.138, 0.206, 0.257, 0.209, 0.013, 0.085, 0.147, 0.125, 0.108, 0.078]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6518 | Steps: 2 | Val loss: 1.8868 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5262 | Steps: 2 | Val loss: 13.1539 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0287 | Steps: 2 | Val loss: 1.9622 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 01:09:52 (running for 00:26:44.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.254 |      0.137 |                   63 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.652 |      0.29  |                   42 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.025 |      0.334 |                   39 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.526 |      0.27  |                   16 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.2994402985074627
[2m[36m(func pid=53465)[0m top5: 0.8498134328358209
[2m[36m(func pid=53465)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=53465)[0m f1_macro: 0.28955057230489006
[2m[36m(func pid=53465)[0m f1_weighted: 0.3186420880120272
[2m[36m(func pid=53465)[0m f1_per_class: [0.368, 0.348, 0.564, 0.464, 0.062, 0.165, 0.254, 0.237, 0.211, 0.222]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2236 | Steps: 2 | Val loss: 2.2508 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=59563)[0m top1: 0.34328358208955223
[2m[36m(func pid=59563)[0m top5: 0.8334888059701493
[2m[36m(func pid=59563)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=59563)[0m f1_macro: 0.30502587424119076
[2m[36m(func pid=59563)[0m f1_weighted: 0.3085257857265448
[2m[36m(func pid=59563)[0m f1_per_class: [0.327, 0.359, 0.8, 0.54, 0.13, 0.232, 0.115, 0.3, 0.125, 0.122]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.353544776119403
[2m[36m(func pid=53984)[0m top5: 0.8731343283582089
[2m[36m(func pid=53984)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=53984)[0m f1_macro: 0.3284789943379598
[2m[36m(func pid=53984)[0m f1_weighted: 0.3704867815159946
[2m[36m(func pid=53984)[0m f1_per_class: [0.444, 0.424, 0.522, 0.487, 0.077, 0.184, 0.341, 0.257, 0.222, 0.326]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5678 | Steps: 2 | Val loss: 1.8699 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=48412)[0m top1: 0.15671641791044777
[2m[36m(func pid=48412)[0m top5: 0.6194029850746269
[2m[36m(func pid=48412)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=48412)[0m f1_macro: 0.13916324750268833
[2m[36m(func pid=48412)[0m f1_weighted: 0.16735063404101413
[2m[36m(func pid=48412)[0m f1_per_class: [0.147, 0.205, 0.247, 0.219, 0.013, 0.082, 0.149, 0.137, 0.114, 0.08]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1694 | Steps: 2 | Val loss: 12.5272 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0213 | Steps: 2 | Val loss: 1.9633 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:09:57 (running for 00:26:49.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.224 |      0.139 |                   64 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.568 |      0.291 |                   43 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.029 |      0.328 |                   40 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  1.526 |      0.305 |                   17 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.30830223880597013
[2m[36m(func pid=53465)[0m top5: 0.8540111940298507
[2m[36m(func pid=53465)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=53465)[0m f1_macro: 0.2910550229267423
[2m[36m(func pid=53465)[0m f1_weighted: 0.32785808207270933
[2m[36m(func pid=53465)[0m f1_per_class: [0.366, 0.345, 0.55, 0.483, 0.065, 0.165, 0.271, 0.231, 0.21, 0.226]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2308 | Steps: 2 | Val loss: 2.2507 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=59563)[0m top1: 0.34281716417910446
[2m[36m(func pid=59563)[0m top5: 0.8218283582089553
[2m[36m(func pid=59563)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=59563)[0m f1_macro: 0.3373135417254051
[2m[36m(func pid=59563)[0m f1_weighted: 0.3162360131021055
[2m[36m(func pid=59563)[0m f1_per_class: [0.441, 0.417, 0.8, 0.479, 0.175, 0.302, 0.117, 0.348, 0.124, 0.17]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35494402985074625
[2m[36m(func pid=53984)[0m top5: 0.8717350746268657
[2m[36m(func pid=53984)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=53984)[0m f1_macro: 0.33582971093690483
[2m[36m(func pid=53984)[0m f1_weighted: 0.3744229532717475
[2m[36m(func pid=53984)[0m f1_per_class: [0.45, 0.424, 0.558, 0.49, 0.081, 0.182, 0.352, 0.248, 0.217, 0.356]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5897 | Steps: 2 | Val loss: 1.8623 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=48412)[0m top1: 0.15485074626865672
[2m[36m(func pid=48412)[0m top5: 0.6166044776119403
[2m[36m(func pid=48412)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=48412)[0m f1_macro: 0.13626371268941845
[2m[36m(func pid=48412)[0m f1_weighted: 0.1659484030514507
[2m[36m(func pid=48412)[0m f1_per_class: [0.133, 0.209, 0.244, 0.219, 0.013, 0.085, 0.145, 0.124, 0.112, 0.08]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0108 | Steps: 2 | Val loss: 15.0559 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 01:10:02 (running for 00:26:54.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.231 |      0.136 |                   65 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.59  |      0.293 |                   44 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.021 |      0.336 |                   41 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.169 |      0.337 |                   18 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.314365671641791
[2m[36m(func pid=53465)[0m top5: 0.8586753731343284
[2m[36m(func pid=53465)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=53465)[0m f1_macro: 0.29349151249554906
[2m[36m(func pid=53465)[0m f1_weighted: 0.3338587908390283
[2m[36m(func pid=53465)[0m f1_per_class: [0.355, 0.343, 0.55, 0.491, 0.066, 0.173, 0.282, 0.234, 0.205, 0.236]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0222 | Steps: 2 | Val loss: 1.9620 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1896 | Steps: 2 | Val loss: 2.2467 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=59563)[0m top1: 0.30830223880597013
[2m[36m(func pid=59563)[0m top5: 0.7971082089552238
[2m[36m(func pid=59563)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=59563)[0m f1_macro: 0.32498661104307175
[2m[36m(func pid=59563)[0m f1_weighted: 0.254333995964565
[2m[36m(func pid=59563)[0m f1_per_class: [0.611, 0.414, 0.783, 0.292, 0.056, 0.35, 0.062, 0.32, 0.124, 0.239]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3572761194029851
[2m[36m(func pid=53984)[0m top5: 0.8740671641791045
[2m[36m(func pid=53984)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=53984)[0m f1_macro: 0.3328303453374609
[2m[36m(func pid=53984)[0m f1_weighted: 0.3790260913903571
[2m[36m(func pid=53984)[0m f1_per_class: [0.443, 0.421, 0.533, 0.496, 0.077, 0.179, 0.367, 0.244, 0.22, 0.348]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5437 | Steps: 2 | Val loss: 1.8547 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=48412)[0m top1: 0.15764925373134328
[2m[36m(func pid=48412)[0m top5: 0.6180037313432836
[2m[36m(func pid=48412)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=48412)[0m f1_macro: 0.13974604181643094
[2m[36m(func pid=48412)[0m f1_weighted: 0.16772737656362985
[2m[36m(func pid=48412)[0m f1_per_class: [0.143, 0.213, 0.25, 0.222, 0.012, 0.084, 0.142, 0.136, 0.114, 0.08]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0000 | Steps: 2 | Val loss: 18.4660 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 01:10:07 (running for 00:27:00.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.19  |      0.14  |                   66 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.544 |      0.295 |                   45 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.022 |      0.333 |                   42 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.011 |      0.325 |                   19 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.31716417910447764
[2m[36m(func pid=53465)[0m top5: 0.8605410447761194
[2m[36m(func pid=53465)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=53465)[0m f1_macro: 0.2948576436868476
[2m[36m(func pid=53465)[0m f1_weighted: 0.3327961651671452
[2m[36m(func pid=53465)[0m f1_per_class: [0.359, 0.347, 0.537, 0.501, 0.073, 0.178, 0.264, 0.232, 0.209, 0.25]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0246 | Steps: 2 | Val loss: 1.9515 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2151 | Steps: 2 | Val loss: 2.2464 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=59563)[0m top1: 0.29151119402985076
[2m[36m(func pid=59563)[0m top5: 0.761660447761194
[2m[36m(func pid=59563)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=59563)[0m f1_macro: 0.31182558741821353
[2m[36m(func pid=59563)[0m f1_weighted: 0.2067770041478828
[2m[36m(func pid=59563)[0m f1_per_class: [0.615, 0.422, 0.727, 0.134, 0.069, 0.353, 0.042, 0.321, 0.122, 0.312]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5103 | Steps: 2 | Val loss: 1.8478 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=53984)[0m top1: 0.3596082089552239
[2m[36m(func pid=53984)[0m top5: 0.8763992537313433
[2m[36m(func pid=53984)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=53984)[0m f1_macro: 0.3336971267988216
[2m[36m(func pid=53984)[0m f1_weighted: 0.38423893895760103
[2m[36m(func pid=53984)[0m f1_per_class: [0.456, 0.421, 0.522, 0.5, 0.079, 0.187, 0.381, 0.232, 0.214, 0.348]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.15904850746268656
[2m[36m(func pid=48412)[0m top5: 0.6245335820895522
[2m[36m(func pid=48412)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=48412)[0m f1_macro: 0.14060940917878276
[2m[36m(func pid=48412)[0m f1_weighted: 0.17109755822764972
[2m[36m(func pid=48412)[0m f1_per_class: [0.142, 0.204, 0.247, 0.229, 0.012, 0.093, 0.151, 0.126, 0.113, 0.089]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0446 | Steps: 2 | Val loss: 21.1575 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=53465)[0m top1: 0.3180970149253731
[2m[36m(func pid=53465)[0m top5: 0.8638059701492538
[2m[36m(func pid=53465)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=53465)[0m f1_macro: 0.2970462966024363
[2m[36m(func pid=53465)[0m f1_weighted: 0.3318824696024382
[2m[36m(func pid=53465)[0m f1_per_class: [0.365, 0.343, 0.55, 0.506, 0.073, 0.171, 0.258, 0.241, 0.209, 0.254]
[2m[36m(func pid=53465)[0m 
== Status ==
Current time: 2024-01-07 01:10:12 (running for 00:27:05.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.215 |      0.141 |                   67 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.51  |      0.297 |                   46 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.025 |      0.334 |                   43 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.312 |                   20 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0213 | Steps: 2 | Val loss: 1.9610 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=59563)[0m top1: 0.28125
[2m[36m(func pid=59563)[0m top5: 0.7313432835820896
[2m[36m(func pid=59563)[0m f1_micro: 0.28125
[2m[36m(func pid=59563)[0m f1_macro: 0.292449653977188
[2m[36m(func pid=59563)[0m f1_weighted: 0.18201337149515345
[2m[36m(func pid=59563)[0m f1_per_class: [0.622, 0.422, 0.6, 0.067, 0.069, 0.351, 0.027, 0.312, 0.1, 0.354]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2083 | Steps: 2 | Val loss: 2.2446 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5591 | Steps: 2 | Val loss: 1.8441 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=53984)[0m top1: 0.3568097014925373
[2m[36m(func pid=53984)[0m top5: 0.8763992537313433
[2m[36m(func pid=53984)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=53984)[0m f1_macro: 0.3320620695169227
[2m[36m(func pid=53984)[0m f1_weighted: 0.38097898684511927
[2m[36m(func pid=53984)[0m f1_per_class: [0.439, 0.419, 0.533, 0.501, 0.081, 0.183, 0.372, 0.233, 0.216, 0.344]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.1599813432835821
[2m[36m(func pid=48412)[0m top5: 0.6236007462686567
[2m[36m(func pid=48412)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=48412)[0m f1_macro: 0.1395510044967479
[2m[36m(func pid=48412)[0m f1_weighted: 0.17268424730042406
[2m[36m(func pid=48412)[0m f1_per_class: [0.143, 0.207, 0.238, 0.232, 0.013, 0.092, 0.153, 0.126, 0.102, 0.09]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0374 | Steps: 2 | Val loss: 23.3523 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 01:10:18 (running for 00:27:10.50)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.208 |      0.14  |                   68 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.559 |      0.294 |                   47 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.021 |      0.332 |                   44 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.045 |      0.292 |                   21 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.31763059701492535
[2m[36m(func pid=53465)[0m top5: 0.8647388059701493
[2m[36m(func pid=53465)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=53465)[0m f1_macro: 0.29413360177771686
[2m[36m(func pid=53465)[0m f1_weighted: 0.3280335866456178
[2m[36m(func pid=53465)[0m f1_per_class: [0.37, 0.344, 0.537, 0.511, 0.075, 0.172, 0.24, 0.24, 0.206, 0.246]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0250 | Steps: 2 | Val loss: 1.9510 | Batch size: 32 | lr: 0.01 | Duration: 2.62s
[2m[36m(func pid=59563)[0m top1: 0.2630597014925373
[2m[36m(func pid=59563)[0m top5: 0.6884328358208955
[2m[36m(func pid=59563)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=59563)[0m f1_macro: 0.27450653990846857
[2m[36m(func pid=59563)[0m f1_weighted: 0.16072589250432573
[2m[36m(func pid=59563)[0m f1_per_class: [0.606, 0.414, 0.556, 0.036, 0.045, 0.286, 0.018, 0.293, 0.117, 0.375]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1889 | Steps: 2 | Val loss: 2.2456 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5420 | Steps: 2 | Val loss: 1.8245 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=53984)[0m top1: 0.35634328358208955
[2m[36m(func pid=53984)[0m top5: 0.8763992537313433
[2m[36m(func pid=53984)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=53984)[0m f1_macro: 0.3340171241208044
[2m[36m(func pid=53984)[0m f1_weighted: 0.3831526829848572
[2m[36m(func pid=53984)[0m f1_per_class: [0.434, 0.403, 0.585, 0.509, 0.077, 0.184, 0.382, 0.23, 0.199, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.15951492537313433
[2m[36m(func pid=48412)[0m top5: 0.6226679104477612
[2m[36m(func pid=48412)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=48412)[0m f1_macro: 0.13829796629336918
[2m[36m(func pid=48412)[0m f1_weighted: 0.17264614281091778
[2m[36m(func pid=48412)[0m f1_per_class: [0.138, 0.207, 0.225, 0.233, 0.012, 0.095, 0.151, 0.124, 0.106, 0.091]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2039 | Steps: 2 | Val loss: 24.7364 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:10:23 (running for 00:27:15.53)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.189 |      0.138 |                   69 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.542 |      0.298 |                   48 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.025 |      0.334 |                   45 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.037 |      0.275 |                   22 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.322294776119403
[2m[36m(func pid=53465)[0m top5: 0.8694029850746269
[2m[36m(func pid=53465)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=53465)[0m f1_macro: 0.29821946088489615
[2m[36m(func pid=53465)[0m f1_weighted: 0.3335509069008591
[2m[36m(func pid=53465)[0m f1_per_class: [0.383, 0.346, 0.55, 0.51, 0.076, 0.172, 0.258, 0.242, 0.202, 0.243]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0215 | Steps: 2 | Val loss: 1.9465 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=59563)[0m top1: 0.25279850746268656
[2m[36m(func pid=59563)[0m top5: 0.6702425373134329
[2m[36m(func pid=59563)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=59563)[0m f1_macro: 0.2674295931789713
[2m[36m(func pid=59563)[0m f1_weighted: 0.15149107795138492
[2m[36m(func pid=59563)[0m f1_per_class: [0.548, 0.396, 0.471, 0.01, 0.132, 0.21, 0.048, 0.311, 0.136, 0.414]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1933 | Steps: 2 | Val loss: 2.2473 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5375 | Steps: 2 | Val loss: 1.8254 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=53984)[0m top1: 0.35494402985074625
[2m[36m(func pid=53984)[0m top5: 0.8782649253731343
[2m[36m(func pid=53984)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=53984)[0m f1_macro: 0.3333940947707705
[2m[36m(func pid=53984)[0m f1_weighted: 0.38092609922985227
[2m[36m(func pid=53984)[0m f1_per_class: [0.429, 0.405, 0.585, 0.496, 0.074, 0.187, 0.385, 0.225, 0.203, 0.344]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=48412)[0m top1: 0.16184701492537312
[2m[36m(func pid=48412)[0m top5: 0.6217350746268657
[2m[36m(func pid=48412)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=48412)[0m f1_macro: 0.13973313607058452
[2m[36m(func pid=48412)[0m f1_weighted: 0.1751658845702832
[2m[36m(func pid=48412)[0m f1_per_class: [0.151, 0.213, 0.213, 0.237, 0.012, 0.095, 0.152, 0.127, 0.109, 0.09]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0005 | Steps: 2 | Val loss: 25.5061 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:10:28 (running for 00:27:20.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.193 |      0.14  |                   70 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.537 |      0.3   |                   49 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.022 |      0.333 |                   46 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.204 |      0.267 |                   23 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3246268656716418
[2m[36m(func pid=53465)[0m top5: 0.8703358208955224
[2m[36m(func pid=53465)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=53465)[0m f1_macro: 0.3000717104459051
[2m[36m(func pid=53465)[0m f1_weighted: 0.3389672111766297
[2m[36m(func pid=53465)[0m f1_per_class: [0.385, 0.332, 0.55, 0.512, 0.074, 0.17, 0.281, 0.248, 0.206, 0.244]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2513992537313433
[2m[36m(func pid=59563)[0m top5: 0.6609141791044776
[2m[36m(func pid=59563)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=59563)[0m f1_macro: 0.2668032905491663
[2m[36m(func pid=59563)[0m f1_weighted: 0.16600443744521773
[2m[36m(func pid=59563)[0m f1_per_class: [0.511, 0.377, 0.471, 0.003, 0.094, 0.14, 0.136, 0.326, 0.17, 0.44]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0258 | Steps: 2 | Val loss: 1.9453 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1753 | Steps: 2 | Val loss: 2.2473 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4732 | Steps: 2 | Val loss: 1.8156 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=53984)[0m top1: 0.3596082089552239
[2m[36m(func pid=53984)[0m top5: 0.8768656716417911
[2m[36m(func pid=53984)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=53984)[0m f1_macro: 0.3329257410502116
[2m[36m(func pid=53984)[0m f1_weighted: 0.3847473559757506
[2m[36m(func pid=53984)[0m f1_per_class: [0.424, 0.408, 0.571, 0.509, 0.073, 0.191, 0.38, 0.248, 0.209, 0.317]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0525 | Steps: 2 | Val loss: 24.6724 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=48412)[0m top1: 0.1609141791044776
[2m[36m(func pid=48412)[0m top5: 0.6273320895522388
[2m[36m(func pid=48412)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=48412)[0m f1_macro: 0.13881781148772032
[2m[36m(func pid=48412)[0m f1_weighted: 0.17524012274620415
[2m[36m(func pid=48412)[0m f1_per_class: [0.146, 0.216, 0.215, 0.233, 0.012, 0.092, 0.156, 0.123, 0.108, 0.087]
[2m[36m(func pid=48412)[0m 
== Status ==
Current time: 2024-01-07 01:10:33 (running for 00:27:25.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.175 |      0.139 |                   71 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.473 |      0.304 |                   50 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.026 |      0.333 |                   47 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.001 |      0.267 |                   24 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.332089552238806
[2m[36m(func pid=53465)[0m top5: 0.875
[2m[36m(func pid=53465)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=53465)[0m f1_macro: 0.30422950693533074
[2m[36m(func pid=53465)[0m f1_weighted: 0.3475484432337091
[2m[36m(func pid=53465)[0m f1_per_class: [0.383, 0.339, 0.55, 0.515, 0.076, 0.171, 0.303, 0.244, 0.21, 0.252]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.26259328358208955
[2m[36m(func pid=59563)[0m top5: 0.6637126865671642
[2m[36m(func pid=59563)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=59563)[0m f1_macro: 0.26300692132106646
[2m[36m(func pid=59563)[0m f1_weighted: 0.2002378796123552
[2m[36m(func pid=59563)[0m f1_per_class: [0.416, 0.388, 0.471, 0.003, 0.072, 0.116, 0.267, 0.286, 0.188, 0.423]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0285 | Steps: 2 | Val loss: 1.9363 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2100 | Steps: 2 | Val loss: 2.2439 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4993 | Steps: 2 | Val loss: 1.8069 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=48412)[0m top1: 0.1609141791044776
[2m[36m(func pid=48412)[0m top5: 0.6301305970149254
[2m[36m(func pid=48412)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=48412)[0m f1_macro: 0.13866240844054262
[2m[36m(func pid=48412)[0m f1_weighted: 0.17550074439995184
[2m[36m(func pid=48412)[0m f1_per_class: [0.141, 0.213, 0.215, 0.236, 0.012, 0.09, 0.156, 0.122, 0.116, 0.085]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3614738805970149
[2m[36m(func pid=53984)[0m top5: 0.8805970149253731
[2m[36m(func pid=53984)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=53984)[0m f1_macro: 0.3321585469649476
[2m[36m(func pid=53984)[0m f1_weighted: 0.386317583824671
[2m[36m(func pid=53984)[0m f1_per_class: [0.426, 0.415, 0.558, 0.511, 0.074, 0.201, 0.376, 0.247, 0.208, 0.305]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0516 | Steps: 2 | Val loss: 23.9804 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=53465)[0m top1: 0.33675373134328357
[2m[36m(func pid=53465)[0m top5: 0.8736007462686567
[2m[36m(func pid=53465)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=53465)[0m f1_macro: 0.30573121792354263
[2m[36m(func pid=53465)[0m f1_weighted: 0.35379823297912144
[2m[36m(func pid=53465)[0m f1_per_class: [0.389, 0.363, 0.55, 0.513, 0.076, 0.16, 0.318, 0.235, 0.214, 0.241]
[2m[36m(func pid=53465)[0m 
== Status ==
Current time: 2024-01-07 01:10:39 (running for 00:27:31.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.21  |      0.139 |                   72 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.499 |      0.306 |                   51 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.029 |      0.332 |                   48 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.052 |      0.263 |                   25 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2728544776119403
[2m[36m(func pid=59563)[0m top5: 0.6637126865671642
[2m[36m(func pid=59563)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=59563)[0m f1_macro: 0.2556264554448219
[2m[36m(func pid=59563)[0m f1_weighted: 0.2275667098030828
[2m[36m(func pid=59563)[0m f1_per_class: [0.344, 0.406, 0.471, 0.007, 0.061, 0.078, 0.38, 0.233, 0.149, 0.429]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1593 | Steps: 2 | Val loss: 2.2435 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0234 | Steps: 2 | Val loss: 1.9573 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4296 | Steps: 2 | Val loss: 1.8152 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=48412)[0m top1: 0.166044776119403
[2m[36m(func pid=48412)[0m top5: 0.6291977611940298
[2m[36m(func pid=48412)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=48412)[0m f1_macro: 0.1423137817808917
[2m[36m(func pid=48412)[0m f1_weighted: 0.18306492544875527
[2m[36m(func pid=48412)[0m f1_per_class: [0.144, 0.226, 0.215, 0.235, 0.011, 0.098, 0.171, 0.123, 0.117, 0.082]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3572761194029851
[2m[36m(func pid=53984)[0m top5: 0.8791977611940298
[2m[36m(func pid=53984)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=53984)[0m f1_macro: 0.33270704056894584
[2m[36m(func pid=53984)[0m f1_weighted: 0.38031653634743096
[2m[36m(func pid=53984)[0m f1_per_class: [0.435, 0.411, 0.558, 0.513, 0.071, 0.209, 0.349, 0.264, 0.204, 0.312]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0001 | Steps: 2 | Val loss: 23.4735 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:10:44 (running for 00:27:36.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.159 |      0.142 |                   73 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.43  |      0.304 |                   52 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.023 |      0.333 |                   49 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.052 |      0.256 |                   26 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.33115671641791045
[2m[36m(func pid=53465)[0m top5: 0.8680037313432836
[2m[36m(func pid=53465)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=53465)[0m f1_macro: 0.30430346985300816
[2m[36m(func pid=53465)[0m f1_weighted: 0.35038111626911195
[2m[36m(func pid=53465)[0m f1_per_class: [0.395, 0.355, 0.55, 0.499, 0.07, 0.166, 0.321, 0.237, 0.217, 0.234]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2737873134328358
[2m[36m(func pid=59563)[0m top5: 0.6665111940298507
[2m[36m(func pid=59563)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=59563)[0m f1_macro: 0.24399617619315794
[2m[36m(func pid=59563)[0m f1_weighted: 0.24795986929967348
[2m[36m(func pid=59563)[0m f1_per_class: [0.266, 0.413, 0.444, 0.017, 0.05, 0.065, 0.454, 0.209, 0.129, 0.393]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1045 | Steps: 2 | Val loss: 2.2391 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0189 | Steps: 2 | Val loss: 1.9740 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4391 | Steps: 2 | Val loss: 1.8190 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0003 | Steps: 2 | Val loss: 23.3757 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=48412)[0m top1: 0.1646455223880597
[2m[36m(func pid=48412)[0m top5: 0.6296641791044776
[2m[36m(func pid=48412)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=48412)[0m f1_macro: 0.1424078886424115
[2m[36m(func pid=48412)[0m f1_weighted: 0.181756705495005
[2m[36m(func pid=48412)[0m f1_per_class: [0.138, 0.22, 0.23, 0.233, 0.011, 0.098, 0.172, 0.122, 0.117, 0.082]
[2m[36m(func pid=48412)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35401119402985076
[2m[36m(func pid=53984)[0m top5: 0.8791977611940298
[2m[36m(func pid=53984)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=53984)[0m f1_macro: 0.3351835748222196
[2m[36m(func pid=53984)[0m f1_weighted: 0.37560325546667667
[2m[36m(func pid=53984)[0m f1_per_class: [0.44, 0.412, 0.571, 0.518, 0.069, 0.221, 0.323, 0.263, 0.207, 0.327]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:10:49 (running for 00:27:41.75)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.35824999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00012 | RUNNING    | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.104 |      0.142 |                   74 |
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.439 |      0.301 |                   53 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.019 |      0.335 |                   50 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.244 |                   27 |
| train_57e67_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3260261194029851
[2m[36m(func pid=53465)[0m top5: 0.8680037313432836
[2m[36m(func pid=53465)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=53465)[0m f1_macro: 0.3014526876511737
[2m[36m(func pid=53465)[0m f1_weighted: 0.34601067169943117
[2m[36m(func pid=53465)[0m f1_per_class: [0.402, 0.342, 0.55, 0.492, 0.071, 0.161, 0.322, 0.237, 0.208, 0.229]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.27098880597014924
[2m[36m(func pid=59563)[0m top5: 0.667910447761194
[2m[36m(func pid=59563)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=59563)[0m f1_macro: 0.2284510301050771
[2m[36m(func pid=59563)[0m f1_weighted: 0.2606645595254943
[2m[36m(func pid=59563)[0m f1_per_class: [0.23, 0.397, 0.353, 0.02, 0.046, 0.066, 0.518, 0.162, 0.124, 0.369]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=48412)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1074 | Steps: 2 | Val loss: 2.2363 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0282 | Steps: 2 | Val loss: 1.9861 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4478 | Steps: 2 | Val loss: 1.8215 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=48412)[0m top1: 0.166044776119403
[2m[36m(func pid=48412)[0m top5: 0.6352611940298507
[2m[36m(func pid=48412)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=48412)[0m f1_macro: 0.14271416863203304
[2m[36m(func pid=48412)[0m f1_weighted: 0.18334723253448224
[2m[36m(func pid=48412)[0m f1_per_class: [0.142, 0.22, 0.222, 0.234, 0.011, 0.098, 0.177, 0.125, 0.113, 0.084]
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5105 | Steps: 2 | Val loss: 22.3590 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=53984)[0m top1: 0.3493470149253731
[2m[36m(func pid=53984)[0m top5: 0.8791977611940298
[2m[36m(func pid=53984)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=53984)[0m f1_macro: 0.33304583673603383
[2m[36m(func pid=53984)[0m f1_weighted: 0.37207028859383484
[2m[36m(func pid=53984)[0m f1_per_class: [0.434, 0.419, 0.571, 0.511, 0.077, 0.221, 0.317, 0.251, 0.197, 0.333]
[2m[36m(func pid=53465)[0m top1: 0.3246268656716418
[2m[36m(func pid=53465)[0m top5: 0.8661380597014925
[2m[36m(func pid=53465)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=53465)[0m f1_macro: 0.30184188598152545
[2m[36m(func pid=53465)[0m f1_weighted: 0.34573875862387315
[2m[36m(func pid=53465)[0m f1_per_class: [0.402, 0.357, 0.55, 0.486, 0.071, 0.164, 0.32, 0.23, 0.204, 0.235]
[2m[36m(func pid=59563)[0m top1: 0.28171641791044777
[2m[36m(func pid=59563)[0m top5: 0.6828358208955224
[2m[36m(func pid=59563)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=59563)[0m f1_macro: 0.23581029298303316
[2m[36m(func pid=59563)[0m f1_weighted: 0.270491480599412
[2m[36m(func pid=59563)[0m f1_per_class: [0.19, 0.386, 0.444, 0.03, 0.055, 0.075, 0.55, 0.122, 0.155, 0.351]
== Status ==
Current time: 2024-01-07 01:10:54 (running for 00:27:46.80)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.439 |      0.301 |                   53 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.019 |      0.335 |                   50 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.228 |                   28 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 01:11:03 (running for 00:27:55.41)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.439 |      0.301 |                   53 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.019 |      0.335 |                   50 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.511 |      0.236 |                   29 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=66105)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=66105)[0m Configuration completed!
[2m[36m(func pid=66105)[0m New optimizer parameters:
[2m[36m(func pid=66105)[0m SGD (
[2m[36m(func pid=66105)[0m Parameter Group 0
[2m[36m(func pid=66105)[0m     dampening: 0
[2m[36m(func pid=66105)[0m     differentiable: False
[2m[36m(func pid=66105)[0m     foreach: None
[2m[36m(func pid=66105)[0m     lr: 0.0001
[2m[36m(func pid=66105)[0m     maximize: False
[2m[36m(func pid=66105)[0m     momentum: 0.99
[2m[36m(func pid=66105)[0m     nesterov: False
[2m[36m(func pid=66105)[0m     weight_decay: 1e-05
[2m[36m(func pid=66105)[0m )
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4373 | Steps: 2 | Val loss: 1.8272 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0177 | Steps: 2 | Val loss: 2.0056 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0323 | Steps: 2 | Val loss: 20.7934 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1463 | Steps: 2 | Val loss: 2.5149 | Batch size: 32 | lr: 0.0001 | Duration: 4.53s
== Status ==
Current time: 2024-01-07 01:11:08 (running for 00:28:00.42)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.448 |      0.302 |                   54 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.028 |      0.333 |                   51 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.511 |      0.236 |                   29 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3278917910447761
[2m[36m(func pid=53465)[0m top5: 0.8661380597014925
[2m[36m(func pid=53465)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=53465)[0m f1_macro: 0.3079808289706564
[2m[36m(func pid=53465)[0m f1_weighted: 0.3487093910009229
[2m[36m(func pid=53465)[0m f1_per_class: [0.427, 0.364, 0.55, 0.485, 0.07, 0.166, 0.318, 0.247, 0.213, 0.239]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.28638059701492535
[2m[36m(func pid=59563)[0m top5: 0.6982276119402985
[2m[36m(func pid=59563)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=59563)[0m f1_macro: 0.26959050921894223
[2m[36m(func pid=59563)[0m f1_weighted: 0.28111089220347857
[2m[36m(func pid=59563)[0m f1_per_class: [0.146, 0.381, 0.727, 0.045, 0.072, 0.136, 0.546, 0.123, 0.157, 0.361]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3474813432835821
[2m[36m(func pid=53984)[0m top5: 0.8782649253731343
[2m[36m(func pid=53984)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=53984)[0m f1_macro: 0.33322460530592257
[2m[36m(func pid=53984)[0m f1_weighted: 0.36877353320584083
[2m[36m(func pid=53984)[0m f1_per_class: [0.438, 0.421, 0.571, 0.509, 0.07, 0.216, 0.307, 0.249, 0.199, 0.351]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06716417910447761
[2m[36m(func pid=66105)[0m top5: 0.4864738805970149
[2m[36m(func pid=66105)[0m f1_micro: 0.06716417910447761
[2m[36m(func pid=66105)[0m f1_macro: 0.04170136951806362
[2m[36m(func pid=66105)[0m f1_weighted: 0.03718248646989157
[2m[36m(func pid=66105)[0m f1_per_class: [0.14, 0.01, 0.0, 0.083, 0.0, 0.019, 0.0, 0.106, 0.022, 0.037]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4507 | Steps: 2 | Val loss: 1.8243 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0015 | Steps: 2 | Val loss: 19.8617 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0266 | Steps: 2 | Val loss: 2.0225 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1158 | Steps: 2 | Val loss: 2.5361 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=59563)[0m top1: 0.2644589552238806
[2m[36m(func pid=59563)[0m top5: 0.7122201492537313
[2m[36m(func pid=59563)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=59563)[0m f1_macro: 0.27014208593768024
[2m[36m(func pid=59563)[0m f1_weighted: 0.2706331069679636
[2m[36m(func pid=59563)[0m f1_per_class: [0.125, 0.383, 0.783, 0.061, 0.091, 0.146, 0.491, 0.137, 0.148, 0.338]
[2m[36m(func pid=59563)[0m 
== Status ==
Current time: 2024-01-07 01:11:13 (running for 00:28:06.06)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.437 |      0.308 |                   55 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.333 |                   52 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.001 |      0.27  |                   31 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.146 |      0.042 |                    1 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3246268656716418
[2m[36m(func pid=53465)[0m top5: 0.8661380597014925
[2m[36m(func pid=53465)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=53465)[0m f1_macro: 0.30827724841273346
[2m[36m(func pid=53465)[0m f1_weighted: 0.3447888756509281
[2m[36m(func pid=53465)[0m f1_per_class: [0.426, 0.367, 0.55, 0.483, 0.069, 0.158, 0.308, 0.251, 0.205, 0.267]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34701492537313433
[2m[36m(func pid=53984)[0m top5: 0.8768656716417911
[2m[36m(func pid=53984)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=53984)[0m f1_macro: 0.33458931836456196
[2m[36m(func pid=53984)[0m f1_weighted: 0.3681442171377838
[2m[36m(func pid=53984)[0m f1_per_class: [0.428, 0.42, 0.6, 0.505, 0.07, 0.219, 0.306, 0.262, 0.205, 0.33]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06389925373134328
[2m[36m(func pid=66105)[0m top5: 0.4832089552238806
[2m[36m(func pid=66105)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=66105)[0m f1_macro: 0.036209447845362876
[2m[36m(func pid=66105)[0m f1_weighted: 0.03480726956206024
[2m[36m(func pid=66105)[0m f1_per_class: [0.08, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.103, 0.023, 0.048]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4250 | Steps: 2 | Val loss: 1.8337 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0001 | Steps: 2 | Val loss: 19.1907 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0295 | Steps: 2 | Val loss: 2.0385 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1634 | Steps: 2 | Val loss: 2.5515 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:11:18 (running for 00:28:11.19)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.425 |      0.306 |                   57 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.027 |      0.335 |                   53 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.001 |      0.27  |                   31 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.116 |      0.036 |                    2 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3218283582089552
[2m[36m(func pid=53465)[0m top5: 0.8652052238805971
[2m[36m(func pid=53465)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=53465)[0m f1_macro: 0.3057209925462007
[2m[36m(func pid=53465)[0m f1_weighted: 0.34058442566935626
[2m[36m(func pid=53465)[0m f1_per_class: [0.417, 0.36, 0.55, 0.487, 0.068, 0.157, 0.295, 0.249, 0.21, 0.264]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.25466417910447764
[2m[36m(func pid=59563)[0m top5: 0.7229477611940298
[2m[36m(func pid=59563)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=59563)[0m f1_macro: 0.28124581608393234
[2m[36m(func pid=59563)[0m f1_weighted: 0.2669491569677345
[2m[36m(func pid=59563)[0m f1_per_class: [0.119, 0.378, 0.88, 0.08, 0.104, 0.166, 0.449, 0.178, 0.12, 0.338]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.345615671641791
[2m[36m(func pid=53984)[0m top5: 0.8736007462686567
[2m[36m(func pid=53984)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=53984)[0m f1_macro: 0.3317760721164647
[2m[36m(func pid=53984)[0m f1_weighted: 0.3654886213308941
[2m[36m(func pid=53984)[0m f1_per_class: [0.433, 0.428, 0.585, 0.509, 0.068, 0.22, 0.29, 0.253, 0.211, 0.321]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06110074626865672
[2m[36m(func pid=66105)[0m top5: 0.47341417910447764
[2m[36m(func pid=66105)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=66105)[0m f1_macro: 0.030697194181278243
[2m[36m(func pid=66105)[0m f1_weighted: 0.03387903692324111
[2m[36m(func pid=66105)[0m f1_per_class: [0.055, 0.015, 0.0, 0.078, 0.0, 0.02, 0.0, 0.099, 0.0, 0.041]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3984 | Steps: 2 | Val loss: 1.8445 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0000 | Steps: 2 | Val loss: 19.0223 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0228 | Steps: 2 | Val loss: 2.0563 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1249 | Steps: 2 | Val loss: 2.5546 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 01:11:24 (running for 00:28:16.60)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.398 |      0.304 |                   58 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.029 |      0.332 |                   54 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.281 |                   32 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.163 |      0.031 |                    3 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.31763059701492535
[2m[36m(func pid=53465)[0m top5: 0.8614738805970149
[2m[36m(func pid=53465)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=53465)[0m f1_macro: 0.30430712286654055
[2m[36m(func pid=53465)[0m f1_weighted: 0.33465493764665577
[2m[36m(func pid=53465)[0m f1_per_class: [0.407, 0.359, 0.55, 0.485, 0.067, 0.162, 0.274, 0.26, 0.21, 0.269]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2416044776119403
[2m[36m(func pid=59563)[0m top5: 0.7318097014925373
[2m[36m(func pid=59563)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=59563)[0m f1_macro: 0.281508176346445
[2m[36m(func pid=59563)[0m f1_weighted: 0.2591749270651357
[2m[36m(func pid=59563)[0m f1_per_class: [0.111, 0.379, 0.88, 0.095, 0.122, 0.195, 0.396, 0.183, 0.129, 0.325]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.33861940298507465
[2m[36m(func pid=53984)[0m top5: 0.8736007462686567
[2m[36m(func pid=53984)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=53984)[0m f1_macro: 0.33032012998003457
[2m[36m(func pid=53984)[0m f1_weighted: 0.3589834266522584
[2m[36m(func pid=53984)[0m f1_per_class: [0.438, 0.426, 0.585, 0.503, 0.078, 0.197, 0.284, 0.25, 0.21, 0.333]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06063432835820896
[2m[36m(func pid=66105)[0m top5: 0.46968283582089554
[2m[36m(func pid=66105)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=66105)[0m f1_macro: 0.03407594316691895
[2m[36m(func pid=66105)[0m f1_weighted: 0.037141685491452636
[2m[36m(func pid=66105)[0m f1_per_class: [0.052, 0.018, 0.0, 0.085, 0.0, 0.02, 0.0, 0.097, 0.024, 0.044]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.3737 | Steps: 2 | Val loss: 1.8587 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0000 | Steps: 2 | Val loss: 18.8748 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0200 | Steps: 2 | Val loss: 2.0555 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0866 | Steps: 2 | Val loss: 2.5565 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:11:29 (running for 00:28:21.74)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.374 |      0.301 |                   59 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.023 |      0.33  |                   55 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   33 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.125 |      0.034 |                    4 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.314365671641791
[2m[36m(func pid=53465)[0m top5: 0.8582089552238806
[2m[36m(func pid=53465)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=53465)[0m f1_macro: 0.3009445285217418
[2m[36m(func pid=53465)[0m f1_weighted: 0.33158215158249543
[2m[36m(func pid=53465)[0m f1_per_class: [0.377, 0.365, 0.55, 0.482, 0.061, 0.157, 0.266, 0.265, 0.22, 0.267]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22621268656716417
[2m[36m(func pid=59563)[0m top5: 0.7402052238805971
[2m[36m(func pid=59563)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=59563)[0m f1_macro: 0.2763530081037168
[2m[36m(func pid=59563)[0m f1_weighted: 0.24434113138461702
[2m[36m(func pid=59563)[0m f1_per_class: [0.107, 0.362, 0.846, 0.103, 0.148, 0.198, 0.346, 0.189, 0.126, 0.338]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.33488805970149255
[2m[36m(func pid=53984)[0m top5: 0.8722014925373134
[2m[36m(func pid=53984)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=53984)[0m f1_macro: 0.32831332624924325
[2m[36m(func pid=53984)[0m f1_weighted: 0.35715104356457467
[2m[36m(func pid=53984)[0m f1_per_class: [0.439, 0.422, 0.585, 0.502, 0.071, 0.189, 0.285, 0.251, 0.203, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.0625
[2m[36m(func pid=66105)[0m top5: 0.466884328358209
[2m[36m(func pid=66105)[0m f1_micro: 0.0625
[2m[36m(func pid=66105)[0m f1_macro: 0.03796125027177964
[2m[36m(func pid=66105)[0m f1_weighted: 0.04508986965442291
[2m[36m(func pid=66105)[0m f1_per_class: [0.045, 0.06, 0.0, 0.089, 0.0, 0.019, 0.0, 0.092, 0.024, 0.049]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.3827 | Steps: 2 | Val loss: 1.8567 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0001 | Steps: 2 | Val loss: 18.5883 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0323 | Steps: 2 | Val loss: 2.0335 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0533 | Steps: 2 | Val loss: 2.5493 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:11:34 (running for 00:28:26.93)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.383 |      0.3   |                   60 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.02  |      0.328 |                   56 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.276 |                   34 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.087 |      0.038 |                    5 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.31156716417910446
[2m[36m(func pid=53465)[0m top5: 0.8572761194029851
[2m[36m(func pid=53465)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=53465)[0m f1_macro: 0.299548546339255
[2m[36m(func pid=53465)[0m f1_weighted: 0.3291640349711293
[2m[36m(func pid=53465)[0m f1_per_class: [0.367, 0.351, 0.55, 0.48, 0.061, 0.169, 0.264, 0.257, 0.224, 0.271]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22061567164179105
[2m[36m(func pid=59563)[0m top5: 0.7486007462686567
[2m[36m(func pid=59563)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=59563)[0m f1_macro: 0.2752441310541327
[2m[36m(func pid=59563)[0m f1_weighted: 0.2400945237873418
[2m[36m(func pid=59563)[0m f1_per_class: [0.105, 0.366, 0.846, 0.124, 0.137, 0.188, 0.311, 0.199, 0.139, 0.338]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.34328358208955223
[2m[36m(func pid=53984)[0m top5: 0.8763992537313433
[2m[36m(func pid=53984)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=53984)[0m f1_macro: 0.3384791362384306
[2m[36m(func pid=53984)[0m f1_weighted: 0.3678876359058831
[2m[36m(func pid=53984)[0m f1_per_class: [0.447, 0.412, 0.632, 0.51, 0.081, 0.201, 0.311, 0.261, 0.193, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.061567164179104475
[2m[36m(func pid=66105)[0m top5: 0.4552238805970149
[2m[36m(func pid=66105)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=66105)[0m f1_macro: 0.037672505924359556
[2m[36m(func pid=66105)[0m f1_weighted: 0.04605719847884528
[2m[36m(func pid=66105)[0m f1_per_class: [0.041, 0.065, 0.0, 0.09, 0.0, 0.018, 0.0, 0.094, 0.024, 0.043]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3829 | Steps: 2 | Val loss: 1.8672 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 18.7333 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0157 | Steps: 2 | Val loss: 2.0321 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0226 | Steps: 2 | Val loss: 2.5380 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:11:40 (running for 00:28:32.27)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.383 |      0.3   |                   61 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.032 |      0.338 |                   57 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.275 |                   35 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.053 |      0.038 |                    6 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3125
[2m[36m(func pid=53465)[0m top5: 0.8530783582089553
[2m[36m(func pid=53465)[0m f1_micro: 0.3125
[2m[36m(func pid=53465)[0m f1_macro: 0.3003623938487618
[2m[36m(func pid=53465)[0m f1_weighted: 0.3301695403102123
[2m[36m(func pid=53465)[0m f1_per_class: [0.366, 0.353, 0.55, 0.481, 0.061, 0.169, 0.265, 0.264, 0.213, 0.281]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2140858208955224
[2m[36m(func pid=59563)[0m top5: 0.7527985074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=59563)[0m f1_macro: 0.27496381098086636
[2m[36m(func pid=59563)[0m f1_weighted: 0.2360775025434381
[2m[36m(func pid=59563)[0m f1_per_class: [0.102, 0.351, 0.846, 0.144, 0.154, 0.197, 0.284, 0.196, 0.142, 0.333]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m top1: 0.345615671641791
[2m[36m(func pid=53984)[0m top5: 0.875
[2m[36m(func pid=53984)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=53984)[0m f1_macro: 0.3406742840058624
[2m[36m(func pid=53984)[0m f1_weighted: 0.3712576810474189
[2m[36m(func pid=53984)[0m f1_per_class: [0.429, 0.416, 0.649, 0.511, 0.078, 0.199, 0.32, 0.259, 0.202, 0.344]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06669776119402986
[2m[36m(func pid=66105)[0m top5: 0.45009328358208955
[2m[36m(func pid=66105)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=66105)[0m f1_macro: 0.04456909538717232
[2m[36m(func pid=66105)[0m f1_weighted: 0.0568765969980559
[2m[36m(func pid=66105)[0m f1_per_class: [0.049, 0.078, 0.0, 0.112, 0.0, 0.024, 0.003, 0.095, 0.045, 0.039]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3780 | Steps: 2 | Val loss: 1.8702 | Batch size: 32 | lr: 0.001 | Duration: 2.66s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0242 | Steps: 2 | Val loss: 18.5646 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0289 | Steps: 2 | Val loss: 2.0336 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 01:11:45 (running for 00:28:37.33)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.383 |      0.3   |                   61 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.016 |      0.341 |                   58 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.024 |      0.273 |                   37 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.023 |      0.045 |                    7 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2150186567164179
[2m[36m(func pid=59563)[0m top5: 0.7551305970149254
[2m[36m(func pid=59563)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=59563)[0m f1_macro: 0.2734838981620113
[2m[36m(func pid=59563)[0m f1_weighted: 0.2328138770780656
[2m[36m(func pid=59563)[0m f1_per_class: [0.108, 0.367, 0.846, 0.15, 0.152, 0.181, 0.262, 0.205, 0.159, 0.306]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.3162313432835821
[2m[36m(func pid=53465)[0m top5: 0.8512126865671642
[2m[36m(func pid=53465)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=53465)[0m f1_macro: 0.3026710686655066
[2m[36m(func pid=53465)[0m f1_weighted: 0.3327252420353786
[2m[36m(func pid=53465)[0m f1_per_class: [0.356, 0.378, 0.55, 0.482, 0.059, 0.179, 0.253, 0.268, 0.226, 0.276]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0270 | Steps: 2 | Val loss: 2.5244 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=53984)[0m top1: 0.3474813432835821
[2m[36m(func pid=53984)[0m top5: 0.8726679104477612
[2m[36m(func pid=53984)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=53984)[0m f1_macro: 0.3424701738575246
[2m[36m(func pid=53984)[0m f1_weighted: 0.37456376809258324
[2m[36m(func pid=53984)[0m f1_per_class: [0.433, 0.409, 0.667, 0.514, 0.081, 0.199, 0.333, 0.256, 0.2, 0.333]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06623134328358209
[2m[36m(func pid=66105)[0m top5: 0.439365671641791
[2m[36m(func pid=66105)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=66105)[0m f1_macro: 0.04625222603857068
[2m[36m(func pid=66105)[0m f1_weighted: 0.06224403483041599
[2m[36m(func pid=66105)[0m f1_per_class: [0.041, 0.093, 0.0, 0.124, 0.0, 0.022, 0.003, 0.091, 0.058, 0.031]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4016 | Steps: 2 | Val loss: 1.8570 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 18.2294 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:11:50 (running for 00:28:42.45)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.378 |      0.303 |                   62 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.029 |      0.342 |                   59 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   38 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  3.027 |      0.046 |                    8 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2196828358208955
[2m[36m(func pid=59563)[0m top5: 0.7532649253731343
[2m[36m(func pid=59563)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=59563)[0m f1_macro: 0.27661172809415924
[2m[36m(func pid=59563)[0m f1_weighted: 0.231814626432536
[2m[36m(func pid=59563)[0m f1_per_class: [0.125, 0.37, 0.815, 0.164, 0.171, 0.181, 0.241, 0.198, 0.188, 0.315]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0182 | Steps: 2 | Val loss: 1.9903 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=53465)[0m top1: 0.31949626865671643
[2m[36m(func pid=53465)[0m top5: 0.8582089552238806
[2m[36m(func pid=53465)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=53465)[0m f1_macro: 0.30862274609967
[2m[36m(func pid=53465)[0m f1_weighted: 0.33768512716423693
[2m[36m(func pid=53465)[0m f1_per_class: [0.365, 0.377, 0.564, 0.481, 0.059, 0.193, 0.264, 0.271, 0.223, 0.288]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9178 | Steps: 2 | Val loss: 2.5131 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=53984)[0m top1: 0.3582089552238806
[2m[36m(func pid=53984)[0m top5: 0.8773320895522388
[2m[36m(func pid=53984)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=53984)[0m f1_macro: 0.3450098450493762
[2m[36m(func pid=53984)[0m f1_weighted: 0.38840604620731667
[2m[36m(func pid=53984)[0m f1_per_class: [0.416, 0.406, 0.667, 0.518, 0.081, 0.207, 0.377, 0.245, 0.203, 0.33]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06623134328358209
[2m[36m(func pid=66105)[0m top5: 0.42677238805970147
[2m[36m(func pid=66105)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=66105)[0m f1_macro: 0.049934285040812446
[2m[36m(func pid=66105)[0m f1_weighted: 0.06637709816107318
[2m[36m(func pid=66105)[0m f1_per_class: [0.049, 0.099, 0.0, 0.123, 0.0, 0.04, 0.006, 0.087, 0.07, 0.025]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.8919 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3845 | Steps: 2 | Val loss: 1.8446 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:11:55 (running for 00:28:47.86)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.402 |      0.309 |                   63 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.345 |                   60 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.278 |                   39 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.918 |      0.05  |                    9 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0155 | Steps: 2 | Val loss: 1.9591 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=59563)[0m top1: 0.22201492537313433
[2m[36m(func pid=59563)[0m top5: 0.7541977611940298
[2m[36m(func pid=59563)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=59563)[0m f1_macro: 0.27767667051580813
[2m[36m(func pid=59563)[0m f1_weighted: 0.23207830606711402
[2m[36m(func pid=59563)[0m f1_per_class: [0.142, 0.382, 0.815, 0.166, 0.158, 0.186, 0.231, 0.184, 0.19, 0.322]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8707 | Steps: 2 | Val loss: 2.4979 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=53465)[0m top1: 0.3246268656716418
[2m[36m(func pid=53465)[0m top5: 0.8605410447761194
[2m[36m(func pid=53465)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=53465)[0m f1_macro: 0.3100899938892307
[2m[36m(func pid=53465)[0m f1_weighted: 0.34376459903016726
[2m[36m(func pid=53465)[0m f1_per_class: [0.374, 0.388, 0.564, 0.481, 0.062, 0.195, 0.278, 0.265, 0.224, 0.269]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m top1: 0.37080223880597013
[2m[36m(func pid=53984)[0m top5: 0.8777985074626866
[2m[36m(func pid=53984)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=53984)[0m f1_macro: 0.35088610135754
[2m[36m(func pid=53984)[0m f1_weighted: 0.4013757040084028
[2m[36m(func pid=53984)[0m f1_per_class: [0.407, 0.411, 0.667, 0.523, 0.085, 0.206, 0.411, 0.259, 0.207, 0.333]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06576492537313433
[2m[36m(func pid=66105)[0m top5: 0.42024253731343286
[2m[36m(func pid=66105)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=66105)[0m f1_macro: 0.049824236351498324
[2m[36m(func pid=66105)[0m f1_weighted: 0.06979650804429331
[2m[36m(func pid=66105)[0m f1_per_class: [0.05, 0.1, 0.0, 0.135, 0.0, 0.049, 0.006, 0.073, 0.059, 0.026]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.5689 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3579 | Steps: 2 | Val loss: 1.8416 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:12:00 (running for 00:28:52.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.385 |      0.31  |                   64 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.016 |      0.351 |                   61 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.278 |                   40 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.871 |      0.05  |                   10 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.22574626865671643
[2m[36m(func pid=59563)[0m top5: 0.753731343283582
[2m[36m(func pid=59563)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=59563)[0m f1_macro: 0.27845574780445803
[2m[36m(func pid=59563)[0m f1_weighted: 0.23537385959620005
[2m[36m(func pid=59563)[0m f1_per_class: [0.155, 0.385, 0.786, 0.174, 0.175, 0.185, 0.234, 0.181, 0.188, 0.322]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.324160447761194
[2m[36m(func pid=53465)[0m top5: 0.8614738805970149
[2m[36m(func pid=53465)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=53465)[0m f1_macro: 0.30906767637721055
[2m[36m(func pid=53465)[0m f1_weighted: 0.34384125302987256
[2m[36m(func pid=53465)[0m f1_per_class: [0.367, 0.388, 0.564, 0.485, 0.061, 0.198, 0.276, 0.26, 0.225, 0.267]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0154 | Steps: 2 | Val loss: 1.9456 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8703 | Steps: 2 | Val loss: 2.4839 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.5175 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=66105)[0m top1: 0.06763059701492537
[2m[36m(func pid=66105)[0m top5: 0.4099813432835821
[2m[36m(func pid=66105)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=66105)[0m f1_macro: 0.05241085771573395
[2m[36m(func pid=66105)[0m f1_weighted: 0.07246293666979149
[2m[36m(func pid=66105)[0m f1_per_class: [0.043, 0.111, 0.0, 0.137, 0.0, 0.043, 0.006, 0.078, 0.078, 0.027]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.37919776119402987
[2m[36m(func pid=53984)[0m top5: 0.878731343283582
[2m[36m(func pid=53984)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=53984)[0m f1_macro: 0.3545548431964355
[2m[36m(func pid=53984)[0m f1_weighted: 0.4104124756520184
[2m[36m(func pid=53984)[0m f1_per_class: [0.402, 0.411, 0.667, 0.527, 0.085, 0.207, 0.438, 0.258, 0.206, 0.345]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3980 | Steps: 2 | Val loss: 1.8257 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=59563)[0m top1: 0.22574626865671643
[2m[36m(func pid=59563)[0m top5: 0.7504664179104478
[2m[36m(func pid=59563)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=59563)[0m f1_macro: 0.27631361718133773
[2m[36m(func pid=59563)[0m f1_weighted: 0.23197588215789172
[2m[36m(func pid=59563)[0m f1_per_class: [0.172, 0.383, 0.786, 0.169, 0.159, 0.181, 0.229, 0.185, 0.189, 0.311]
[2m[36m(func pid=59563)[0m 
== Status ==
Current time: 2024-01-07 01:12:05 (running for 00:28:58.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.358 |      0.309 |                   65 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.015 |      0.355 |                   62 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.276 |                   41 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.87  |      0.052 |                   11 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.3278917910447761
[2m[36m(func pid=53465)[0m top5: 0.8689365671641791
[2m[36m(func pid=53465)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=53465)[0m f1_macro: 0.30844690983497797
[2m[36m(func pid=53465)[0m f1_weighted: 0.34643228851545743
[2m[36m(func pid=53465)[0m f1_per_class: [0.379, 0.38, 0.564, 0.498, 0.062, 0.195, 0.28, 0.253, 0.212, 0.262]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8383 | Steps: 2 | Val loss: 2.4711 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0175 | Steps: 2 | Val loss: 1.9511 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3715 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=66105)[0m top1: 0.06856343283582089
[2m[36m(func pid=66105)[0m top5: 0.41091417910447764
[2m[36m(func pid=66105)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=66105)[0m f1_macro: 0.05661528284170157
[2m[36m(func pid=66105)[0m f1_weighted: 0.07536754867669601
[2m[36m(func pid=66105)[0m f1_per_class: [0.04, 0.112, 0.027, 0.136, 0.0, 0.055, 0.012, 0.073, 0.086, 0.026]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3680 | Steps: 2 | Val loss: 1.8123 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=53984)[0m top1: 0.37453358208955223
[2m[36m(func pid=53984)[0m top5: 0.8815298507462687
[2m[36m(func pid=53984)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=53984)[0m f1_macro: 0.3481950482979427
[2m[36m(func pid=53984)[0m f1_weighted: 0.40532628632378176
[2m[36m(func pid=53984)[0m f1_per_class: [0.391, 0.403, 0.647, 0.522, 0.086, 0.205, 0.432, 0.256, 0.205, 0.333]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:12:11 (running for 00:29:03.36)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.398 |      0.308 |                   66 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.348 |                   63 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.28  |                   42 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.838 |      0.057 |                   12 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23041044776119404
[2m[36m(func pid=59563)[0m top5: 0.7541977611940298
[2m[36m(func pid=59563)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=59563)[0m f1_macro: 0.28019555645138083
[2m[36m(func pid=59563)[0m f1_weighted: 0.2383392725472787
[2m[36m(func pid=59563)[0m f1_per_class: [0.183, 0.393, 0.786, 0.179, 0.154, 0.174, 0.237, 0.181, 0.192, 0.323]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.33255597014925375
[2m[36m(func pid=53465)[0m top5: 0.8708022388059702
[2m[36m(func pid=53465)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=53465)[0m f1_macro: 0.30927822205822336
[2m[36m(func pid=53465)[0m f1_weighted: 0.3519148078841143
[2m[36m(func pid=53465)[0m f1_per_class: [0.383, 0.376, 0.564, 0.506, 0.064, 0.191, 0.295, 0.254, 0.216, 0.243]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8062 | Steps: 2 | Val loss: 2.4609 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0165 | Steps: 2 | Val loss: 1.9658 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3902 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=66105)[0m top1: 0.06902985074626866
[2m[36m(func pid=66105)[0m top5: 0.40951492537313433
[2m[36m(func pid=66105)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=66105)[0m f1_macro: 0.05585535416955736
[2m[36m(func pid=66105)[0m f1_weighted: 0.07774251834231703
[2m[36m(func pid=66105)[0m f1_per_class: [0.044, 0.124, 0.019, 0.116, 0.0, 0.061, 0.032, 0.061, 0.073, 0.027]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3223 | Steps: 2 | Val loss: 1.7899 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=53984)[0m top1: 0.36847014925373134
[2m[36m(func pid=53984)[0m top5: 0.8768656716417911
[2m[36m(func pid=53984)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=53984)[0m f1_macro: 0.34132401117387157
[2m[36m(func pid=53984)[0m f1_weighted: 0.3976708058285103
[2m[36m(func pid=53984)[0m f1_per_class: [0.375, 0.398, 0.629, 0.526, 0.087, 0.201, 0.409, 0.258, 0.201, 0.33]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:12:16 (running for 00:29:08.66)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.368 |      0.309 |                   67 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.016 |      0.341 |                   64 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.278 |                   43 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.806 |      0.056 |                   13 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.228544776119403
[2m[36m(func pid=59563)[0m top5: 0.753731343283582
[2m[36m(func pid=59563)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=59563)[0m f1_macro: 0.27806837833359527
[2m[36m(func pid=59563)[0m f1_weighted: 0.2358234595537174
[2m[36m(func pid=59563)[0m f1_per_class: [0.191, 0.389, 0.786, 0.174, 0.146, 0.175, 0.236, 0.179, 0.186, 0.319]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.34281716417910446
[2m[36m(func pid=53465)[0m top5: 0.8745335820895522
[2m[36m(func pid=53465)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=53465)[0m f1_macro: 0.3166492302812041
[2m[36m(func pid=53465)[0m f1_weighted: 0.36131607405663174
[2m[36m(func pid=53465)[0m f1_per_class: [0.383, 0.37, 0.564, 0.518, 0.067, 0.198, 0.314, 0.254, 0.223, 0.276]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0186 | Steps: 2 | Val loss: 1.9763 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7466 | Steps: 2 | Val loss: 2.4528 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3791 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=53984)[0m top1: 0.3656716417910448
[2m[36m(func pid=53984)[0m top5: 0.875
[2m[36m(func pid=53984)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=53984)[0m f1_macro: 0.34589803434918603
[2m[36m(func pid=53984)[0m f1_weighted: 0.39393156153891506
[2m[36m(func pid=53984)[0m f1_per_class: [0.369, 0.401, 0.688, 0.529, 0.092, 0.203, 0.392, 0.249, 0.2, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06529850746268656
[2m[36m(func pid=66105)[0m top5: 0.41091417910447764
[2m[36m(func pid=66105)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=66105)[0m f1_macro: 0.05123209430919062
[2m[36m(func pid=66105)[0m f1_weighted: 0.07467011607649608
[2m[36m(func pid=66105)[0m f1_per_class: [0.042, 0.124, 0.014, 0.106, 0.0, 0.066, 0.037, 0.027, 0.065, 0.032]
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3074 | Steps: 2 | Val loss: 1.7694 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=66105)[0m 
== Status ==
Current time: 2024-01-07 01:12:21 (running for 00:29:13.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.322 |      0.317 |                   68 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.019 |      0.346 |                   65 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.278 |                   44 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.747 |      0.051 |                   14 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2271455223880597
[2m[36m(func pid=59563)[0m top5: 0.7518656716417911
[2m[36m(func pid=59563)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=59563)[0m f1_macro: 0.27758595945927894
[2m[36m(func pid=59563)[0m f1_weighted: 0.2326529360243418
[2m[36m(func pid=59563)[0m f1_per_class: [0.194, 0.393, 0.786, 0.171, 0.136, 0.178, 0.224, 0.18, 0.188, 0.326]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.3512126865671642
[2m[36m(func pid=53465)[0m top5: 0.8791977611940298
[2m[36m(func pid=53465)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=53465)[0m f1_macro: 0.3201323298979574
[2m[36m(func pid=53465)[0m f1_weighted: 0.3678720855850298
[2m[36m(func pid=53465)[0m f1_per_class: [0.391, 0.379, 0.564, 0.525, 0.073, 0.198, 0.323, 0.248, 0.23, 0.269]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0150 | Steps: 2 | Val loss: 1.9724 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6979 | Steps: 2 | Val loss: 2.4410 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3508 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=53984)[0m top1: 0.3628731343283582
[2m[36m(func pid=53984)[0m top5: 0.8759328358208955
[2m[36m(func pid=53984)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=53984)[0m f1_macro: 0.34409822637413245
[2m[36m(func pid=53984)[0m f1_weighted: 0.39114300264525
[2m[36m(func pid=53984)[0m f1_per_class: [0.381, 0.399, 0.688, 0.536, 0.07, 0.194, 0.38, 0.249, 0.197, 0.348]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.06576492537313433
[2m[36m(func pid=66105)[0m top5: 0.41511194029850745
[2m[36m(func pid=66105)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=66105)[0m f1_macro: 0.05293096903882747
[2m[36m(func pid=66105)[0m f1_weighted: 0.07523296938831449
[2m[36m(func pid=66105)[0m f1_per_class: [0.039, 0.121, 0.034, 0.076, 0.0, 0.082, 0.065, 0.011, 0.078, 0.024]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2950 | Steps: 2 | Val loss: 1.7622 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=59563)[0m top1: 0.22901119402985073
[2m[36m(func pid=59563)[0m top5: 0.7527985074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=59563)[0m f1_macro: 0.27884564885517815
[2m[36m(func pid=59563)[0m f1_weighted: 0.23415366994171385
[2m[36m(func pid=59563)[0m f1_per_class: [0.206, 0.399, 0.759, 0.174, 0.139, 0.178, 0.222, 0.176, 0.191, 0.345]
[2m[36m(func pid=59563)[0m 
== Status ==
Current time: 2024-01-07 01:12:27 (running for 00:29:19.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.307 |      0.32  |                   69 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.015 |      0.344 |                   66 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.279 |                   45 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.698 |      0.053 |                   15 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.35634328358208955
[2m[36m(func pid=53465)[0m top5: 0.8824626865671642
[2m[36m(func pid=53465)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=53465)[0m f1_macro: 0.32312659539161676
[2m[36m(func pid=53465)[0m f1_weighted: 0.37325045960969744
[2m[36m(func pid=53465)[0m f1_per_class: [0.395, 0.379, 0.564, 0.529, 0.074, 0.207, 0.334, 0.251, 0.229, 0.269]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7475 | Steps: 2 | Val loss: 2.4327 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0176 | Steps: 2 | Val loss: 1.9884 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1743 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2925 | Steps: 2 | Val loss: 1.7630 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=66105)[0m top1: 0.07136194029850747
[2m[36m(func pid=66105)[0m top5: 0.4291044776119403
[2m[36m(func pid=66105)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=66105)[0m f1_macro: 0.057655143410605236
[2m[36m(func pid=66105)[0m f1_weighted: 0.08079162400996055
[2m[36m(func pid=66105)[0m f1_per_class: [0.049, 0.117, 0.04, 0.052, 0.0, 0.099, 0.1, 0.012, 0.078, 0.029]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.36427238805970147
[2m[36m(func pid=53984)[0m top5: 0.8722014925373134
[2m[36m(func pid=53984)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=53984)[0m f1_macro: 0.34699678557468855
[2m[36m(func pid=53984)[0m f1_weighted: 0.3926799043610746
[2m[36m(func pid=53984)[0m f1_per_class: [0.393, 0.406, 0.688, 0.533, 0.071, 0.196, 0.381, 0.247, 0.214, 0.34]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:12:32 (running for 00:29:25.17)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.295 |      0.323 |                   70 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.347 |                   67 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.28  |                   46 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.748 |      0.058 |                   16 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=53465)[0m top1: 0.353544776119403
[2m[36m(func pid=53465)[0m top5: 0.8791977611940298
[2m[36m(func pid=53465)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=53465)[0m f1_macro: 0.3236356609885055
[2m[36m(func pid=53465)[0m f1_weighted: 0.37052691265993865
[2m[36m(func pid=53465)[0m f1_per_class: [0.409, 0.374, 0.564, 0.528, 0.074, 0.2, 0.331, 0.245, 0.22, 0.291]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23180970149253732
[2m[36m(func pid=59563)[0m top5: 0.7527985074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=59563)[0m f1_macro: 0.28046031873501914
[2m[36m(func pid=59563)[0m f1_weighted: 0.23492913017867575
[2m[36m(func pid=59563)[0m f1_per_class: [0.222, 0.416, 0.759, 0.181, 0.132, 0.18, 0.206, 0.175, 0.192, 0.341]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6613 | Steps: 2 | Val loss: 2.4248 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0149 | Steps: 2 | Val loss: 2.0110 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.0442 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3392 | Steps: 2 | Val loss: 1.7634 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=66105)[0m top1: 0.0732276119402985
[2m[36m(func pid=66105)[0m top5: 0.45009328358208955
[2m[36m(func pid=66105)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=66105)[0m f1_macro: 0.05898298170287928
[2m[36m(func pid=66105)[0m f1_weighted: 0.08226122164592804
[2m[36m(func pid=66105)[0m f1_per_class: [0.054, 0.107, 0.045, 0.036, 0.0, 0.102, 0.125, 0.014, 0.071, 0.037]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35867537313432835
[2m[36m(func pid=53984)[0m top5: 0.8717350746268657
[2m[36m(func pid=53984)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=53984)[0m f1_macro: 0.34427943499793484
[2m[36m(func pid=53984)[0m f1_weighted: 0.3853658162628692
[2m[36m(func pid=53984)[0m f1_per_class: [0.398, 0.41, 0.667, 0.524, 0.073, 0.2, 0.361, 0.251, 0.209, 0.352]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:12:38 (running for 00:29:30.31)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.292 |      0.324 |                   71 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.015 |      0.344 |                   68 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.279 |                   47 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.661 |      0.059 |                   17 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2332089552238806
[2m[36m(func pid=59563)[0m top5: 0.7546641791044776
[2m[36m(func pid=59563)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=59563)[0m f1_macro: 0.27861603062782453
[2m[36m(func pid=59563)[0m f1_weighted: 0.23749853215535732
[2m[36m(func pid=59563)[0m f1_per_class: [0.229, 0.427, 0.733, 0.189, 0.131, 0.176, 0.205, 0.172, 0.183, 0.341]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.3521455223880597
[2m[36m(func pid=53465)[0m top5: 0.8791977611940298
[2m[36m(func pid=53465)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=53465)[0m f1_macro: 0.32226542073542186
[2m[36m(func pid=53465)[0m f1_weighted: 0.36976077424233755
[2m[36m(func pid=53465)[0m f1_per_class: [0.422, 0.37, 0.564, 0.528, 0.073, 0.2, 0.33, 0.246, 0.223, 0.266]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5872 | Steps: 2 | Val loss: 2.4145 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0130 | Steps: 2 | Val loss: 2.0317 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8717 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4220 | Steps: 2 | Val loss: 1.7693 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=66105)[0m top1: 0.08115671641791045
[2m[36m(func pid=66105)[0m top5: 0.45988805970149255
[2m[36m(func pid=66105)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=66105)[0m f1_macro: 0.0609876795098687
[2m[36m(func pid=66105)[0m f1_weighted: 0.0915932664685544
[2m[36m(func pid=66105)[0m f1_per_class: [0.067, 0.106, 0.039, 0.034, 0.0, 0.096, 0.163, 0.0, 0.074, 0.031]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3512126865671642
[2m[36m(func pid=53984)[0m top5: 0.8684701492537313
[2m[36m(func pid=53984)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=53984)[0m f1_macro: 0.3391521477857503
[2m[36m(func pid=53984)[0m f1_weighted: 0.37654069887730773
[2m[36m(func pid=53984)[0m f1_per_class: [0.404, 0.41, 0.629, 0.512, 0.075, 0.202, 0.342, 0.244, 0.218, 0.356]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:12:43 (running for 00:29:35.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.339 |      0.322 |                   72 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.013 |      0.339 |                   69 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   48 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.587 |      0.061 |                   18 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23600746268656717
[2m[36m(func pid=59563)[0m top5: 0.7588619402985075
[2m[36m(func pid=59563)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=59563)[0m f1_macro: 0.28228776900207236
[2m[36m(func pid=59563)[0m f1_weighted: 0.2413810482706296
[2m[36m(func pid=59563)[0m f1_per_class: [0.237, 0.429, 0.733, 0.192, 0.15, 0.177, 0.212, 0.171, 0.188, 0.333]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.353544776119403
[2m[36m(func pid=53465)[0m top5: 0.8740671641791045
[2m[36m(func pid=53465)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=53465)[0m f1_macro: 0.32291129726809087
[2m[36m(func pid=53465)[0m f1_weighted: 0.371855373252092
[2m[36m(func pid=53465)[0m f1_per_class: [0.407, 0.376, 0.564, 0.519, 0.075, 0.214, 0.337, 0.25, 0.222, 0.264]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5718 | Steps: 2 | Val loss: 2.4061 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0184 | Steps: 2 | Val loss: 2.0422 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9307 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3058 | Steps: 2 | Val loss: 1.7735 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=53984)[0m top1: 0.34794776119402987
[2m[36m(func pid=53984)[0m top5: 0.867070895522388
[2m[36m(func pid=53984)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=53984)[0m f1_macro: 0.33779307469548464
[2m[36m(func pid=53984)[0m f1_weighted: 0.3714817836791494
[2m[36m(func pid=53984)[0m f1_per_class: [0.398, 0.409, 0.647, 0.511, 0.078, 0.206, 0.325, 0.247, 0.22, 0.337]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.08955223880597014
[2m[36m(func pid=66105)[0m top5: 0.46875
[2m[36m(func pid=66105)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=66105)[0m f1_macro: 0.06435892806041875
[2m[36m(func pid=66105)[0m f1_weighted: 0.09908193127802929
[2m[36m(func pid=66105)[0m f1_per_class: [0.067, 0.107, 0.038, 0.025, 0.0, 0.114, 0.189, 0.0, 0.067, 0.036]
[2m[36m(func pid=66105)[0m 
== Status ==
Current time: 2024-01-07 01:12:48 (running for 00:29:40.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.422 |      0.323 |                   73 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.338 |                   70 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   49 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.572 |      0.064 |                   19 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2355410447761194
[2m[36m(func pid=59563)[0m top5: 0.7597947761194029
[2m[36m(func pid=59563)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=59563)[0m f1_macro: 0.2816351870762623
[2m[36m(func pid=59563)[0m f1_weighted: 0.24013861211861307
[2m[36m(func pid=59563)[0m f1_per_class: [0.235, 0.426, 0.733, 0.189, 0.147, 0.176, 0.213, 0.173, 0.187, 0.337]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.34888059701492535
[2m[36m(func pid=53465)[0m top5: 0.875
[2m[36m(func pid=53465)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=53465)[0m f1_macro: 0.3216459899391171
[2m[36m(func pid=53465)[0m f1_weighted: 0.3696375643084588
[2m[36m(func pid=53465)[0m f1_per_class: [0.414, 0.365, 0.564, 0.517, 0.07, 0.205, 0.342, 0.243, 0.226, 0.27]
[2m[36m(func pid=53465)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5536 | Steps: 2 | Val loss: 2.3990 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0233 | Steps: 2 | Val loss: 2.0340 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8178 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=53465)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3104 | Steps: 2 | Val loss: 1.7773 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=66105)[0m top1: 0.09794776119402986
[2m[36m(func pid=66105)[0m top5: 0.480410447761194
[2m[36m(func pid=66105)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=66105)[0m f1_macro: 0.0687287087092004
[2m[36m(func pid=66105)[0m f1_weighted: 0.10569713607462582
[2m[36m(func pid=66105)[0m f1_per_class: [0.072, 0.105, 0.049, 0.022, 0.0, 0.114, 0.214, 0.0, 0.072, 0.038]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3512126865671642
[2m[36m(func pid=53984)[0m top5: 0.8675373134328358
[2m[36m(func pid=53984)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=53984)[0m f1_macro: 0.33872843054009677
[2m[36m(func pid=53984)[0m f1_weighted: 0.3747481115328838
[2m[36m(func pid=53984)[0m f1_per_class: [0.4, 0.417, 0.647, 0.509, 0.072, 0.217, 0.328, 0.252, 0.218, 0.327]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23694029850746268
[2m[36m(func pid=59563)[0m top5: 0.761660447761194== Status ==
Current time: 2024-01-07 01:12:53 (running for 00:29:46.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.355
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00013 | RUNNING    | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.306 |      0.322 |                   74 |
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.023 |      0.339 |                   71 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   50 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.554 |      0.069 |                   20 |
| train_57e67_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=59563)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=59563)[0m f1_macro: 0.2815865999321506
[2m[36m(func pid=59563)[0m f1_weighted: 0.24352614992902413
[2m[36m(func pid=59563)[0m f1_per_class: [0.249, 0.427, 0.733, 0.197, 0.132, 0.177, 0.217, 0.169, 0.181, 0.333]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=53465)[0m top1: 0.34421641791044777
[2m[36m(func pid=53465)[0m top5: 0.8796641791044776
[2m[36m(func pid=53465)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=53465)[0m f1_macro: 0.32204161757819255
[2m[36m(func pid=53465)[0m f1_weighted: 0.36633188789041427
[2m[36m(func pid=53465)[0m f1_per_class: [0.414, 0.368, 0.595, 0.51, 0.068, 0.221, 0.333, 0.228, 0.213, 0.269]
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0272 | Steps: 2 | Val loss: 2.0387 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5799 | Steps: 2 | Val loss: 2.3890 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8608 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=53984)[0m top1: 0.34794776119402987
[2m[36m(func pid=53984)[0m top5: 0.8684701492537313
[2m[36m(func pid=53984)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=53984)[0m f1_macro: 0.33572588355360017
[2m[36m(func pid=53984)[0m f1_weighted: 0.3708194111547429
[2m[36m(func pid=53984)[0m f1_per_class: [0.402, 0.408, 0.647, 0.513, 0.072, 0.208, 0.322, 0.251, 0.211, 0.323]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=66105)[0m top1: 0.10774253731343283
[2m[36m(func pid=66105)[0m top5: 0.4906716417910448
[2m[36m(func pid=66105)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=66105)[0m f1_macro: 0.0735964834853312
[2m[36m(func pid=66105)[0m f1_weighted: 0.10992536592737508
[2m[36m(func pid=66105)[0m f1_per_class: [0.071, 0.098, 0.069, 0.013, 0.007, 0.119, 0.238, 0.0, 0.078, 0.043]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23600746268656717
[2m[36m(func pid=59563)[0m top5: 0.761660447761194
[2m[36m(func pid=59563)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=59563)[0m f1_macro: 0.28263740154595396
[2m[36m(func pid=59563)[0m f1_weighted: 0.24307608169453168
[2m[36m(func pid=59563)[0m f1_per_class: [0.253, 0.428, 0.733, 0.195, 0.133, 0.176, 0.217, 0.166, 0.184, 0.341]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5369 | Steps: 2 | Val loss: 2.3762 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0247 | Steps: 2 | Val loss: 2.0175 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.8632 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:12:59 (running for 00:29:51.42)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.027 |      0.336 |                   72 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.283 |                   51 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.58  |      0.074 |                   21 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=70967)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=70967)[0m Configuration completed!
[2m[36m(func pid=70967)[0m New optimizer parameters:
[2m[36m(func pid=70967)[0m SGD (
[2m[36m(func pid=70967)[0m Parameter Group 0
[2m[36m(func pid=70967)[0m     dampening: 0
[2m[36m(func pid=70967)[0m     differentiable: False
[2m[36m(func pid=70967)[0m     foreach: None
[2m[36m(func pid=70967)[0m     lr: 0.001
[2m[36m(func pid=70967)[0m     maximize: False
[2m[36m(func pid=70967)[0m     momentum: 0.99
[2m[36m(func pid=70967)[0m     nesterov: False
[2m[36m(func pid=70967)[0m     weight_decay: 1e-05
[2m[36m(func pid=70967)[0m )
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m top1: 0.1226679104477612
[2m[36m(func pid=66105)[0m top5: 0.5013992537313433
[2m[36m(func pid=66105)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=66105)[0m f1_macro: 0.0788032595934725
[2m[36m(func pid=66105)[0m f1_weighted: 0.12201392100760387
[2m[36m(func pid=66105)[0m f1_per_class: [0.079, 0.091, 0.068, 0.013, 0.008, 0.117, 0.282, 0.0, 0.079, 0.051]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.3521455223880597
[2m[36m(func pid=53984)[0m top5: 0.8736007462686567
[2m[36m(func pid=53984)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=53984)[0m f1_macro: 0.3383411665859947
[2m[36m(func pid=53984)[0m f1_weighted: 0.37660727241050873
[2m[36m(func pid=53984)[0m f1_per_class: [0.428, 0.41, 0.611, 0.518, 0.072, 0.208, 0.334, 0.245, 0.209, 0.348]
[2m[36m(func pid=53984)[0m 
== Status ==
Current time: 2024-01-07 01:13:04 (running for 00:29:56.67)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.025 |      0.338 |                   73 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.281 |                   52 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.537 |      0.079 |                   22 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23647388059701493
[2m[36m(func pid=59563)[0m top5: 0.7607276119402985
[2m[36m(func pid=59563)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=59563)[0m f1_macro: 0.28077065505576526
[2m[36m(func pid=59563)[0m f1_weighted: 0.24115346412547425
[2m[36m(func pid=59563)[0m f1_per_class: [0.257, 0.436, 0.71, 0.192, 0.145, 0.176, 0.209, 0.167, 0.183, 0.333]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4945 | Steps: 2 | Val loss: 2.3648 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0186 | Steps: 2 | Val loss: 2.0297 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1790 | Steps: 2 | Val loss: 2.4961 | Batch size: 32 | lr: 0.001 | Duration: 4.44s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.7734 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=66105)[0m top1: 0.13759328358208955
[2m[36m(func pid=66105)[0m top5: 0.5149253731343284
[2m[36m(func pid=66105)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=66105)[0m f1_macro: 0.08561400962178814
[2m[36m(func pid=66105)[0m f1_weighted: 0.13243522600539867
[2m[36m(func pid=66105)[0m f1_per_class: [0.1, 0.093, 0.068, 0.013, 0.009, 0.12, 0.312, 0.0, 0.086, 0.055]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35401119402985076
[2m[36m(func pid=53984)[0m top5: 0.8717350746268657
[2m[36m(func pid=53984)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=53984)[0m f1_macro: 0.3368509854552123
[2m[36m(func pid=53984)[0m f1_weighted: 0.3780712113644545
[2m[36m(func pid=53984)[0m f1_per_class: [0.416, 0.407, 0.595, 0.522, 0.069, 0.211, 0.336, 0.254, 0.209, 0.352]
[2m[36m(func pid=53984)[0m 
[2m[36m(func pid=70967)[0m top1: 0.06669776119402986
[2m[36m(func pid=70967)[0m top5: 0.4808768656716418
[2m[36m(func pid=70967)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=70967)[0m f1_macro: 0.041825627684428754
[2m[36m(func pid=70967)[0m f1_weighted: 0.04080983218471407
[2m[36m(func pid=70967)[0m f1_per_class: [0.134, 0.01, 0.0, 0.098, 0.0, 0.017, 0.0, 0.102, 0.021, 0.035]
[2m[36m(func pid=70967)[0m 
== Status ==
Current time: 2024-01-07 01:13:09 (running for 00:30:01.77)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00014 | RUNNING    | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.019 |      0.337 |                   74 |
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.285 |                   53 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.494 |      0.086 |                   23 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  3.179 |      0.042 |                    1 |
| train_57e67_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23740671641791045
[2m[36m(func pid=59563)[0m top5: 0.7630597014925373
[2m[36m(func pid=59563)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=59563)[0m f1_macro: 0.28453649877040027
[2m[36m(func pid=59563)[0m f1_weighted: 0.2444525222869934
[2m[36m(func pid=59563)[0m f1_per_class: [0.267, 0.432, 0.71, 0.202, 0.148, 0.176, 0.211, 0.165, 0.18, 0.353]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4077 | Steps: 2 | Val loss: 2.3538 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=53984)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0180 | Steps: 2 | Val loss: 1.9958 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0807 | Steps: 2 | Val loss: 2.4676 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.7670 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=66105)[0m top1: 0.1501865671641791
[2m[36m(func pid=66105)[0m top5: 0.523320895522388
[2m[36m(func pid=66105)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=66105)[0m f1_macro: 0.09026231214735998
[2m[36m(func pid=66105)[0m f1_weighted: 0.14068522797621869
[2m[36m(func pid=66105)[0m f1_per_class: [0.107, 0.093, 0.068, 0.016, 0.01, 0.125, 0.333, 0.0, 0.099, 0.051]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=53984)[0m top1: 0.35774253731343286
[2m[36m(func pid=53984)[0m top5: 0.8768656716417911
[2m[36m(func pid=53984)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=53984)[0m f1_macro: 0.336817061769716
[2m[36m(func pid=53984)[0m f1_weighted: 0.38233756212846215
[2m[36m(func pid=53984)[0m f1_per_class: [0.419, 0.403, 0.595, 0.534, 0.081, 0.189, 0.351, 0.244, 0.213, 0.341]
[2m[36m(func pid=70967)[0m top1: 0.06576492537313433
[2m[36m(func pid=70967)[0m top5: 0.46222014925373134
[2m[36m(func pid=70967)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=70967)[0m f1_macro: 0.05020818771022031
[2m[36m(func pid=70967)[0m f1_weighted: 0.05065008441573981
[2m[36m(func pid=70967)[0m f1_per_class: [0.115, 0.049, 0.0, 0.105, 0.0, 0.024, 0.0, 0.093, 0.057, 0.059]
[2m[36m(func pid=59563)[0m top1: 0.23367537313432835
[2m[36m(func pid=59563)[0m top5: 0.7658582089552238
[2m[36m(func pid=59563)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=59563)[0m f1_macro: 0.28372999835574236
[2m[36m(func pid=59563)[0m f1_weighted: 0.24215463413594593
[2m[36m(func pid=59563)[0m f1_per_class: [0.266, 0.42, 0.733, 0.21, 0.14, 0.178, 0.204, 0.164, 0.178, 0.345]
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3857 | Steps: 2 | Val loss: 2.3420 | Batch size: 32 | lr: 0.0001 | Duration: 2.69s
[2m[36m(func pid=66105)[0m top1: 0.16184701492537312
[2m[36m(func pid=66105)[0m top5: 0.5387126865671642
[2m[36m(func pid=66105)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=66105)[0m f1_macro: 0.09526853420317227
[2m[36m(func pid=66105)[0m f1_weighted: 0.14705541595910554
[2m[36m(func pid=66105)[0m f1_per_class: [0.129, 0.094, 0.074, 0.016, 0.011, 0.126, 0.352, 0.0, 0.103, 0.048]
== Status ==
Current time: 2024-01-07 01:13:14 (running for 00:30:06.82)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.285 |                   53 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.408 |      0.09  |                   24 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  3.179 |      0.042 |                    1 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=72016)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=72016)[0m Configuration completed!
[2m[36m(func pid=72016)[0m New optimizer parameters:
[2m[36m(func pid=72016)[0m SGD (
[2m[36m(func pid=72016)[0m Parameter Group 0
[2m[36m(func pid=72016)[0m     dampening: 0
[2m[36m(func pid=72016)[0m     differentiable: False
[2m[36m(func pid=72016)[0m     foreach: None
[2m[36m(func pid=72016)[0m     lr: 0.01
[2m[36m(func pid=72016)[0m     maximize: False
[2m[36m(func pid=72016)[0m     momentum: 0.99
[2m[36m(func pid=72016)[0m     nesterov: False
[2m[36m(func pid=72016)[0m     weight_decay: 1e-05
[2m[36m(func pid=72016)[0m )
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:13:23 (running for 00:30:15.44)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.285 |                   53 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.386 |      0.095 |                   25 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  3.179 |      0.042 |                    1 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3035 | Steps: 2 | Val loss: 2.3268 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0206 | Steps: 2 | Val loss: 2.4444 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.6783 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0856 | Steps: 2 | Val loss: 2.4000 | Batch size: 32 | lr: 0.01 | Duration: 4.75s
== Status ==
Current time: 2024-01-07 01:13:28 (running for 00:30:20.46)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.284 |                   54 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.386 |      0.095 |                   25 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  3.081 |      0.05  |                    2 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.16791044776119404
[2m[36m(func pid=66105)[0m top5: 0.5550373134328358
[2m[36m(func pid=66105)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=66105)[0m f1_macro: 0.10124775088142127
[2m[36m(func pid=66105)[0m f1_weighted: 0.15161412960446932
[2m[36m(func pid=66105)[0m f1_per_class: [0.126, 0.094, 0.077, 0.019, 0.011, 0.129, 0.361, 0.0, 0.106, 0.089]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.05970149253731343
[2m[36m(func pid=70967)[0m top5: 0.4291044776119403
[2m[36m(func pid=70967)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=70967)[0m f1_macro: 0.05171221829828052
[2m[36m(func pid=70967)[0m f1_weighted: 0.05913529688051429
[2m[36m(func pid=70967)[0m f1_per_class: [0.082, 0.08, 0.0, 0.099, 0.0, 0.031, 0.015, 0.086, 0.081, 0.044]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2392723880597015
[2m[36m(func pid=59563)[0m top5: 0.7677238805970149
[2m[36m(func pid=59563)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=59563)[0m f1_macro: 0.2932955081102593
[2m[36m(func pid=59563)[0m f1_weighted: 0.24927518963095568
[2m[36m(func pid=59563)[0m f1_per_class: [0.273, 0.42, 0.786, 0.205, 0.137, 0.181, 0.228, 0.165, 0.194, 0.345]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m top1: 0.05783582089552239
[2m[36m(func pid=72016)[0m top5: 0.447294776119403
[2m[36m(func pid=72016)[0m f1_micro: 0.05783582089552239
[2m[36m(func pid=72016)[0m f1_macro: 0.06879071988155924
[2m[36m(func pid=72016)[0m f1_weighted: 0.058338357507252726
[2m[36m(func pid=72016)[0m f1_per_class: [0.113, 0.102, 0.091, 0.057, 0.009, 0.04, 0.029, 0.058, 0.148, 0.041]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3894 | Steps: 2 | Val loss: 2.3111 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.7508 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8634 | Steps: 2 | Val loss: 2.4153 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8048 | Steps: 2 | Val loss: 2.3617 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 01:13:33 (running for 00:30:25.94)
Memory usage on this node: 24.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.291 |                   56 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.303 |      0.101 |                   26 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  3.021 |      0.052 |                    3 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  3.086 |      0.069 |                    1 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23647388059701493
[2m[36m(func pid=59563)[0m top5: 0.7625932835820896
[2m[36m(func pid=59563)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=59563)[0m f1_macro: 0.2914493618441591
[2m[36m(func pid=59563)[0m f1_weighted: 0.24584410275225804
[2m[36m(func pid=59563)[0m f1_per_class: [0.268, 0.423, 0.786, 0.202, 0.136, 0.172, 0.221, 0.165, 0.185, 0.357]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.17397388059701493
[2m[36m(func pid=66105)[0m top5: 0.5666977611940298
[2m[36m(func pid=66105)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=66105)[0m f1_macro: 0.10554699640328383
[2m[36m(func pid=66105)[0m f1_weighted: 0.15586586485070386
[2m[36m(func pid=66105)[0m f1_per_class: [0.123, 0.096, 0.09, 0.023, 0.012, 0.125, 0.371, 0.0, 0.111, 0.104]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.057369402985074626
[2m[36m(func pid=70967)[0m top5: 0.44402985074626866
[2m[36m(func pid=70967)[0m f1_micro: 0.057369402985074626
[2m[36m(func pid=70967)[0m f1_macro: 0.06055163738297277
[2m[36m(func pid=70967)[0m f1_weighted: 0.06264083631275955
[2m[36m(func pid=70967)[0m f1_per_class: [0.107, 0.08, 0.04, 0.046, 0.006, 0.038, 0.07, 0.078, 0.106, 0.036]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.1357276119402985
[2m[36m(func pid=72016)[0m top5: 0.5634328358208955
[2m[36m(func pid=72016)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=72016)[0m f1_macro: 0.0939412610150036
[2m[36m(func pid=72016)[0m f1_weighted: 0.12071270196639663
[2m[36m(func pid=72016)[0m f1_per_class: [0.168, 0.139, 0.147, 0.0, 0.014, 0.047, 0.283, 0.0, 0.046, 0.095]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7405 | Steps: 2 | Val loss: 2.3956 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9948 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2352 | Steps: 2 | Val loss: 2.2919 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:13:39 (running for 00:30:31.28)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.291 |                   56 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.389 |      0.106 |                   27 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.74  |      0.076 |                    5 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  2.805 |      0.094 |                    2 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.10401119402985075
[2m[36m(func pid=70967)[0m top5: 0.48367537313432835
[2m[36m(func pid=70967)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=70967)[0m f1_macro: 0.07639336906865532
[2m[36m(func pid=70967)[0m f1_weighted: 0.10194287789928981
[2m[36m(func pid=70967)[0m f1_per_class: [0.106, 0.07, 0.105, 0.019, 0.013, 0.057, 0.238, 0.0, 0.121, 0.037]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m top1: 0.18190298507462688
[2m[36m(func pid=66105)[0m top5: 0.5834888059701493
[2m[36m(func pid=66105)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=66105)[0m f1_macro: 0.10810153584734963
[2m[36m(func pid=66105)[0m f1_weighted: 0.16145554078759494
[2m[36m(func pid=66105)[0m f1_per_class: [0.13, 0.101, 0.098, 0.029, 0.012, 0.122, 0.382, 0.0, 0.11, 0.096]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4158 | Steps: 2 | Val loss: 2.2319 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=59563)[0m top1: 0.23414179104477612
[2m[36m(func pid=59563)[0m top5: 0.7597947761194029
[2m[36m(func pid=59563)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=59563)[0m f1_macro: 0.2905051431162595
[2m[36m(func pid=59563)[0m f1_weighted: 0.24204897414216475
[2m[36m(func pid=59563)[0m f1_per_class: [0.263, 0.423, 0.786, 0.192, 0.137, 0.171, 0.218, 0.164, 0.194, 0.357]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m top1: 0.14085820895522388
[2m[36m(func pid=72016)[0m top5: 0.679570895522388
[2m[36m(func pid=72016)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=72016)[0m f1_macro: 0.13431696005598756
[2m[36m(func pid=72016)[0m f1_weighted: 0.1301493726329149
[2m[36m(func pid=72016)[0m f1_per_class: [0.293, 0.206, 0.297, 0.01, 0.051, 0.141, 0.212, 0.0, 0.133, 0.0]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.1968 | Steps: 2 | Val loss: 2.2701 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6557 | Steps: 2 | Val loss: 2.3732 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9881 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:13:44 (running for 00:30:36.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.291 |                   57 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.197 |      0.113 |                   29 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.74  |      0.076 |                    5 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  2.416 |      0.134 |                    3 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.1875
[2m[36m(func pid=66105)[0m top5: 0.6002798507462687
[2m[36m(func pid=66105)[0m f1_micro: 0.1875
[2m[36m(func pid=66105)[0m f1_macro: 0.11273769505337194
[2m[36m(func pid=66105)[0m f1_weighted: 0.16920087709171086
[2m[36m(func pid=66105)[0m f1_per_class: [0.131, 0.116, 0.106, 0.041, 0.025, 0.125, 0.387, 0.0, 0.106, 0.091]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.17537313432835822
[2m[36m(func pid=70967)[0m top5: 0.5214552238805971
[2m[36m(func pid=70967)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=70967)[0m f1_macro: 0.0984317223314135
[2m[36m(func pid=70967)[0m f1_weighted: 0.1292086698743152
[2m[36m(func pid=70967)[0m f1_per_class: [0.122, 0.034, 0.154, 0.007, 0.02, 0.071, 0.349, 0.0, 0.142, 0.085]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23507462686567165
[2m[36m(func pid=59563)[0m top5: 0.757929104477612
[2m[36m(func pid=59563)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=59563)[0m f1_macro: 0.2890367450920281
[2m[36m(func pid=59563)[0m f1_weighted: 0.2425932879635287
[2m[36m(func pid=59563)[0m f1_per_class: [0.261, 0.427, 0.786, 0.195, 0.138, 0.171, 0.216, 0.165, 0.191, 0.341]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.8081 | Steps: 2 | Val loss: 2.0994 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1118 | Steps: 2 | Val loss: 2.2518 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5217 | Steps: 2 | Val loss: 2.3496 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=72016)[0m top1: 0.21175373134328357
[2m[36m(func pid=72016)[0m top5: 0.7607276119402985
[2m[36m(func pid=72016)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=72016)[0m f1_macro: 0.24648153273942688
[2m[36m(func pid=72016)[0m f1_weighted: 0.20674093336186788
[2m[36m(func pid=72016)[0m f1_per_class: [0.456, 0.275, 0.647, 0.388, 0.073, 0.199, 0.018, 0.029, 0.144, 0.235]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.2329 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:13:49 (running for 00:30:41.56)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.289 |                   58 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.112 |      0.12  |                   30 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.656 |      0.098 |                    6 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  1.808 |      0.246 |                    4 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.19169776119402984
[2m[36m(func pid=66105)[0m top5: 0.6194029850746269
[2m[36m(func pid=66105)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=66105)[0m f1_macro: 0.11952990866877589
[2m[36m(func pid=66105)[0m f1_weighted: 0.17655378886991127
[2m[36m(func pid=66105)[0m f1_per_class: [0.13, 0.14, 0.12, 0.054, 0.025, 0.126, 0.385, 0.0, 0.108, 0.107]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.21735074626865672
[2m[36m(func pid=70967)[0m top5: 0.5368470149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=70967)[0m f1_macro: 0.11876216558022304
[2m[36m(func pid=70967)[0m f1_weighted: 0.14197333397730233
[2m[36m(func pid=70967)[0m f1_per_class: [0.189, 0.02, 0.219, 0.0, 0.014, 0.084, 0.394, 0.0, 0.142, 0.125]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23180970149253732
[2m[36m(func pid=59563)[0m top5: 0.7569962686567164
[2m[36m(func pid=59563)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=59563)[0m f1_macro: 0.2831479302699522
[2m[36m(func pid=59563)[0m f1_weighted: 0.2365391040361228
[2m[36m(func pid=59563)[0m f1_per_class: [0.257, 0.428, 0.733, 0.176, 0.143, 0.166, 0.215, 0.164, 0.183, 0.366]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.2168 | Steps: 2 | Val loss: 2.0622 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.1090 | Steps: 2 | Val loss: 2.2308 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4346 | Steps: 2 | Val loss: 2.3150 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1422 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=72016)[0m top1: 0.32882462686567165
[2m[36m(func pid=72016)[0m top5: 0.7574626865671642
[2m[36m(func pid=72016)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=72016)[0m f1_macro: 0.2945507799633117
[2m[36m(func pid=72016)[0m f1_weighted: 0.22157310289856375
[2m[36m(func pid=72016)[0m f1_per_class: [0.549, 0.061, 0.733, 0.544, 0.13, 0.148, 0.0, 0.304, 0.122, 0.355]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:13:54 (running for 00:30:46.69)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.283 |                   59 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.109 |      0.13  |                   31 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.522 |      0.119 |                    7 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  1.217 |      0.295 |                    5 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.20289179104477612
[2m[36m(func pid=66105)[0m top5: 0.6338619402985075
[2m[36m(func pid=66105)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=66105)[0m f1_macro: 0.13009962462064045
[2m[36m(func pid=66105)[0m f1_weighted: 0.18968114327563226
[2m[36m(func pid=66105)[0m f1_per_class: [0.154, 0.167, 0.134, 0.077, 0.038, 0.118, 0.392, 0.0, 0.112, 0.11]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.22621268656716417
[2m[36m(func pid=70967)[0m top5: 0.5652985074626866
[2m[36m(func pid=70967)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=70967)[0m f1_macro: 0.13079386718565597
[2m[36m(func pid=70967)[0m f1_weighted: 0.14926445077442152
[2m[36m(func pid=70967)[0m f1_per_class: [0.199, 0.02, 0.267, 0.0, 0.024, 0.09, 0.414, 0.0, 0.137, 0.158]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2332089552238806
[2m[36m(func pid=59563)[0m top5: 0.7555970149253731
[2m[36m(func pid=59563)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=59563)[0m f1_macro: 0.28854898893526093
[2m[36m(func pid=59563)[0m f1_weighted: 0.24035467853178535
[2m[36m(func pid=59563)[0m f1_per_class: [0.26, 0.428, 0.786, 0.187, 0.136, 0.17, 0.215, 0.164, 0.19, 0.35]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.7878 | Steps: 2 | Val loss: 1.9545 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0783 | Steps: 2 | Val loss: 2.2129 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2801 | Steps: 2 | Val loss: 2.2567 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3462 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=72016)[0m top1: 0.3064365671641791
[2m[36m(func pid=72016)[0m top5: 0.8731343283582089
[2m[36m(func pid=72016)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=72016)[0m f1_macro: 0.2935086163588866
[2m[36m(func pid=72016)[0m f1_weighted: 0.24386675919903225
[2m[36m(func pid=72016)[0m f1_per_class: [0.415, 0.213, 0.815, 0.578, 0.2, 0.107, 0.0, 0.216, 0.084, 0.308]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:13:59 (running for 00:30:51.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.289 |                   60 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  2.078 |      0.136 |                   32 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.435 |      0.131 |                    8 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.788 |      0.294 |                    6 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.20522388059701493
[2m[36m(func pid=66105)[0m top5: 0.6529850746268657
[2m[36m(func pid=66105)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=66105)[0m f1_macro: 0.13572296236700201
[2m[36m(func pid=66105)[0m f1_weighted: 0.1971469759615498
[2m[36m(func pid=66105)[0m f1_per_class: [0.148, 0.193, 0.149, 0.09, 0.038, 0.123, 0.386, 0.0, 0.124, 0.106]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22667910447761194
[2m[36m(func pid=59563)[0m top5: 0.7527985074626866
[2m[36m(func pid=59563)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=59563)[0m f1_macro: 0.28043674336386615
[2m[36m(func pid=59563)[0m f1_weighted: 0.23357135354365047
[2m[36m(func pid=59563)[0m f1_per_class: [0.249, 0.407, 0.759, 0.173, 0.13, 0.175, 0.217, 0.164, 0.183, 0.346]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m top1: 0.20988805970149255
[2m[36m(func pid=70967)[0m top5: 0.6156716417910447
[2m[36m(func pid=70967)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=70967)[0m f1_macro: 0.12810257289056887
[2m[36m(func pid=70967)[0m f1_weighted: 0.15918543256752127
[2m[36m(func pid=70967)[0m f1_per_class: [0.155, 0.053, 0.225, 0.016, 0.028, 0.095, 0.413, 0.0, 0.157, 0.14]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5085 | Steps: 2 | Val loss: 1.7424 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.9202 | Steps: 2 | Val loss: 2.1873 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3934 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0594 | Steps: 2 | Val loss: 2.1907 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=72016)[0m top1: 0.32369402985074625
[2m[36m(func pid=72016)[0m top5: 0.9071828358208955
[2m[36m(func pid=72016)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=72016)[0m f1_macro: 0.32280234678397957
[2m[36m(func pid=72016)[0m f1_weighted: 0.33582972902079916
[2m[36m(func pid=72016)[0m f1_per_class: [0.329, 0.443, 0.815, 0.518, 0.126, 0.229, 0.191, 0.222, 0.113, 0.242]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:14:05 (running for 00:30:57.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.28  |                   61 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.92  |      0.142 |                   33 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  2.28  |      0.128 |                    9 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.508 |      0.323 |                    7 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.208955223880597
[2m[36m(func pid=66105)[0m top5: 0.6763059701492538
[2m[36m(func pid=66105)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=66105)[0m f1_macro: 0.14189456985473692
[2m[36m(func pid=66105)[0m f1_weighted: 0.20489597321653727
[2m[36m(func pid=66105)[0m f1_per_class: [0.16, 0.195, 0.177, 0.118, 0.026, 0.115, 0.385, 0.0, 0.139, 0.104]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22761194029850745
[2m[36m(func pid=59563)[0m top5: 0.7523320895522388
[2m[36m(func pid=59563)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=59563)[0m f1_macro: 0.2804300922266288
[2m[36m(func pid=59563)[0m f1_weighted: 0.23353304604444922
[2m[36m(func pid=59563)[0m f1_per_class: [0.248, 0.417, 0.733, 0.174, 0.137, 0.171, 0.213, 0.162, 0.183, 0.366]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m top1: 0.1828358208955224
[2m[36m(func pid=70967)[0m top5: 0.6879664179104478
[2m[36m(func pid=70967)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=70967)[0m f1_macro: 0.13502113515229985
[2m[36m(func pid=70967)[0m f1_weighted: 0.18505134437048137
[2m[36m(func pid=70967)[0m f1_per_class: [0.143, 0.094, 0.234, 0.136, 0.035, 0.122, 0.36, 0.0, 0.105, 0.12]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3030 | Steps: 2 | Val loss: 1.8074 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.9470 | Steps: 2 | Val loss: 2.1710 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8650 | Steps: 2 | Val loss: 2.1308 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.3159 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=72016)[0m top1: 0.3451492537313433
[2m[36m(func pid=72016)[0m top5: 0.8936567164179104
[2m[36m(func pid=72016)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=72016)[0m f1_macro: 0.32077459117312673
[2m[36m(func pid=72016)[0m f1_weighted: 0.3553890152401656
[2m[36m(func pid=72016)[0m f1_per_class: [0.368, 0.437, 0.786, 0.288, 0.087, 0.245, 0.473, 0.203, 0.114, 0.207]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m top1: 0.21128731343283583
[2m[36m(func pid=70967)[0m top5: 0.7392723880597015
[2m[36m(func pid=70967)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=70967)[0m f1_macro: 0.1788810474055677
[2m[36m(func pid=70967)[0m f1_weighted: 0.2358906673819353
[2m[36m(func pid=70967)[0m f1_per_class: [0.18, 0.154, 0.31, 0.345, 0.038, 0.151, 0.279, 0.0, 0.128, 0.203]
== Status ==
Current time: 2024-01-07 01:14:10 (running for 00:31:02.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.28  |                   62 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.92  |      0.142 |                   33 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.865 |      0.179 |                   11 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.303 |      0.321 |                    8 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m top1: 0.2080223880597015
[2m[36m(func pid=66105)[0m top5: 0.6902985074626866
[2m[36m(func pid=66105)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=66105)[0m f1_macro: 0.1461446067261612
[2m[36m(func pid=66105)[0m f1_weighted: 0.20892945936682872
[2m[36m(func pid=66105)[0m f1_per_class: [0.171, 0.199, 0.195, 0.141, 0.026, 0.114, 0.374, 0.0, 0.147, 0.094]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.228544776119403
[2m[36m(func pid=59563)[0m top5: 0.7532649253731343
[2m[36m(func pid=59563)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=59563)[0m f1_macro: 0.2816862933237564
[2m[36m(func pid=59563)[0m f1_weighted: 0.23538887574844614
[2m[36m(func pid=59563)[0m f1_per_class: [0.255, 0.42, 0.733, 0.176, 0.138, 0.167, 0.216, 0.162, 0.183, 0.366]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.2191 | Steps: 2 | Val loss: 2.0296 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.6163 | Steps: 2 | Val loss: 2.0763 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8368 | Steps: 2 | Val loss: 2.1522 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.4084 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=72016)[0m top1: 0.3316231343283582
[2m[36m(func pid=72016)[0m top5: 0.8731343283582089
[2m[36m(func pid=72016)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=72016)[0m f1_macro: 0.3162651709111181
[2m[36m(func pid=72016)[0m f1_weighted: 0.3296134404457434
[2m[36m(func pid=72016)[0m f1_per_class: [0.453, 0.428, 0.815, 0.183, 0.077, 0.251, 0.497, 0.105, 0.147, 0.207]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:14:15 (running for 00:31:07.68)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   63 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.947 |      0.146 |                   34 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.616 |      0.194 |                   12 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.219 |      0.316 |                    9 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.24253731343283583
[2m[36m(func pid=70967)[0m top5: 0.7509328358208955
[2m[36m(func pid=70967)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=70967)[0m f1_macro: 0.19429474720155748
[2m[36m(func pid=70967)[0m f1_weighted: 0.23473054175034164
[2m[36m(func pid=70967)[0m f1_per_class: [0.239, 0.188, 0.4, 0.467, 0.041, 0.151, 0.137, 0.0, 0.121, 0.198]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m top1: 0.2103544776119403
[2m[36m(func pid=66105)[0m top5: 0.7089552238805971
[2m[36m(func pid=66105)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=66105)[0m f1_macro: 0.15280705152933888
[2m[36m(func pid=66105)[0m f1_weighted: 0.21576606140813528
[2m[36m(func pid=66105)[0m f1_per_class: [0.169, 0.215, 0.237, 0.164, 0.039, 0.112, 0.367, 0.0, 0.133, 0.091]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22434701492537312
[2m[36m(func pid=59563)[0m top5: 0.7546641791044776
[2m[36m(func pid=59563)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=59563)[0m f1_macro: 0.27849741373425807
[2m[36m(func pid=59563)[0m f1_weighted: 0.23070896518626313
[2m[36m(func pid=59563)[0m f1_per_class: [0.255, 0.416, 0.733, 0.174, 0.136, 0.159, 0.209, 0.16, 0.182, 0.361]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.1254 | Steps: 2 | Val loss: 2.2559 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8234 | Steps: 2 | Val loss: 2.1324 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.4200 | Steps: 2 | Val loss: 2.0395 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.5488 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=72016)[0m top1: 0.32649253731343286
[2m[36m(func pid=72016)[0m top5: 0.8610074626865671
[2m[36m(func pid=72016)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=72016)[0m f1_macro: 0.34685350932480674
[2m[36m(func pid=72016)[0m f1_weighted: 0.335991577894607
[2m[36m(func pid=72016)[0m f1_per_class: [0.602, 0.405, 0.815, 0.197, 0.069, 0.247, 0.489, 0.161, 0.227, 0.258]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m top1: 0.2719216417910448
[2m[36m(func pid=70967)[0m top5: 0.742070895522388
[2m[36m(func pid=70967)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=70967)[0m f1_macro: 0.2151736183462977
[2m[36m(func pid=70967)[0m f1_weighted: 0.22972664153740235
[2m[36m(func pid=70967)[0m f1_per_class: [0.308, 0.181, 0.524, 0.505, 0.037, 0.127, 0.073, 0.088, 0.139, 0.17]
== Status ==
Current time: 2024-01-07 01:14:20 (running for 00:31:12.86)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.278 |                   64 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.837 |      0.153 |                   35 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.42  |      0.215 |                   13 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.125 |      0.347 |                   10 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.21595149253731344
[2m[36m(func pid=66105)[0m top5: 0.7271455223880597
[2m[36m(func pid=66105)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=66105)[0m f1_macro: 0.1606891445750743
[2m[36m(func pid=66105)[0m f1_weighted: 0.22981885058574109
[2m[36m(func pid=66105)[0m f1_per_class: [0.173, 0.207, 0.275, 0.218, 0.039, 0.117, 0.367, 0.0, 0.128, 0.083]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.2248134328358209
[2m[36m(func pid=59563)[0m top5: 0.7523320895522388
[2m[36m(func pid=59563)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=59563)[0m f1_macro: 0.2762861562181066
[2m[36m(func pid=59563)[0m f1_weighted: 0.23090791880803527
[2m[36m(func pid=59563)[0m f1_per_class: [0.251, 0.417, 0.71, 0.171, 0.131, 0.16, 0.212, 0.161, 0.185, 0.366]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.1038 | Steps: 2 | Val loss: 2.4931 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7671 | Steps: 2 | Val loss: 2.1175 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2036 | Steps: 2 | Val loss: 2.0282 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.4622 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=72016)[0m top1: 0.30223880597014924
[2m[36m(func pid=72016)[0m top5: 0.8666044776119403
[2m[36m(func pid=72016)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=72016)[0m f1_macro: 0.35943674931338354
[2m[36m(func pid=72016)[0m f1_weighted: 0.3260902901657696
[2m[36m(func pid=72016)[0m f1_per_class: [0.644, 0.396, 0.786, 0.242, 0.072, 0.241, 0.406, 0.209, 0.198, 0.4]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:14:25 (running for 00:31:17.99)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.276 |                   65 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.767 |      0.169 |                   37 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.42  |      0.215 |                   13 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.104 |      0.359 |                   11 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.21735074626865672
[2m[36m(func pid=66105)[0m top5: 0.7388059701492538
[2m[36m(func pid=66105)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=66105)[0m f1_macro: 0.16918968441431437
[2m[36m(func pid=66105)[0m f1_weighted: 0.237019275699094
[2m[36m(func pid=66105)[0m f1_per_class: [0.195, 0.218, 0.314, 0.258, 0.038, 0.126, 0.342, 0.0, 0.126, 0.074]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.3045708955223881
[2m[36m(func pid=70967)[0m top5: 0.7276119402985075
[2m[36m(func pid=70967)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=70967)[0m f1_macro: 0.2469805968836066
[2m[36m(func pid=70967)[0m f1_weighted: 0.24088369765404086
[2m[36m(func pid=70967)[0m f1_per_class: [0.366, 0.248, 0.524, 0.526, 0.058, 0.118, 0.009, 0.313, 0.116, 0.192]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=59563)[0m top1: 0.22574626865671643
[2m[36m(func pid=59563)[0m top5: 0.7513992537313433
[2m[36m(func pid=59563)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=59563)[0m f1_macro: 0.27651707227702016
[2m[36m(func pid=59563)[0m f1_weighted: 0.23037100151842063
[2m[36m(func pid=59563)[0m f1_per_class: [0.241, 0.413, 0.71, 0.168, 0.145, 0.175, 0.209, 0.165, 0.182, 0.357]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.0839 | Steps: 2 | Val loss: 2.7297 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7631 | Steps: 2 | Val loss: 2.1017 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0001 | Steps: 2 | Val loss: 17.0076 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.0839 | Steps: 2 | Val loss: 2.0271 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=72016)[0m top1: 0.2896455223880597
[2m[36m(func pid=72016)[0m top5: 0.8610074626865671
[2m[36m(func pid=72016)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=72016)[0m f1_macro: 0.3584095323716917
[2m[36m(func pid=72016)[0m f1_weighted: 0.3261257946617035
[2m[36m(func pid=72016)[0m f1_per_class: [0.59, 0.39, 0.786, 0.339, 0.067, 0.217, 0.326, 0.241, 0.179, 0.449]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:14:30 (running for 00:31:23.20)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   66 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.763 |      0.179 |                   38 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.204 |      0.247 |                   14 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.084 |      0.358 |                   12 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2178171641791045
[2m[36m(func pid=66105)[0m top5: 0.7532649253731343
[2m[36m(func pid=66105)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=66105)[0m f1_macro: 0.1791942037461302
[2m[36m(func pid=66105)[0m f1_weighted: 0.2412609925135374
[2m[36m(func pid=66105)[0m f1_per_class: [0.202, 0.225, 0.355, 0.279, 0.038, 0.12, 0.324, 0.041, 0.134, 0.074]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23227611940298507
[2m[36m(func pid=59563)[0m top5: 0.7607276119402985
[2m[36m(func pid=59563)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=59563)[0m f1_macro: 0.2819348442781987
[2m[36m(func pid=59563)[0m f1_weighted: 0.2401658035818065
[2m[36m(func pid=59563)[0m f1_per_class: [0.259, 0.422, 0.733, 0.19, 0.136, 0.162, 0.22, 0.165, 0.188, 0.345]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m top1: 0.31716417910447764
[2m[36m(func pid=70967)[0m top5: 0.7075559701492538
[2m[36m(func pid=70967)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=70967)[0m f1_macro: 0.2643930154646441
[2m[36m(func pid=70967)[0m f1_weighted: 0.25559653094239343
[2m[36m(func pid=70967)[0m f1_per_class: [0.408, 0.31, 0.512, 0.526, 0.067, 0.135, 0.003, 0.353, 0.137, 0.193]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.0538 | Steps: 2 | Val loss: 2.8795 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6128 | Steps: 2 | Val loss: 2.0923 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9642 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.9713 | Steps: 2 | Val loss: 2.0281 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:14:36 (running for 00:31:28.38)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   67 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.763 |      0.179 |                   38 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  1.084 |      0.264 |                   15 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.084 |      0.358 |                   12 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2224813432835821
[2m[36m(func pid=66105)[0m top5: 0.7546641791044776
[2m[36m(func pid=66105)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=66105)[0m f1_macro: 0.18742056287161837
[2m[36m(func pid=66105)[0m f1_weighted: 0.24739325117345712
[2m[36m(func pid=66105)[0m f1_per_class: [0.218, 0.231, 0.393, 0.312, 0.038, 0.123, 0.308, 0.039, 0.138, 0.076]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m top1: 0.2971082089552239
[2m[36m(func pid=72016)[0m top5: 0.8586753731343284
[2m[36m(func pid=72016)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=72016)[0m f1_macro: 0.35709691399333726
[2m[36m(func pid=72016)[0m f1_weighted: 0.33087526442823917
[2m[36m(func pid=72016)[0m f1_per_class: [0.59, 0.39, 0.759, 0.427, 0.067, 0.216, 0.251, 0.298, 0.185, 0.387]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23274253731343283
[2m[36m(func pid=59563)[0m top5: 0.7611940298507462
[2m[36m(func pid=59563)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=59563)[0m f1_macro: 0.2819448000871432
[2m[36m(func pid=59563)[0m f1_weighted: 0.24004651635640312
[2m[36m(func pid=59563)[0m f1_per_class: [0.275, 0.422, 0.733, 0.189, 0.142, 0.163, 0.22, 0.165, 0.178, 0.333]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m top1: 0.31716417910447764
[2m[36m(func pid=70967)[0m top5: 0.7159514925373134
[2m[36m(func pid=70967)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=70967)[0m f1_macro: 0.2721309330811727
[2m[36m(func pid=70967)[0m f1_weighted: 0.26308969170397783
[2m[36m(func pid=70967)[0m f1_per_class: [0.449, 0.361, 0.512, 0.527, 0.094, 0.147, 0.0, 0.29, 0.157, 0.184]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.5819 | Steps: 2 | Val loss: 2.0832 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.0306 | Steps: 2 | Val loss: 3.0773 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9242 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.9889 | Steps: 2 | Val loss: 2.0221 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:14:41 (running for 00:31:33.62)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.282 |                   68 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.613 |      0.187 |                   39 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.971 |      0.272 |                   16 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.054 |      0.357 |                   13 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.22388059701492538
[2m[36m(func pid=66105)[0m top5: 0.7611940298507462
[2m[36m(func pid=66105)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=66105)[0m f1_macro: 0.19581783074575287
[2m[36m(func pid=66105)[0m f1_weighted: 0.24903239277708256
[2m[36m(func pid=66105)[0m f1_per_class: [0.228, 0.243, 0.431, 0.33, 0.037, 0.132, 0.279, 0.06, 0.139, 0.077]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23600746268656717
[2m[36m(func pid=59563)[0m top5: 0.7625932835820896
[2m[36m(func pid=59563)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=59563)[0m f1_macro: 0.2840814837615412
[2m[36m(func pid=59563)[0m f1_weighted: 0.2438707752441103
[2m[36m(func pid=59563)[0m f1_per_class: [0.269, 0.423, 0.71, 0.194, 0.14, 0.178, 0.221, 0.165, 0.188, 0.353]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=70967)[0m top1: 0.30597014925373134
[2m[36m(func pid=70967)[0m top5: 0.7341417910447762
[2m[36m(func pid=70967)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=70967)[0m f1_macro: 0.28135843320949805
[2m[36m(func pid=70967)[0m f1_weighted: 0.2676092620869811
[2m[36m(func pid=70967)[0m f1_per_class: [0.472, 0.416, 0.55, 0.506, 0.078, 0.159, 0.0, 0.245, 0.202, 0.187]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3045708955223881
[2m[36m(func pid=72016)[0m top5: 0.8446828358208955
[2m[36m(func pid=72016)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=72016)[0m f1_macro: 0.3547302905746586
[2m[36m(func pid=72016)[0m f1_weighted: 0.328883371059615
[2m[36m(func pid=72016)[0m f1_per_class: [0.561, 0.395, 0.759, 0.479, 0.066, 0.207, 0.192, 0.33, 0.194, 0.364]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.5994 | Steps: 2 | Val loss: 2.0698 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0008 | Steps: 2 | Val loss: 17.1943 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.0119 | Steps: 2 | Val loss: 3.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7033 | Steps: 2 | Val loss: 1.9911 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:14:46 (running for 00:31:39.03)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0.001 |      0.277 |                   70 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.582 |      0.196 |                   40 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.989 |      0.281 |                   17 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.031 |      0.355 |                   14 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.22901119402985073
[2m[36m(func pid=59563)[0m top5: 0.7583955223880597
[2m[36m(func pid=59563)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=59563)[0m f1_macro: 0.27716009074373293
[2m[36m(func pid=59563)[0m f1_weighted: 0.23361789645820752
[2m[36m(func pid=59563)[0m f1_per_class: [0.269, 0.412, 0.71, 0.176, 0.133, 0.166, 0.215, 0.166, 0.183, 0.341]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.228544776119403
[2m[36m(func pid=66105)[0m top5: 0.7667910447761194
[2m[36m(func pid=66105)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=66105)[0m f1_macro: 0.20588351404179056
[2m[36m(func pid=66105)[0m f1_weighted: 0.2517128321002733
[2m[36m(func pid=66105)[0m f1_per_class: [0.254, 0.259, 0.449, 0.347, 0.038, 0.139, 0.251, 0.094, 0.147, 0.081]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.2943097014925373
[2m[36m(func pid=70967)[0m top5: 0.7831156716417911
[2m[36m(func pid=70967)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=70967)[0m f1_macro: 0.2849824531652519
[2m[36m(func pid=70967)[0m f1_weighted: 0.26790315972491713
[2m[36m(func pid=70967)[0m f1_per_class: [0.479, 0.457, 0.564, 0.487, 0.089, 0.146, 0.003, 0.219, 0.204, 0.2]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.30363805970149255
[2m[36m(func pid=72016)[0m top5: 0.8292910447761194
[2m[36m(func pid=72016)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=72016)[0m f1_macro: 0.34894889600384227
[2m[36m(func pid=72016)[0m f1_weighted: 0.3175213773884603
[2m[36m(func pid=72016)[0m f1_per_class: [0.6, 0.392, 0.759, 0.504, 0.065, 0.18, 0.141, 0.33, 0.194, 0.323]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.2711 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5358 | Steps: 2 | Val loss: 2.0618 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6256 | Steps: 2 | Val loss: 1.9686 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0063 | Steps: 2 | Val loss: 3.4754 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:14:51 (running for 00:31:44.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.275 |                   71 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.599 |      0.206 |                   41 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.703 |      0.285 |                   18 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.012 |      0.349 |                   15 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.22994402985074627
[2m[36m(func pid=59563)[0m top5: 0.7541977611940298
[2m[36m(func pid=59563)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=59563)[0m f1_macro: 0.27474408198208733
[2m[36m(func pid=59563)[0m f1_weighted: 0.23216701217151534
[2m[36m(func pid=59563)[0m f1_per_class: [0.264, 0.42, 0.71, 0.17, 0.145, 0.17, 0.211, 0.169, 0.176, 0.312]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.23274253731343283
[2m[36m(func pid=66105)[0m top5: 0.7714552238805971
[2m[36m(func pid=66105)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=66105)[0m f1_macro: 0.21214606144737985
[2m[36m(func pid=66105)[0m f1_weighted: 0.249946233361455
[2m[36m(func pid=66105)[0m f1_per_class: [0.268, 0.28, 0.458, 0.371, 0.036, 0.136, 0.203, 0.13, 0.154, 0.086]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.2691231343283582
[2m[36m(func pid=70967)[0m top5: 0.8255597014925373
[2m[36m(func pid=70967)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=70967)[0m f1_macro: 0.28308431123032496
[2m[36m(func pid=70967)[0m f1_weighted: 0.2437645267465514
[2m[36m(func pid=70967)[0m f1_per_class: [0.508, 0.453, 0.595, 0.396, 0.098, 0.141, 0.012, 0.21, 0.181, 0.238]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.31529850746268656
[2m[36m(func pid=72016)[0m top5: 0.8129664179104478
[2m[36m(func pid=72016)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=72016)[0m f1_macro: 0.34209750225327545
[2m[36m(func pid=72016)[0m f1_weighted: 0.31635793863623923
[2m[36m(func pid=72016)[0m f1_per_class: [0.571, 0.402, 0.759, 0.545, 0.068, 0.168, 0.099, 0.338, 0.208, 0.262]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1168 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4398 | Steps: 2 | Val loss: 2.0599 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.7014 | Steps: 2 | Val loss: 1.9260 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0177 | Steps: 2 | Val loss: 3.6766 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:14:57 (running for 00:31:49.44)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.275 |                   72 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.536 |      0.212 |                   42 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.626 |      0.283 |                   19 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.006 |      0.342 |                   16 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23134328358208955
[2m[36m(func pid=59563)[0m top5: 0.7574626865671642
[2m[36m(func pid=59563)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=59563)[0m f1_macro: 0.27495049333217836
[2m[36m(func pid=59563)[0m f1_weighted: 0.23511806229454923
[2m[36m(func pid=59563)[0m f1_per_class: [0.267, 0.422, 0.71, 0.176, 0.132, 0.171, 0.215, 0.167, 0.174, 0.316]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.23180970149253732
[2m[36m(func pid=66105)[0m top5: 0.7723880597014925
[2m[36m(func pid=66105)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=66105)[0m f1_macro: 0.2178643549550745
[2m[36m(func pid=66105)[0m f1_weighted: 0.24235501093633685
[2m[36m(func pid=66105)[0m f1_per_class: [0.286, 0.288, 0.468, 0.377, 0.035, 0.14, 0.153, 0.189, 0.156, 0.087]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.25886194029850745
[2m[36m(func pid=70967)[0m top5: 0.8530783582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=70967)[0m f1_macro: 0.28676131916316605
[2m[36m(func pid=70967)[0m f1_weighted: 0.23518996253974156
[2m[36m(func pid=70967)[0m f1_per_class: [0.517, 0.448, 0.595, 0.324, 0.115, 0.144, 0.048, 0.203, 0.219, 0.256]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3199626865671642
[2m[36m(func pid=72016)[0m top5: 0.8027052238805971
[2m[36m(func pid=72016)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=72016)[0m f1_macro: 0.33737193848088765
[2m[36m(func pid=72016)[0m f1_weighted: 0.314515955447067
[2m[36m(func pid=72016)[0m f1_per_class: [0.542, 0.407, 0.759, 0.558, 0.068, 0.162, 0.082, 0.336, 0.22, 0.239]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.1053 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.4834 | Steps: 2 | Val loss: 2.0540 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4346 | Steps: 2 | Val loss: 1.8697 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0020 | Steps: 2 | Val loss: 3.8794 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:15:02 (running for 00:31:54.80)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   73 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.44  |      0.218 |                   43 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.701 |      0.287 |                   20 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.018 |      0.337 |                   17 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.23227611940298507
[2m[36m(func pid=59563)[0m top5: 0.7551305970149254
[2m[36m(func pid=59563)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=59563)[0m f1_macro: 0.27694063240446687
[2m[36m(func pid=59563)[0m f1_weighted: 0.2352407849683875
[2m[36m(func pid=59563)[0m f1_per_class: [0.265, 0.427, 0.71, 0.17, 0.137, 0.176, 0.215, 0.166, 0.178, 0.326]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.24253731343283583
[2m[36m(func pid=66105)[0m top5: 0.7630597014925373
[2m[36m(func pid=66105)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=66105)[0m f1_macro: 0.22597538408545
[2m[36m(func pid=66105)[0m f1_weighted: 0.24491105598298524
[2m[36m(func pid=66105)[0m f1_per_class: [0.295, 0.31, 0.489, 0.403, 0.037, 0.138, 0.118, 0.215, 0.157, 0.097]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.27705223880597013
[2m[36m(func pid=70967)[0m top5: 0.8698694029850746
[2m[36m(func pid=70967)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=70967)[0m f1_macro: 0.3096875758249845
[2m[36m(func pid=70967)[0m f1_weighted: 0.26695389820463983
[2m[36m(func pid=70967)[0m f1_per_class: [0.525, 0.437, 0.629, 0.298, 0.126, 0.2, 0.158, 0.212, 0.236, 0.276]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3185634328358209
[2m[36m(func pid=72016)[0m top5: 0.7971082089552238
[2m[36m(func pid=72016)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.33493558868394246
[2m[36m(func pid=72016)[0m f1_weighted: 0.3065409145104834
[2m[36m(func pid=72016)[0m f1_per_class: [0.559, 0.382, 0.786, 0.56, 0.069, 0.143, 0.074, 0.332, 0.238, 0.207]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 17.0261 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3394 | Steps: 2 | Val loss: 2.0495 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3645 | Steps: 2 | Val loss: 1.8089 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0213 | Steps: 2 | Val loss: 4.0947 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:15:07 (running for 00:32:00.07)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.276 |                   74 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.483 |      0.226 |                   44 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.435 |      0.31  |                   21 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.002 |      0.335 |                   18 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59563)[0m top1: 0.2332089552238806
[2m[36m(func pid=59563)[0m top5: 0.7569962686567164
[2m[36m(func pid=59563)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=59563)[0m f1_macro: 0.2757458379463428
[2m[36m(func pid=59563)[0m f1_weighted: 0.23708873979373607
[2m[36m(func pid=59563)[0m f1_per_class: [0.261, 0.429, 0.71, 0.175, 0.137, 0.182, 0.213, 0.168, 0.186, 0.297]
[2m[36m(func pid=59563)[0m 
[2m[36m(func pid=66105)[0m top1: 0.24860074626865672
[2m[36m(func pid=66105)[0m top5: 0.7621268656716418
[2m[36m(func pid=66105)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=66105)[0m f1_macro: 0.2340678423747244
[2m[36m(func pid=66105)[0m f1_weighted: 0.24127301866449521
[2m[36m(func pid=66105)[0m f1_per_class: [0.339, 0.327, 0.5, 0.405, 0.051, 0.133, 0.089, 0.24, 0.153, 0.105]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.3050373134328358
[2m[36m(func pid=70967)[0m top5: 0.8805970149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=70967)[0m f1_macro: 0.32622202220506824
[2m[36m(func pid=70967)[0m f1_weighted: 0.3155535868850288
[2m[36m(func pid=70967)[0m f1_per_class: [0.508, 0.419, 0.667, 0.303, 0.11, 0.207, 0.325, 0.215, 0.219, 0.288]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.32509328358208955
[2m[36m(func pid=72016)[0m top5: 0.7915111940298507
[2m[36m(func pid=72016)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=72016)[0m f1_macro: 0.3363736565466561
[2m[36m(func pid=72016)[0m f1_weighted: 0.3056910772812372
[2m[36m(func pid=72016)[0m f1_per_class: [0.567, 0.374, 0.786, 0.564, 0.072, 0.138, 0.071, 0.342, 0.234, 0.216]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=59563)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 16.9643 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.3747 | Steps: 2 | Val loss: 2.0397 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3213 | Steps: 2 | Val loss: 1.7815 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0226 | Steps: 2 | Val loss: 4.2551 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:15:12 (running for 00:32:05.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00015 | RUNNING    | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.276 |                   74 |
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.375 |      0.243 |                   46 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.364 |      0.326 |                   22 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.021 |      0.336 |                   19 |
| train_57e67_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2579291044776119
[2m[36m(func pid=66105)[0m top5: 0.7588619402985075
[2m[36m(func pid=66105)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=66105)[0m f1_macro: 0.2431852406330668
[2m[36m(func pid=66105)[0m f1_weighted: 0.2475352872992324
[2m[36m(func pid=66105)[0m f1_per_class: [0.344, 0.338, 0.55, 0.429, 0.054, 0.13, 0.079, 0.246, 0.155, 0.108]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=59563)[0m top1: 0.23367537313432835
[2m[36m(func pid=59563)[0m top5: 0.7555970149253731
[2m[36m(func pid=59563)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=59563)[0m f1_macro: 0.27654609903974936
[2m[36m(func pid=59563)[0m f1_weighted: 0.23728202564572748
[2m[36m(func pid=59563)[0m f1_per_class: [0.252, 0.423, 0.71, 0.179, 0.14, 0.185, 0.213, 0.17, 0.185, 0.309]
[2m[36m(func pid=70967)[0m top1: 0.33861940298507465
[2m[36m(func pid=70967)[0m top5: 0.8833955223880597
[2m[36m(func pid=70967)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=70967)[0m f1_macro: 0.34030507869819554
[2m[36m(func pid=70967)[0m f1_weighted: 0.35673799199794887
[2m[36m(func pid=70967)[0m f1_per_class: [0.5, 0.392, 0.688, 0.32, 0.103, 0.213, 0.464, 0.2, 0.212, 0.312]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3269589552238806
[2m[36m(func pid=72016)[0m top5: 0.7966417910447762
[2m[36m(func pid=72016)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=72016)[0m f1_macro: 0.33231027038881544
[2m[36m(func pid=72016)[0m f1_weighted: 0.30338139142274134
[2m[36m(func pid=72016)[0m f1_per_class: [0.541, 0.352, 0.786, 0.562, 0.074, 0.143, 0.077, 0.355, 0.224, 0.21]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.2747 | Steps: 2 | Val loss: 2.0411 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2820 | Steps: 2 | Val loss: 1.7720 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0453 | Steps: 2 | Val loss: 4.4079 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=66105)[0m top1: 0.25886194029850745
[2m[36m(func pid=66105)[0m top5: 0.7574626865671642
[2m[36m(func pid=66105)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=66105)[0m f1_macro: 0.2449802581329113
[2m[36m(func pid=66105)[0m f1_weighted: 0.2434973981770865
[2m[36m(func pid=66105)[0m f1_per_class: [0.344, 0.344, 0.55, 0.429, 0.055, 0.129, 0.059, 0.251, 0.17, 0.119]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.37220149253731344
[2m[36m(func pid=70967)[0m top5: 0.8815298507462687
[2m[36m(func pid=70967)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=70967)[0m f1_macro: 0.3499341370322625
[2m[36m(func pid=70967)[0m f1_weighted: 0.3864224975196295
[2m[36m(func pid=70967)[0m f1_per_class: [0.481, 0.382, 0.71, 0.347, 0.108, 0.227, 0.545, 0.175, 0.199, 0.326]
[2m[36m(func pid=72016)[0m top1: 0.333955223880597
[2m[36m(func pid=72016)[0m top5: 0.8036380597014925
[2m[36m(func pid=72016)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=72016)[0m f1_macro: 0.3384520796886773
[2m[36m(func pid=72016)[0m f1_weighted: 0.313101828480016
[2m[36m(func pid=72016)[0m f1_per_class: [0.559, 0.344, 0.815, 0.563, 0.074, 0.131, 0.115, 0.371, 0.209, 0.204]
== Status ==
Current time: 2024-01-07 01:15:18 (running for 00:32:10.36)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.275 |      0.245 |                   47 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.321 |      0.34  |                   23 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.023 |      0.332 |                   20 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=76915)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=76915)[0m Configuration completed!
[2m[36m(func pid=76915)[0m New optimizer parameters:
[2m[36m(func pid=76915)[0m SGD (
[2m[36m(func pid=76915)[0m Parameter Group 0
[2m[36m(func pid=76915)[0m     dampening: 0
[2m[36m(func pid=76915)[0m     differentiable: False
[2m[36m(func pid=76915)[0m     foreach: None
[2m[36m(func pid=76915)[0m     lr: 0.1
[2m[36m(func pid=76915)[0m     maximize: False
[2m[36m(func pid=76915)[0m     momentum: 0.99
[2m[36m(func pid=76915)[0m     nesterov: False
[2m[36m(func pid=76915)[0m     weight_decay: 1e-05
[2m[36m(func pid=76915)[0m )
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2433 | Steps: 2 | Val loss: 2.0399 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 01:15:23 (running for 00:32:16.07)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.243 |      0.253 |                   48 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.282 |      0.35  |                   24 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.045 |      0.338 |                   21 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26259328358208955
[2m[36m(func pid=66105)[0m top5: 0.7546641791044776
[2m[36m(func pid=66105)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=66105)[0m f1_macro: 0.25264494289302286
[2m[36m(func pid=66105)[0m f1_weighted: 0.2453297148451979
[2m[36m(func pid=66105)[0m f1_per_class: [0.354, 0.35, 0.595, 0.443, 0.055, 0.129, 0.048, 0.238, 0.188, 0.129]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2426 | Steps: 2 | Val loss: 1.7716 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0275 | Steps: 2 | Val loss: 4.4040 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.2477 | Steps: 2 | Val loss: 2.8804 | Batch size: 32 | lr: 0.1 | Duration: 4.57s
[2m[36m(func pid=70967)[0m top1: 0.39132462686567165
[2m[36m(func pid=70967)[0m top5: 0.8824626865671642
[2m[36m(func pid=70967)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=70967)[0m f1_macro: 0.3548802678816436
[2m[36m(func pid=70967)[0m f1_weighted: 0.4004120671787928
[2m[36m(func pid=70967)[0m f1_per_class: [0.469, 0.361, 0.71, 0.373, 0.106, 0.243, 0.579, 0.157, 0.184, 0.368]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.2680 | Steps: 2 | Val loss: 2.0416 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=72016)[0m top1: 0.34701492537313433
[2m[36m(func pid=72016)[0m top5: 0.8236940298507462
[2m[36m(func pid=72016)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=72016)[0m f1_macro: 0.3423343873799249
[2m[36m(func pid=72016)[0m f1_weighted: 0.3272004871411602
[2m[36m(func pid=72016)[0m f1_per_class: [0.563, 0.341, 0.815, 0.568, 0.074, 0.139, 0.158, 0.362, 0.203, 0.2]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.21548507462686567
[2m[36m(func pid=76915)[0m top5: 0.8572761194029851
[2m[36m(func pid=76915)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=76915)[0m f1_macro: 0.048349569958369434
[2m[36m(func pid=76915)[0m f1_weighted: 0.11967661540409781
[2m[36m(func pid=76915)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.133, 0.351, 0.0, 0.0, 0.0]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:15:28 (running for 00:32:21.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.268 |      0.256 |                   49 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.243 |      0.355 |                   25 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.027 |      0.342 |                   22 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  4.248 |      0.048 |                    1 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26492537313432835
[2m[36m(func pid=66105)[0m top5: 0.753731343283582
[2m[36m(func pid=66105)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=66105)[0m f1_macro: 0.25634018439361295
[2m[36m(func pid=66105)[0m f1_weighted: 0.24540094723464687
[2m[36m(func pid=66105)[0m f1_per_class: [0.366, 0.356, 0.611, 0.449, 0.055, 0.126, 0.039, 0.228, 0.189, 0.144]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2198 | Steps: 2 | Val loss: 1.7680 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0014 | Steps: 2 | Val loss: 4.4888 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.7679 | Steps: 2 | Val loss: 13.2670 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=70967)[0m top1: 0.3978544776119403
[2m[36m(func pid=70967)[0m top5: 0.8843283582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.36015699489503494
[2m[36m(func pid=70967)[0m f1_weighted: 0.4035242546346441
[2m[36m(func pid=70967)[0m f1_per_class: [0.484, 0.346, 0.733, 0.395, 0.103, 0.237, 0.579, 0.138, 0.191, 0.394]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1668 | Steps: 2 | Val loss: 2.0423 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=72016)[0m top1: 0.34841417910447764
[2m[36m(func pid=72016)[0m top5: 0.8395522388059702
[2m[36m(func pid=72016)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=72016)[0m f1_macro: 0.3388457875106367
[2m[36m(func pid=72016)[0m f1_weighted: 0.33167195326731935
[2m[36m(func pid=72016)[0m f1_per_class: [0.547, 0.331, 0.815, 0.56, 0.079, 0.133, 0.198, 0.318, 0.201, 0.206]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.28171641791044777
[2m[36m(func pid=76915)[0m top5: 0.6539179104477612
[2m[36m(func pid=76915)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=76915)[0m f1_macro: 0.06725884503710075
[2m[36m(func pid=76915)[0m f1_weighted: 0.12679801872452984
[2m[36m(func pid=76915)[0m f1_per_class: [0.235, 0.0, 0.0, 0.437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:15:34 (running for 00:32:26.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.167 |      0.258 |                   50 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.22  |      0.36  |                   26 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.339 |                   23 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  7.768 |      0.067 |                    2 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2635261194029851
[2m[36m(func pid=66105)[0m top5: 0.7486007462686567
[2m[36m(func pid=66105)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=66105)[0m f1_macro: 0.2576853465710192
[2m[36m(func pid=66105)[0m f1_weighted: 0.24005165955336197
[2m[36m(func pid=66105)[0m f1_per_class: [0.37, 0.361, 0.629, 0.454, 0.058, 0.114, 0.018, 0.222, 0.19, 0.161]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2273 | Steps: 2 | Val loss: 1.7738 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0007 | Steps: 2 | Val loss: 4.5481 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 13.8733 | Steps: 2 | Val loss: 29.1711 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=70967)[0m top1: 0.40345149253731344
[2m[36m(func pid=70967)[0m top5: 0.886660447761194
[2m[36m(func pid=70967)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=70967)[0m f1_macro: 0.3615585272480122
[2m[36m(func pid=70967)[0m f1_weighted: 0.41132283196844693
[2m[36m(func pid=70967)[0m f1_per_class: [0.458, 0.326, 0.733, 0.432, 0.129, 0.237, 0.582, 0.149, 0.181, 0.387]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0769 | Steps: 2 | Val loss: 2.0484 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=72016)[0m top1: 0.3512126865671642
[2m[36m(func pid=72016)[0m top5: 0.8502798507462687
[2m[36m(func pid=72016)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=72016)[0m f1_macro: 0.33074919993679347
[2m[36m(func pid=72016)[0m f1_weighted: 0.33914418767072463
[2m[36m(func pid=72016)[0m f1_per_class: [0.51, 0.33, 0.786, 0.557, 0.08, 0.131, 0.239, 0.283, 0.19, 0.203]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.11240671641791045
[2m[36m(func pid=76915)[0m top5: 0.3773320895522388
[2m[36m(func pid=76915)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=76915)[0m f1_macro: 0.09479851723251506
[2m[36m(func pid=76915)[0m f1_weighted: 0.06580227631201911
[2m[36m(func pid=76915)[0m f1_per_class: [0.044, 0.159, 0.238, 0.0, 0.056, 0.196, 0.0, 0.192, 0.062, 0.0]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=66105)[0m top1: 0.2621268656716418
[2m[36m(func pid=66105)[0m top5: 0.7425373134328358
[2m[36m(func pid=66105)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=66105)[0m f1_macro: 0.25494599060172857
[2m[36m(func pid=66105)[0m f1_weighted: 0.23715967054031392
[2m[36m(func pid=66105)[0m f1_per_class: [0.368, 0.361, 0.611, 0.453, 0.06, 0.108, 0.012, 0.221, 0.191, 0.164]
[2m[36m(func pid=66105)[0m 
== Status ==
Current time: 2024-01-07 01:15:39 (running for 00:32:31.52)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.077 |      0.255 |                   51 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.227 |      0.362 |                   27 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.331 |                   24 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  | 13.873 |      0.095 |                    3 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.1662 | Steps: 2 | Val loss: 1.7887 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0005 | Steps: 2 | Val loss: 4.5433 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 18.4977 | Steps: 2 | Val loss: 42.6532 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=70967)[0m top1: 0.41138059701492535
[2m[36m(func pid=70967)[0m top5: 0.886660447761194
[2m[36m(func pid=70967)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=70967)[0m f1_macro: 0.37267947514108857
[2m[36m(func pid=70967)[0m f1_weighted: 0.4215121720879377
[2m[36m(func pid=70967)[0m f1_per_class: [0.486, 0.323, 0.759, 0.46, 0.139, 0.243, 0.585, 0.164, 0.182, 0.387]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.1618 | Steps: 2 | Val loss: 2.0466 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=72016)[0m top1: 0.3614738805970149
[2m[36m(func pid=72016)[0m top5: 0.8638059701492538
[2m[36m(func pid=72016)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=72016)[0m f1_macro: 0.3307520185715376
[2m[36m(func pid=72016)[0m f1_weighted: 0.3533727566839124
[2m[36m(func pid=72016)[0m f1_per_class: [0.49, 0.338, 0.759, 0.558, 0.075, 0.112, 0.29, 0.282, 0.185, 0.219]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.14458955223880596
[2m[36m(func pid=76915)[0m top5: 0.38759328358208955
[2m[36m(func pid=76915)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=76915)[0m f1_macro: 0.12684535648925327
[2m[36m(func pid=76915)[0m f1_weighted: 0.10055442683366056
[2m[36m(func pid=76915)[0m f1_per_class: [0.0, 0.269, 0.083, 0.0, 0.25, 0.278, 0.0, 0.293, 0.096, 0.0]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2156 | Steps: 2 | Val loss: 1.8096 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:15:44 (running for 00:32:37.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.162 |      0.263 |                   52 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.166 |      0.373 |                   28 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.331 |                   25 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  | 18.498 |      0.127 |                    4 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26492537313432835
[2m[36m(func pid=66105)[0m top5: 0.7495335820895522
[2m[36m(func pid=66105)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=66105)[0m f1_macro: 0.2633215422839508
[2m[36m(func pid=66105)[0m f1_weighted: 0.2411721323892884
[2m[36m(func pid=66105)[0m f1_per_class: [0.386, 0.367, 0.647, 0.46, 0.062, 0.109, 0.012, 0.218, 0.196, 0.176]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0003 | Steps: 2 | Val loss: 4.6129 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 24.4947 | Steps: 2 | Val loss: 44.7965 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=70967)[0m top1: 0.4099813432835821
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=70967)[0m f1_macro: 0.37074027045379393
[2m[36m(func pid=70967)[0m f1_weighted: 0.423415317882557
[2m[36m(func pid=70967)[0m f1_per_class: [0.482, 0.297, 0.759, 0.485, 0.133, 0.242, 0.585, 0.158, 0.173, 0.393]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0600 | Steps: 2 | Val loss: 2.0481 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=72016)[0m top1: 0.37080223880597013
[2m[36m(func pid=72016)[0m top5: 0.8703358208955224
[2m[36m(func pid=72016)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=72016)[0m f1_macro: 0.3346929256972697
[2m[36m(func pid=72016)[0m f1_weighted: 0.36518947573216665
[2m[36m(func pid=72016)[0m f1_per_class: [0.477, 0.342, 0.759, 0.557, 0.075, 0.103, 0.332, 0.284, 0.196, 0.223]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.10494402985074627
[2m[36m(func pid=76915)[0m top5: 0.375
[2m[36m(func pid=76915)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=76915)[0m f1_macro: 0.11673453353528299
[2m[36m(func pid=76915)[0m f1_weighted: 0.0765843774479026
[2m[36m(func pid=76915)[0m f1_per_class: [0.0, 0.232, 0.276, 0.0, 0.08, 0.122, 0.0, 0.262, 0.138, 0.058]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1888 | Steps: 2 | Val loss: 1.8283 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=66105)[0m top1: 0.2681902985074627
[2m[36m(func pid=66105)[0m top5: 0.7476679104477612
[2m[36m(func pid=66105)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=66105)[0m f1_macro: 0.2618279508049526
[2m[36m(func pid=66105)[0m f1_weighted: 0.24390349187604268
[2m[36m(func pid=66105)[0m f1_per_class: [0.388, 0.377, 0.611, 0.468, 0.059, 0.111, 0.009, 0.217, 0.192, 0.187]
== Status ==
Current time: 2024-01-07 01:15:50 (running for 00:32:42.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.06  |      0.262 |                   53 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.216 |      0.371 |                   29 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.335 |                   26 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  | 24.495 |      0.117 |                    5 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0009 | Steps: 2 | Val loss: 4.6517 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 16.9097 | Steps: 2 | Val loss: 37.3597 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=70967)[0m top1: 0.40904850746268656
[2m[36m(func pid=70967)[0m top5: 0.8857276119402985
[2m[36m(func pid=70967)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=70967)[0m f1_macro: 0.3785044375939432
[2m[36m(func pid=70967)[0m f1_weighted: 0.4279461536704082
[2m[36m(func pid=70967)[0m f1_per_class: [0.474, 0.299, 0.815, 0.508, 0.128, 0.221, 0.579, 0.187, 0.182, 0.393]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.0516 | Steps: 2 | Val loss: 2.0470 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=72016)[0m top1: 0.3810634328358209
[2m[36m(func pid=72016)[0m top5: 0.8763992537313433
[2m[36m(func pid=72016)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.3350487995081721
[2m[36m(func pid=72016)[0m f1_weighted: 0.3778344358152118
[2m[36m(func pid=72016)[0m f1_per_class: [0.474, 0.353, 0.759, 0.559, 0.071, 0.106, 0.374, 0.233, 0.202, 0.22]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.12360074626865672
[2m[36m(func pid=76915)[0m top5: 0.404384328358209
[2m[36m(func pid=76915)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=76915)[0m f1_macro: 0.23631576254217418
[2m[36m(func pid=76915)[0m f1_weighted: 0.1019095252226356
[2m[36m(func pid=76915)[0m f1_per_class: [0.5, 0.331, 0.727, 0.0, 0.029, 0.103, 0.0, 0.145, 0.175, 0.353]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2029 | Steps: 2 | Val loss: 1.8440 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:15:55 (running for 00:32:47.56)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.052 |      0.264 |                   54 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.189 |      0.379 |                   30 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.335 |                   27 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  | 16.91  |      0.236 |                    6 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2667910447761194
[2m[36m(func pid=66105)[0m top5: 0.7509328358208955
[2m[36m(func pid=66105)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=66105)[0m f1_macro: 0.2641797836861347
[2m[36m(func pid=66105)[0m f1_weighted: 0.24328365768135843
[2m[36m(func pid=66105)[0m f1_per_class: [0.388, 0.369, 0.629, 0.471, 0.062, 0.117, 0.006, 0.215, 0.191, 0.194]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0009 | Steps: 2 | Val loss: 4.6662 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 6.9348 | Steps: 2 | Val loss: 22.9793 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=70967)[0m top1: 0.40111940298507465
[2m[36m(func pid=70967)[0m top5: 0.8871268656716418
[2m[36m(func pid=70967)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=70967)[0m f1_macro: 0.3803515458708983
[2m[36m(func pid=70967)[0m f1_weighted: 0.4250336154650795
[2m[36m(func pid=70967)[0m f1_per_class: [0.465, 0.297, 0.815, 0.52, 0.129, 0.227, 0.552, 0.212, 0.188, 0.4]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9937 | Steps: 2 | Val loss: 2.0496 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=72016)[0m top1: 0.3880597014925373
[2m[36m(func pid=72016)[0m top5: 0.882929104477612
[2m[36m(func pid=72016)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=72016)[0m f1_macro: 0.34060495646414335
[2m[36m(func pid=72016)[0m f1_weighted: 0.3851214659228001
[2m[36m(func pid=72016)[0m f1_per_class: [0.471, 0.349, 0.774, 0.56, 0.073, 0.113, 0.396, 0.238, 0.205, 0.228]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.14878731343283583
[2m[36m(func pid=76915)[0m top5: 0.6012126865671642
[2m[36m(func pid=76915)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=76915)[0m f1_macro: 0.12916587940665894
[2m[36m(func pid=76915)[0m f1_weighted: 0.11817975949297439
[2m[36m(func pid=76915)[0m f1_per_class: [0.217, 0.22, 0.267, 0.0, 0.069, 0.209, 0.149, 0.0, 0.161, 0.0]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:16:00 (running for 00:32:52.97)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.994 |      0.265 |                   55 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.203 |      0.38  |                   31 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.341 |                   28 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  6.935 |      0.129 |                    7 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26492537313432835
[2m[36m(func pid=66105)[0m top5: 0.7541977611940298
[2m[36m(func pid=66105)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=66105)[0m f1_macro: 0.2647594090654069
[2m[36m(func pid=66105)[0m f1_weighted: 0.2416084154006156
[2m[36m(func pid=66105)[0m f1_per_class: [0.395, 0.368, 0.629, 0.467, 0.058, 0.108, 0.006, 0.218, 0.197, 0.203]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0945 | Steps: 2 | Val loss: 1.8518 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0002 | Steps: 2 | Val loss: 4.6946 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.9087 | Steps: 2 | Val loss: 13.6299 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=70967)[0m top1: 0.3927238805970149
[2m[36m(func pid=70967)[0m top5: 0.8931902985074627
[2m[36m(func pid=70967)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=70967)[0m f1_macro: 0.37671850876422525
[2m[36m(func pid=70967)[0m f1_weighted: 0.421913527772731
[2m[36m(func pid=70967)[0m f1_per_class: [0.454, 0.319, 0.815, 0.523, 0.111, 0.236, 0.52, 0.229, 0.203, 0.358]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9633 | Steps: 2 | Val loss: 2.0536 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=76915)[0m top1: 0.39552238805970147
[2m[36m(func pid=76915)[0m top5: 0.8722014925373134
[2m[36m(func pid=76915)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=76915)[0m f1_macro: 0.20438092492377388
[2m[36m(func pid=76915)[0m f1_weighted: 0.37396872349803323
[2m[36m(func pid=76915)[0m f1_per_class: [0.163, 0.021, 0.143, 0.509, 0.19, 0.346, 0.607, 0.0, 0.064, 0.0]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3908582089552239
[2m[36m(func pid=72016)[0m top5: 0.8871268656716418
[2m[36m(func pid=72016)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=72016)[0m f1_macro: 0.32850920673182454
[2m[36m(func pid=72016)[0m f1_weighted: 0.3861131798675922
[2m[36m(func pid=72016)[0m f1_per_class: [0.462, 0.34, 0.706, 0.558, 0.074, 0.108, 0.421, 0.189, 0.184, 0.244]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:16:05 (running for 00:32:58.18)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.963 |      0.267 |                   56 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.094 |      0.377 |                   32 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.329 |                   29 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  3.909 |      0.204 |                    8 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2630597014925373
[2m[36m(func pid=66105)[0m top5: 0.7588619402985075
[2m[36m(func pid=66105)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=66105)[0m f1_macro: 0.26727302712021483
[2m[36m(func pid=66105)[0m f1_weighted: 0.24194988451317864
[2m[36m(func pid=66105)[0m f1_per_class: [0.392, 0.376, 0.629, 0.462, 0.068, 0.109, 0.006, 0.213, 0.205, 0.212]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1170 | Steps: 2 | Val loss: 1.8989 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 7.9515 | Steps: 2 | Val loss: 16.2712 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0016 | Steps: 2 | Val loss: 4.7976 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=70967)[0m top1: 0.3805970149253731
[2m[36m(func pid=70967)[0m top5: 0.8903917910447762
[2m[36m(func pid=70967)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=70967)[0m f1_macro: 0.37557064595941564
[2m[36m(func pid=70967)[0m f1_weighted: 0.4139071181319232
[2m[36m(func pid=70967)[0m f1_per_class: [0.458, 0.336, 0.815, 0.528, 0.101, 0.237, 0.474, 0.244, 0.209, 0.353]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.0134 | Steps: 2 | Val loss: 2.0501 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=76915)[0m top1: 0.42350746268656714
[2m[36m(func pid=76915)[0m top5: 0.898320895522388
[2m[36m(func pid=76915)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=76915)[0m f1_macro: 0.2616522834879979
[2m[36m(func pid=76915)[0m f1_weighted: 0.3436472299591106
[2m[36m(func pid=76915)[0m f1_per_class: [0.466, 0.011, 0.632, 0.561, 0.16, 0.138, 0.504, 0.0, 0.146, 0.0]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.39365671641791045
[2m[36m(func pid=72016)[0m top5: 0.8871268656716418
[2m[36m(func pid=72016)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=72016)[0m f1_macro: 0.3252006418260952
[2m[36m(func pid=72016)[0m f1_weighted: 0.38755538952597907
[2m[36m(func pid=72016)[0m f1_per_class: [0.46, 0.343, 0.706, 0.559, 0.075, 0.109, 0.432, 0.144, 0.179, 0.246]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:16:11 (running for 00:33:03.40)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  1.013 |      0.267 |                   57 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.117 |      0.376 |                   33 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.002 |      0.325 |                   30 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  7.951 |      0.262 |                    9 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.259794776119403
[2m[36m(func pid=66105)[0m top5: 0.7639925373134329
[2m[36m(func pid=66105)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=66105)[0m f1_macro: 0.26688733049694335
[2m[36m(func pid=66105)[0m f1_weighted: 0.23901005333611292
[2m[36m(func pid=66105)[0m f1_per_class: [0.405, 0.362, 0.629, 0.46, 0.069, 0.11, 0.006, 0.213, 0.198, 0.217]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0564 | Steps: 2 | Val loss: 1.9304 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 8.5781 | Steps: 2 | Val loss: 23.4608 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0017 | Steps: 2 | Val loss: 4.8906 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=70967)[0m top1: 0.3666044776119403
[2m[36m(func pid=70967)[0m top5: 0.8899253731343284
[2m[36m(func pid=70967)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.37124374180295144
[2m[36m(func pid=70967)[0m f1_weighted: 0.3982561635789783
[2m[36m(func pid=70967)[0m f1_per_class: [0.442, 0.345, 0.815, 0.534, 0.098, 0.221, 0.412, 0.273, 0.214, 0.358]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.9536 | Steps: 2 | Val loss: 2.0409 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=76915)[0m top1: 0.33488805970149255
[2m[36m(func pid=76915)[0m top5: 0.9300373134328358
[2m[36m(func pid=76915)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=76915)[0m f1_macro: 0.23582858315042654
[2m[36m(func pid=76915)[0m f1_weighted: 0.25794811878184354
[2m[36m(func pid=76915)[0m f1_per_class: [0.62, 0.005, 0.556, 0.534, 0.091, 0.008, 0.258, 0.158, 0.128, 0.0]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3969216417910448
[2m[36m(func pid=72016)[0m top5: 0.8880597014925373
[2m[36m(func pid=72016)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=72016)[0m f1_macro: 0.3246528681623401
[2m[36m(func pid=72016)[0m f1_weighted: 0.3925741791392294
[2m[36m(func pid=72016)[0m f1_per_class: [0.446, 0.344, 0.706, 0.563, 0.072, 0.11, 0.445, 0.148, 0.182, 0.232]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0778 | Steps: 2 | Val loss: 1.9722 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:16:16 (running for 00:33:08.95)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.954 |      0.271 |                   58 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.056 |      0.371 |                   34 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.002 |      0.325 |                   31 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  8.578 |      0.236 |                   10 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26026119402985076
[2m[36m(func pid=66105)[0m top5: 0.7719216417910447
[2m[36m(func pid=66105)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=66105)[0m f1_macro: 0.2711555901137262
[2m[36m(func pid=66105)[0m f1_weighted: 0.23939332389719117
[2m[36m(func pid=66105)[0m f1_per_class: [0.418, 0.361, 0.647, 0.46, 0.066, 0.108, 0.006, 0.213, 0.201, 0.231]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.3502 | Steps: 2 | Val loss: 25.1481 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0011 | Steps: 2 | Val loss: 4.9592 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70967)[0m top1: 0.36100746268656714
[2m[36m(func pid=70967)[0m top5: 0.8847947761194029
[2m[36m(func pid=70967)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=70967)[0m f1_macro: 0.3718450226089022
[2m[36m(func pid=70967)[0m f1_weighted: 0.38901721770554204
[2m[36m(func pid=70967)[0m f1_per_class: [0.455, 0.365, 0.815, 0.548, 0.097, 0.23, 0.352, 0.273, 0.215, 0.369]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9381 | Steps: 2 | Val loss: 2.0331 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=76915)[0m top1: 0.25
[2m[36m(func pid=76915)[0m top5: 0.8586753731343284
[2m[36m(func pid=76915)[0m f1_micro: 0.25
[2m[36m(func pid=76915)[0m f1_macro: 0.22919013684359918
[2m[36m(func pid=76915)[0m f1_weighted: 0.21405391808472068
[2m[36m(func pid=76915)[0m f1_per_class: [0.407, 0.067, 0.7, 0.559, 0.108, 0.0, 0.06, 0.16, 0.154, 0.077]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.40205223880597013
[2m[36m(func pid=72016)[0m top5: 0.8908582089552238
[2m[36m(func pid=72016)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=72016)[0m f1_macro: 0.3241158856122238
[2m[36m(func pid=72016)[0m f1_weighted: 0.3946659246711144
[2m[36m(func pid=72016)[0m f1_per_class: [0.454, 0.335, 0.706, 0.564, 0.075, 0.112, 0.461, 0.11, 0.189, 0.236]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0413 | Steps: 2 | Val loss: 2.0262 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:16:22 (running for 00:33:14.37)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.938 |      0.273 |                   59 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.078 |      0.372 |                   35 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.324 |                   32 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  4.35  |      0.229 |                   11 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2621268656716418
[2m[36m(func pid=66105)[0m top5: 0.777518656716418
[2m[36m(func pid=66105)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=66105)[0m f1_macro: 0.27323913147511125
[2m[36m(func pid=66105)[0m f1_weighted: 0.24170998765093962
[2m[36m(func pid=66105)[0m f1_per_class: [0.42, 0.366, 0.647, 0.461, 0.062, 0.107, 0.009, 0.219, 0.205, 0.235]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4517 | Steps: 2 | Val loss: 29.9611 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0046 | Steps: 2 | Val loss: 5.0929 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=70967)[0m top1: 0.3498134328358209
[2m[36m(func pid=70967)[0m top5: 0.8805970149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=70967)[0m f1_macro: 0.3664351010320452
[2m[36m(func pid=70967)[0m f1_weighted: 0.37064039145441957
[2m[36m(func pid=70967)[0m f1_per_class: [0.466, 0.369, 0.815, 0.534, 0.095, 0.236, 0.295, 0.295, 0.217, 0.343]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8254 | Steps: 2 | Val loss: 2.0197 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=76915)[0m top1: 0.23647388059701493
[2m[36m(func pid=76915)[0m top5: 0.7238805970149254
[2m[36m(func pid=76915)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=76915)[0m f1_macro: 0.2891277790728635
[2m[36m(func pid=76915)[0m f1_weighted: 0.23173841326042094
[2m[36m(func pid=76915)[0m f1_per_class: [0.296, 0.382, 0.783, 0.448, 0.118, 0.023, 0.015, 0.181, 0.185, 0.458]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.40205223880597013
[2m[36m(func pid=72016)[0m top5: 0.8931902985074627
[2m[36m(func pid=72016)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=72016)[0m f1_macro: 0.3280412989803883
[2m[36m(func pid=72016)[0m f1_weighted: 0.3903837628740806
[2m[36m(func pid=72016)[0m f1_per_class: [0.455, 0.331, 0.774, 0.565, 0.073, 0.08, 0.462, 0.084, 0.196, 0.26]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0855 | Steps: 2 | Val loss: 2.0982 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:16:27 (running for 00:33:19.68)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.825 |      0.276 |                   60 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.041 |      0.366 |                   36 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.005 |      0.328 |                   33 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.452 |      0.289 |                   12 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.26725746268656714
[2m[36m(func pid=66105)[0m top5: 0.7831156716417911
[2m[36m(func pid=66105)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=66105)[0m f1_macro: 0.2760041678955238
[2m[36m(func pid=66105)[0m f1_weighted: 0.24599422923774372
[2m[36m(func pid=66105)[0m f1_per_class: [0.428, 0.371, 0.629, 0.468, 0.077, 0.119, 0.009, 0.22, 0.2, 0.239]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7975 | Steps: 2 | Val loss: 42.3298 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0034 | Steps: 2 | Val loss: 5.1970 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=70967)[0m top1: 0.3414179104477612
[2m[36m(func pid=70967)[0m top5: 0.8726679104477612
[2m[36m(func pid=70967)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=70967)[0m f1_macro: 0.36037072650094026
[2m[36m(func pid=70967)[0m f1_weighted: 0.3572402399392394
[2m[36m(func pid=70967)[0m f1_per_class: [0.467, 0.383, 0.815, 0.526, 0.094, 0.229, 0.252, 0.295, 0.218, 0.324]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7869 | Steps: 2 | Val loss: 2.0061 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=76915)[0m top1: 0.20708955223880596
[2m[36m(func pid=76915)[0m top5: 0.5848880597014925
[2m[36m(func pid=76915)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=76915)[0m f1_macro: 0.2467842977356852
[2m[36m(func pid=76915)[0m f1_weighted: 0.1544558483919676
[2m[36m(func pid=76915)[0m f1_per_class: [0.163, 0.385, 0.88, 0.175, 0.058, 0.023, 0.006, 0.263, 0.212, 0.302]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.4025186567164179
[2m[36m(func pid=72016)[0m top5: 0.8964552238805971
[2m[36m(func pid=72016)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=72016)[0m f1_macro: 0.31870502997413414
[2m[36m(func pid=72016)[0m f1_weighted: 0.39120609042720567
[2m[36m(func pid=72016)[0m f1_per_class: [0.433, 0.345, 0.706, 0.56, 0.076, 0.088, 0.467, 0.057, 0.194, 0.262]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0413 | Steps: 2 | Val loss: 2.1551 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:16:32 (running for 00:33:25.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.787 |      0.278 |                   61 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.085 |      0.36  |                   37 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.003 |      0.319 |                   34 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.797 |      0.247 |                   13 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2681902985074627
[2m[36m(func pid=66105)[0m top5: 0.7943097014925373
[2m[36m(func pid=66105)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=66105)[0m f1_macro: 0.2776551508546378
[2m[36m(func pid=66105)[0m f1_weighted: 0.24718120268108357
[2m[36m(func pid=66105)[0m f1_per_class: [0.43, 0.367, 0.629, 0.467, 0.071, 0.125, 0.012, 0.224, 0.204, 0.248]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1385 | Steps: 2 | Val loss: 58.5471 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=70967)[0m top1: 0.3353544776119403
[2m[36m(func pid=70967)[0m top5: 0.867070895522388
[2m[36m(func pid=70967)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.3569126217101494
[2m[36m(func pid=70967)[0m f1_weighted: 0.3444361043947958
[2m[36m(func pid=70967)[0m f1_per_class: [0.459, 0.392, 0.815, 0.529, 0.101, 0.22, 0.205, 0.288, 0.223, 0.338]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0080 | Steps: 2 | Val loss: 5.3261 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.8069 | Steps: 2 | Val loss: 1.9919 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=76915)[0m top1: 0.1958955223880597
[2m[36m(func pid=76915)[0m top5: 0.48367537313432835
[2m[36m(func pid=76915)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=76915)[0m f1_macro: 0.24245255879664657
[2m[36m(func pid=76915)[0m f1_weighted: 0.11328342644796367
[2m[36m(func pid=76915)[0m f1_per_class: [0.2, 0.336, 0.88, 0.026, 0.053, 0.03, 0.009, 0.351, 0.26, 0.28]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.404384328358209
[2m[36m(func pid=72016)[0m top5: 0.8964552238805971
[2m[36m(func pid=72016)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.3198689907563247
[2m[36m(func pid=72016)[0m f1_weighted: 0.3914702237547073
[2m[36m(func pid=72016)[0m f1_per_class: [0.443, 0.351, 0.706, 0.56, 0.075, 0.076, 0.47, 0.043, 0.196, 0.279]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0433 | Steps: 2 | Val loss: 2.2082 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:16:37 (running for 00:33:30.21)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.807 |      0.278 |                   62 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.041 |      0.357 |                   38 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.008 |      0.32  |                   35 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.138 |      0.242 |                   14 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.27005597014925375
[2m[36m(func pid=66105)[0m top5: 0.8027052238805971
[2m[36m(func pid=66105)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=66105)[0m f1_macro: 0.277943727315767
[2m[36m(func pid=66105)[0m f1_weighted: 0.24989459084684867
[2m[36m(func pid=66105)[0m f1_per_class: [0.433, 0.372, 0.611, 0.468, 0.07, 0.142, 0.012, 0.225, 0.194, 0.252]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.3917 | Steps: 2 | Val loss: 73.2859 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=70967)[0m top1: 0.33255597014925375
[2m[36m(func pid=70967)[0m top5: 0.8638059701492538
[2m[36m(func pid=70967)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=70967)[0m f1_macro: 0.35463147394265243
[2m[36m(func pid=70967)[0m f1_weighted: 0.33693366755200727
[2m[36m(func pid=70967)[0m f1_per_class: [0.464, 0.399, 0.815, 0.528, 0.096, 0.224, 0.174, 0.295, 0.227, 0.325]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0089 | Steps: 2 | Val loss: 5.4224 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8549 | Steps: 2 | Val loss: 1.9749 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=76915)[0m top1: 0.19076492537313433
[2m[36m(func pid=76915)[0m top5: 0.43423507462686567
[2m[36m(func pid=76915)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=76915)[0m f1_macro: 0.2426212287533628
[2m[36m(func pid=76915)[0m f1_weighted: 0.10504187574831472
[2m[36m(func pid=76915)[0m f1_per_class: [0.269, 0.313, 0.815, 0.0, 0.053, 0.023, 0.003, 0.42, 0.294, 0.236]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0554 | Steps: 2 | Val loss: 2.2571 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=72016)[0m top1: 0.4048507462686567
[2m[36m(func pid=72016)[0m top5: 0.8955223880597015
[2m[36m(func pid=72016)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=72016)[0m f1_macro: 0.3193041772813974
[2m[36m(func pid=72016)[0m f1_weighted: 0.39499913201759795
[2m[36m(func pid=72016)[0m f1_per_class: [0.435, 0.367, 0.706, 0.563, 0.072, 0.075, 0.471, 0.057, 0.173, 0.273]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:16:43 (running for 00:33:35.45)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.855 |      0.282 |                   63 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.043 |      0.355 |                   39 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.009 |      0.319 |                   36 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.392 |      0.243 |                   15 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2719216417910448
[2m[36m(func pid=66105)[0m top5: 0.8125
[2m[36m(func pid=66105)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=66105)[0m f1_macro: 0.2824108341418608
[2m[36m(func pid=66105)[0m f1_weighted: 0.2546905423993012
[2m[36m(func pid=66105)[0m f1_per_class: [0.433, 0.374, 0.629, 0.467, 0.065, 0.137, 0.027, 0.231, 0.201, 0.26]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.2880 | Steps: 2 | Val loss: 79.1495 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=70967)[0m top1: 0.3292910447761194
[2m[36m(func pid=70967)[0m top5: 0.8619402985074627
[2m[36m(func pid=70967)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=70967)[0m f1_macro: 0.35190015343597925
[2m[36m(func pid=70967)[0m f1_weighted: 0.3289493684867317
[2m[36m(func pid=70967)[0m f1_per_class: [0.47, 0.396, 0.815, 0.539, 0.093, 0.209, 0.144, 0.294, 0.23, 0.329]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0018 | Steps: 2 | Val loss: 5.4838 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6966 | Steps: 2 | Val loss: 1.9633 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=76915)[0m top1: 0.16744402985074627
[2m[36m(func pid=76915)[0m top5: 0.4253731343283582
[2m[36m(func pid=76915)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=76915)[0m f1_macro: 0.2503871753153376
[2m[36m(func pid=76915)[0m f1_weighted: 0.10584029709297989
[2m[36m(func pid=76915)[0m f1_per_class: [0.516, 0.291, 0.759, 0.0, 0.041, 0.036, 0.006, 0.388, 0.299, 0.168]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0394 | Steps: 2 | Val loss: 2.2978 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:16:48 (running for 00:33:40.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.697 |      0.287 |                   64 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.055 |      0.352 |                   40 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.009 |      0.319 |                   36 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.288 |      0.25  |                   16 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2728544776119403
[2m[36m(func pid=66105)[0m top5: 0.8166977611940298
[2m[36m(func pid=66105)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=66105)[0m f1_macro: 0.28712095817829336
[2m[36m(func pid=66105)[0m f1_weighted: 0.2576587844286712
[2m[36m(func pid=66105)[0m f1_per_class: [0.443, 0.369, 0.647, 0.467, 0.072, 0.143, 0.036, 0.234, 0.196, 0.263]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m top1: 0.4099813432835821
[2m[36m(func pid=72016)[0m top5: 0.8955223880597015
[2m[36m(func pid=72016)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=72016)[0m f1_macro: 0.31835999961034067
[2m[36m(func pid=72016)[0m f1_weighted: 0.3990692274282474
[2m[36m(func pid=72016)[0m f1_per_class: [0.435, 0.367, 0.706, 0.565, 0.074, 0.076, 0.485, 0.044, 0.164, 0.266]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1139 | Steps: 2 | Val loss: 84.3003 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=70967)[0m top1: 0.3278917910447761
[2m[36m(func pid=70967)[0m top5: 0.8628731343283582
[2m[36m(func pid=70967)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=70967)[0m f1_macro: 0.3534136895158114
[2m[36m(func pid=70967)[0m f1_weighted: 0.32767381554078406
[2m[36m(func pid=70967)[0m f1_per_class: [0.473, 0.405, 0.815, 0.532, 0.092, 0.207, 0.141, 0.292, 0.227, 0.35]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7159 | Steps: 2 | Val loss: 1.9474 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0006 | Steps: 2 | Val loss: 5.6400 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=76915)[0m top1: 0.13805970149253732
[2m[36m(func pid=76915)[0m top5: 0.4207089552238806
[2m[36m(func pid=76915)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=76915)[0m f1_macro: 0.23573543751489293
[2m[36m(func pid=76915)[0m f1_weighted: 0.10047235841989191
[2m[36m(func pid=76915)[0m f1_per_class: [0.63, 0.275, 0.71, 0.0, 0.04, 0.022, 0.019, 0.32, 0.24, 0.102]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0218 | Steps: 2 | Val loss: 2.3372 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=66105)[0m top1: 0.28125
[2m[36m(func pid=66105)[0m top5: 0.8236940298507462
[2m[36m(func pid=66105)[0m f1_micro: 0.28125
[2m[36m(func pid=66105)[0m f1_macro: 0.29265480472184463
[2m[36m(func pid=66105)[0m f1_weighted: 0.27097609557509783
[2m[36m(func pid=66105)[0m f1_per_class: [0.443, 0.37, 0.647, 0.476, 0.061, 0.146, 0.068, 0.243, 0.206, 0.265]
[2m[36m(func pid=66105)[0m 
== Status ==
Current time: 2024-01-07 01:16:53 (running for 00:33:45.78)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.716 |      0.293 |                   65 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.039 |      0.353 |                   41 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.002 |      0.318 |                   37 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.114 |      0.236 |                   17 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=72016)[0m top1: 0.40578358208955223
[2m[36m(func pid=72016)[0m top5: 0.8955223880597015
[2m[36m(func pid=72016)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=72016)[0m f1_macro: 0.3155358020843838
[2m[36m(func pid=72016)[0m f1_weighted: 0.39256947933552716
[2m[36m(func pid=72016)[0m f1_per_class: [0.443, 0.348, 0.686, 0.56, 0.076, 0.076, 0.476, 0.058, 0.169, 0.264]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.9896 | Steps: 2 | Val loss: 88.4259 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=70967)[0m top1: 0.3278917910447761
[2m[36m(func pid=70967)[0m top5: 0.855410447761194
[2m[36m(func pid=70967)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=70967)[0m f1_macro: 0.35444318815042875
[2m[36m(func pid=70967)[0m f1_weighted: 0.3242227798318795
[2m[36m(func pid=70967)[0m f1_per_class: [0.464, 0.405, 0.815, 0.53, 0.094, 0.213, 0.128, 0.292, 0.233, 0.37]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6881 | Steps: 2 | Val loss: 1.9303 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=76915)[0m top1: 0.12546641791044777
[2m[36m(func pid=76915)[0m top5: 0.4314365671641791
[2m[36m(func pid=76915)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=76915)[0m f1_macro: 0.2240119128110576
[2m[36m(func pid=76915)[0m f1_weighted: 0.10141147328235292
[2m[36m(func pid=76915)[0m f1_per_class: [0.63, 0.274, 0.629, 0.007, 0.034, 0.034, 0.019, 0.303, 0.225, 0.086]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0030 | Steps: 2 | Val loss: 5.6942 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0287 | Steps: 2 | Val loss: 2.3763 | Batch size: 32 | lr: 0.001 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 01:16:58 (running for 00:33:51.11)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.688 |      0.3   |                   66 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.022 |      0.354 |                   42 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.316 |                   38 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.99  |      0.224 |                   18 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.28591417910447764
[2m[36m(func pid=66105)[0m top5: 0.8306902985074627
[2m[36m(func pid=66105)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=66105)[0m f1_macro: 0.300182814318957
[2m[36m(func pid=66105)[0m f1_weighted: 0.2778655556308722
[2m[36m(func pid=66105)[0m f1_per_class: [0.437, 0.372, 0.667, 0.472, 0.069, 0.147, 0.09, 0.252, 0.219, 0.277]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m top1: 0.40904850746268656
[2m[36m(func pid=72016)[0m top5: 0.8969216417910447
[2m[36m(func pid=72016)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=72016)[0m f1_macro: 0.31880841807467497
[2m[36m(func pid=72016)[0m f1_weighted: 0.39543977925080215
[2m[36m(func pid=72016)[0m f1_per_class: [0.448, 0.365, 0.706, 0.56, 0.078, 0.076, 0.478, 0.043, 0.169, 0.264]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7108 | Steps: 2 | Val loss: 88.8300 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=70967)[0m top1: 0.3302238805970149
[2m[36m(func pid=70967)[0m top5: 0.8530783582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=70967)[0m f1_macro: 0.35466142939583467
[2m[36m(func pid=70967)[0m f1_weighted: 0.32147442339874965
[2m[36m(func pid=70967)[0m f1_per_class: [0.473, 0.413, 0.815, 0.542, 0.095, 0.21, 0.104, 0.293, 0.227, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6162 | Steps: 2 | Val loss: 1.9189 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=76915)[0m top1: 0.11893656716417911
[2m[36m(func pid=76915)[0m top5: 0.43703358208955223
[2m[36m(func pid=76915)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=76915)[0m f1_macro: 0.21209198421157804
[2m[36m(func pid=76915)[0m f1_weighted: 0.10069742310992534
[2m[36m(func pid=76915)[0m f1_per_class: [0.554, 0.254, 0.615, 0.017, 0.034, 0.065, 0.022, 0.264, 0.203, 0.092]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.1052 | Steps: 2 | Val loss: 5.8388 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0298 | Steps: 2 | Val loss: 2.4034 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=66105)[0m top1: 0.28824626865671643
[2m[36m(func pid=66105)[0m top5: 0.8353544776119403
[2m[36m(func pid=66105)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=66105)[0m f1_macro: 0.30117884756978086
[2m[36m(func pid=66105)[0m f1_weighted: 0.28014776908674544
[2m[36m(func pid=66105)[0m f1_per_class: [0.422, 0.363, 0.667, 0.474, 0.068, 0.149, 0.098, 0.261, 0.234, 0.277]
[2m[36m(func pid=66105)[0m 
== Status ==
Current time: 2024-01-07 01:17:04 (running for 00:33:56.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.616 |      0.301 |                   67 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.029 |      0.355 |                   43 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.003 |      0.319 |                   39 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.711 |      0.212 |                   19 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=72016)[0m top1: 0.40578358208955223
[2m[36m(func pid=72016)[0m top5: 0.8969216417910447
[2m[36m(func pid=72016)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=72016)[0m f1_macro: 0.3146773645742432
[2m[36m(func pid=72016)[0m f1_weighted: 0.3926159824460626
[2m[36m(func pid=72016)[0m f1_per_class: [0.435, 0.352, 0.686, 0.562, 0.078, 0.076, 0.474, 0.057, 0.168, 0.26]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.1201 | Steps: 2 | Val loss: 88.6478 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=70967)[0m top1: 0.3316231343283582
[2m[36m(func pid=70967)[0m top5: 0.8549440298507462
[2m[36m(func pid=70967)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=70967)[0m f1_macro: 0.35360164922709675
[2m[36m(func pid=70967)[0m f1_weighted: 0.3233186523504487
[2m[36m(func pid=70967)[0m f1_per_class: [0.476, 0.424, 0.815, 0.546, 0.093, 0.195, 0.107, 0.289, 0.228, 0.364]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6256 | Steps: 2 | Val loss: 1.9010 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=76915)[0m top1: 0.12033582089552239
[2m[36m(func pid=76915)[0m top5: 0.44822761194029853
[2m[36m(func pid=76915)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=76915)[0m f1_macro: 0.19595410176331046
[2m[36m(func pid=76915)[0m f1_weighted: 0.10586062787294691
[2m[36m(func pid=76915)[0m f1_per_class: [0.402, 0.248, 0.571, 0.036, 0.035, 0.078, 0.031, 0.277, 0.177, 0.103]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0126 | Steps: 2 | Val loss: 5.9380 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0453 | Steps: 2 | Val loss: 2.4213 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 01:17:09 (running for 00:34:01.80)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.626 |      0.302 |                   68 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.03  |      0.354 |                   44 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.105 |      0.315 |                   40 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.12  |      0.196 |                   20 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.2933768656716418
[2m[36m(func pid=66105)[0m top5: 0.8442164179104478
[2m[36m(func pid=66105)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=66105)[0m f1_macro: 0.302062839263851
[2m[36m(func pid=66105)[0m f1_weighted: 0.28796954552037163
[2m[36m(func pid=66105)[0m f1_per_class: [0.409, 0.356, 0.647, 0.484, 0.073, 0.154, 0.116, 0.27, 0.228, 0.283]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m top1: 0.40345149253731344
[2m[36m(func pid=72016)[0m top5: 0.8922574626865671
[2m[36m(func pid=72016)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=72016)[0m f1_macro: 0.3169541489009243
[2m[36m(func pid=72016)[0m f1_weighted: 0.3952721853727521
[2m[36m(func pid=72016)[0m f1_per_class: [0.428, 0.362, 0.686, 0.563, 0.072, 0.081, 0.469, 0.082, 0.166, 0.26]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0000 | Steps: 2 | Val loss: 87.8100 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=70967)[0m top1: 0.33348880597014924
[2m[36m(func pid=70967)[0m top5: 0.8540111940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=70967)[0m f1_macro: 0.35202486338222727
[2m[36m(func pid=70967)[0m f1_weighted: 0.32564070421945374
[2m[36m(func pid=70967)[0m f1_per_class: [0.467, 0.432, 0.786, 0.536, 0.09, 0.201, 0.118, 0.294, 0.222, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5923 | Steps: 2 | Val loss: 1.8866 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=76915)[0m top1: 0.12406716417910447
[2m[36m(func pid=76915)[0m top5: 0.45615671641791045
[2m[36m(func pid=76915)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=76915)[0m f1_macro: 0.17923449673162498
[2m[36m(func pid=76915)[0m f1_weighted: 0.10890459217024567
[2m[36m(func pid=76915)[0m f1_per_class: [0.311, 0.232, 0.471, 0.045, 0.04, 0.121, 0.04, 0.244, 0.171, 0.118]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0691 | Steps: 2 | Val loss: 5.9922 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0295 | Steps: 2 | Val loss: 2.4243 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=66105)[0m top1: 0.2989738805970149
[2m[36m(func pid=66105)[0m top5: 0.8540111940298507
[2m[36m(func pid=66105)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=66105)[0m f1_macro: 0.306490675914319
[2m[36m(func pid=66105)[0m f1_weighted: 0.2990433498023522
[2m[36m(func pid=66105)[0m f1_per_class: [0.409, 0.358, 0.647, 0.481, 0.071, 0.16, 0.152, 0.281, 0.227, 0.28]
== Status ==
Current time: 2024-01-07 01:17:14 (running for 00:34:07.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.592 |      0.306 |                   69 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.045 |      0.352 |                   45 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.013 |      0.317 |                   41 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.179 |                   21 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=72016)[0m top1: 0.40531716417910446
[2m[36m(func pid=72016)[0m top5: 0.8899253731343284
[2m[36m(func pid=72016)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=72016)[0m f1_macro: 0.32193545380334154
[2m[36m(func pid=72016)[0m f1_weighted: 0.3989132257789728
[2m[36m(func pid=72016)[0m f1_per_class: [0.42, 0.382, 0.686, 0.564, 0.072, 0.081, 0.463, 0.118, 0.167, 0.267]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0000 | Steps: 2 | Val loss: 89.8368 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=70967)[0m top1: 0.3353544776119403
[2m[36m(func pid=70967)[0m top5: 0.8549440298507462
[2m[36m(func pid=70967)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.35813650845096645
[2m[36m(func pid=70967)[0m f1_weighted: 0.32870900786269946
[2m[36m(func pid=70967)[0m f1_per_class: [0.464, 0.434, 0.815, 0.532, 0.093, 0.206, 0.128, 0.287, 0.222, 0.4]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6339 | Steps: 2 | Val loss: 1.8719 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=76915)[0m top1: 0.12453358208955224
[2m[36m(func pid=76915)[0m top5: 0.45149253731343286
[2m[36m(func pid=76915)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=76915)[0m f1_macro: 0.1663812229625017
[2m[36m(func pid=76915)[0m f1_weighted: 0.1087501256632359
[2m[36m(func pid=76915)[0m f1_per_class: [0.225, 0.195, 0.4, 0.055, 0.046, 0.151, 0.045, 0.256, 0.163, 0.127]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.0570 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0143 | Steps: 2 | Val loss: 2.4408 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=66105)[0m top1: 0.3041044776119403
[2m[36m(func pid=66105)[0m top5: 0.8642723880597015
[2m[36m(func pid=66105)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=66105)[0m f1_macro: 0.3133710983767689
[2m[36m(func pid=66105)[0m f1_weighted: 0.30879043834353453
[2m[36m(func pid=66105)[0m f1_per_class: [0.419, 0.364, 0.667, 0.489, 0.07, 0.157, 0.174, 0.281, 0.219, 0.295]
== Status ==
Current time: 2024-01-07 01:17:20 (running for 00:34:12.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.634 |      0.313 |                   70 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.03  |      0.358 |                   46 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.069 |      0.322 |                   42 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.166 |                   22 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5108 | Steps: 2 | Val loss: 91.9656 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=70967)[0m top1: 0.3381529850746269
[2m[36m(func pid=70967)[0m top5: 0.8544776119402985
[2m[36m(func pid=70967)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=70967)[0m f1_macro: 0.35535407204098546
[2m[36m(func pid=70967)[0m f1_weighted: 0.33087151884081406
[2m[36m(func pid=70967)[0m f1_per_class: [0.464, 0.433, 0.786, 0.539, 0.094, 0.206, 0.131, 0.289, 0.217, 0.395]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.404384328358209
[2m[36m(func pid=72016)[0m top5: 0.8871268656716418
[2m[36m(func pid=72016)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.3268900855556187
[2m[36m(func pid=72016)[0m f1_weighted: 0.402858825871915
[2m[36m(func pid=72016)[0m f1_per_class: [0.409, 0.396, 0.686, 0.564, 0.071, 0.093, 0.455, 0.151, 0.184, 0.259]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5610 | Steps: 2 | Val loss: 1.8622 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=76915)[0m top1: 0.12313432835820895
[2m[36m(func pid=76915)[0m top5: 0.4566231343283582
[2m[36m(func pid=76915)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=76915)[0m f1_macro: 0.15537719632938957
[2m[36m(func pid=76915)[0m f1_weighted: 0.10573904792593633
[2m[36m(func pid=76915)[0m f1_per_class: [0.178, 0.148, 0.343, 0.067, 0.052, 0.179, 0.048, 0.253, 0.118, 0.167]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0157 | Steps: 2 | Val loss: 2.4806 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.0625 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:17:25 (running for 00:34:17.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.561 |      0.312 |                   71 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.014 |      0.355 |                   47 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.327 |                   43 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.511 |      0.155 |                   23 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.3050373134328358
[2m[36m(func pid=66105)[0m top5: 0.8656716417910447
[2m[36m(func pid=66105)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=66105)[0m f1_macro: 0.3116194111599726
[2m[36m(func pid=66105)[0m f1_weighted: 0.3116583150808079
[2m[36m(func pid=66105)[0m f1_per_class: [0.419, 0.354, 0.647, 0.486, 0.071, 0.16, 0.191, 0.286, 0.213, 0.289]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.33488805970149255
[2m[36m(func pid=70967)[0m top5: 0.8535447761194029
[2m[36m(func pid=70967)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=70967)[0m f1_macro: 0.3571611250418683
[2m[36m(func pid=70967)[0m f1_weighted: 0.32946817745931983
[2m[36m(func pid=70967)[0m f1_per_class: [0.467, 0.431, 0.815, 0.53, 0.088, 0.206, 0.134, 0.292, 0.224, 0.385]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0378 | Steps: 2 | Val loss: 93.8765 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=72016)[0m top1: 0.40671641791044777
[2m[36m(func pid=72016)[0m top5: 0.8861940298507462
[2m[36m(func pid=72016)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=72016)[0m f1_macro: 0.3340064170944223
[2m[36m(func pid=72016)[0m f1_weighted: 0.40753715210538854
[2m[36m(func pid=72016)[0m f1_per_class: [0.411, 0.398, 0.686, 0.567, 0.071, 0.092, 0.456, 0.202, 0.194, 0.263]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6139 | Steps: 2 | Val loss: 1.8396 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=76915)[0m top1: 0.12779850746268656
[2m[36m(func pid=76915)[0m top5: 0.46408582089552236
[2m[36m(func pid=76915)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=76915)[0m f1_macro: 0.15719694799887476
[2m[36m(func pid=76915)[0m f1_weighted: 0.10662760187939771
[2m[36m(func pid=76915)[0m f1_per_class: [0.152, 0.138, 0.329, 0.077, 0.062, 0.195, 0.042, 0.251, 0.122, 0.204]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0104 | Steps: 2 | Val loss: 2.4922 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0001 | Steps: 2 | Val loss: 6.1442 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:17:30 (running for 00:34:23.09)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.614 |      0.319 |                   72 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.016 |      0.357 |                   48 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.334 |                   44 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.038 |      0.157 |                   24 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.31669776119402987
[2m[36m(func pid=66105)[0m top5: 0.871268656716418
[2m[36m(func pid=66105)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=66105)[0m f1_macro: 0.3190207831764688
[2m[36m(func pid=66105)[0m f1_weighted: 0.3289011719520013
[2m[36m(func pid=66105)[0m f1_per_class: [0.414, 0.358, 0.667, 0.49, 0.074, 0.162, 0.242, 0.282, 0.221, 0.28]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1011 | Steps: 2 | Val loss: 96.3256 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=70967)[0m top1: 0.3344216417910448
[2m[36m(func pid=70967)[0m top5: 0.8563432835820896
[2m[36m(func pid=70967)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=70967)[0m f1_macro: 0.35642506302254606
[2m[36m(func pid=70967)[0m f1_weighted: 0.32970829281441105
[2m[36m(func pid=70967)[0m f1_per_class: [0.473, 0.431, 0.815, 0.526, 0.088, 0.2, 0.141, 0.29, 0.221, 0.38]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.4048507462686567
[2m[36m(func pid=72016)[0m top5: 0.882929104477612
[2m[36m(func pid=72016)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=72016)[0m f1_macro: 0.33769028633095316
[2m[36m(func pid=72016)[0m f1_weighted: 0.40833244255432816
[2m[36m(func pid=72016)[0m f1_per_class: [0.409, 0.401, 0.686, 0.571, 0.075, 0.092, 0.448, 0.232, 0.184, 0.279]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.13013059701492538
[2m[36m(func pid=76915)[0m top5: 0.4766791044776119
[2m[36m(func pid=76915)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=76915)[0m f1_macro: 0.1565263292322116
[2m[36m(func pid=76915)[0m f1_weighted: 0.10608130988494795
[2m[36m(func pid=76915)[0m f1_per_class: [0.132, 0.113, 0.3, 0.079, 0.074, 0.218, 0.045, 0.25, 0.12, 0.235]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4899 | Steps: 2 | Val loss: 1.8294 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0133 | Steps: 2 | Val loss: 2.5083 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0006 | Steps: 2 | Val loss: 6.2498 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:17:36 (running for 00:34:28.70)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.49  |      0.318 |                   73 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.01  |      0.356 |                   49 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.338 |                   45 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.101 |      0.157 |                   25 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.3208955223880597
[2m[36m(func pid=66105)[0m top5: 0.8763992537313433
[2m[36m(func pid=66105)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=66105)[0m f1_macro: 0.31806973015664797
[2m[36m(func pid=66105)[0m f1_weighted: 0.33805065836629705
[2m[36m(func pid=66105)[0m f1_per_class: [0.414, 0.36, 0.647, 0.492, 0.073, 0.162, 0.276, 0.257, 0.218, 0.283]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0008 | Steps: 2 | Val loss: 98.2338 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=70967)[0m top1: 0.3376865671641791
[2m[36m(func pid=70967)[0m top5: 0.8544776119402985
[2m[36m(func pid=70967)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=70967)[0m f1_macro: 0.36072812570504126
[2m[36m(func pid=70967)[0m f1_weighted: 0.33360073874778257
[2m[36m(func pid=70967)[0m f1_per_class: [0.484, 0.43, 0.815, 0.524, 0.09, 0.199, 0.154, 0.295, 0.226, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.404384328358209
[2m[36m(func pid=72016)[0m top5: 0.8791977611940298
[2m[36m(func pid=72016)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.33929606996621875
[2m[36m(func pid=72016)[0m f1_weighted: 0.41045887090976574
[2m[36m(func pid=72016)[0m f1_per_class: [0.406, 0.413, 0.667, 0.572, 0.075, 0.091, 0.443, 0.254, 0.204, 0.27]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.13526119402985073
[2m[36m(func pid=76915)[0m top5: 0.48973880597014924
[2m[36m(func pid=76915)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=76915)[0m f1_macro: 0.16864459087258635
[2m[36m(func pid=76915)[0m f1_weighted: 0.11225013238296708
[2m[36m(func pid=76915)[0m f1_per_class: [0.11, 0.126, 0.312, 0.079, 0.084, 0.235, 0.048, 0.25, 0.137, 0.306]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5156 | Steps: 2 | Val loss: 1.8140 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0115 | Steps: 2 | Val loss: 2.5099 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.3476 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0000 | Steps: 2 | Val loss: 101.1997 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:17:41 (running for 00:34:34.17)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00016 | RUNNING    | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.516 |      0.322 |                   74 |
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.013 |      0.361 |                   50 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.339 |                   46 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.001 |      0.169 |                   26 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.3269589552238806
[2m[36m(func pid=66105)[0m top5: 0.8819962686567164
[2m[36m(func pid=66105)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=66105)[0m f1_macro: 0.3222219593195505
[2m[36m(func pid=66105)[0m f1_weighted: 0.3479904752650524
[2m[36m(func pid=66105)[0m f1_per_class: [0.419, 0.356, 0.647, 0.491, 0.065, 0.164, 0.309, 0.265, 0.215, 0.292]
[2m[36m(func pid=66105)[0m 
[2m[36m(func pid=70967)[0m top1: 0.3423507462686567
[2m[36m(func pid=70967)[0m top5: 0.8591417910447762
[2m[36m(func pid=70967)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=70967)[0m f1_macro: 0.36226895400313974
[2m[36m(func pid=70967)[0m f1_weighted: 0.33962965904800924
[2m[36m(func pid=70967)[0m f1_per_class: [0.494, 0.434, 0.815, 0.533, 0.089, 0.206, 0.162, 0.292, 0.228, 0.37]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.4006529850746269
[2m[36m(func pid=72016)[0m top5: 0.8759328358208955
[2m[36m(func pid=72016)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=72016)[0m f1_macro: 0.34153883195546814
[2m[36m(func pid=72016)[0m f1_weighted: 0.4078156667827955
[2m[36m(func pid=72016)[0m f1_per_class: [0.398, 0.419, 0.686, 0.571, 0.076, 0.091, 0.427, 0.28, 0.205, 0.264]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.13899253731343283
[2m[36m(func pid=76915)[0m top5: 0.5
[2m[36m(func pid=76915)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=76915)[0m f1_macro: 0.17423832616616347
[2m[36m(func pid=76915)[0m f1_weighted: 0.11888582763430336
[2m[36m(func pid=76915)[0m f1_per_class: [0.099, 0.142, 0.293, 0.083, 0.097, 0.247, 0.053, 0.243, 0.143, 0.344]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=66105)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5261 | Steps: 2 | Val loss: 1.8079 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0081 | Steps: 2 | Val loss: 2.5164 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.4814 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 01:17:47 (running for 00:34:39.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.012 |      0.362 |                   51 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.342 |                   47 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.174 |                   27 |
| train_57e67_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=66105)[0m top1: 0.332089552238806
[2m[36m(func pid=66105)[0m top5: 0.8838619402985075
[2m[36m(func pid=66105)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=66105)[0m f1_macro: 0.3216489808842095
[2m[36m(func pid=66105)[0m f1_weighted: 0.35485039166486054
[2m[36m(func pid=66105)[0m f1_per_class: [0.417, 0.362, 0.629, 0.489, 0.065, 0.165, 0.333, 0.261, 0.215, 0.283]
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0000 | Steps: 2 | Val loss: 105.1908 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=70967)[0m top1: 0.3460820895522388
[2m[36m(func pid=70967)[0m top5: 0.8596082089552238
[2m[36m(func pid=70967)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=70967)[0m f1_macro: 0.364214604962292
[2m[36m(func pid=70967)[0m f1_weighted: 0.34331807889654997
[2m[36m(func pid=70967)[0m f1_per_class: [0.5, 0.429, 0.815, 0.54, 0.09, 0.205, 0.169, 0.299, 0.229, 0.366]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.39365671641791045
[2m[36m(func pid=72016)[0m top5: 0.8722014925373134
[2m[36m(func pid=72016)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=72016)[0m f1_macro: 0.3379064313357112
[2m[36m(func pid=72016)[0m f1_weighted: 0.39982480056434483
[2m[36m(func pid=72016)[0m f1_per_class: [0.402, 0.412, 0.649, 0.573, 0.076, 0.09, 0.401, 0.289, 0.198, 0.29]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.14085820895522388
[2m[36m(func pid=76915)[0m top5: 0.5060634328358209
[2m[36m(func pid=76915)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=76915)[0m f1_macro: 0.18118321850595315
[2m[36m(func pid=76915)[0m f1_weighted: 0.1241181350841625
[2m[36m(func pid=76915)[0m f1_per_class: [0.09, 0.14, 0.282, 0.095, 0.102, 0.268, 0.051, 0.257, 0.12, 0.407]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0096 | Steps: 2 | Val loss: 2.5244 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.6705 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0011 | Steps: 2 | Val loss: 107.4746 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=70967)[0m top1: 0.34794776119402987
[2m[36m(func pid=70967)[0m top5: 0.8586753731343284
[2m[36m(func pid=70967)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=70967)[0m f1_macro: 0.36510712492887737
[2m[36m(func pid=70967)[0m f1_weighted: 0.3458183960853446
[2m[36m(func pid=70967)[0m f1_per_class: [0.494, 0.428, 0.815, 0.538, 0.091, 0.203, 0.179, 0.306, 0.232, 0.366]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3843283582089552
[2m[36m(func pid=72016)[0m top5: 0.8652052238805971
[2m[36m(func pid=72016)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=72016)[0m f1_macro: 0.3318940456040532
[2m[36m(func pid=72016)[0m f1_weighted: 0.3911103597561557
[2m[36m(func pid=72016)[0m f1_per_class: [0.4, 0.421, 0.649, 0.569, 0.075, 0.089, 0.373, 0.284, 0.199, 0.261]
[2m[36m(func pid=76915)[0m top1: 0.14039179104477612
[2m[36m(func pid=76915)[0m top5: 0.5163246268656716
[2m[36m(func pid=76915)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=76915)[0m f1_macro: 0.18471007176140045
[2m[36m(func pid=76915)[0m f1_weighted: 0.12816936349054672
[2m[36m(func pid=76915)[0m f1_per_class: [0.084, 0.135, 0.286, 0.107, 0.092, 0.282, 0.051, 0.251, 0.121, 0.439]
== Status ==
Current time: 2024-01-07 01:17:52 (running for 00:34:45.17)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.01  |      0.365 |                   53 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.338 |                   48 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.181 |                   28 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=83527)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=83527)[0m Configuration completed!
[2m[36m(func pid=83527)[0m New optimizer parameters:
[2m[36m(func pid=83527)[0m SGD (
[2m[36m(func pid=83527)[0m Parameter Group 0
[2m[36m(func pid=83527)[0m     dampening: 0
[2m[36m(func pid=83527)[0m     differentiable: False
[2m[36m(func pid=83527)[0m     foreach: None
[2m[36m(func pid=83527)[0m     lr: 0.0001
[2m[36m(func pid=83527)[0m     maximize: False
[2m[36m(func pid=83527)[0m     momentum: 0.9
[2m[36m(func pid=83527)[0m     nesterov: False
[2m[36m(func pid=83527)[0m     weight_decay: 1e-05
[2m[36m(func pid=83527)[0m )
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0098 | Steps: 2 | Val loss: 2.5374 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=70967)[0m top1: 0.345615671641791
[2m[36m(func pid=70967)[0m top5: 0.8596082089552238
[2m[36m(func pid=70967)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=70967)[0m f1_macro: 0.36331765812819283
[2m[36m(func pid=70967)[0m f1_weighted: 0.34488636490993624
[2m[36m(func pid=70967)[0m f1_per_class: [0.5, 0.427, 0.815, 0.526, 0.089, 0.201, 0.189, 0.313, 0.228, 0.346]
[2m[36m(func pid=70967)[0m 
== Status ==
Current time: 2024-01-07 01:17:58 (running for 00:34:50.29)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.01  |      0.363 |                   54 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.332 |                   49 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.001 |      0.185 |                   29 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0000 | Steps: 2 | Val loss: 112.4996 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 6.8130 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1621 | Steps: 2 | Val loss: 2.5189 | Batch size: 32 | lr: 0.0001 | Duration: 4.63s
[2m[36m(func pid=76915)[0m top1: 0.13619402985074627
[2m[36m(func pid=76915)[0m top5: 0.5181902985074627
[2m[36m(func pid=76915)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=76915)[0m f1_macro: 0.17548158220068158
[2m[36m(func pid=76915)[0m f1_weighted: 0.12538211334454208
[2m[36m(func pid=76915)[0m f1_per_class: [0.077, 0.14, 0.261, 0.094, 0.103, 0.283, 0.051, 0.254, 0.138, 0.353]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.37826492537313433
[2m[36m(func pid=72016)[0m top5: 0.8605410447761194
[2m[36m(func pid=72016)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=72016)[0m f1_macro: 0.32907111205765827
[2m[36m(func pid=72016)[0m f1_weighted: 0.38512243898508375
[2m[36m(func pid=72016)[0m f1_per_class: [0.4, 0.427, 0.632, 0.563, 0.08, 0.089, 0.354, 0.285, 0.198, 0.262]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0131 | Steps: 2 | Val loss: 2.5830 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=83527)[0m top1: 0.06669776119402986
[2m[36m(func pid=83527)[0m top5: 0.48367537313432835
[2m[36m(func pid=83527)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=83527)[0m f1_macro: 0.03820680869114543
[2m[36m(func pid=83527)[0m f1_weighted: 0.03778110205743507
[2m[36m(func pid=83527)[0m f1_per_class: [0.101, 0.01, 0.0, 0.088, 0.0, 0.019, 0.0, 0.105, 0.022, 0.037]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:03 (running for 00:34:55.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.013 |      0.366 |                   55 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.329 |                   50 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.175 |                   30 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.162 |      0.038 |                    1 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.34654850746268656
[2m[36m(func pid=70967)[0m top5: 0.8582089552238806
[2m[36m(func pid=70967)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=70967)[0m f1_macro: 0.3661106019933992
[2m[36m(func pid=70967)[0m f1_weighted: 0.3449161598262148
[2m[36m(func pid=70967)[0m f1_per_class: [0.5, 0.429, 0.815, 0.525, 0.095, 0.201, 0.186, 0.323, 0.226, 0.361]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5173 | Steps: 2 | Val loss: 116.8712 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.0082 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.2346 | Steps: 2 | Val loss: 2.5396 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=76915)[0m top1: 0.13386194029850745
[2m[36m(func pid=76915)[0m top5: 0.5228544776119403
[2m[36m(func pid=76915)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=76915)[0m f1_macro: 0.16730685769301523
[2m[36m(func pid=76915)[0m f1_weighted: 0.12271967547135927
[2m[36m(func pid=76915)[0m f1_per_class: [0.073, 0.136, 0.255, 0.088, 0.113, 0.297, 0.048, 0.255, 0.14, 0.267]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0112 | Steps: 2 | Val loss: 2.5912 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=72016)[0m top1: 0.373134328358209
[2m[36m(func pid=72016)[0m top5: 0.8535447761194029
[2m[36m(func pid=72016)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=72016)[0m f1_macro: 0.32701081453313646
[2m[36m(func pid=72016)[0m f1_weighted: 0.3783144877818562
[2m[36m(func pid=72016)[0m f1_per_class: [0.396, 0.444, 0.632, 0.562, 0.079, 0.076, 0.328, 0.283, 0.199, 0.271]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06296641791044776
[2m[36m(func pid=83527)[0m top5: 0.4808768656716418
[2m[36m(func pid=83527)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=83527)[0m f1_macro: 0.03457387270415993
[2m[36m(func pid=83527)[0m f1_weighted: 0.03459531888034176
[2m[36m(func pid=83527)[0m f1_per_class: [0.079, 0.01, 0.0, 0.079, 0.0, 0.019, 0.0, 0.102, 0.023, 0.034]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:08 (running for 00:35:00.60)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.011 |      0.365 |                   56 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.327 |                   51 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.517 |      0.167 |                   31 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.235 |      0.035 |                    2 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3474813432835821
[2m[36m(func pid=70967)[0m top5: 0.8596082089552238
[2m[36m(func pid=70967)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=70967)[0m f1_macro: 0.36462636893493144
[2m[36m(func pid=70967)[0m f1_weighted: 0.34482499892123786
[2m[36m(func pid=70967)[0m f1_per_class: [0.503, 0.427, 0.815, 0.53, 0.086, 0.196, 0.183, 0.325, 0.23, 0.35]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0001 | Steps: 2 | Val loss: 119.8050 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.2287 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0943 | Steps: 2 | Val loss: 2.5498 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0154 | Steps: 2 | Val loss: 2.6113 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=76915)[0m top1: 0.134794776119403
[2m[36m(func pid=76915)[0m top5: 0.5214552238805971
[2m[36m(func pid=76915)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=76915)[0m f1_macro: 0.15364201788035328
[2m[36m(func pid=76915)[0m f1_weighted: 0.1233298850720348
[2m[36m(func pid=76915)[0m f1_per_class: [0.074, 0.136, 0.231, 0.092, 0.118, 0.302, 0.051, 0.258, 0.127, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.37080223880597013
[2m[36m(func pid=72016)[0m top5: 0.8493470149253731
[2m[36m(func pid=72016)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=72016)[0m f1_macro: 0.3245669930278593
[2m[36m(func pid=72016)[0m f1_weighted: 0.3758375804127751
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.443, 0.632, 0.563, 0.079, 0.077, 0.319, 0.294, 0.193, 0.259]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=83527)[0m top1: 0.061567164179104475
[2m[36m(func pid=83527)[0m top5: 0.4780783582089552
[2m[36m(func pid=83527)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=83527)[0m f1_macro: 0.031144812667558024
[2m[36m(func pid=83527)[0m f1_weighted: 0.03398558296745085
[2m[36m(func pid=83527)[0m f1_per_class: [0.056, 0.015, 0.0, 0.078, 0.0, 0.019, 0.0, 0.1, 0.0, 0.044]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:13 (running for 00:35:05.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.015 |      0.37  |                   57 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.325 |                   52 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.154 |                   32 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.094 |      0.031 |                    3 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.35074626865671643
[2m[36m(func pid=70967)[0m top5: 0.8628731343283582
[2m[36m(func pid=70967)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=70967)[0m f1_macro: 0.3698003921644517
[2m[36m(func pid=70967)[0m f1_weighted: 0.3487889848250562
[2m[36m(func pid=70967)[0m f1_per_class: [0.521, 0.437, 0.815, 0.528, 0.084, 0.197, 0.19, 0.325, 0.233, 0.368]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.7326 | Steps: 2 | Val loss: 117.8284 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.4179 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1314 | Steps: 2 | Val loss: 2.5622 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0081 | Steps: 2 | Val loss: 2.6145 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=76915)[0m top1: 0.14598880597014927
[2m[36m(func pid=76915)[0m top5: 0.5340485074626866
[2m[36m(func pid=76915)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=76915)[0m f1_macro: 0.16056553582024963
[2m[36m(func pid=76915)[0m f1_weighted: 0.13430314802317145
[2m[36m(func pid=76915)[0m f1_per_class: [0.079, 0.159, 0.216, 0.107, 0.126, 0.308, 0.054, 0.276, 0.133, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m top1: 0.363339552238806
[2m[36m(func pid=72016)[0m top5: 0.8428171641791045
[2m[36m(func pid=72016)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=72016)[0m f1_macro: 0.32095367516404283
[2m[36m(func pid=72016)[0m f1_weighted: 0.36872140174257456
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.442, 0.632, 0.557, 0.079, 0.083, 0.3, 0.29, 0.185, 0.254]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=83527)[0m top1: 0.060167910447761194
[2m[36m(func pid=83527)[0m top5: 0.46781716417910446
[2m[36m(func pid=83527)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=83527)[0m f1_macro: 0.03234889407407586
[2m[36m(func pid=83527)[0m f1_weighted: 0.03722735339418547
[2m[36m(func pid=83527)[0m f1_per_class: [0.052, 0.036, 0.0, 0.078, 0.0, 0.02, 0.0, 0.094, 0.0, 0.043]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:18 (running for 00:35:11.16)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.008 |      0.369 |                   58 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.321 |                   53 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.733 |      0.161 |                   33 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.131 |      0.032 |                    4 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.35074626865671643
[2m[36m(func pid=70967)[0m top5: 0.8624067164179104
[2m[36m(func pid=70967)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=70967)[0m f1_macro: 0.3689440497075258
[2m[36m(func pid=70967)[0m f1_weighted: 0.3495758967510689
[2m[36m(func pid=70967)[0m f1_per_class: [0.525, 0.44, 0.815, 0.524, 0.069, 0.195, 0.195, 0.329, 0.233, 0.364]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 4.0035 | Steps: 2 | Val loss: 111.6747 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0897 | Steps: 2 | Val loss: 2.5709 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.5719 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0089 | Steps: 2 | Val loss: 2.6067 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=76915)[0m top1: 0.16511194029850745
[2m[36m(func pid=76915)[0m top5: 0.5513059701492538
[2m[36m(func pid=76915)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=76915)[0m f1_macro: 0.1692913320605154
[2m[36m(func pid=76915)[0m f1_weighted: 0.15543832038782493
[2m[36m(func pid=76915)[0m f1_per_class: [0.092, 0.187, 0.207, 0.135, 0.13, 0.318, 0.079, 0.268, 0.128, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06110074626865672
[2m[36m(func pid=83527)[0m top5: 0.45988805970149255
[2m[36m(func pid=83527)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=83527)[0m f1_macro: 0.033933540351958454
[2m[36m(func pid=83527)[0m f1_weighted: 0.0404787573239326
[2m[36m(func pid=83527)[0m f1_per_class: [0.045, 0.052, 0.0, 0.08, 0.0, 0.02, 0.0, 0.094, 0.0, 0.048]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3619402985074627
[2m[36m(func pid=72016)[0m top5: 0.8386194029850746
[2m[36m(func pid=72016)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=72016)[0m f1_macro: 0.32001868400115985
[2m[36m(func pid=72016)[0m f1_weighted: 0.3652685196217885
[2m[36m(func pid=72016)[0m f1_per_class: [0.378, 0.436, 0.615, 0.557, 0.083, 0.088, 0.289, 0.296, 0.191, 0.266]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m top1: 0.353544776119403
[2m[36m(func pid=70967)[0m top5: 0.8661380597014925
[2m[36m(func pid=70967)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.37030393171424725
[2m[36m(func pid=70967)[0m f1_weighted: 0.3556782652468033
[2m[36m(func pid=70967)[0m f1_per_class: [0.529, 0.442, 0.815, 0.523, 0.068, 0.195, 0.216, 0.327, 0.234, 0.354]
== Status ==
Current time: 2024-01-07 01:18:24 (running for 00:35:16.32)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.009 |      0.37  |                   59 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.32  |                   54 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  4.004 |      0.169 |                   34 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.09  |      0.034 |                    5 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2071 | Steps: 2 | Val loss: 102.8135 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0331 | Steps: 2 | Val loss: 2.5683 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.6686 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=76915)[0m top1: 0.1884328358208955
[2m[36m(func pid=76915)[0m top5: 0.590018656716418
[2m[36m(func pid=76915)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=76915)[0m f1_macro: 0.1774039085862012
[2m[36m(func pid=76915)[0m f1_weighted: 0.17830958398256977
[2m[36m(func pid=76915)[0m f1_per_class: [0.135, 0.236, 0.171, 0.152, 0.125, 0.309, 0.114, 0.271, 0.111, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0091 | Steps: 2 | Val loss: 2.6242 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=83527)[0m top1: 0.06203358208955224
[2m[36m(func pid=83527)[0m top5: 0.45382462686567165
[2m[36m(func pid=83527)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=83527)[0m f1_macro: 0.035368078184273524
[2m[36m(func pid=83527)[0m f1_weighted: 0.04497066501426601
[2m[36m(func pid=83527)[0m f1_per_class: [0.043, 0.073, 0.0, 0.086, 0.0, 0.013, 0.0, 0.092, 0.0, 0.045]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m top1: 0.36007462686567165
[2m[36m(func pid=72016)[0m top5: 0.8348880597014925
[2m[36m(func pid=72016)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=72016)[0m f1_macro: 0.32024601002844055
[2m[36m(func pid=72016)[0m f1_weighted: 0.36362152538379366
[2m[36m(func pid=72016)[0m f1_per_class: [0.372, 0.435, 0.632, 0.558, 0.083, 0.094, 0.281, 0.293, 0.19, 0.264]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:18:29 (running for 00:35:21.60)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.009 |      0.373 |                   60 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.32  |                   55 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.207 |      0.177 |                   35 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.033 |      0.035 |                    6 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.35401119402985076
[2m[36m(func pid=70967)[0m top5: 0.8656716417910447
[2m[36m(func pid=70967)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=70967)[0m f1_macro: 0.3734105456895345
[2m[36m(func pid=70967)[0m f1_weighted: 0.3568772228197046
[2m[36m(func pid=70967)[0m f1_per_class: [0.544, 0.441, 0.815, 0.522, 0.074, 0.189, 0.223, 0.325, 0.233, 0.368]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 98.1463 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0495 | Steps: 2 | Val loss: 2.5657 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0000 | Steps: 2 | Val loss: 7.8784 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0095 | Steps: 2 | Val loss: 2.6657 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=76915)[0m top1: 0.19962686567164178
[2m[36m(func pid=76915)[0m top5: 0.6156716417910447
[2m[36m(func pid=76915)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=76915)[0m f1_macro: 0.18628259230271407
[2m[36m(func pid=76915)[0m f1_weighted: 0.19137732815907352
[2m[36m(func pid=76915)[0m f1_per_class: [0.202, 0.261, 0.145, 0.146, 0.107, 0.305, 0.154, 0.229, 0.099, 0.214]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06576492537313433
[2m[36m(func pid=83527)[0m top5: 0.45149253731343286
[2m[36m(func pid=83527)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=83527)[0m f1_macro: 0.04097794034889389
[2m[36m(func pid=83527)[0m f1_weighted: 0.05097492539750024
[2m[36m(func pid=83527)[0m f1_per_class: [0.057, 0.089, 0.0, 0.093, 0.0, 0.013, 0.0, 0.098, 0.026, 0.034]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3568097014925373
[2m[36m(func pid=72016)[0m top5: 0.8325559701492538
[2m[36m(func pid=72016)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=72016)[0m f1_macro: 0.3187713798217554
[2m[36m(func pid=72016)[0m f1_weighted: 0.35966098249292006
[2m[36m(func pid=72016)[0m f1_per_class: [0.374, 0.437, 0.632, 0.56, 0.082, 0.076, 0.27, 0.307, 0.182, 0.268]
[2m[36m(func pid=72016)[0m 
== Status ==
Current time: 2024-01-07 01:18:34 (running for 00:35:26.76)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.01  |      0.372 |                   61 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.319 |                   56 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.186 |                   36 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.049 |      0.041 |                    7 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3493470149253731
[2m[36m(func pid=70967)[0m top5: 0.863339552238806
[2m[36m(func pid=70967)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=70967)[0m f1_macro: 0.37225389813251347
[2m[36m(func pid=70967)[0m f1_weighted: 0.35101810624504054
[2m[36m(func pid=70967)[0m f1_per_class: [0.556, 0.438, 0.815, 0.513, 0.073, 0.183, 0.214, 0.333, 0.224, 0.373]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0000 | Steps: 2 | Val loss: 94.9606 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0441 | Steps: 2 | Val loss: 2.5616 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.0337 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0119 | Steps: 2 | Val loss: 2.6445 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=76915)[0m top1: 0.21361940298507462
[2m[36m(func pid=76915)[0m top5: 0.6347947761194029
[2m[36m(func pid=76915)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=76915)[0m f1_macro: 0.20716525869025829
[2m[36m(func pid=76915)[0m f1_weighted: 0.20977575301160015
[2m[36m(func pid=76915)[0m f1_per_class: [0.288, 0.269, 0.141, 0.146, 0.11, 0.306, 0.202, 0.233, 0.101, 0.276]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06389925373134328
[2m[36m(func pid=83527)[0m top5: 0.45009328358208955
[2m[36m(func pid=83527)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=83527)[0m f1_macro: 0.04233179659957115
[2m[36m(func pid=83527)[0m f1_weighted: 0.05187239242105744
[2m[36m(func pid=83527)[0m f1_per_class: [0.048, 0.086, 0.0, 0.095, 0.0, 0.019, 0.0, 0.096, 0.05, 0.03]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:39 (running for 00:35:31.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.01  |      0.372 |                   61 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.319 |                   57 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.207 |                   37 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  3.044 |      0.042 |                    8 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=72016)[0m top1: 0.355410447761194
[2m[36m(func pid=72016)[0m top5: 0.8311567164179104
[2m[36m(func pid=72016)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=72016)[0m f1_macro: 0.31898298483714027
[2m[36m(func pid=72016)[0m f1_weighted: 0.3586497187535275
[2m[36m(func pid=72016)[0m f1_per_class: [0.378, 0.431, 0.632, 0.564, 0.082, 0.082, 0.262, 0.315, 0.195, 0.25]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m top1: 0.35774253731343286
[2m[36m(func pid=70967)[0m top5: 0.867070895522388
[2m[36m(func pid=70967)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=70967)[0m f1_macro: 0.37974348624934107
[2m[36m(func pid=70967)[0m f1_weighted: 0.36051083633550113
[2m[36m(func pid=70967)[0m f1_per_class: [0.569, 0.438, 0.815, 0.524, 0.083, 0.189, 0.23, 0.329, 0.244, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0000 | Steps: 2 | Val loss: 92.8582 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9735 | Steps: 2 | Val loss: 2.5565 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0236 | Steps: 2 | Val loss: 2.6870 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.2605 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=76915)[0m top1: 0.2234141791044776
[2m[36m(func pid=76915)[0m top5: 0.6497201492537313
[2m[36m(func pid=76915)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=76915)[0m f1_macro: 0.2227203328349304
[2m[36m(func pid=76915)[0m f1_weighted: 0.22264988927012155
[2m[36m(func pid=76915)[0m f1_per_class: [0.4, 0.287, 0.133, 0.132, 0.115, 0.31, 0.238, 0.231, 0.105, 0.276]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06389925373134328
[2m[36m(func pid=83527)[0m top5: 0.44169776119402987
[2m[36m(func pid=83527)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=83527)[0m f1_macro: 0.0423739888521836
[2m[36m(func pid=83527)[0m f1_weighted: 0.055502048618387145
[2m[36m(func pid=83527)[0m f1_per_class: [0.042, 0.089, 0.0, 0.11, 0.0, 0.012, 0.0, 0.091, 0.047, 0.033]
[2m[36m(func pid=83527)[0m 
== Status ==
Current time: 2024-01-07 01:18:44 (running for 00:35:37.22)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.024 |      0.376 |                   63 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.319 |                   57 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.223 |                   38 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.973 |      0.042 |                    9 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.355410447761194
[2m[36m(func pid=70967)[0m top5: 0.863339552238806
[2m[36m(func pid=70967)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=70967)[0m f1_macro: 0.376104063851722
[2m[36m(func pid=70967)[0m f1_weighted: 0.3567492016711705
[2m[36m(func pid=70967)[0m f1_per_class: [0.578, 0.443, 0.815, 0.521, 0.082, 0.187, 0.22, 0.332, 0.229, 0.354]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.35027985074626866
[2m[36m(func pid=72016)[0m top5: 0.8246268656716418
[2m[36m(func pid=72016)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=72016)[0m f1_macro: 0.3160745980201308
[2m[36m(func pid=72016)[0m f1_weighted: 0.3545020038809109
[2m[36m(func pid=72016)[0m f1_per_class: [0.376, 0.437, 0.632, 0.558, 0.075, 0.082, 0.252, 0.31, 0.191, 0.248]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0000 | Steps: 2 | Val loss: 91.4801 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9897 | Steps: 2 | Val loss: 2.5480 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0028 | Steps: 2 | Val loss: 2.6943 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=76915)[0m top1: 0.2294776119402985
[2m[36m(func pid=76915)[0m top5: 0.667910447761194
[2m[36m(func pid=76915)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=76915)[0m f1_macro: 0.23282565040374936
[2m[36m(func pid=76915)[0m f1_weighted: 0.2357380179861007
[2m[36m(func pid=76915)[0m f1_per_class: [0.473, 0.289, 0.129, 0.129, 0.113, 0.31, 0.28, 0.221, 0.107, 0.276]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0001 | Steps: 2 | Val loss: 8.4667 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=83527)[0m top1: 0.06436567164179105
[2m[36m(func pid=83527)[0m top5: 0.43423507462686567
[2m[36m(func pid=83527)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=83527)[0m f1_macro: 0.04412283354445448
[2m[36m(func pid=83527)[0m f1_weighted: 0.059102616586522716
[2m[36m(func pid=83527)[0m f1_per_class: [0.045, 0.101, 0.0, 0.112, 0.0, 0.023, 0.0, 0.089, 0.045, 0.026]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.36007462686567165
[2m[36m(func pid=70967)[0m top5: 0.8642723880597015
[2m[36m(func pid=70967)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=70967)[0m f1_macro: 0.38078885902527226
[2m[36m(func pid=70967)[0m f1_weighted: 0.36188025943992663
[2m[36m(func pid=70967)[0m f1_per_class: [0.565, 0.445, 0.815, 0.518, 0.084, 0.19, 0.235, 0.342, 0.241, 0.375]
[2m[36m(func pid=70967)[0m 
== Status ==
Current time: 2024-01-07 01:18:50 (running for 00:35:42.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.381 |                   64 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.316 |                   58 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.233 |                   39 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.99  |      0.044 |                   10 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=72016)[0m top1: 0.34375
[2m[36m(func pid=72016)[0m top5: 0.8194962686567164
[2m[36m(func pid=72016)[0m f1_micro: 0.34375
[2m[36m(func pid=72016)[0m f1_macro: 0.3101473367663127
[2m[36m(func pid=72016)[0m f1_weighted: 0.34455712860710286
[2m[36m(func pid=72016)[0m f1_per_class: [0.371, 0.428, 0.615, 0.562, 0.076, 0.07, 0.225, 0.306, 0.196, 0.252]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 89.4459 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9839 | Steps: 2 | Val loss: 2.5405 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.6766 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=76915)[0m top1: 0.23880597014925373
[2m[36m(func pid=76915)[0m top5: 0.6842350746268657
[2m[36m(func pid=76915)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=76915)[0m f1_macro: 0.2379369888236103
[2m[36m(func pid=76915)[0m f1_weighted: 0.2530369341300789
[2m[36m(func pid=76915)[0m f1_per_class: [0.474, 0.291, 0.136, 0.135, 0.103, 0.308, 0.335, 0.215, 0.107, 0.276]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.5399 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=83527)[0m top1: 0.06110074626865672
[2m[36m(func pid=83527)[0m top5: 0.4230410447761194
[2m[36m(func pid=83527)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=83527)[0m f1_macro: 0.0446936657947107
[2m[36m(func pid=83527)[0m f1_weighted: 0.05783230270087822
[2m[36m(func pid=83527)[0m f1_per_class: [0.04, 0.094, 0.0, 0.104, 0.0, 0.039, 0.0, 0.085, 0.062, 0.021]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.3596082089552239
[2m[36m(func pid=70967)[0m top5: 0.8689365671641791
[2m[36m(func pid=70967)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=70967)[0m f1_macro: 0.3804680552451183
[2m[36m(func pid=70967)[0m f1_weighted: 0.3625106692177096
[2m[36m(func pid=70967)[0m f1_per_class: [0.583, 0.438, 0.815, 0.515, 0.073, 0.192, 0.244, 0.331, 0.23, 0.385]
== Status ==
Current time: 2024-01-07 01:18:55 (running for 00:35:47.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.38  |                   65 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.31  |                   59 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.238 |                   40 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.984 |      0.045 |                   11 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m top1: 0.34048507462686567
[2m[36m(func pid=72016)[0m top5: 0.8218283582089553
[2m[36m(func pid=72016)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=72016)[0m f1_macro: 0.3121471589749666
[2m[36m(func pid=72016)[0m f1_weighted: 0.3401471863514024
[2m[36m(func pid=72016)[0m f1_per_class: [0.386, 0.432, 0.632, 0.558, 0.076, 0.075, 0.209, 0.304, 0.192, 0.257]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9396 | Steps: 2 | Val loss: 88.4419 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9681 | Steps: 2 | Val loss: 2.5338 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0093 | Steps: 2 | Val loss: 2.6995 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=76915)[0m top1: 0.25046641791044777
[2m[36m(func pid=76915)[0m top5: 0.6940298507462687
[2m[36m(func pid=76915)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=76915)[0m f1_macro: 0.23003942417688233
[2m[36m(func pid=76915)[0m f1_weighted: 0.2701765249331312
[2m[36m(func pid=76915)[0m f1_per_class: [0.442, 0.301, 0.135, 0.146, 0.112, 0.313, 0.38, 0.216, 0.108, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06110074626865672
[2m[36m(func pid=83527)[0m top5: 0.41744402985074625
[2m[36m(func pid=83527)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=83527)[0m f1_macro: 0.045529059996092275
[2m[36m(func pid=83527)[0m f1_weighted: 0.06038122995185997
[2m[36m(func pid=83527)[0m f1_per_class: [0.045, 0.102, 0.0, 0.106, 0.0, 0.043, 0.003, 0.077, 0.059, 0.022]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0080 | Steps: 2 | Val loss: 8.8702 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:19:00 (running for 00:35:53.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.009 |      0.379 |                   66 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.312 |                   60 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.94  |      0.23  |                   41 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.968 |      0.046 |                   12 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3605410447761194
[2m[36m(func pid=70967)[0m top5: 0.871268656716418
[2m[36m(func pid=70967)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=70967)[0m f1_macro: 0.3787880212027376
[2m[36m(func pid=70967)[0m f1_weighted: 0.3651356147725165
[2m[36m(func pid=70967)[0m f1_per_class: [0.578, 0.442, 0.786, 0.516, 0.07, 0.189, 0.253, 0.328, 0.233, 0.395]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9772 | Steps: 2 | Val loss: 2.5301 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0000 | Steps: 2 | Val loss: 87.6996 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=72016)[0m top1: 0.33302238805970147
[2m[36m(func pid=72016)[0m top5: 0.8143656716417911
[2m[36m(func pid=72016)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=72016)[0m f1_macro: 0.307053585605021
[2m[36m(func pid=72016)[0m f1_weighted: 0.33012871407492445
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.424, 0.632, 0.556, 0.075, 0.075, 0.183, 0.305, 0.187, 0.247]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0299 | Steps: 2 | Val loss: 2.7060 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=76915)[0m top1: 0.26072761194029853
[2m[36m(func pid=76915)[0m top5: 0.7056902985074627
[2m[36m(func pid=76915)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=76915)[0m f1_macro: 0.23328676404099563
[2m[36m(func pid=76915)[0m f1_weighted: 0.28171228889351074
[2m[36m(func pid=76915)[0m f1_per_class: [0.412, 0.312, 0.148, 0.143, 0.124, 0.319, 0.417, 0.207, 0.103, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.06296641791044776
[2m[36m(func pid=83527)[0m top5: 0.4137126865671642
[2m[36m(func pid=83527)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=83527)[0m f1_macro: 0.047314079068724854
[2m[36m(func pid=83527)[0m f1_weighted: 0.06373895763663043
[2m[36m(func pid=83527)[0m f1_per_class: [0.05, 0.117, 0.0, 0.107, 0.0, 0.042, 0.006, 0.068, 0.057, 0.026]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.9762 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:19:06 (running for 00:35:58.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.03  |      0.382 |                   67 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.008 |      0.307 |                   61 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.233 |                   42 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.977 |      0.047 |                   13 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.36380597014925375
[2m[36m(func pid=70967)[0m top5: 0.8717350746268657
[2m[36m(func pid=70967)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=70967)[0m f1_macro: 0.3817718132152244
[2m[36m(func pid=70967)[0m f1_weighted: 0.3682036230511352
[2m[36m(func pid=70967)[0m f1_per_class: [0.569, 0.441, 0.786, 0.508, 0.072, 0.195, 0.266, 0.333, 0.237, 0.41]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0000 | Steps: 2 | Val loss: 86.7240 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8572 | Steps: 2 | Val loss: 2.5214 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=72016)[0m top1: 0.333955223880597
[2m[36m(func pid=72016)[0m top5: 0.8115671641791045
[2m[36m(func pid=72016)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=72016)[0m f1_macro: 0.3065609150189852
[2m[36m(func pid=72016)[0m f1_weighted: 0.3299393021738828
[2m[36m(func pid=72016)[0m f1_per_class: [0.39, 0.435, 0.615, 0.56, 0.079, 0.074, 0.174, 0.3, 0.186, 0.254]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0038 | Steps: 2 | Val loss: 2.7195 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=83527)[0m top1: 0.06669776119402986
[2m[36m(func pid=83527)[0m top5: 0.40718283582089554
[2m[36m(func pid=83527)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=83527)[0m f1_macro: 0.050943171627265825
[2m[36m(func pid=83527)[0m f1_weighted: 0.06828108997640371
[2m[36m(func pid=83527)[0m f1_per_class: [0.048, 0.123, 0.0, 0.114, 0.0, 0.051, 0.006, 0.072, 0.069, 0.027]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.2733208955223881
[2m[36m(func pid=76915)[0m top5: 0.7080223880597015
[2m[36m(func pid=76915)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=76915)[0m f1_macro: 0.22190824627389846
[2m[36m(func pid=76915)[0m f1_weighted: 0.2948607825287563
[2m[36m(func pid=76915)[0m f1_per_class: [0.3, 0.331, 0.151, 0.146, 0.14, 0.314, 0.462, 0.195, 0.103, 0.077]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0008 | Steps: 2 | Val loss: 9.2199 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:19:11 (running for 00:36:03.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.375 |                   68 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.307 |                   62 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.222 |                   43 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.857 |      0.051 |                   14 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3614738805970149
[2m[36m(func pid=70967)[0m top5: 0.8722014925373134
[2m[36m(func pid=70967)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=70967)[0m f1_macro: 0.3752197190878697
[2m[36m(func pid=70967)[0m f1_weighted: 0.3677032759712382
[2m[36m(func pid=70967)[0m f1_per_class: [0.562, 0.438, 0.759, 0.508, 0.072, 0.204, 0.267, 0.321, 0.241, 0.38]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8780 | Steps: 2 | Val loss: 2.5147 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0000 | Steps: 2 | Val loss: 87.2670 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=72016)[0m top1: 0.332089552238806
[2m[36m(func pid=72016)[0m top5: 0.804570895522388
[2m[36m(func pid=72016)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=72016)[0m f1_macro: 0.30606864091034114
[2m[36m(func pid=72016)[0m f1_weighted: 0.32505806430200945
[2m[36m(func pid=72016)[0m f1_per_class: [0.386, 0.435, 0.615, 0.553, 0.079, 0.069, 0.164, 0.302, 0.19, 0.267]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0029 | Steps: 2 | Val loss: 2.7487 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=83527)[0m top1: 0.06763059701492537
[2m[36m(func pid=83527)[0m top5: 0.40671641791044777
[2m[36m(func pid=83527)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=83527)[0m f1_macro: 0.05203945567145964
[2m[36m(func pid=83527)[0m f1_weighted: 0.06907424297554157
[2m[36m(func pid=83527)[0m f1_per_class: [0.043, 0.122, 0.0, 0.119, 0.0, 0.041, 0.006, 0.078, 0.083, 0.029]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.28031716417910446
[2m[36m(func pid=76915)[0m top5: 0.7117537313432836
[2m[36m(func pid=76915)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=76915)[0m f1_macro: 0.22656174349303987
[2m[36m(func pid=76915)[0m f1_weighted: 0.30333232087616874
[2m[36m(func pid=76915)[0m f1_per_class: [0.296, 0.336, 0.157, 0.159, 0.149, 0.315, 0.474, 0.198, 0.105, 0.077]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.4193 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:19:16 (running for 00:36:08.79)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.371 |                   69 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.306 |                   63 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.227 |                   44 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.878 |      0.052 |                   15 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3568097014925373
[2m[36m(func pid=70967)[0m top5: 0.8698694029850746
[2m[36m(func pid=70967)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=70967)[0m f1_macro: 0.3708773667200361
[2m[36m(func pid=70967)[0m f1_weighted: 0.36391307683800234
[2m[36m(func pid=70967)[0m f1_per_class: [0.545, 0.435, 0.759, 0.496, 0.073, 0.19, 0.277, 0.305, 0.239, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8621 | Steps: 2 | Val loss: 2.5031 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 88.2519 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=72016)[0m top1: 0.32742537313432835
[2m[36m(func pid=72016)[0m top5: 0.8003731343283582
[2m[36m(func pid=72016)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=72016)[0m f1_macro: 0.3035351227014426
[2m[36m(func pid=72016)[0m f1_weighted: 0.3192165205812785
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.432, 0.615, 0.548, 0.079, 0.069, 0.151, 0.299, 0.188, 0.265]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.7590 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=83527)[0m top1: 0.06809701492537314
[2m[36m(func pid=83527)[0m top5: 0.4085820895522388
[2m[36m(func pid=83527)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=83527)[0m f1_macro: 0.05281831842188604
[2m[36m(func pid=83527)[0m f1_weighted: 0.07066459764839388
[2m[36m(func pid=83527)[0m f1_per_class: [0.042, 0.123, 0.0, 0.122, 0.0, 0.044, 0.006, 0.081, 0.091, 0.02]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.2891791044776119
[2m[36m(func pid=76915)[0m top5: 0.7164179104477612
[2m[36m(func pid=76915)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=76915)[0m f1_macro: 0.2231525738027163
[2m[36m(func pid=76915)[0m f1_weighted: 0.3093934254396509
[2m[36m(func pid=76915)[0m f1_per_class: [0.235, 0.341, 0.163, 0.148, 0.152, 0.312, 0.508, 0.191, 0.104, 0.077]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.5111 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:19:21 (running for 00:36:14.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.373 |                   70 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.304 |                   64 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.223 |                   45 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.862 |      0.053 |                   16 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.35774253731343286
[2m[36m(func pid=70967)[0m top5: 0.8694029850746269
[2m[36m(func pid=70967)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=70967)[0m f1_macro: 0.37263446467597244
[2m[36m(func pid=70967)[0m f1_weighted: 0.36348334923553177
[2m[36m(func pid=70967)[0m f1_per_class: [0.541, 0.434, 0.759, 0.494, 0.074, 0.178, 0.281, 0.306, 0.25, 0.41]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8208 | Steps: 2 | Val loss: 2.4938 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1445 | Steps: 2 | Val loss: 87.3097 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=72016)[0m top1: 0.3292910447761194
[2m[36m(func pid=72016)[0m top5: 0.8013059701492538
[2m[36m(func pid=72016)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=72016)[0m f1_macro: 0.3050467842181469
[2m[36m(func pid=72016)[0m f1_weighted: 0.3217647806591606
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.441, 0.615, 0.553, 0.08, 0.069, 0.151, 0.291, 0.189, 0.273]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0053 | Steps: 2 | Val loss: 2.7273 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=83527)[0m top1: 0.06856343283582089
[2m[36m(func pid=83527)[0m top5: 0.4099813432835821
[2m[36m(func pid=83527)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=83527)[0m f1_macro: 0.05289899576882086
[2m[36m(func pid=83527)[0m f1_weighted: 0.07043744596031613
[2m[36m(func pid=83527)[0m f1_per_class: [0.047, 0.12, 0.0, 0.124, 0.0, 0.039, 0.006, 0.084, 0.082, 0.026]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.30597014925373134
[2m[36m(func pid=76915)[0m top5: 0.7229477611940298
[2m[36m(func pid=76915)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=76915)[0m f1_macro: 0.22159357279126196
[2m[36m(func pid=76915)[0m f1_weighted: 0.32493121876795567
[2m[36m(func pid=76915)[0m f1_per_class: [0.125, 0.34, 0.174, 0.17, 0.171, 0.32, 0.544, 0.187, 0.109, 0.077]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.7452 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:19:26 (running for 00:36:19.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.005 |      0.374 |                   71 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.305 |                   65 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  2.144 |      0.222 |                   46 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.821 |      0.053 |                   17 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.365205223880597
[2m[36m(func pid=70967)[0m top5: 0.8740671641791045
[2m[36m(func pid=70967)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=70967)[0m f1_macro: 0.3739991784663159
[2m[36m(func pid=70967)[0m f1_weighted: 0.3755871584739865
[2m[36m(func pid=70967)[0m f1_per_class: [0.554, 0.438, 0.759, 0.499, 0.07, 0.191, 0.312, 0.297, 0.251, 0.37]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8084 | Steps: 2 | Val loss: 2.4834 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 88.5008 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=72016)[0m top1: 0.32276119402985076
[2m[36m(func pid=72016)[0m top5: 0.7947761194029851
[2m[36m(func pid=72016)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=72016)[0m f1_macro: 0.29907633940780187
[2m[36m(func pid=72016)[0m f1_weighted: 0.3137690846701793
[2m[36m(func pid=72016)[0m f1_per_class: [0.388, 0.437, 0.6, 0.544, 0.08, 0.063, 0.139, 0.291, 0.183, 0.266]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0017 | Steps: 2 | Val loss: 2.7222 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=83527)[0m top1: 0.07042910447761194
[2m[36m(func pid=83527)[0m top5: 0.41138059701492535
[2m[36m(func pid=83527)[0m f1_micro: 0.07042910447761194
[2m[36m(func pid=83527)[0m f1_macro: 0.05467043238500878
[2m[36m(func pid=83527)[0m f1_weighted: 0.07473857538900672
[2m[36m(func pid=83527)[0m f1_per_class: [0.047, 0.124, 0.0, 0.134, 0.0, 0.043, 0.009, 0.072, 0.092, 0.027]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.30363805970149255
[2m[36m(func pid=76915)[0m top5: 0.7196828358208955
[2m[36m(func pid=76915)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=76915)[0m f1_macro: 0.22233918556749632
[2m[36m(func pid=76915)[0m f1_weighted: 0.32226069735117796
[2m[36m(func pid=76915)[0m f1_per_class: [0.085, 0.332, 0.175, 0.17, 0.175, 0.312, 0.546, 0.17, 0.11, 0.148]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.7902 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:19:31 (running for 00:36:24.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.375 |                   72 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.299 |                   66 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.222 |                   47 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.808 |      0.055 |                   18 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.36613805970149255
[2m[36m(func pid=70967)[0m top5: 0.875
[2m[36m(func pid=70967)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=70967)[0m f1_macro: 0.3749578014031062
[2m[36m(func pid=70967)[0m f1_weighted: 0.3784051096058152
[2m[36m(func pid=70967)[0m f1_per_class: [0.55, 0.437, 0.759, 0.5, 0.068, 0.207, 0.317, 0.292, 0.245, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8391 | Steps: 2 | Val loss: 2.4771 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 88.9977 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=72016)[0m top1: 0.3218283582089552
[2m[36m(func pid=72016)[0m top5: 0.7971082089552238
[2m[36m(func pid=72016)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=72016)[0m f1_macro: 0.3001038343844093
[2m[36m(func pid=72016)[0m f1_weighted: 0.31176909640390194
[2m[36m(func pid=72016)[0m f1_per_class: [0.392, 0.437, 0.585, 0.542, 0.084, 0.063, 0.134, 0.284, 0.193, 0.288]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.7168 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=83527)[0m top1: 0.07369402985074627
[2m[36m(func pid=83527)[0m top5: 0.4146455223880597
[2m[36m(func pid=83527)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=83527)[0m f1_macro: 0.057979884082811114
[2m[36m(func pid=83527)[0m f1_weighted: 0.0783256318447099
[2m[36m(func pid=83527)[0m f1_per_class: [0.047, 0.138, 0.024, 0.128, 0.0, 0.056, 0.015, 0.073, 0.077, 0.022]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3045708955223881
[2m[36m(func pid=76915)[0m top5: 0.7201492537313433
[2m[36m(func pid=76915)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=76915)[0m f1_macro: 0.22579797196546428
[2m[36m(func pid=76915)[0m f1_weighted: 0.32453509281701753
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.324, 0.169, 0.179, 0.184, 0.321, 0.548, 0.165, 0.111, 0.214]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0007 | Steps: 2 | Val loss: 9.9302 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 01:19:37 (running for 00:36:29.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.379 |                   73 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.3   |                   67 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.226 |                   48 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.839 |      0.058 |                   19 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.36986940298507465
[2m[36m(func pid=70967)[0m top5: 0.8777985074626866
[2m[36m(func pid=70967)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=70967)[0m f1_macro: 0.3785124036583468
[2m[36m(func pid=70967)[0m f1_weighted: 0.38508282979164143
[2m[36m(func pid=70967)[0m f1_per_class: [0.558, 0.439, 0.759, 0.505, 0.065, 0.204, 0.333, 0.291, 0.241, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8151 | Steps: 2 | Val loss: 2.4698 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7520 | Steps: 2 | Val loss: 89.9108 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=72016)[0m top1: 0.31902985074626866
[2m[36m(func pid=72016)[0m top5: 0.7971082089552238
[2m[36m(func pid=72016)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=72016)[0m f1_macro: 0.2970385481608767
[2m[36m(func pid=72016)[0m f1_weighted: 0.3078838657186268
[2m[36m(func pid=72016)[0m f1_per_class: [0.385, 0.437, 0.585, 0.544, 0.085, 0.063, 0.121, 0.281, 0.185, 0.283]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.6832 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=83527)[0m top1: 0.07182835820895522
[2m[36m(func pid=83527)[0m top5: 0.4146455223880597
[2m[36m(func pid=83527)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=83527)[0m f1_macro: 0.05654473837543076
[2m[36m(func pid=83527)[0m f1_weighted: 0.07572964016005505
[2m[36m(func pid=83527)[0m f1_per_class: [0.052, 0.136, 0.021, 0.121, 0.0, 0.056, 0.015, 0.068, 0.075, 0.023]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3069029850746269
[2m[36m(func pid=76915)[0m top5: 0.7201492537313433
[2m[36m(func pid=76915)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=76915)[0m f1_macro: 0.22483130887790087
[2m[36m(func pid=76915)[0m f1_weighted: 0.326220503852932
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.314, 0.178, 0.184, 0.184, 0.307, 0.559, 0.174, 0.097, 0.207]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.1521 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:19:42 (running for 00:36:34.56)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.344
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.39  |                   74 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0.001 |      0.297 |                   68 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.752 |      0.225 |                   49 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.815 |      0.057 |                   20 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.37593283582089554
[2m[36m(func pid=70967)[0m top5: 0.878731343283582
[2m[36m(func pid=70967)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=70967)[0m f1_macro: 0.39037140785121727
[2m[36m(func pid=70967)[0m f1_weighted: 0.39215921863408576
[2m[36m(func pid=70967)[0m f1_per_class: [0.592, 0.443, 0.815, 0.513, 0.066, 0.199, 0.345, 0.296, 0.246, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7580 | Steps: 2 | Val loss: 2.4623 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 90.7745 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=72016)[0m top1: 0.31296641791044777
[2m[36m(func pid=72016)[0m top5: 0.7901119402985075
[2m[36m(func pid=72016)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=72016)[0m f1_macro: 0.29380611860273587
[2m[36m(func pid=72016)[0m f1_weighted: 0.29981352880326073
[2m[36m(func pid=72016)[0m f1_per_class: [0.379, 0.432, 0.585, 0.53, 0.087, 0.064, 0.11, 0.277, 0.185, 0.288]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0031 | Steps: 2 | Val loss: 2.6832 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=83527)[0m top1: 0.07276119402985075
[2m[36m(func pid=83527)[0m top5: 0.4123134328358209
[2m[36m(func pid=83527)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=83527)[0m f1_macro: 0.05754200490711875
[2m[36m(func pid=83527)[0m f1_weighted: 0.07808158565779741
[2m[36m(func pid=83527)[0m f1_per_class: [0.058, 0.138, 0.02, 0.119, 0.0, 0.057, 0.024, 0.065, 0.074, 0.023]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.30830223880597013
[2m[36m(func pid=76915)[0m top5: 0.7201492537313433
[2m[36m(func pid=76915)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=76915)[0m f1_macro: 0.2295689317464155
[2m[36m(func pid=76915)[0m f1_weighted: 0.3274398768162935
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.316, 0.179, 0.187, 0.187, 0.297, 0.563, 0.165, 0.101, 0.258]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:19:47 (running for 00:36:39.76)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.387 |                   75 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.294 |                   69 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.23  |                   50 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.058 |                   21 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3763992537313433
[2m[36m(func pid=70967)[0m top5: 0.8805970149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=70967)[0m f1_macro: 0.3870526877192706
[2m[36m(func pid=70967)[0m f1_weighted: 0.39415538608787937
[2m[36m(func pid=70967)[0m f1_per_class: [0.595, 0.438, 0.786, 0.514, 0.066, 0.196, 0.356, 0.297, 0.246, 0.378]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.2563 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7473 | Steps: 2 | Val loss: 2.4550 | Batch size: 32 | lr: 0.0001 | Duration: 2.63s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 91.3268 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=72016)[0m top1: 0.31156716417910446
[2m[36m(func pid=72016)[0m top5: 0.7938432835820896
[2m[36m(func pid=72016)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=72016)[0m f1_macro: 0.2972984593076293
[2m[36m(func pid=72016)[0m f1_weighted: 0.29950476563984846
[2m[36m(func pid=72016)[0m f1_per_class: [0.387, 0.432, 0.615, 0.531, 0.087, 0.064, 0.107, 0.274, 0.185, 0.29]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0188 | Steps: 2 | Val loss: 2.7367 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=83527)[0m top1: 0.07369402985074627
[2m[36m(func pid=83527)[0m top5: 0.4207089552238806
[2m[36m(func pid=83527)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=83527)[0m f1_macro: 0.05936581779575707
[2m[36m(func pid=83527)[0m f1_weighted: 0.07985100036083664
[2m[36m(func pid=83527)[0m f1_per_class: [0.055, 0.133, 0.037, 0.115, 0.0, 0.057, 0.035, 0.068, 0.069, 0.024]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.31343283582089554
[2m[36m(func pid=76915)[0m top5: 0.7192164179104478
[2m[36m(func pid=76915)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=76915)[0m f1_macro: 0.23907290091567596
[2m[36m(func pid=76915)[0m f1_weighted: 0.3314173918092082
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.309, 0.176, 0.192, 0.197, 0.3, 0.57, 0.17, 0.098, 0.333]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:19:52 (running for 00:36:45.06)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.019 |      0.381 |                   76 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.297 |                   70 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.239 |                   51 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.747 |      0.059 |                   22 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.37173507462686567
[2m[36m(func pid=70967)[0m top5: 0.878731343283582
[2m[36m(func pid=70967)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=70967)[0m f1_macro: 0.3807478167245705
[2m[36m(func pid=70967)[0m f1_weighted: 0.3882194526822269
[2m[36m(func pid=70967)[0m f1_per_class: [0.571, 0.44, 0.786, 0.51, 0.071, 0.2, 0.342, 0.276, 0.25, 0.361]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.4403 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8232 | Steps: 2 | Val loss: 2.4495 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7526 | Steps: 2 | Val loss: 91.7170 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=72016)[0m top1: 0.30736940298507465
[2m[36m(func pid=72016)[0m top5: 0.7915111940298507
[2m[36m(func pid=72016)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=72016)[0m f1_macro: 0.2962478981056095
[2m[36m(func pid=72016)[0m f1_weighted: 0.29679398114445343
[2m[36m(func pid=72016)[0m f1_per_class: [0.387, 0.43, 0.632, 0.527, 0.082, 0.071, 0.102, 0.272, 0.183, 0.276]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.7653 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=83527)[0m top1: 0.07229477611940298
[2m[36m(func pid=83527)[0m top5: 0.4239738805970149
[2m[36m(func pid=83527)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=83527)[0m f1_macro: 0.05815194126540407
[2m[36m(func pid=83527)[0m f1_weighted: 0.07909460719986337
[2m[36m(func pid=83527)[0m f1_per_class: [0.054, 0.129, 0.034, 0.113, 0.0, 0.057, 0.038, 0.067, 0.067, 0.024]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3208955223880597
[2m[36m(func pid=76915)[0m top5: 0.7280783582089553
[2m[36m(func pid=76915)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=76915)[0m f1_macro: 0.2422433472857079
[2m[36m(func pid=76915)[0m f1_weighted: 0.3367029322966644
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.312, 0.183, 0.203, 0.192, 0.295, 0.578, 0.164, 0.101, 0.35]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:19:58 (running for 00:36:50.28)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.38  |                   77 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.296 |                   71 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.753 |      0.242 |                   52 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.823 |      0.058 |                   23 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3694029850746269
[2m[36m(func pid=70967)[0m top5: 0.878731343283582
[2m[36m(func pid=70967)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=70967)[0m f1_macro: 0.3799974186344023
[2m[36m(func pid=70967)[0m f1_weighted: 0.38424300028848357
[2m[36m(func pid=70967)[0m f1_per_class: [0.576, 0.438, 0.759, 0.5, 0.071, 0.195, 0.336, 0.3, 0.246, 0.378]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.5251 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7492 | Steps: 2 | Val loss: 2.4394 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6853 | Steps: 2 | Val loss: 92.1733 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0014 | Steps: 2 | Val loss: 2.7490 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=72016)[0m top1: 0.30363805970149255
[2m[36m(func pid=72016)[0m top5: 0.7905783582089553
[2m[36m(func pid=72016)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=72016)[0m f1_macro: 0.29012295949908246
[2m[36m(func pid=72016)[0m f1_weighted: 0.29418685981333087
[2m[36m(func pid=72016)[0m f1_per_class: [0.392, 0.434, 0.6, 0.519, 0.082, 0.058, 0.105, 0.267, 0.175, 0.269]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=83527)[0m top1: 0.07555970149253731
[2m[36m(func pid=83527)[0m top5: 0.43330223880597013
[2m[36m(func pid=83527)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=83527)[0m f1_macro: 0.060965597231057864
[2m[36m(func pid=83527)[0m f1_weighted: 0.08272113865653777
[2m[36m(func pid=83527)[0m f1_per_class: [0.06, 0.13, 0.033, 0.114, 0.0, 0.061, 0.046, 0.068, 0.066, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3204291044776119
[2m[36m(func pid=76915)[0m top5: 0.730410447761194
[2m[36m(func pid=76915)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=76915)[0m f1_macro: 0.2504829236174853
[2m[36m(func pid=76915)[0m f1_weighted: 0.3358639084275234
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.307, 0.186, 0.213, 0.182, 0.27, 0.575, 0.155, 0.105, 0.468]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:20:03 (running for 00:36:55.39)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.387 |                   78 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.29  |                   72 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.685 |      0.25  |                   53 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.749 |      0.061 |                   24 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.375
[2m[36m(func pid=70967)[0m top5: 0.8815298507462687
[2m[36m(func pid=70967)[0m f1_micro: 0.375
[2m[36m(func pid=70967)[0m f1_macro: 0.38733305896909653
[2m[36m(func pid=70967)[0m f1_weighted: 0.3903449639100625
[2m[36m(func pid=70967)[0m f1_per_class: [0.583, 0.442, 0.815, 0.516, 0.066, 0.189, 0.343, 0.279, 0.252, 0.389]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.7307 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6774 | Steps: 2 | Val loss: 2.4352 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3857 | Steps: 2 | Val loss: 90.3585 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.7629 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=72016)[0m top1: 0.30550373134328357
[2m[36m(func pid=72016)[0m top5: 0.7868470149253731
[2m[36m(func pid=72016)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=72016)[0m f1_macro: 0.2892430800160091
[2m[36m(func pid=72016)[0m f1_weighted: 0.29462112934623896
[2m[36m(func pid=72016)[0m f1_per_class: [0.394, 0.435, 0.585, 0.536, 0.079, 0.058, 0.091, 0.268, 0.168, 0.278]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=83527)[0m top1: 0.07882462686567164
[2m[36m(func pid=83527)[0m top5: 0.43423507462686567
[2m[36m(func pid=83527)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=83527)[0m f1_macro: 0.06452028091624315
[2m[36m(func pid=83527)[0m f1_weighted: 0.08753290692287162
[2m[36m(func pid=83527)[0m f1_per_class: [0.061, 0.134, 0.059, 0.123, 0.0, 0.061, 0.051, 0.065, 0.066, 0.026]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3278917910447761
[2m[36m(func pid=76915)[0m top5: 0.7322761194029851
[2m[36m(func pid=76915)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=76915)[0m f1_macro: 0.2538866040347926
[2m[36m(func pid=76915)[0m f1_weighted: 0.3452782944711365
[2m[36m(func pid=76915)[0m f1_per_class: [0.043, 0.325, 0.182, 0.238, 0.173, 0.266, 0.577, 0.144, 0.1, 0.49]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:20:08 (running for 00:37:00.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.388 |                   79 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.289 |                   73 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.386 |      0.254 |                   54 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.677 |      0.065 |                   25 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.376865671641791
[2m[36m(func pid=70967)[0m top5: 0.8805970149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=70967)[0m f1_macro: 0.3879569381703821
[2m[36m(func pid=70967)[0m f1_weighted: 0.3915520244175123
[2m[36m(func pid=70967)[0m f1_per_class: [0.587, 0.439, 0.815, 0.516, 0.068, 0.193, 0.345, 0.293, 0.249, 0.373]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7114 | Steps: 2 | Val loss: 2.4294 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0002 | Steps: 2 | Val loss: 10.9282 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.3208 | Steps: 2 | Val loss: 88.7750 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0057 | Steps: 2 | Val loss: 2.7202 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=83527)[0m top1: 0.08022388059701492
[2m[36m(func pid=83527)[0m top5: 0.435634328358209
[2m[36m(func pid=83527)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=83527)[0m f1_macro: 0.0675023523837262
[2m[36m(func pid=83527)[0m f1_weighted: 0.08860257110551721
[2m[36m(func pid=83527)[0m f1_per_class: [0.06, 0.133, 0.069, 0.124, 0.0, 0.059, 0.054, 0.068, 0.077, 0.033]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m top1: 0.3064365671641791
[2m[36m(func pid=72016)[0m top5: 0.784981343283582
[2m[36m(func pid=72016)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=72016)[0m f1_macro: 0.290756246297459
[2m[36m(func pid=72016)[0m f1_weighted: 0.29394008528673893
[2m[36m(func pid=72016)[0m f1_per_class: [0.398, 0.439, 0.585, 0.533, 0.083, 0.059, 0.088, 0.268, 0.166, 0.288]
[2m[36m(func pid=72016)[0m 
[2m[36m(func pid=76915)[0m top1: 0.33722014925373134
[2m[36m(func pid=76915)[0m top5: 0.7397388059701493
[2m[36m(func pid=76915)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=76915)[0m f1_macro: 0.25452970900856786
[2m[36m(func pid=76915)[0m f1_weighted: 0.35338002440287103
[2m[36m(func pid=76915)[0m f1_per_class: [0.085, 0.331, 0.178, 0.257, 0.171, 0.246, 0.591, 0.142, 0.089, 0.456]
[2m[36m(func pid=76915)[0m 
== Status ==
Current time: 2024-01-07 01:20:13 (running for 00:37:05.85)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.35224999999999995
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.006 |      0.392 |                   80 |
| train_57e67_00018 | RUNNING    | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.291 |                   74 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.321 |      0.255 |                   55 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.711 |      0.068 |                   26 |
| train_57e67_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3843283582089552
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=70967)[0m f1_macro: 0.39188224425618695
[2m[36m(func pid=70967)[0m f1_weighted: 0.4012411380396594
[2m[36m(func pid=70967)[0m f1_per_class: [0.595, 0.435, 0.815, 0.53, 0.071, 0.193, 0.367, 0.293, 0.246, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7123 | Steps: 2 | Val loss: 2.4266 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=72016)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.0989 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7937 | Steps: 2 | Val loss: 87.4736 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0026 | Steps: 2 | Val loss: 2.7314 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=83527)[0m top1: 0.08115671641791045
[2m[36m(func pid=83527)[0m top5: 0.4361007462686567
[2m[36m(func pid=83527)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=83527)[0m f1_macro: 0.06823252049194518
[2m[36m(func pid=83527)[0m f1_weighted: 0.0901976250998067
[2m[36m(func pid=83527)[0m f1_per_class: [0.058, 0.137, 0.065, 0.117, 0.0, 0.059, 0.062, 0.079, 0.074, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=72016)[0m top1: 0.29757462686567165
[2m[36m(func pid=72016)[0m top5: 0.7826492537313433
[2m[36m(func pid=72016)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=72016)[0m f1_macro: 0.28782355065814325
[2m[36m(func pid=72016)[0m f1_weighted: 0.28417523793862465
[2m[36m(func pid=72016)[0m f1_per_class: [0.396, 0.43, 0.6, 0.514, 0.085, 0.059, 0.08, 0.263, 0.164, 0.288]
[2m[36m(func pid=76915)[0m top1: 0.34328358208955223
[2m[36m(func pid=76915)[0m top5: 0.7476679104477612
[2m[36m(func pid=76915)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=76915)[0m f1_macro: 0.2547521685212875
[2m[36m(func pid=76915)[0m f1_weighted: 0.3562232044467842
[2m[36m(func pid=76915)[0m f1_per_class: [0.125, 0.346, 0.187, 0.266, 0.177, 0.233, 0.586, 0.14, 0.104, 0.382]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m top1: 0.38619402985074625
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=70967)[0m f1_macro: 0.39353648399208174
[2m[36m(func pid=70967)[0m f1_weighted: 0.40315606348445393
[2m[36m(func pid=70967)[0m f1_per_class: [0.579, 0.439, 0.815, 0.531, 0.071, 0.203, 0.366, 0.298, 0.245, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6805 | Steps: 2 | Val loss: 2.4220 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2735 | Steps: 2 | Val loss: 86.1957 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.7414 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=83527)[0m top1: 0.08208955223880597
[2m[36m(func pid=83527)[0m top5: 0.4398320895522388
[2m[36m(func pid=83527)[0m f1_micro: 0.08208955223880597
[2m[36m(func pid=83527)[0m f1_macro: 0.06857101790453146
[2m[36m(func pid=83527)[0m f1_weighted: 0.0930825480820111
[2m[36m(func pid=83527)[0m f1_per_class: [0.057, 0.136, 0.063, 0.117, 0.0, 0.059, 0.073, 0.076, 0.074, 0.032]
[2m[36m(func pid=76915)[0m top1: 0.35447761194029853
[2m[36m(func pid=76915)[0m top5: 0.7555970149253731
[2m[36m(func pid=76915)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=76915)[0m f1_macro: 0.27881685932510913
[2m[36m(func pid=76915)[0m f1_weighted: 0.36585681043982693
[2m[36m(func pid=76915)[0m f1_per_class: [0.269, 0.375, 0.203, 0.271, 0.187, 0.238, 0.582, 0.138, 0.124, 0.4]
[2m[36m(func pid=70967)[0m top1: 0.38152985074626866
[2m[36m(func pid=70967)[0m top5: 0.8843283582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=70967)[0m f1_macro: 0.38564428490304425
[2m[36m(func pid=70967)[0m f1_weighted: 0.39876824223068763
[2m[36m(func pid=70967)[0m f1_per_class: [0.567, 0.435, 0.786, 0.524, 0.072, 0.202, 0.366, 0.276, 0.244, 0.385]
== Status ==
Current time: 2024-01-07 01:20:18 (running for 00:37:11.09)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.394 |                   81 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.794 |      0.255 |                   56 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.712 |      0.068 |                   27 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 01:20:26 (running for 00:37:18.83)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.394 |                   81 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  1.794 |      0.255 |                   56 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.681 |      0.069 |                   28 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=89821)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=89821)[0m Configuration completed!
[2m[36m(func pid=89821)[0m New optimizer parameters:
[2m[36m(func pid=89821)[0m SGD (
[2m[36m(func pid=89821)[0m Parameter Group 0
[2m[36m(func pid=89821)[0m     dampening: 0
[2m[36m(func pid=89821)[0m     differentiable: False
[2m[36m(func pid=89821)[0m     foreach: None
[2m[36m(func pid=89821)[0m     lr: 0.001
[2m[36m(func pid=89821)[0m     maximize: False
[2m[36m(func pid=89821)[0m     momentum: 0.9
[2m[36m(func pid=89821)[0m     nesterov: False
[2m[36m(func pid=89821)[0m     weight_decay: 1e-05
[2m[36m(func pid=89821)[0m )
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0016 | Steps: 2 | Val loss: 2.7267 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 86.0383 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6472 | Steps: 2 | Val loss: 2.4167 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1330 | Steps: 2 | Val loss: 2.4973 | Batch size: 32 | lr: 0.001 | Duration: 4.65s
== Status ==
Current time: 2024-01-07 01:20:31 (running for 00:37:23.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.386 |                   82 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.273 |      0.279 |                   57 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.681 |      0.069 |                   28 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.38386194029850745
[2m[36m(func pid=70967)[0m top5: 0.886660447761194
[2m[36m(func pid=70967)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=70967)[0m f1_macro: 0.3869864822600041
[2m[36m(func pid=70967)[0m f1_weighted: 0.4035227865433182
[2m[36m(func pid=70967)[0m f1_per_class: [0.576, 0.43, 0.786, 0.526, 0.073, 0.199, 0.384, 0.276, 0.239, 0.38]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.0830223880597015
[2m[36m(func pid=83527)[0m top5: 0.44263059701492535
[2m[36m(func pid=83527)[0m f1_micro: 0.0830223880597015
[2m[36m(func pid=83527)[0m f1_macro: 0.06892186842494621
[2m[36m(func pid=83527)[0m f1_weighted: 0.09494122921546966
[2m[36m(func pid=83527)[0m f1_per_class: [0.058, 0.135, 0.057, 0.121, 0.0, 0.059, 0.074, 0.077, 0.075, 0.031]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.35027985074626866
[2m[36m(func pid=76915)[0m top5: 0.7569962686567164
[2m[36m(func pid=76915)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=76915)[0m f1_macro: 0.28285439476501095
[2m[36m(func pid=76915)[0m f1_weighted: 0.3647083513804617
[2m[36m(func pid=76915)[0m f1_per_class: [0.333, 0.389, 0.235, 0.269, 0.194, 0.245, 0.569, 0.139, 0.106, 0.349]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.06623134328358209
[2m[36m(func pid=89821)[0m top5: 0.48134328358208955
[2m[36m(func pid=89821)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=89821)[0m f1_macro: 0.04196526426047942
[2m[36m(func pid=89821)[0m f1_weighted: 0.040709086225771046
[2m[36m(func pid=89821)[0m f1_per_class: [0.137, 0.01, 0.0, 0.097, 0.0, 0.018, 0.0, 0.101, 0.022, 0.035]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 86.2409 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0013 | Steps: 2 | Val loss: 2.7509 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.6445 | Steps: 2 | Val loss: 2.4125 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0954 | Steps: 2 | Val loss: 2.4854 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:20:37 (running for 00:37:29.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.389 |                   84 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.283 |                   58 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.647 |      0.069 |                   29 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  3.133 |      0.042 |                    1 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=70967)[0m top1: 0.3871268656716418

[2m[36m(func pid=70967)[0m top5: 0.8847947761194029
[2m[36m(func pid=70967)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=70967)[0m f1_macro: 0.3886815475118026
[2m[36m(func pid=70967)[0m f1_weighted: 0.4077263095553269
[2m[36m(func pid=70967)[0m f1_per_class: [0.571, 0.438, 0.786, 0.53, 0.069, 0.206, 0.389, 0.269, 0.239, 0.39]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.34794776119402987
[2m[36m(func pid=76915)[0m top5: 0.7551305970149254
[2m[36m(func pid=76915)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=76915)[0m f1_macro: 0.2886150232256014
[2m[36m(func pid=76915)[0m f1_weighted: 0.3649055649891241
[2m[36m(func pid=76915)[0m f1_per_class: [0.414, 0.393, 0.261, 0.273, 0.187, 0.241, 0.558, 0.149, 0.115, 0.297]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=83527)[0m top1: 0.08488805970149253
[2m[36m(func pid=83527)[0m top5: 0.447294776119403
[2m[36m(func pid=83527)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=83527)[0m f1_macro: 0.07042799301068871
[2m[36m(func pid=83527)[0m f1_weighted: 0.09797795655125408
[2m[36m(func pid=83527)[0m f1_per_class: [0.064, 0.135, 0.056, 0.119, 0.0, 0.059, 0.087, 0.079, 0.075, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.06576492537313433
[2m[36m(func pid=89821)[0m top5: 0.4519589552238806
[2m[36m(func pid=89821)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=89821)[0m f1_macro: 0.05062390820095897
[2m[36m(func pid=89821)[0m f1_weighted: 0.04894567250195827
[2m[36m(func pid=89821)[0m f1_per_class: [0.12, 0.052, 0.0, 0.093, 0.0, 0.03, 0.0, 0.094, 0.06, 0.056]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0044 | Steps: 2 | Val loss: 2.7800 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 86.7589 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6064 | Steps: 2 | Val loss: 2.4044 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9279 | Steps: 2 | Val loss: 2.4733 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 01:20:42 (running for 00:37:34.66)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.389 |                   84 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.289 |                   59 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.606 |      0.074 |                   31 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  3.095 |      0.051 |                    2 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.386660447761194
[2m[36m(func pid=70967)[0m top5: 0.8871268656716418
[2m[36m(func pid=70967)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=70967)[0m f1_macro: 0.3898759863919587
[2m[36m(func pid=70967)[0m f1_weighted: 0.4074675208468877
[2m[36m(func pid=70967)[0m f1_per_class: [0.567, 0.438, 0.786, 0.526, 0.076, 0.204, 0.393, 0.261, 0.236, 0.411]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.09048507462686567
[2m[36m(func pid=83527)[0m top5: 0.45149253731343286
[2m[36m(func pid=83527)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=83527)[0m f1_macro: 0.07354036678991255
[2m[36m(func pid=83527)[0m f1_weighted: 0.10555321924379184
[2m[36m(func pid=83527)[0m f1_per_class: [0.066, 0.139, 0.056, 0.137, 0.0, 0.059, 0.092, 0.079, 0.075, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3460820895522388
[2m[36m(func pid=76915)[0m top5: 0.7551305970149254
[2m[36m(func pid=76915)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=76915)[0m f1_macro: 0.2912358071412892
[2m[36m(func pid=76915)[0m f1_weighted: 0.36504070647900294
[2m[36m(func pid=76915)[0m f1_per_class: [0.441, 0.397, 0.273, 0.273, 0.177, 0.246, 0.553, 0.146, 0.116, 0.291]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.055970149253731345
[2m[36m(func pid=89821)[0m top5: 0.4146455223880597
[2m[36m(func pid=89821)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=89821)[0m f1_macro: 0.043069322292848065
[2m[36m(func pid=89821)[0m f1_weighted: 0.04961667189269699
[2m[36m(func pid=89821)[0m f1_per_class: [0.077, 0.089, 0.0, 0.072, 0.0, 0.033, 0.009, 0.082, 0.019, 0.048]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6379 | Steps: 2 | Val loss: 2.3999 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0015 | Steps: 2 | Val loss: 2.8215 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0159 | Steps: 2 | Val loss: 86.6057 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8868 | Steps: 2 | Val loss: 2.4642 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:20:47 (running for 00:37:39.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.39  |                   85 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.291 |                   60 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.638 |      0.073 |                   32 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.928 |      0.043 |                    3 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.08768656716417911
[2m[36m(func pid=83527)[0m top5: 0.4542910447761194
[2m[36m(func pid=83527)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=83527)[0m f1_macro: 0.07263801140148789
[2m[36m(func pid=83527)[0m f1_weighted: 0.1009841571432677
[2m[36m(func pid=83527)[0m f1_per_class: [0.067, 0.137, 0.069, 0.124, 0.0, 0.058, 0.092, 0.073, 0.075, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.3805970149253731
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=70967)[0m f1_macro: 0.38479100441961894
[2m[36m(func pid=70967)[0m f1_weighted: 0.4019608172095137
[2m[36m(func pid=70967)[0m f1_per_class: [0.562, 0.432, 0.786, 0.516, 0.068, 0.195, 0.393, 0.258, 0.239, 0.4]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.341884328358209
[2m[36m(func pid=76915)[0m top5: 0.757929104477612
[2m[36m(func pid=76915)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=76915)[0m f1_macro: 0.2994307414038023
[2m[36m(func pid=76915)[0m f1_weighted: 0.363233796232487
[2m[36m(func pid=76915)[0m f1_per_class: [0.5, 0.399, 0.32, 0.281, 0.179, 0.246, 0.534, 0.154, 0.098, 0.283]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.05083955223880597
[2m[36m(func pid=89821)[0m top5: 0.394589552238806
[2m[36m(func pid=89821)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=89821)[0m f1_macro: 0.05001957941281996
[2m[36m(func pid=89821)[0m f1_weighted: 0.05051488670992083
[2m[36m(func pid=89821)[0m f1_per_class: [0.104, 0.097, 0.0, 0.043, 0.0, 0.039, 0.026, 0.072, 0.094, 0.026]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6103 | Steps: 2 | Val loss: 2.3912 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.8390 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 87.0349 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7921 | Steps: 2 | Val loss: 2.4502 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=70967)[0m top1: 0.37919776119402987
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=70967)[0m f1_macro: 0.3830393231260729
[2m[36m(func pid=70967)[0m f1_weighted: 0.4009392941414604
[2m[36m(func pid=70967)[0m f1_per_class: [0.553, 0.432, 0.786, 0.514, 0.075, 0.195, 0.393, 0.257, 0.237, 0.39]
== Status ==
Current time: 2024-01-07 01:20:52 (running for 00:37:45.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.383 |                   87 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.016 |      0.299 |                   61 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.638 |      0.073 |                   32 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.887 |      0.05  |                    4 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.33861940298507465
[2m[36m(func pid=76915)[0m top5: 0.7625932835820896
[2m[36m(func pid=76915)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=76915)[0m f1_macro: 0.30584770765997815
[2m[36m(func pid=76915)[0m f1_weighted: 0.36356606571932404
[2m[36m(func pid=76915)[0m f1_per_class: [0.556, 0.403, 0.358, 0.292, 0.171, 0.226, 0.526, 0.152, 0.091, 0.283]
[2m[36m(func pid=83527)[0m top1: 0.09001865671641791
[2m[36m(func pid=83527)[0m top5: 0.46548507462686567
[2m[36m(func pid=83527)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=83527)[0m f1_macro: 0.07371788282841885
[2m[36m(func pid=83527)[0m f1_weighted: 0.10465281549957561
[2m[36m(func pid=83527)[0m f1_per_class: [0.068, 0.136, 0.066, 0.133, 0.0, 0.058, 0.096, 0.074, 0.074, 0.033]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.05503731343283582
[2m[36m(func pid=89821)[0m top5: 0.41884328358208955
[2m[36m(func pid=89821)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=89821)[0m f1_macro: 0.056625804706391815
[2m[36m(func pid=89821)[0m f1_weighted: 0.05870280352710042
[2m[36m(func pid=89821)[0m f1_per_class: [0.097, 0.103, 0.043, 0.029, 0.007, 0.053, 0.06, 0.056, 0.108, 0.011]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0018 | Steps: 2 | Val loss: 2.8783 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6128 | Steps: 2 | Val loss: 2.3844 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 88.6735 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7185 | Steps: 2 | Val loss: 2.4442 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:20:58 (running for 00:37:50.62)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.383 |                   87 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.306 |                   62 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.613 |      0.076 |                   34 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.792 |      0.057 |                    5 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.376865671641791
[2m[36m(func pid=70967)[0m top5: 0.8852611940298507
[2m[36m(func pid=70967)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=70967)[0m f1_macro: 0.3828260492372676
[2m[36m(func pid=70967)[0m f1_weighted: 0.39728894386313474
[2m[36m(func pid=70967)[0m f1_per_class: [0.557, 0.433, 0.786, 0.516, 0.073, 0.192, 0.378, 0.256, 0.236, 0.4]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.09328358208955224
[2m[36m(func pid=83527)[0m top5: 0.4701492537313433
[2m[36m(func pid=83527)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=83527)[0m f1_macro: 0.07569006658031882
[2m[36m(func pid=83527)[0m f1_weighted: 0.10865535471340561
[2m[36m(func pid=83527)[0m f1_per_class: [0.065, 0.14, 0.065, 0.14, 0.0, 0.058, 0.101, 0.075, 0.077, 0.038]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.33348880597014924
[2m[36m(func pid=76915)[0m top5: 0.7597947761194029
[2m[36m(func pid=76915)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=76915)[0m f1_macro: 0.306296237192682
[2m[36m(func pid=76915)[0m f1_weighted: 0.3603683558585812
[2m[36m(func pid=76915)[0m f1_per_class: [0.579, 0.401, 0.375, 0.287, 0.169, 0.239, 0.515, 0.155, 0.089, 0.254]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.07182835820895522
[2m[36m(func pid=89821)[0m top5: 0.44263059701492535
[2m[36m(func pid=89821)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=89821)[0m f1_macro: 0.06653061465676206
[2m[36m(func pid=89821)[0m f1_weighted: 0.07535403847899307
[2m[36m(func pid=89821)[0m f1_per_class: [0.097, 0.092, 0.065, 0.016, 0.011, 0.081, 0.123, 0.038, 0.126, 0.016]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6198 | Steps: 2 | Val loss: 2.3761 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0036 | Steps: 2 | Val loss: 2.8953 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.1249 | Steps: 2 | Val loss: 90.6715 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6456 | Steps: 2 | Val loss: 2.4300 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:21:03 (running for 00:37:55.73)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.383 |                   88 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.306 |                   63 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.62  |      0.08  |                   35 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.718 |      0.067 |                    6 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.09701492537313433
[2m[36m(func pid=83527)[0m top5: 0.47901119402985076
[2m[36m(func pid=83527)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=83527)[0m f1_macro: 0.08019059321189709
[2m[36m(func pid=83527)[0m f1_weighted: 0.11075380162291343
[2m[36m(func pid=83527)[0m f1_per_class: [0.084, 0.144, 0.074, 0.139, 0.0, 0.058, 0.102, 0.08, 0.081, 0.038]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.373134328358209
[2m[36m(func pid=70967)[0m top5: 0.8819962686567164
[2m[36m(func pid=70967)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=70967)[0m f1_macro: 0.3795648029487754
[2m[36m(func pid=70967)[0m f1_weighted: 0.39243198679080926
[2m[36m(func pid=70967)[0m f1_per_class: [0.571, 0.427, 0.786, 0.512, 0.068, 0.202, 0.366, 0.256, 0.238, 0.37]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3246268656716418
[2m[36m(func pid=76915)[0m top5: 0.7597947761194029
[2m[36m(func pid=76915)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=76915)[0m f1_macro: 0.3075733231001867
[2m[36m(func pid=76915)[0m f1_weighted: 0.3518339992370299
[2m[36m(func pid=76915)[0m f1_per_class: [0.6, 0.384, 0.421, 0.284, 0.187, 0.245, 0.497, 0.157, 0.069, 0.231]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.08955223880597014
[2m[36m(func pid=89821)[0m top5: 0.4626865671641791
[2m[36m(func pid=89821)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=89821)[0m f1_macro: 0.07335934341430835
[2m[36m(func pid=89821)[0m f1_weighted: 0.09034866183693895
[2m[36m(func pid=89821)[0m f1_per_class: [0.1, 0.073, 0.092, 0.007, 0.015, 0.088, 0.193, 0.023, 0.123, 0.019]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5938 | Steps: 2 | Val loss: 2.3679 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0011 | Steps: 2 | Val loss: 2.9132 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 92.1836 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.5963 | Steps: 2 | Val loss: 2.3894 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 01:21:08 (running for 00:38:00.86)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.004 |      0.38  |                   89 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0.125 |      0.308 |                   64 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.594 |      0.082 |                   36 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.646 |      0.073 |                    7 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.10121268656716417
[2m[36m(func pid=83527)[0m top5: 0.4878731343283582
[2m[36m(func pid=83527)[0m f1_micro: 0.10121268656716416
[2m[36m(func pid=83527)[0m f1_macro: 0.08240761277019462
[2m[36m(func pid=83527)[0m f1_weighted: 0.11694008159866827
[2m[36m(func pid=83527)[0m f1_per_class: [0.091, 0.146, 0.074, 0.148, 0.0, 0.059, 0.113, 0.08, 0.08, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.37220149253731344
[2m[36m(func pid=70967)[0m top5: 0.8815298507462687
[2m[36m(func pid=70967)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=70967)[0m f1_macro: 0.3782539750181514
[2m[36m(func pid=70967)[0m f1_weighted: 0.3919925585761589
[2m[36m(func pid=70967)[0m f1_per_class: [0.557, 0.425, 0.786, 0.509, 0.067, 0.192, 0.372, 0.259, 0.24, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3199626865671642
[2m[36m(func pid=76915)[0m top5: 0.7597947761194029
[2m[36m(func pid=76915)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=76915)[0m f1_macro: 0.3133031429619617
[2m[36m(func pid=76915)[0m f1_weighted: 0.3500673864394779
[2m[36m(func pid=76915)[0m f1_per_class: [0.614, 0.389, 0.48, 0.294, 0.182, 0.257, 0.471, 0.165, 0.072, 0.209]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.11986940298507463
[2m[36m(func pid=89821)[0m top5: 0.49673507462686567
[2m[36m(func pid=89821)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=89821)[0m f1_macro: 0.0890328137403017
[2m[36m(func pid=89821)[0m f1_weighted: 0.11616369134057547
[2m[36m(func pid=89821)[0m f1_per_class: [0.116, 0.093, 0.109, 0.01, 0.015, 0.098, 0.26, 0.012, 0.124, 0.053]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0010 | Steps: 2 | Val loss: 2.9075 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5551 | Steps: 2 | Val loss: 2.3627 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 95.7537 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4617 | Steps: 2 | Val loss: 2.3335 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:21:13 (running for 00:38:06.11)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.378 |                   90 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.313 |                   65 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.555 |      0.084 |                   37 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.596 |      0.089 |                    8 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3773320895522388
[2m[36m(func pid=70967)[0m top5: 0.882929104477612
[2m[36m(func pid=70967)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=70967)[0m f1_macro: 0.3848766347149958
[2m[36m(func pid=70967)[0m f1_weighted: 0.3972475018096846
[2m[36m(func pid=70967)[0m f1_per_class: [0.569, 0.432, 0.815, 0.518, 0.068, 0.192, 0.376, 0.259, 0.235, 0.385]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.10307835820895522
[2m[36m(func pid=83527)[0m top5: 0.4962686567164179
[2m[36m(func pid=83527)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=83527)[0m f1_macro: 0.08412370269128124
[2m[36m(func pid=83527)[0m f1_weighted: 0.1198520984428953
[2m[36m(func pid=83527)[0m f1_per_class: [0.087, 0.141, 0.076, 0.157, 0.0, 0.068, 0.113, 0.087, 0.079, 0.032]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3064365671641791
[2m[36m(func pid=76915)[0m top5: 0.7523320895522388
[2m[36m(func pid=76915)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=76915)[0m f1_macro: 0.31072181846200386
[2m[36m(func pid=76915)[0m f1_weighted: 0.3381958970114531
[2m[36m(func pid=76915)[0m f1_per_class: [0.6, 0.385, 0.522, 0.285, 0.182, 0.249, 0.447, 0.158, 0.074, 0.205]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.14412313432835822
[2m[36m(func pid=89821)[0m top5: 0.5424440298507462
[2m[36m(func pid=89821)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=89821)[0m f1_macro: 0.10243475893044998
[2m[36m(func pid=89821)[0m f1_weighted: 0.13788876921649248
[2m[36m(func pid=89821)[0m f1_per_class: [0.131, 0.121, 0.138, 0.025, 0.013, 0.095, 0.302, 0.012, 0.12, 0.066]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5776 | Steps: 2 | Val loss: 2.3603 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0022 | Steps: 2 | Val loss: 2.8983 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 98.3844 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3454 | Steps: 2 | Val loss: 2.2731 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:21:19 (running for 00:38:11.39)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.385 |                   91 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.311 |                   66 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.578 |      0.084 |                   38 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.462 |      0.102 |                    9 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.1021455223880597
[2m[36m(func pid=83527)[0m top5: 0.49580223880597013
[2m[36m(func pid=83527)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=83527)[0m f1_macro: 0.08425794185500364
[2m[36m(func pid=83527)[0m f1_weighted: 0.11784366547693977
[2m[36m(func pid=83527)[0m f1_per_class: [0.086, 0.146, 0.082, 0.145, 0.0, 0.067, 0.116, 0.077, 0.078, 0.044]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=70967)[0m top1: 0.37826492537313433
[2m[36m(func pid=70967)[0m top5: 0.8805970149253731
[2m[36m(func pid=70967)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=70967)[0m f1_macro: 0.38366514423907316
[2m[36m(func pid=70967)[0m f1_weighted: 0.39768434764438004
[2m[36m(func pid=70967)[0m f1_per_class: [0.56, 0.434, 0.815, 0.519, 0.069, 0.193, 0.375, 0.259, 0.242, 0.37]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=76915)[0m top1: 0.3003731343283582
[2m[36m(func pid=76915)[0m top5: 0.7490671641791045
[2m[36m(func pid=76915)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=76915)[0m f1_macro: 0.30908751054100053
[2m[36m(func pid=76915)[0m f1_weighted: 0.3340775849685431
[2m[36m(func pid=76915)[0m f1_per_class: [0.593, 0.387, 0.545, 0.287, 0.182, 0.25, 0.43, 0.161, 0.075, 0.18]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.16511194029850745
[2m[36m(func pid=89821)[0m top5: 0.6007462686567164
[2m[36m(func pid=89821)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=89821)[0m f1_macro: 0.12035837562379284
[2m[36m(func pid=89821)[0m f1_weighted: 0.1672304197460168
[2m[36m(func pid=89821)[0m f1_per_class: [0.131, 0.182, 0.185, 0.071, 0.01, 0.099, 0.315, 0.046, 0.112, 0.052]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5132 | Steps: 2 | Val loss: 2.3580 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0009 | Steps: 2 | Val loss: 2.9058 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 102.0688 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2164 | Steps: 2 | Val loss: 2.2229 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:21:24 (running for 00:38:16.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.385 |                   93 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.309 |                   67 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.578 |      0.084 |                   38 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.345 |      0.12  |                   10 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.37919776119402987
[2m[36m(func pid=70967)[0m top5: 0.8819962686567164
[2m[36m(func pid=70967)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=70967)[0m f1_macro: 0.38469729769436695
[2m[36m(func pid=70967)[0m f1_weighted: 0.39873089806347806
[2m[36m(func pid=70967)[0m f1_per_class: [0.55, 0.431, 0.815, 0.521, 0.076, 0.196, 0.378, 0.259, 0.242, 0.38]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1044776119402985
[2m[36m(func pid=83527)[0m top5: 0.5013992537313433
[2m[36m(func pid=83527)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=83527)[0m f1_macro: 0.08608282106974466
[2m[36m(func pid=83527)[0m f1_weighted: 0.12020020730796266
[2m[36m(func pid=83527)[0m f1_per_class: [0.099, 0.143, 0.078, 0.148, 0.0, 0.068, 0.122, 0.084, 0.077, 0.044]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.29197761194029853
[2m[36m(func pid=76915)[0m top5: 0.7397388059701493
[2m[36m(func pid=76915)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=76915)[0m f1_macro: 0.307281329495689
[2m[36m(func pid=76915)[0m f1_weighted: 0.32646934529882354
[2m[36m(func pid=76915)[0m f1_per_class: [0.604, 0.392, 0.571, 0.279, 0.182, 0.249, 0.41, 0.159, 0.063, 0.163]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.17723880597014927
[2m[36m(func pid=89821)[0m top5: 0.6394589552238806
[2m[36m(func pid=89821)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=89821)[0m f1_macro: 0.14187598929168724
[2m[36m(func pid=89821)[0m f1_weighted: 0.18868257096480984
[2m[36m(func pid=89821)[0m f1_per_class: [0.151, 0.223, 0.238, 0.135, 0.014, 0.085, 0.297, 0.087, 0.113, 0.076]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0006 | Steps: 2 | Val loss: 2.9284 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5460 | Steps: 2 | Val loss: 2.3569 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 103.9367 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1036 | Steps: 2 | Val loss: 2.1968 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 01:21:29 (running for 00:38:21.62)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.383 |                   94 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.307 |                   68 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.546 |      0.086 |                   40 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.216 |      0.142 |                   11 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.37826492537313433
[2m[36m(func pid=70967)[0m top5: 0.8824626865671642
[2m[36m(func pid=70967)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=70967)[0m f1_macro: 0.3834077872501923
[2m[36m(func pid=70967)[0m f1_weighted: 0.39886588327297157
[2m[36m(func pid=70967)[0m f1_per_class: [0.554, 0.431, 0.815, 0.52, 0.068, 0.196, 0.38, 0.252, 0.238, 0.38]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1044776119402985
[2m[36m(func pid=83527)[0m top5: 0.5013992537313433
[2m[36m(func pid=83527)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=83527)[0m f1_macro: 0.08605454055093097
[2m[36m(func pid=83527)[0m f1_weighted: 0.12032735730368181
[2m[36m(func pid=83527)[0m f1_per_class: [0.097, 0.143, 0.076, 0.141, 0.0, 0.068, 0.128, 0.088, 0.076, 0.044]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.28451492537313433
[2m[36m(func pid=76915)[0m top5: 0.7374067164179104
[2m[36m(func pid=76915)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=76915)[0m f1_macro: 0.30616136964591584
[2m[36m(func pid=76915)[0m f1_weighted: 0.3187046979974442
[2m[36m(func pid=76915)[0m f1_per_class: [0.608, 0.391, 0.6, 0.279, 0.167, 0.247, 0.386, 0.155, 0.064, 0.165]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.1912313432835821
[2m[36m(func pid=89821)[0m top5: 0.6739738805970149
[2m[36m(func pid=89821)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=89821)[0m f1_macro: 0.17094964277419597
[2m[36m(func pid=89821)[0m f1_weighted: 0.20441495498639767
[2m[36m(func pid=89821)[0m f1_per_class: [0.199, 0.253, 0.344, 0.24, 0.015, 0.102, 0.21, 0.145, 0.122, 0.079]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0007 | Steps: 2 | Val loss: 2.9194 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4894 | Steps: 2 | Val loss: 2.3517 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 106.7721 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=70967)[0m top1: 0.384794776119403
[2m[36m(func pid=70967)[0m top5: 0.8824626865671642
[2m[36m(func pid=70967)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=70967)[0m f1_macro: 0.38593487631998613
[2m[36m(func pid=70967)[0m f1_weighted: 0.4057589080418209
[2m[36m(func pid=70967)[0m f1_per_class: [0.55, 0.431, 0.815, 0.527, 0.078, 0.196, 0.398, 0.25, 0.24, 0.375]
[2m[36m(func pid=70967)[0m == Status ==
Current time: 2024-01-07 01:21:34 (running for 00:38:26.83)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.001 |      0.386 |                   95 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.306 |                   69 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.546 |      0.086 |                   40 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.104 |      0.171 |                   12 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0538 | Steps: 2 | Val loss: 2.1860 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=83527)[0m top1: 0.10727611940298508
[2m[36m(func pid=83527)[0m top5: 0.5088619402985075
[2m[36m(func pid=83527)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=83527)[0m f1_macro: 0.08838739923268674
[2m[36m(func pid=83527)[0m f1_weighted: 0.12306392354558388
[2m[36m(func pid=83527)[0m f1_per_class: [0.098, 0.146, 0.076, 0.15, 0.0, 0.067, 0.126, 0.087, 0.087, 0.047]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=76915)[0m top1: 0.2751865671641791
[2m[36m(func pid=76915)[0m top5: 0.7360074626865671
[2m[36m(func pid=76915)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=76915)[0m f1_macro: 0.3041191581015115
[2m[36m(func pid=76915)[0m f1_weighted: 0.3089478784756319
[2m[36m(func pid=76915)[0m f1_per_class: [0.564, 0.377, 0.632, 0.293, 0.192, 0.25, 0.348, 0.157, 0.066, 0.163]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=89821)[0m top1: 0.18470149253731344
[2m[36m(func pid=89821)[0m top5: 0.6772388059701493
[2m[36m(func pid=89821)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=89821)[0m f1_macro: 0.16799250061568677
[2m[36m(func pid=89821)[0m f1_weighted: 0.18793393759877303
[2m[36m(func pid=89821)[0m f1_per_class: [0.232, 0.274, 0.379, 0.268, 0.014, 0.076, 0.125, 0.156, 0.089, 0.067]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0023 | Steps: 2 | Val loss: 2.8990 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.5400 | Steps: 2 | Val loss: 2.3456 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 110.4209 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:21:39 (running for 00:38:31.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.388 |                   96 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.304 |                   70 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.489 |      0.088 |                   41 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  2.054 |      0.168 |                   13 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3871268656716418
[2m[36m(func pid=70967)[0m top5: 0.8843283582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=70967)[0m f1_macro: 0.38770542565739224
[2m[36m(func pid=70967)[0m f1_weighted: 0.408142860416621
[2m[36m(func pid=70967)[0m f1_per_class: [0.545, 0.432, 0.815, 0.526, 0.07, 0.195, 0.404, 0.262, 0.243, 0.385]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.10960820895522388
[2m[36m(func pid=83527)[0m top5: 0.5167910447761194
[2m[36m(func pid=83527)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=83527)[0m f1_macro: 0.08974469154782652
[2m[36m(func pid=83527)[0m f1_weighted: 0.12579408485104113
[2m[36m(func pid=83527)[0m f1_per_class: [0.104, 0.144, 0.08, 0.149, 0.0, 0.074, 0.136, 0.077, 0.088, 0.045]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9127 | Steps: 2 | Val loss: 2.1730 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=76915)[0m top1: 0.2635261194029851
[2m[36m(func pid=76915)[0m top5: 0.730410447761194
[2m[36m(func pid=76915)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=76915)[0m f1_macro: 0.2926412485170721
[2m[36m(func pid=76915)[0m f1_weighted: 0.2950384669315727
[2m[36m(func pid=76915)[0m f1_per_class: [0.53, 0.379, 0.649, 0.288, 0.189, 0.227, 0.319, 0.16, 0.034, 0.151]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0024 | Steps: 2 | Val loss: 2.9442 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=89821)[0m top1: 0.1958955223880597
[2m[36m(func pid=89821)[0m top5: 0.6861007462686567
[2m[36m(func pid=89821)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=89821)[0m f1_macro: 0.1854892233220156
[2m[36m(func pid=89821)[0m f1_weighted: 0.1906942621391807
[2m[36m(func pid=89821)[0m f1_per_class: [0.28, 0.282, 0.468, 0.314, 0.013, 0.088, 0.072, 0.186, 0.082, 0.069]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4913 | Steps: 2 | Val loss: 2.3409 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0000 | Steps: 2 | Val loss: 114.5173 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 01:21:44 (running for 00:38:37.00)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.388 |                   97 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.293 |                   71 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.54  |      0.09  |                   42 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.913 |      0.185 |                   14 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.38572761194029853
[2m[36m(func pid=70967)[0m top5: 0.8843283582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=70967)[0m f1_macro: 0.3882037297266434
[2m[36m(func pid=70967)[0m f1_weighted: 0.4054621843180903
[2m[36m(func pid=70967)[0m f1_per_class: [0.563, 0.433, 0.815, 0.524, 0.079, 0.194, 0.396, 0.267, 0.238, 0.375]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.11240671641791045
[2m[36m(func pid=83527)[0m top5: 0.5219216417910447
[2m[36m(func pid=83527)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=83527)[0m f1_macro: 0.09267762353709531
[2m[36m(func pid=83527)[0m f1_weighted: 0.12803264788337215
[2m[36m(func pid=83527)[0m f1_per_class: [0.102, 0.146, 0.098, 0.154, 0.0, 0.074, 0.136, 0.084, 0.087, 0.046]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8346 | Steps: 2 | Val loss: 2.1742 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=76915)[0m top1: 0.25093283582089554
[2m[36m(func pid=76915)[0m top5: 0.7243470149253731
[2m[36m(func pid=76915)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=76915)[0m f1_macro: 0.2834473517298899
[2m[36m(func pid=76915)[0m f1_weighted: 0.2781843939959577
[2m[36m(func pid=76915)[0m f1_per_class: [0.489, 0.372, 0.686, 0.29, 0.171, 0.214, 0.272, 0.164, 0.036, 0.14]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0072 | Steps: 2 | Val loss: 2.9321 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=89821)[0m top1: 0.1982276119402985
[2m[36m(func pid=89821)[0m top5: 0.683768656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=89821)[0m f1_macro: 0.19613918500229374
[2m[36m(func pid=89821)[0m f1_weighted: 0.19038785605251568
[2m[36m(func pid=89821)[0m f1_per_class: [0.301, 0.277, 0.524, 0.332, 0.012, 0.1, 0.044, 0.204, 0.099, 0.067]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4558 | Steps: 2 | Val loss: 2.3391 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 116.9123 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=70967)[0m top1: 0.38759328358208955
[2m[36m(func pid=70967)[0m top5: 0.8871268656716418
[2m[36m(func pid=70967)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=70967)[0m f1_macro: 0.3870565606985191
[2m[36m(func pid=70967)[0m f1_weighted: 0.4078092966291318
[2m[36m(func pid=70967)[0m f1_per_class: [0.54, 0.428, 0.815, 0.526, 0.079, 0.201, 0.406, 0.25, 0.241, 0.385]
[2m[36m(func pid=70967)[0m 
== Status ==
Current time: 2024-01-07 01:21:49 (running for 00:38:42.25)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.007 |      0.387 |                   98 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.283 |                   72 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.491 |      0.093 |                   43 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.835 |      0.196 |                   15 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.11333955223880597
[2m[36m(func pid=83527)[0m top5: 0.5228544776119403
[2m[36m(func pid=83527)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=83527)[0m f1_macro: 0.09287919951720411
[2m[36m(func pid=83527)[0m f1_weighted: 0.12957606440396974
[2m[36m(func pid=83527)[0m f1_per_class: [0.096, 0.147, 0.101, 0.168, 0.0, 0.071, 0.128, 0.085, 0.086, 0.046]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7268 | Steps: 2 | Val loss: 2.1719 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=76915)[0m top1: 0.2416044776119403
[2m[36m(func pid=76915)[0m top5: 0.7248134328358209
[2m[36m(func pid=76915)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=76915)[0m f1_macro: 0.2778040454659326
[2m[36m(func pid=76915)[0m f1_weighted: 0.26576858715100826
[2m[36m(func pid=76915)[0m f1_per_class: [0.482, 0.366, 0.706, 0.289, 0.167, 0.18, 0.248, 0.163, 0.038, 0.14]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0021 | Steps: 2 | Val loss: 2.9665 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=89821)[0m top1: 0.20522388059701493
[2m[36m(func pid=89821)[0m top5: 0.6861007462686567
[2m[36m(func pid=89821)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=89821)[0m f1_macro: 0.19820713591726152
[2m[36m(func pid=89821)[0m f1_weighted: 0.19447763012885752
[2m[36m(func pid=89821)[0m f1_per_class: [0.307, 0.297, 0.489, 0.346, 0.037, 0.094, 0.036, 0.207, 0.096, 0.075]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4589 | Steps: 2 | Val loss: 2.3375 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 117.8140 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:21:55 (running for 00:38:47.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.3495
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00017 | RUNNING    | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.002 |      0.383 |                   99 |
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.278 |                   73 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.456 |      0.093 |                   44 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.727 |      0.198 |                   16 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=70967)[0m top1: 0.3829291044776119
[2m[36m(func pid=70967)[0m top5: 0.8838619402985075
[2m[36m(func pid=70967)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=70967)[0m f1_macro: 0.38346806315486953
[2m[36m(func pid=70967)[0m f1_weighted: 0.40317514913847974
[2m[36m(func pid=70967)[0m f1_per_class: [0.551, 0.428, 0.815, 0.52, 0.078, 0.191, 0.4, 0.25, 0.236, 0.366]
[2m[36m(func pid=70967)[0m 
[2m[36m(func pid=83527)[0m top1: 0.11473880597014925
[2m[36m(func pid=83527)[0m top5: 0.5279850746268657
[2m[36m(func pid=83527)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=83527)[0m f1_macro: 0.09489705126537676
[2m[36m(func pid=83527)[0m f1_weighted: 0.13025466372795655
[2m[36m(func pid=83527)[0m f1_per_class: [0.101, 0.147, 0.105, 0.168, 0.0, 0.074, 0.129, 0.085, 0.088, 0.051]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7065 | Steps: 2 | Val loss: 2.1735 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=76915)[0m top1: 0.23833955223880596
[2m[36m(func pid=76915)[0m top5: 0.7257462686567164
[2m[36m(func pid=76915)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=76915)[0m f1_macro: 0.27778934246404113
[2m[36m(func pid=76915)[0m f1_weighted: 0.26476898687369416
[2m[36m(func pid=76915)[0m f1_per_class: [0.472, 0.356, 0.727, 0.296, 0.176, 0.177, 0.246, 0.158, 0.039, 0.129]
[2m[36m(func pid=76915)[0m 
[2m[36m(func pid=70967)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0033 | Steps: 2 | Val loss: 2.9688 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=89821)[0m top1: 0.2042910447761194
[2m[36m(func pid=89821)[0m top5: 0.6875
[2m[36m(func pid=89821)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=89821)[0m f1_macro: 0.19532312498252064
[2m[36m(func pid=89821)[0m f1_weighted: 0.19549461432467574
[2m[36m(func pid=89821)[0m f1_per_class: [0.294, 0.294, 0.458, 0.353, 0.027, 0.086, 0.036, 0.215, 0.105, 0.084]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4437 | Steps: 2 | Val loss: 2.3373 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=76915)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0979 | Steps: 2 | Val loss: 121.2876 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=70967)[0m top1: 0.3810634328358209
[2m[36m(func pid=70967)[0m top5: 0.8843283582089553
[2m[36m(func pid=70967)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=70967)[0m f1_macro: 0.3826204187670066
[2m[36m(func pid=70967)[0m f1_weighted: 0.40105821624809807
[2m[36m(func pid=70967)[0m f1_per_class: [0.548, 0.426, 0.815, 0.519, 0.072, 0.192, 0.395, 0.251, 0.239, 0.37]
== Status ==
Current time: 2024-01-07 01:22:00 (running for 00:38:52.58)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3495
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 3 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00019 | RUNNING    | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  0     |      0.278 |                   74 |
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.459 |      0.095 |                   45 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.707 |      0.195 |                   17 |
| train_57e67_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.11473880597014925
[2m[36m(func pid=83527)[0m top5: 0.523320895522388
[2m[36m(func pid=83527)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=83527)[0m f1_macro: 0.09493065824778815
[2m[36m(func pid=83527)[0m f1_weighted: 0.13014379182561978
[2m[36m(func pid=83527)[0m f1_per_class: [0.104, 0.151, 0.106, 0.165, 0.0, 0.075, 0.13, 0.084, 0.085, 0.051]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.5579 | Steps: 2 | Val loss: 2.1538 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=76915)[0m top1: 0.23740671641791045
[2m[36m(func pid=76915)[0m top5: 0.7215485074626866
[2m[36m(func pid=76915)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=76915)[0m f1_macro: 0.2761659591863196
[2m[36m(func pid=76915)[0m f1_weighted: 0.26228955528660247
[2m[36m(func pid=76915)[0m f1_per_class: [0.463, 0.361, 0.727, 0.29, 0.182, 0.158, 0.247, 0.166, 0.039, 0.129]
[2m[36m(func pid=89821)[0m top1: 0.2150186567164179
[2m[36m(func pid=89821)[0m top5: 0.699160447761194
[2m[36m(func pid=89821)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=89821)[0m f1_macro: 0.2024730181088747
[2m[36m(func pid=89821)[0m f1_weighted: 0.20718684025381437
[2m[36m(func pid=89821)[0m f1_per_class: [0.296, 0.289, 0.415, 0.371, 0.035, 0.097, 0.051, 0.222, 0.156, 0.094]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.4490 | Steps: 2 | Val loss: 2.3314 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=83527)[0m top1: 0.11753731343283583
[2m[36m(func pid=83527)[0m top5: 0.5298507462686567
[2m[36m(func pid=83527)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=83527)[0m f1_macro: 0.09755176329737697
[2m[36m(func pid=83527)[0m f1_weighted: 0.13205680708575673
[2m[36m(func pid=83527)[0m f1_per_class: [0.11, 0.156, 0.102, 0.171, 0.0, 0.074, 0.125, 0.088, 0.096, 0.054]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.4683 | Steps: 2 | Val loss: 2.1254 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4670 | Steps: 2 | Val loss: 2.3271 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=89821)[0m top1: 0.2224813432835821
[2m[36m(func pid=89821)[0m top5: 0.730410447761194
[2m[36m(func pid=89821)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=89821)[0m f1_macro: 0.20684721968160735
[2m[36m(func pid=89821)[0m f1_weighted: 0.21745806342978685
[2m[36m(func pid=89821)[0m f1_per_class: [0.265, 0.282, 0.415, 0.383, 0.037, 0.11, 0.073, 0.223, 0.157, 0.123]
[2m[36m(func pid=94112)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=94112)[0m Configuration completed!
[2m[36m(func pid=94112)[0m New optimizer parameters:
[2m[36m(func pid=94112)[0m SGD (
[2m[36m(func pid=94112)[0m Parameter Group 0
[2m[36m(func pid=94112)[0m     dampening: 0
[2m[36m(func pid=94112)[0m     differentiable: False
[2m[36m(func pid=94112)[0m     foreach: None
[2m[36m(func pid=94112)[0m     lr: 0.01
[2m[36m(func pid=94112)[0m     maximize: False
[2m[36m(func pid=94112)[0m     momentum: 0.9
[2m[36m(func pid=94112)[0m     nesterov: False
[2m[36m(func pid=94112)[0m     weight_decay: 1e-05
[2m[36m(func pid=94112)[0m )
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:22:06 (running for 00:38:58.68)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.449 |      0.098 |                   47 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.558 |      0.202 |                   18 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94182)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=94182)[0m Configuration completed!
[2m[36m(func pid=94182)[0m New optimizer parameters:
[2m[36m(func pid=94182)[0m SGD (
[2m[36m(func pid=94182)[0m Parameter Group 0
[2m[36m(func pid=94182)[0m     dampening: 0
[2m[36m(func pid=94182)[0m     differentiable: False
[2m[36m(func pid=94182)[0m     foreach: None
[2m[36m(func pid=94182)[0m     lr: 0.1
[2m[36m(func pid=94182)[0m     maximize: False
[2m[36m(func pid=94182)[0m     momentum: 0.9
[2m[36m(func pid=94182)[0m     nesterov: False
[2m[36m(func pid=94182)[0m     weight_decay: 1e-05
[2m[36m(func pid=94182)[0m )
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:22:11 (running for 00:39:04.10)
Memory usage on this node: 23.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.467 |      0.098 |                   48 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.468 |      0.207 |                   19 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.11753731343283583
[2m[36m(func pid=83527)[0m top5: 0.534981343283582
[2m[36m(func pid=83527)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=83527)[0m f1_macro: 0.09758764777631909
[2m[36m(func pid=83527)[0m f1_weighted: 0.13155148292957278
[2m[36m(func pid=83527)[0m f1_per_class: [0.109, 0.16, 0.104, 0.166, 0.0, 0.078, 0.126, 0.083, 0.098, 0.053]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1458 | Steps: 2 | Val loss: 2.3856 | Batch size: 32 | lr: 0.01 | Duration: 4.50s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4496 | Steps: 2 | Val loss: 2.0882 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4141 | Steps: 2 | Val loss: 2.3229 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.7484 | Steps: 2 | Val loss: 2.9401 | Batch size: 32 | lr: 0.1 | Duration: 4.22s
[2m[36m(func pid=94112)[0m top1: 0.06623134328358209
[2m[36m(func pid=94112)[0m top5: 0.44869402985074625
[2m[36m(func pid=94112)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=94112)[0m f1_macro: 0.07175661924839985
[2m[36m(func pid=94112)[0m f1_weighted: 0.06834977959419018
[2m[36m(func pid=94112)[0m f1_per_class: [0.117, 0.1, 0.093, 0.091, 0.006, 0.042, 0.032, 0.067, 0.125, 0.045]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m top1: 0.23134328358208955
[2m[36m(func pid=89821)[0m top5: 0.7560634328358209
[2m[36m(func pid=89821)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=89821)[0m f1_macro: 0.21516327675106908
[2m[36m(func pid=89821)[0m f1_weighted: 0.22948806691471096
[2m[36m(func pid=89821)[0m f1_per_class: [0.269, 0.277, 0.407, 0.39, 0.048, 0.129, 0.101, 0.225, 0.147, 0.158]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:22:17 (running for 00:39:09.29)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.414 |      0.102 |                   49 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.45  |      0.215 |                   20 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  3.146 |      0.072 |                    1 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.12220149253731344
[2m[36m(func pid=83527)[0m top5: 0.5354477611940298
[2m[36m(func pid=83527)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=83527)[0m f1_macro: 0.10188967647443983
[2m[36m(func pid=83527)[0m f1_weighted: 0.13639402669048206
[2m[36m(func pid=83527)[0m f1_per_class: [0.113, 0.158, 0.112, 0.186, 0.0, 0.069, 0.123, 0.098, 0.102, 0.058]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m top1: 0.04617537313432836
[2m[36m(func pid=94182)[0m top5: 0.7229477611940298
[2m[36m(func pid=94182)[0m f1_micro: 0.04617537313432836
[2m[36m(func pid=94182)[0m f1_macro: 0.014605549521187278
[2m[36m(func pid=94182)[0m f1_weighted: 0.02600052167758088
[2m[36m(func pid=94182)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.066, 0.0]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4373 | Steps: 2 | Val loss: 2.0559 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8547 | Steps: 2 | Val loss: 2.3221 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3666 | Steps: 2 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 7.8594 | Steps: 2 | Val loss: 9.2668 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=89821)[0m top1: 0.240205223880597
[2m[36m(func pid=89821)[0m top5: 0.7831156716417911
[2m[36m(func pid=89821)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=89821)[0m f1_macro: 0.22792997793085706
[2m[36m(func pid=89821)[0m f1_weighted: 0.24393796670716536
[2m[36m(func pid=89821)[0m f1_per_class: [0.27, 0.289, 0.431, 0.396, 0.043, 0.138, 0.13, 0.227, 0.167, 0.189]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:22:22 (running for 00:39:14.52)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.414 |      0.102 |                   49 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.437 |      0.228 |                   21 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  2.855 |      0.085 |                    2 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  3.748 |      0.015 |                    1 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.12220149253731344
[2m[36m(func pid=83527)[0m top5: 0.5396455223880597
[2m[36m(func pid=83527)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=83527)[0m f1_macro: 0.10226682572593826
[2m[36m(func pid=83527)[0m f1_weighted: 0.13592114086943288
[2m[36m(func pid=83527)[0m f1_per_class: [0.116, 0.163, 0.12, 0.177, 0.0, 0.076, 0.126, 0.097, 0.091, 0.057]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94112)[0m top1: 0.11147388059701492
[2m[36m(func pid=94112)[0m top5: 0.5862873134328358
[2m[36m(func pid=94112)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=94112)[0m f1_macro: 0.0851702185267291
[2m[36m(func pid=94112)[0m f1_weighted: 0.10790841387599184
[2m[36m(func pid=94112)[0m f1_per_class: [0.102, 0.074, 0.206, 0.026, 0.021, 0.07, 0.248, 0.0, 0.064, 0.042]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.27052238805970147
[2m[36m(func pid=94182)[0m top5: 0.7238805970149254
[2m[36m(func pid=94182)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=94182)[0m f1_macro: 0.11161624843915163
[2m[36m(func pid=94182)[0m f1_weighted: 0.13590587849394994
[2m[36m(func pid=94182)[0m f1_per_class: [0.531, 0.0, 0.0, 0.442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.143]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3035 | Steps: 2 | Val loss: 2.0186 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3509 | Steps: 2 | Val loss: 2.3157 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3376 | Steps: 2 | Val loss: 2.0733 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 15.8681 | Steps: 2 | Val loss: 19.0305 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 01:22:27 (running for 00:39:19.70)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.367 |      0.102 |                   50 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.304 |      0.24  |                   22 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  2.855 |      0.085 |                    2 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  7.859 |      0.112 |                    2 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.2523320895522388
[2m[36m(func pid=89821)[0m top5: 0.8064365671641791
[2m[36m(func pid=89821)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=89821)[0m f1_macro: 0.2403708571100318
[2m[36m(func pid=89821)[0m f1_weighted: 0.260161712930128
[2m[36m(func pid=89821)[0m f1_per_class: [0.296, 0.307, 0.44, 0.4, 0.045, 0.136, 0.167, 0.227, 0.17, 0.215]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.12639925373134328
[2m[36m(func pid=83527)[0m top5: 0.542910447761194
[2m[36m(func pid=83527)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=83527)[0m f1_macro: 0.10766628409007714
[2m[36m(func pid=83527)[0m f1_weighted: 0.139045246007319
[2m[36m(func pid=83527)[0m f1_per_class: [0.122, 0.171, 0.129, 0.18, 0.0, 0.081, 0.123, 0.104, 0.102, 0.063]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94112)[0m top1: 0.2019589552238806
[2m[36m(func pid=94112)[0m top5: 0.7826492537313433
[2m[36m(func pid=94112)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=94112)[0m f1_macro: 0.18345028664961013
[2m[36m(func pid=94112)[0m f1_weighted: 0.19001820967210747
[2m[36m(func pid=94112)[0m f1_per_class: [0.253, 0.047, 0.541, 0.252, 0.09, 0.153, 0.271, 0.0, 0.08, 0.148]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.08348880597014925
[2m[36m(func pid=94182)[0m top5: 0.7392723880597015
[2m[36m(func pid=94182)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=94182)[0m f1_macro: 0.07954966337267583
[2m[36m(func pid=94182)[0m f1_weighted: 0.03730741588582584
[2m[36m(func pid=94182)[0m f1_per_class: [0.291, 0.0, 0.0, 0.0, 0.186, 0.204, 0.0, 0.114, 0.0, 0.0]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3994 | Steps: 2 | Val loss: 2.3148 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1965 | Steps: 2 | Val loss: 1.9719 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.9603 | Steps: 2 | Val loss: 1.9916 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 24.7782 | Steps: 2 | Val loss: 22.8584 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:22:32 (running for 00:39:24.95)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.399 |      0.109 |                   52 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.304 |      0.24  |                   22 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  2.338 |      0.183 |                    3 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  | 15.868 |      0.08  |                    3 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.12826492537313433
[2m[36m(func pid=83527)[0m top5: 0.5447761194029851
[2m[36m(func pid=83527)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=83527)[0m f1_macro: 0.10896939341468845
[2m[36m(func pid=83527)[0m f1_weighted: 0.1412938774818242
[2m[36m(func pid=83527)[0m f1_per_class: [0.124, 0.18, 0.131, 0.183, 0.0, 0.081, 0.123, 0.098, 0.104, 0.064]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2681902985074627
[2m[36m(func pid=89821)[0m top5: 0.8390858208955224
[2m[36m(func pid=89821)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=89821)[0m f1_macro: 0.2544910920006426
[2m[36m(func pid=89821)[0m f1_weighted: 0.279440527653505
[2m[36m(func pid=89821)[0m f1_per_class: [0.306, 0.332, 0.478, 0.412, 0.051, 0.136, 0.205, 0.217, 0.175, 0.233]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.261660447761194
[2m[36m(func pid=94112)[0m top5: 0.8353544776119403
[2m[36m(func pid=94112)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.24242239132796572
[2m[36m(func pid=94112)[0m f1_weighted: 0.24862653136360519
[2m[36m(func pid=94112)[0m f1_per_class: [0.394, 0.215, 0.478, 0.486, 0.104, 0.213, 0.089, 0.111, 0.138, 0.195]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.1958955223880597
[2m[36m(func pid=94182)[0m top5: 0.746268656716418
[2m[36m(func pid=94182)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=94182)[0m f1_macro: 0.08505943249374795
[2m[36m(func pid=94182)[0m f1_weighted: 0.13892818483546887
[2m[36m(func pid=94182)[0m f1_per_class: [0.118, 0.0, 0.095, 0.383, 0.0, 0.255, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3225 | Steps: 2 | Val loss: 2.3133 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2024 | Steps: 2 | Val loss: 1.9462 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.4597 | Steps: 2 | Val loss: 2.1744 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 18.2752 | Steps: 2 | Val loss: 20.8524 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:22:37 (running for 00:39:30.22)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.399 |      0.109 |                   52 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.202 |      0.266 |                   24 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  1.96  |      0.242 |                    4 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  | 24.778 |      0.085 |                    4 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.28171641791044777
[2m[36m(func pid=89821)[0m top5: 0.847481343283582
[2m[36m(func pid=89821)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=89821)[0m f1_macro: 0.2655137749680493
[2m[36m(func pid=89821)[0m f1_weighted: 0.295175472304801
[2m[36m(func pid=89821)[0m f1_per_class: [0.327, 0.346, 0.478, 0.425, 0.052, 0.142, 0.231, 0.227, 0.18, 0.247]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.12826492537313433
[2m[36m(func pid=83527)[0m top5: 0.5494402985074627
[2m[36m(func pid=83527)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=83527)[0m f1_macro: 0.11007687361909599
[2m[36m(func pid=83527)[0m f1_weighted: 0.14106205399996913
[2m[36m(func pid=83527)[0m f1_per_class: [0.127, 0.177, 0.142, 0.177, 0.0, 0.083, 0.129, 0.097, 0.104, 0.065]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m top1: 0.3031716417910448
[2m[36m(func pid=94182)[0m top5: 0.6828358208955224
[2m[36m(func pid=94182)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=94182)[0m f1_macro: 0.1444576011161952
[2m[36m(func pid=94182)[0m f1_weighted: 0.2243128314106146
[2m[36m(func pid=94182)[0m f1_per_class: [0.243, 0.0, 0.118, 0.46, 0.0, 0.0, 0.29, 0.0, 0.0, 0.333]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.23694029850746268
[2m[36m(func pid=94112)[0m top5: 0.6921641791044776
[2m[36m(func pid=94112)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=94112)[0m f1_macro: 0.27740664973343565
[2m[36m(func pid=94112)[0m f1_weighted: 0.22834407647570415
[2m[36m(func pid=94112)[0m f1_per_class: [0.504, 0.325, 0.647, 0.431, 0.104, 0.159, 0.0, 0.178, 0.179, 0.248]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1197 | Steps: 2 | Val loss: 1.9282 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.3958 | Steps: 2 | Val loss: 2.3136 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 19.0701 | Steps: 2 | Val loss: 26.5321 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9943 | Steps: 2 | Val loss: 2.1539 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:22:43 (running for 00:39:35.51)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.323 |      0.11  |                   53 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.12  |      0.274 |                   25 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  1.46  |      0.277 |                    5 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  | 18.275 |      0.144 |                    5 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.2971082089552239
[2m[36m(func pid=89821)[0m top5: 0.855410447761194
[2m[36m(func pid=89821)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=89821)[0m f1_macro: 0.2739519191644312
[2m[36m(func pid=89821)[0m f1_weighted: 0.31429141590627324
[2m[36m(func pid=89821)[0m f1_per_class: [0.333, 0.348, 0.478, 0.433, 0.051, 0.143, 0.283, 0.238, 0.194, 0.239]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1287313432835821
[2m[36m(func pid=83527)[0m top5: 0.5489738805970149
[2m[36m(func pid=83527)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=83527)[0m f1_macro: 0.1106079881843632
[2m[36m(func pid=83527)[0m f1_weighted: 0.14263412842462836
[2m[36m(func pid=83527)[0m f1_per_class: [0.122, 0.184, 0.139, 0.177, 0.0, 0.077, 0.132, 0.096, 0.114, 0.065]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m top1: 0.06669776119402986
[2m[36m(func pid=94182)[0m top5: 0.6721082089552238
[2m[36m(func pid=94182)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=94182)[0m f1_macro: 0.12787213244804893
[2m[36m(func pid=94182)[0m f1_weighted: 0.057364279481817514
[2m[36m(func pid=94182)[0m f1_per_class: [0.083, 0.0, 0.69, 0.079, 0.0, 0.0, 0.058, 0.138, 0.068, 0.163]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.269589552238806
[2m[36m(func pid=94112)[0m top5: 0.7145522388059702
[2m[36m(func pid=94112)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=94112)[0m f1_macro: 0.29759947179024426
[2m[36m(func pid=94112)[0m f1_weighted: 0.25312350900141584
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.32, 0.71, 0.5, 0.068, 0.141, 0.003, 0.305, 0.199, 0.286]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1225 | Steps: 2 | Val loss: 1.9250 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.3295 | Steps: 2 | Val loss: 2.3110 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 17.0270 | Steps: 2 | Val loss: 14.5018 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7390 | Steps: 2 | Val loss: 1.9270 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:22:48 (running for 00:39:40.81)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.396 |      0.111 |                   54 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.123 |      0.269 |                   26 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.994 |      0.298 |                    6 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  | 19.07  |      0.128 |                    6 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.29244402985074625
[2m[36m(func pid=89821)[0m top5: 0.8530783582089553
[2m[36m(func pid=89821)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=89821)[0m f1_macro: 0.26927933855477937
[2m[36m(func pid=89821)[0m f1_weighted: 0.3136516204330247
[2m[36m(func pid=89821)[0m f1_per_class: [0.329, 0.354, 0.478, 0.398, 0.045, 0.132, 0.318, 0.219, 0.202, 0.218]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.13013059701492538
[2m[36m(func pid=83527)[0m top5: 0.554570895522388
[2m[36m(func pid=83527)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=83527)[0m f1_macro: 0.11179990622845444
[2m[36m(func pid=83527)[0m f1_weighted: 0.14274966423990199
[2m[36m(func pid=83527)[0m f1_per_class: [0.13, 0.19, 0.136, 0.173, 0.0, 0.078, 0.131, 0.1, 0.112, 0.068]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m top1: 0.14458955223880596
[2m[36m(func pid=94182)[0m top5: 0.6781716417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=94182)[0m f1_macro: 0.18541539278730274
[2m[36m(func pid=94182)[0m f1_weighted: 0.10729505764865621
[2m[36m(func pid=94182)[0m f1_per_class: [0.125, 0.0, 0.688, 0.144, 0.0, 0.15, 0.073, 0.211, 0.181, 0.283]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.2966417910447761
[2m[36m(func pid=94112)[0m top5: 0.8642723880597015
[2m[36m(func pid=94112)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=94112)[0m f1_macro: 0.3015034399787071
[2m[36m(func pid=94112)[0m f1_weighted: 0.3155987193380467
[2m[36m(func pid=94112)[0m f1_per_class: [0.382, 0.271, 0.667, 0.543, 0.053, 0.142, 0.213, 0.278, 0.18, 0.286]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1077 | Steps: 2 | Val loss: 1.9166 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3417 | Steps: 2 | Val loss: 2.3100 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 4.6136 | Steps: 2 | Val loss: 9.1288 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4587 | Steps: 2 | Val loss: 1.7153 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=89821)[0m top1: 0.30130597014925375
[2m[36m(func pid=89821)[0m top5: 0.8549440298507462
[2m[36m(func pid=89821)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=89821)[0m f1_macro: 0.2788182851033819
[2m[36m(func pid=89821)[0m f1_weighted: 0.3248706568149839
[2m[36m(func pid=89821)[0m f1_per_class: [0.335, 0.352, 0.512, 0.419, 0.054, 0.138, 0.334, 0.211, 0.203, 0.23]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:22:53 (running for 00:39:46.22)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.33  |      0.112 |                   55 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.108 |      0.279 |                   27 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.739 |      0.302 |                    7 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  | 17.027 |      0.185 |                    7 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.12919776119402984
[2m[36m(func pid=83527)[0m top5: 0.5541044776119403
[2m[36m(func pid=83527)[0m f1_micro: 0.12919776119402984
[2m[36m(func pid=83527)[0m f1_macro: 0.11111348661891282
[2m[36m(func pid=83527)[0m f1_weighted: 0.14032552426848247
[2m[36m(func pid=83527)[0m f1_per_class: [0.132, 0.193, 0.137, 0.169, 0.0, 0.078, 0.125, 0.102, 0.106, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m top1: 0.2994402985074627
[2m[36m(func pid=94182)[0m top5: 0.8250932835820896
[2m[36m(func pid=94182)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=94182)[0m f1_macro: 0.29116759676591586
[2m[36m(func pid=94182)[0m f1_weighted: 0.24588053941112373
[2m[36m(func pid=94182)[0m f1_per_class: [0.326, 0.441, 0.815, 0.023, 0.074, 0.292, 0.315, 0.362, 0.0, 0.263]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3843283582089552
[2m[36m(func pid=94112)[0m top5: 0.9155783582089553
[2m[36m(func pid=94112)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=94112)[0m f1_macro: 0.3238921384181378
[2m[36m(func pid=94112)[0m f1_weighted: 0.40437500022423445
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.304, 0.579, 0.56, 0.096, 0.224, 0.485, 0.045, 0.186, 0.318]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3214 | Steps: 2 | Val loss: 2.3069 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.3535 | Steps: 2 | Val loss: 16.0614 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0541 | Steps: 2 | Val loss: 1.9119 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3699 | Steps: 2 | Val loss: 1.6459 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 01:22:59 (running for 00:39:51.48)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.342 |      0.111 |                   56 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.108 |      0.279 |                   27 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.459 |      0.324 |                    8 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  1.354 |      0.236 |                    9 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.2887126865671642
[2m[36m(func pid=94182)[0m top5: 0.8642723880597015
[2m[36m(func pid=94182)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=94182)[0m f1_macro: 0.23612737366737827
[2m[36m(func pid=94182)[0m f1_weighted: 0.23011617341812352
[2m[36m(func pid=94182)[0m f1_per_class: [0.264, 0.397, 0.762, 0.0, 0.076, 0.182, 0.379, 0.301, 0.0, 0.0]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2947761194029851
[2m[36m(func pid=89821)[0m top5: 0.8558768656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=89821)[0m f1_macro: 0.27844593143678475
[2m[36m(func pid=89821)[0m f1_weighted: 0.31908671532069927
[2m[36m(func pid=89821)[0m f1_per_class: [0.348, 0.337, 0.524, 0.406, 0.053, 0.135, 0.334, 0.226, 0.19, 0.231]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.13013059701492538
[2m[36m(func pid=83527)[0m top5: 0.5601679104477612
[2m[36m(func pid=83527)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=83527)[0m f1_macro: 0.1117601758656485
[2m[36m(func pid=83527)[0m f1_weighted: 0.14162460589948406
[2m[36m(func pid=83527)[0m f1_per_class: [0.124, 0.196, 0.144, 0.171, 0.0, 0.077, 0.127, 0.102, 0.108, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94112)[0m top1: 0.40578358208955223
[2m[36m(func pid=94112)[0m top5: 0.9263059701492538
[2m[36m(func pid=94112)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=94112)[0m f1_macro: 0.3415657953440572
[2m[36m(func pid=94112)[0m f1_weighted: 0.42134556560014286
[2m[36m(func pid=94112)[0m f1_per_class: [0.524, 0.35, 0.55, 0.558, 0.128, 0.285, 0.483, 0.083, 0.172, 0.282]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 8.0346 | Steps: 2 | Val loss: 18.7574 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2955 | Steps: 2 | Val loss: 2.3066 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0193 | Steps: 2 | Val loss: 1.9190 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.2551 | Steps: 2 | Val loss: 1.7000 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:23:04 (running for 00:39:56.65)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.321 |      0.112 |                   57 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.054 |      0.278 |                   28 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.37  |      0.342 |                    9 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  8.035 |      0.201 |                   10 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.15811567164179105
[2m[36m(func pid=94182)[0m top5: 0.9263059701492538
[2m[36m(func pid=94182)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=94182)[0m f1_macro: 0.20085640262102009
[2m[36m(func pid=94182)[0m f1_weighted: 0.20917804374358884
[2m[36m(func pid=94182)[0m f1_per_class: [0.5, 0.34, 0.471, 0.135, 0.022, 0.034, 0.291, 0.142, 0.0, 0.074]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1296641791044776
[2m[36m(func pid=83527)[0m top5: 0.5597014925373134
[2m[36m(func pid=83527)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=83527)[0m f1_macro: 0.11194973839410324
[2m[36m(func pid=83527)[0m f1_weighted: 0.1404614764298269
[2m[36m(func pid=83527)[0m f1_per_class: [0.129, 0.194, 0.142, 0.172, 0.0, 0.082, 0.12, 0.103, 0.106, 0.07]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2905783582089552
[2m[36m(func pid=89821)[0m top5: 0.8526119402985075
[2m[36m(func pid=89821)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=89821)[0m f1_macro: 0.27937818955525096
[2m[36m(func pid=89821)[0m f1_weighted: 0.3135981501879268
[2m[36m(func pid=89821)[0m f1_per_class: [0.37, 0.342, 0.537, 0.406, 0.054, 0.143, 0.308, 0.236, 0.183, 0.217]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.36847014925373134
[2m[36m(func pid=94112)[0m top5: 0.9155783582089553
[2m[36m(func pid=94112)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=94112)[0m f1_macro: 0.3480989578290989
[2m[36m(func pid=94112)[0m f1_weighted: 0.3794776710160025
[2m[36m(func pid=94112)[0m f1_per_class: [0.532, 0.373, 0.595, 0.569, 0.133, 0.288, 0.282, 0.254, 0.195, 0.261]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.2947 | Steps: 2 | Val loss: 15.1064 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2995 | Steps: 2 | Val loss: 2.3013 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9205 | Steps: 2 | Val loss: 1.9408 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.1970 | Steps: 2 | Val loss: 1.8370 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:23:09 (running for 00:40:01.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.296 |      0.112 |                   58 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  1.019 |      0.279 |                   29 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.255 |      0.348 |                   10 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  4.295 |      0.195 |                   11 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.27705223880597013
[2m[36m(func pid=94182)[0m top5: 0.832089552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=94182)[0m f1_macro: 0.1953765354948332
[2m[36m(func pid=94182)[0m f1_weighted: 0.2874754048275118
[2m[36m(func pid=94182)[0m f1_per_class: [0.34, 0.037, 0.353, 0.566, 0.034, 0.083, 0.318, 0.155, 0.0, 0.069]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1310634328358209
[2m[36m(func pid=83527)[0m top5: 0.5657649253731343
[2m[36m(func pid=83527)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=83527)[0m f1_macro: 0.11288151410259148
[2m[36m(func pid=83527)[0m f1_weighted: 0.14194628959083208
[2m[36m(func pid=83527)[0m f1_per_class: [0.126, 0.198, 0.147, 0.18, 0.0, 0.082, 0.115, 0.104, 0.106, 0.07]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.27472014925373134
[2m[36m(func pid=89821)[0m top5: 0.8418843283582089
[2m[36m(func pid=89821)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=89821)[0m f1_macro: 0.27403274474712225
[2m[36m(func pid=89821)[0m f1_weighted: 0.29435492847426137
[2m[36m(func pid=89821)[0m f1_per_class: [0.354, 0.336, 0.55, 0.378, 0.055, 0.143, 0.271, 0.249, 0.179, 0.226]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3269589552238806
[2m[36m(func pid=94112)[0m top5: 0.8875932835820896
[2m[36m(func pid=94112)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=94112)[0m f1_macro: 0.33232682331587615
[2m[36m(func pid=94112)[0m f1_weighted: 0.31981811307523605
[2m[36m(func pid=94112)[0m f1_per_class: [0.545, 0.383, 0.579, 0.533, 0.14, 0.282, 0.108, 0.255, 0.204, 0.293]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0497 | Steps: 2 | Val loss: 15.7917 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2830 | Steps: 2 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8936 | Steps: 2 | Val loss: 1.9456 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=94182)[0m top1: 0.33722014925373134
[2m[36m(func pid=94182)[0m top5: 0.7957089552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=94182)[0m f1_macro: 0.2514903015728137
[2m[36m(func pid=94182)[0m f1_weighted: 0.2834155381698504
[2m[36m(func pid=94182)[0m f1_per_class: [0.243, 0.005, 0.471, 0.55, 0.113, 0.237, 0.251, 0.181, 0.131, 0.333]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.1540 | Steps: 2 | Val loss: 1.9187 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:23:15 (running for 00:40:07.85)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.283 |      0.114 |                   60 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.921 |      0.274 |                   30 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.197 |      0.332 |                   11 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  1.05  |      0.251 |                   12 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.1310634328358209
[2m[36m(func pid=83527)[0m top5: 0.5676305970149254
[2m[36m(func pid=83527)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=83527)[0m f1_macro: 0.11426450302373362
[2m[36m(func pid=83527)[0m f1_weighted: 0.14024240969500584
[2m[36m(func pid=83527)[0m f1_per_class: [0.135, 0.196, 0.15, 0.174, 0.0, 0.081, 0.114, 0.119, 0.103, 0.071]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2681902985074627
[2m[36m(func pid=89821)[0m top5: 0.8390858208955224
[2m[36m(func pid=89821)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=89821)[0m f1_macro: 0.26776973855701636
[2m[36m(func pid=89821)[0m f1_weighted: 0.2844021317902031
[2m[36m(func pid=89821)[0m f1_per_class: [0.346, 0.342, 0.537, 0.379, 0.06, 0.137, 0.239, 0.235, 0.176, 0.227]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.32509328358208955
[2m[36m(func pid=94112)[0m top5: 0.8684701492537313
[2m[36m(func pid=94112)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=94112)[0m f1_macro: 0.33426076112336783
[2m[36m(func pid=94112)[0m f1_weighted: 0.3129949109244856
[2m[36m(func pid=94112)[0m f1_per_class: [0.549, 0.424, 0.595, 0.53, 0.117, 0.258, 0.071, 0.26, 0.202, 0.336]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.5422 | Steps: 2 | Val loss: 18.6359 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.2488 | Steps: 2 | Val loss: 2.2955 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9802 | Steps: 2 | Val loss: 1.9506 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=94182)[0m top1: 0.34701492537313433
[2m[36m(func pid=94182)[0m top5: 0.7966417910447762
[2m[36m(func pid=94182)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=94182)[0m f1_macro: 0.26576152742203374
[2m[36m(func pid=94182)[0m f1_weighted: 0.2763510869802623
[2m[36m(func pid=94182)[0m f1_per_class: [0.302, 0.0, 0.143, 0.53, 0.3, 0.343, 0.18, 0.263, 0.197, 0.4]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.1346 | Steps: 2 | Val loss: 1.9235 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:23:20 (running for 00:40:13.04)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.249 |      0.117 |                   61 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.894 |      0.268 |                   31 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.154 |      0.334 |                   12 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.542 |      0.266 |                   13 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.134794776119403
[2m[36m(func pid=83527)[0m top5: 0.574160447761194
[2m[36m(func pid=83527)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=83527)[0m f1_macro: 0.11737896515450807
[2m[36m(func pid=83527)[0m f1_weighted: 0.14471422266715683
[2m[36m(func pid=83527)[0m f1_per_class: [0.134, 0.199, 0.16, 0.183, 0.0, 0.088, 0.116, 0.118, 0.104, 0.072]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.27425373134328357
[2m[36m(func pid=89821)[0m top5: 0.8325559701492538
[2m[36m(func pid=89821)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=89821)[0m f1_macro: 0.2749124033574755
[2m[36m(func pid=89821)[0m f1_weighted: 0.2877811364878484
[2m[36m(func pid=89821)[0m f1_per_class: [0.343, 0.331, 0.579, 0.4, 0.062, 0.139, 0.23, 0.257, 0.199, 0.21]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.32742537313432835
[2m[36m(func pid=94112)[0m top5: 0.8656716417910447
[2m[36m(func pid=94112)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=94112)[0m f1_macro: 0.33552512412048846
[2m[36m(func pid=94112)[0m f1_weighted: 0.31180956788986225
[2m[36m(func pid=94112)[0m f1_per_class: [0.532, 0.438, 0.595, 0.516, 0.097, 0.245, 0.074, 0.275, 0.212, 0.372]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.4699 | Steps: 2 | Val loss: 17.6958 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2887 | Steps: 2 | Val loss: 2.2889 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8159 | Steps: 2 | Val loss: 1.9390 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=94182)[0m top1: 0.3376865671641791
[2m[36m(func pid=94182)[0m top5: 0.7971082089552238
[2m[36m(func pid=94182)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=94182)[0m f1_macro: 0.2598517883148377
[2m[36m(func pid=94182)[0m f1_weighted: 0.30573324304864835
[2m[36m(func pid=94182)[0m f1_per_class: [0.286, 0.005, 0.143, 0.561, 0.194, 0.356, 0.244, 0.306, 0.16, 0.345]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.1963 | Steps: 2 | Val loss: 1.8597 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:23:26 (running for 00:40:18.37)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.249 |      0.117 |                   61 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.816 |      0.28  |                   33 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.135 |      0.336 |                   13 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  1.47  |      0.26  |                   14 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.13805970149253732
[2m[36m(func pid=83527)[0m top5: 0.5830223880597015
[2m[36m(func pid=83527)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=83527)[0m f1_macro: 0.1216701089795557
[2m[36m(func pid=83527)[0m f1_weighted: 0.14863128114892074
[2m[36m(func pid=83527)[0m f1_per_class: [0.143, 0.205, 0.182, 0.182, 0.0, 0.089, 0.126, 0.115, 0.105, 0.07]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.28171641791044777
[2m[36m(func pid=89821)[0m top5: 0.8381529850746269
[2m[36m(func pid=89821)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=89821)[0m f1_macro: 0.2795906651139043
[2m[36m(func pid=89821)[0m f1_weighted: 0.2942680111630908
[2m[36m(func pid=89821)[0m f1_per_class: [0.357, 0.334, 0.595, 0.43, 0.068, 0.141, 0.22, 0.257, 0.202, 0.192]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8070 | Steps: 2 | Val loss: 17.4447 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=94112)[0m top1: 0.34794776119402987
[2m[36m(func pid=94112)[0m top5: 0.8791977611940298
[2m[36m(func pid=94112)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=94112)[0m f1_macro: 0.3574468765259305
[2m[36m(func pid=94112)[0m f1_weighted: 0.34506657906420063
[2m[36m(func pid=94112)[0m f1_per_class: [0.461, 0.468, 0.667, 0.513, 0.084, 0.25, 0.167, 0.294, 0.196, 0.476]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8114 | Steps: 2 | Val loss: 1.9323 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2688 | Steps: 2 | Val loss: 2.2889 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=94182)[0m top1: 0.302705223880597
[2m[36m(func pid=94182)[0m top5: 0.8031716417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=94182)[0m f1_macro: 0.24325520179695861
[2m[36m(func pid=94182)[0m f1_weighted: 0.30477672349649615
[2m[36m(func pid=94182)[0m f1_per_class: [0.222, 0.011, 0.143, 0.515, 0.154, 0.342, 0.285, 0.369, 0.132, 0.26]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.1199 | Steps: 2 | Val loss: 1.8508 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 01:23:31 (running for 00:40:23.62)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.289 |      0.122 |                   62 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.811 |      0.284 |                   34 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.196 |      0.357 |                   14 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.807 |      0.243 |                   15 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.28824626865671643
[2m[36m(func pid=89821)[0m top5: 0.8423507462686567
[2m[36m(func pid=89821)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=89821)[0m f1_macro: 0.28354213151953833
[2m[36m(func pid=89821)[0m f1_weighted: 0.29791322870477926
[2m[36m(func pid=89821)[0m f1_per_class: [0.372, 0.331, 0.595, 0.46, 0.074, 0.162, 0.197, 0.252, 0.201, 0.191]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.13852611940298507
[2m[36m(func pid=83527)[0m top5: 0.5830223880597015
[2m[36m(func pid=83527)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=83527)[0m f1_macro: 0.12269116573786783
[2m[36m(func pid=83527)[0m f1_weighted: 0.14986940558975595
[2m[36m(func pid=83527)[0m f1_per_class: [0.136, 0.206, 0.185, 0.182, 0.011, 0.089, 0.129, 0.117, 0.102, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6263 | Steps: 2 | Val loss: 14.1281 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=94112)[0m top1: 0.355410447761194
[2m[36m(func pid=94112)[0m top5: 0.8824626865671642
[2m[36m(func pid=94112)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.3649945588728509
[2m[36m(func pid=94112)[0m f1_weighted: 0.37266247258301416
[2m[36m(func pid=94112)[0m f1_per_class: [0.4, 0.458, 0.759, 0.485, 0.085, 0.234, 0.304, 0.28, 0.192, 0.453]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2516 | Steps: 2 | Val loss: 2.2855 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7938 | Steps: 2 | Val loss: 1.9226 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=94182)[0m top1: 0.33348880597014924
[2m[36m(func pid=94182)[0m top5: 0.8418843283582089
[2m[36m(func pid=94182)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=94182)[0m f1_macro: 0.2934234951089105
[2m[36m(func pid=94182)[0m f1_weighted: 0.3467185241331157
[2m[36m(func pid=94182)[0m f1_per_class: [0.27, 0.066, 0.444, 0.53, 0.2, 0.284, 0.392, 0.365, 0.147, 0.237]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.0625 | Steps: 2 | Val loss: 1.8512 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:23:36 (running for 00:40:28.79)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.252 |      0.123 |                   64 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.811 |      0.284 |                   34 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.12  |      0.365 |                   15 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.626 |      0.293 |                   16 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.13852611940298507
[2m[36m(func pid=83527)[0m top5: 0.585820895522388
[2m[36m(func pid=83527)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=83527)[0m f1_macro: 0.12285822766697929
[2m[36m(func pid=83527)[0m f1_weighted: 0.14987421971948064
[2m[36m(func pid=83527)[0m f1_per_class: [0.143, 0.208, 0.19, 0.188, 0.011, 0.081, 0.126, 0.108, 0.104, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2905783582089552
[2m[36m(func pid=89821)[0m top5: 0.8442164179104478
[2m[36m(func pid=89821)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=89821)[0m f1_macro: 0.278758474065829
[2m[36m(func pid=89821)[0m f1_weighted: 0.30130437659656084
[2m[36m(func pid=89821)[0m f1_per_class: [0.372, 0.329, 0.55, 0.465, 0.074, 0.165, 0.206, 0.252, 0.201, 0.174]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0445 | Steps: 2 | Val loss: 11.6977 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=94112)[0m top1: 0.3619402985074627
[2m[36m(func pid=94112)[0m top5: 0.8908582089552238
[2m[36m(func pid=94112)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=94112)[0m f1_macro: 0.3611386841326484
[2m[36m(func pid=94112)[0m f1_weighted: 0.3923868335014232
[2m[36m(func pid=94112)[0m f1_per_class: [0.336, 0.434, 0.786, 0.476, 0.071, 0.222, 0.409, 0.25, 0.175, 0.453]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2289 | Steps: 2 | Val loss: 2.2832 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7602 | Steps: 2 | Val loss: 1.9101 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=94182)[0m top1: 0.37080223880597013
[2m[36m(func pid=94182)[0m top5: 0.8796641791044776
[2m[36m(func pid=94182)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=94182)[0m f1_macro: 0.3388316739710212
[2m[36m(func pid=94182)[0m f1_weighted: 0.3779525796128293
[2m[36m(func pid=94182)[0m f1_per_class: [0.369, 0.238, 0.72, 0.547, 0.179, 0.158, 0.413, 0.37, 0.187, 0.208]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.1386 | Steps: 2 | Val loss: 1.8928 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:23:41 (running for 00:40:34.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.229 |      0.124 |                   65 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.794 |      0.279 |                   35 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.062 |      0.361 |                   16 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.044 |      0.339 |                   17 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.1394589552238806
[2m[36m(func pid=83527)[0m top5: 0.5872201492537313
[2m[36m(func pid=83527)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=83527)[0m f1_macro: 0.12420392759246834
[2m[36m(func pid=83527)[0m f1_weighted: 0.1492285501362093
[2m[36m(func pid=83527)[0m f1_per_class: [0.145, 0.207, 0.19, 0.188, 0.011, 0.085, 0.121, 0.118, 0.108, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2933768656716418
[2m[36m(func pid=89821)[0m top5: 0.847481343283582
[2m[36m(func pid=89821)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=89821)[0m f1_macro: 0.27850854346869663
[2m[36m(func pid=89821)[0m f1_weighted: 0.30858902865491844
[2m[36m(func pid=89821)[0m f1_per_class: [0.372, 0.326, 0.55, 0.474, 0.066, 0.154, 0.228, 0.254, 0.19, 0.17]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.0572 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=94112)[0m top1: 0.35774253731343286
[2m[36m(func pid=94112)[0m top5: 0.8833955223880597
[2m[36m(func pid=94112)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=94112)[0m f1_macro: 0.3527988380950039
[2m[36m(func pid=94112)[0m f1_weighted: 0.39100009946626685
[2m[36m(func pid=94112)[0m f1_per_class: [0.305, 0.419, 0.786, 0.467, 0.069, 0.223, 0.426, 0.241, 0.186, 0.407]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2357 | Steps: 2 | Val loss: 2.2788 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6959 | Steps: 2 | Val loss: 1.8977 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=94182)[0m top1: 0.3805970149253731
[2m[36m(func pid=94182)[0m top5: 0.8819962686567164
[2m[36m(func pid=94182)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=94182)[0m f1_macro: 0.3487303827152941
[2m[36m(func pid=94182)[0m f1_weighted: 0.3840178371826682
[2m[36m(func pid=94182)[0m f1_per_class: [0.416, 0.342, 0.815, 0.55, 0.161, 0.063, 0.406, 0.343, 0.206, 0.185]
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:23:47 (running for 00:40:39.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.236 |      0.125 |                   66 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.76  |      0.279 |                   36 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.139 |      0.353 |                   17 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.349 |                   18 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.14085820895522388
[2m[36m(func pid=83527)[0m top5: 0.59375
[2m[36m(func pid=83527)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=83527)[0m f1_macro: 0.12475002533029773
[2m[36m(func pid=83527)[0m f1_weighted: 0.15098929872284875
[2m[36m(func pid=83527)[0m f1_per_class: [0.146, 0.206, 0.183, 0.192, 0.012, 0.085, 0.123, 0.121, 0.111, 0.069]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.2971082089552239
[2m[36m(func pid=89821)[0m top5: 0.8493470149253731
[2m[36m(func pid=89821)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=89821)[0m f1_macro: 0.28092625073945776
[2m[36m(func pid=89821)[0m f1_weighted: 0.3150747900384523
[2m[36m(func pid=89821)[0m f1_per_class: [0.378, 0.322, 0.537, 0.48, 0.071, 0.149, 0.25, 0.247, 0.191, 0.184]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.1142 | Steps: 2 | Val loss: 1.9664 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0001 | Steps: 2 | Val loss: 11.1372 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m top1: 0.3362873134328358
[2m[36m(func pid=94112)[0m top5: 0.8675373134328358
[2m[36m(func pid=94112)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=94112)[0m f1_macro: 0.34174580315884784
[2m[36m(func pid=94112)[0m f1_weighted: 0.3657219177747426
[2m[36m(func pid=94112)[0m f1_per_class: [0.312, 0.404, 0.786, 0.468, 0.074, 0.228, 0.345, 0.248, 0.195, 0.358]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1854 | Steps: 2 | Val loss: 2.2713 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6928 | Steps: 2 | Val loss: 1.8855 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=94182)[0m top1: 0.37966417910447764
[2m[36m(func pid=94182)[0m top5: 0.871268656716418
[2m[36m(func pid=94182)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=94182)[0m f1_macro: 0.3534116791995118
[2m[36m(func pid=94182)[0m f1_weighted: 0.37775588461423065
[2m[36m(func pid=94182)[0m f1_per_class: [0.479, 0.387, 0.786, 0.534, 0.185, 0.023, 0.384, 0.33, 0.233, 0.193]
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:23:52 (running for 00:40:44.79)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.236 |      0.125 |                   66 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.693 |      0.285 |                   38 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.114 |      0.342 |                   18 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.353 |                   19 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.300839552238806
[2m[36m(func pid=89821)[0m top5: 0.8530783582089553
[2m[36m(func pid=89821)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=89821)[0m f1_macro: 0.28533117206884695
[2m[36m(func pid=89821)[0m f1_weighted: 0.3200045051801099
[2m[36m(func pid=89821)[0m f1_per_class: [0.378, 0.323, 0.55, 0.481, 0.064, 0.15, 0.262, 0.256, 0.198, 0.192]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=83527)[0m top1: 0.1455223880597015
[2m[36m(func pid=83527)[0m top5: 0.6040111940298507
[2m[36m(func pid=83527)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=83527)[0m f1_macro: 0.1277077426951806
[2m[36m(func pid=83527)[0m f1_weighted: 0.15646963539611003
[2m[36m(func pid=83527)[0m f1_per_class: [0.147, 0.208, 0.183, 0.196, 0.012, 0.093, 0.132, 0.125, 0.106, 0.074]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.0455 | Steps: 2 | Val loss: 2.0116 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0920 | Steps: 2 | Val loss: 10.8553 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=94112)[0m top1: 0.32882462686567165
[2m[36m(func pid=94112)[0m top5: 0.8596082089552238
[2m[36m(func pid=94112)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=94112)[0m f1_macro: 0.34401302257736643
[2m[36m(func pid=94112)[0m f1_weighted: 0.35228778095655827
[2m[36m(func pid=94112)[0m f1_per_class: [0.351, 0.399, 0.786, 0.47, 0.074, 0.216, 0.297, 0.273, 0.209, 0.366]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.3885261194029851
[2m[36m(func pid=94182)[0m top5: 0.8759328358208955
[2m[36m(func pid=94182)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=94182)[0m f1_macro: 0.35178402701652123
[2m[36m(func pid=94182)[0m f1_weighted: 0.38557060362041173
[2m[36m(func pid=94182)[0m f1_per_class: [0.554, 0.415, 0.647, 0.513, 0.145, 0.031, 0.403, 0.347, 0.274, 0.191]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2019 | Steps: 2 | Val loss: 2.2727 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7208 | Steps: 2 | Val loss: 1.8690 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:23:57 (running for 00:40:50.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.202 |      0.125 |                   68 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.693 |      0.285 |                   38 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.045 |      0.344 |                   19 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.092 |      0.352 |                   20 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.14225746268656717
[2m[36m(func pid=83527)[0m top5: 0.601679104477612
[2m[36m(func pid=83527)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=83527)[0m f1_macro: 0.1251443657254632
[2m[36m(func pid=83527)[0m f1_weighted: 0.15255220598222924
[2m[36m(func pid=83527)[0m f1_per_class: [0.143, 0.202, 0.175, 0.191, 0.011, 0.093, 0.127, 0.129, 0.103, 0.076]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.30736940298507465
[2m[36m(func pid=89821)[0m top5: 0.8582089552238806
[2m[36m(func pid=89821)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=89821)[0m f1_macro: 0.2926095150595783
[2m[36m(func pid=89821)[0m f1_weighted: 0.3269516447272424
[2m[36m(func pid=89821)[0m f1_per_class: [0.372, 0.327, 0.579, 0.491, 0.061, 0.147, 0.272, 0.26, 0.194, 0.222]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.0485 | Steps: 2 | Val loss: 2.0909 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0002 | Steps: 2 | Val loss: 11.0701 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=94182)[0m top1: 0.3829291044776119
[2m[36m(func pid=94182)[0m top5: 0.8717350746268657
[2m[36m(func pid=94182)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=94182)[0m f1_macro: 0.3396249785714022
[2m[36m(func pid=94182)[0m f1_weighted: 0.37963854573168204
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.428, 0.585, 0.483, 0.141, 0.031, 0.406, 0.359, 0.259, 0.172]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2070 | Steps: 2 | Val loss: 2.2708 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=94112)[0m top1: 0.30923507462686567
[2m[36m(func pid=94112)[0m top5: 0.8563432835820896
[2m[36m(func pid=94112)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=94112)[0m f1_macro: 0.33064592115092645
[2m[36m(func pid=94112)[0m f1_weighted: 0.33153805992428387
[2m[36m(func pid=94112)[0m f1_per_class: [0.371, 0.385, 0.759, 0.467, 0.083, 0.191, 0.255, 0.235, 0.202, 0.359]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6391 | Steps: 2 | Val loss: 1.8615 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:24:03 (running for 00:40:55.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.207 |      0.127 |                   69 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.721 |      0.293 |                   39 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.049 |      0.331 |                   20 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.34  |                   21 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.14458955223880596
[2m[36m(func pid=83527)[0m top5: 0.6012126865671642
[2m[36m(func pid=83527)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=83527)[0m f1_macro: 0.12678025331503998
[2m[36m(func pid=83527)[0m f1_weighted: 0.15483189122360844
[2m[36m(func pid=83527)[0m f1_per_class: [0.148, 0.206, 0.179, 0.197, 0.011, 0.096, 0.126, 0.129, 0.098, 0.078]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.30783582089552236
[2m[36m(func pid=89821)[0m top5: 0.8596082089552238
[2m[36m(func pid=89821)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=89821)[0m f1_macro: 0.2948863205058443
[2m[36m(func pid=89821)[0m f1_weighted: 0.3290319368701482
[2m[36m(func pid=89821)[0m f1_per_class: [0.383, 0.337, 0.579, 0.481, 0.059, 0.141, 0.285, 0.257, 0.195, 0.233]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0018 | Steps: 2 | Val loss: 11.2186 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.0430 | Steps: 2 | Val loss: 2.1052 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=94182)[0m top1: 0.3843283582089552
[2m[36m(func pid=94182)[0m top5: 0.8740671641791045
[2m[36m(func pid=94182)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=94182)[0m f1_macro: 0.33764099993711916
[2m[36m(func pid=94182)[0m f1_weighted: 0.37783848496205097
[2m[36m(func pid=94182)[0m f1_per_class: [0.542, 0.447, 0.571, 0.458, 0.115, 0.032, 0.411, 0.36, 0.257, 0.182]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2105 | Steps: 2 | Val loss: 2.2704 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6989 | Steps: 2 | Val loss: 1.8510 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=94112)[0m top1: 0.3087686567164179
[2m[36m(func pid=94112)[0m top5: 0.8572761194029851
[2m[36m(func pid=94112)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=94112)[0m f1_macro: 0.3290479615120204
[2m[36m(func pid=94112)[0m f1_weighted: 0.3307869692118592
[2m[36m(func pid=94112)[0m f1_per_class: [0.427, 0.379, 0.71, 0.475, 0.09, 0.178, 0.251, 0.233, 0.198, 0.35]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.6403 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:24:08 (running for 00:41:00.97)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.21  |      0.128 |                   70 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.639 |      0.295 |                   40 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.043 |      0.329 |                   21 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.002 |      0.338 |                   22 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.14598880597014927
[2m[36m(func pid=83527)[0m top5: 0.5993470149253731
[2m[36m(func pid=83527)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=83527)[0m f1_macro: 0.1276789042210459
[2m[36m(func pid=83527)[0m f1_weighted: 0.15643780360719758
[2m[36m(func pid=83527)[0m f1_per_class: [0.146, 0.204, 0.175, 0.204, 0.012, 0.096, 0.125, 0.135, 0.101, 0.078]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.31156716417910446
[2m[36m(func pid=89821)[0m top5: 0.863339552238806
[2m[36m(func pid=89821)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=89821)[0m f1_macro: 0.2953234404380015
[2m[36m(func pid=89821)[0m f1_weighted: 0.33455873940916403
[2m[36m(func pid=89821)[0m f1_per_class: [0.376, 0.341, 0.55, 0.479, 0.059, 0.144, 0.302, 0.26, 0.195, 0.248]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.0486 | Steps: 2 | Val loss: 2.0898 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=94182)[0m top1: 0.37546641791044777
[2m[36m(func pid=94182)[0m top5: 0.8680037313432836
[2m[36m(func pid=94182)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=94182)[0m f1_macro: 0.3275311372648653
[2m[36m(func pid=94182)[0m f1_weighted: 0.370195091374312
[2m[36m(func pid=94182)[0m f1_per_class: [0.559, 0.439, 0.533, 0.437, 0.114, 0.032, 0.414, 0.363, 0.217, 0.168]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2025 | Steps: 2 | Val loss: 2.2732 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6127 | Steps: 2 | Val loss: 1.8533 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=94112)[0m top1: 0.3138992537313433
[2m[36m(func pid=94112)[0m top5: 0.8610074626865671
[2m[36m(func pid=94112)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=94112)[0m f1_macro: 0.33582430429495946
[2m[36m(func pid=94112)[0m f1_weighted: 0.33753829217325815
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.382, 0.71, 0.486, 0.102, 0.177, 0.261, 0.223, 0.206, 0.368]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0001 | Steps: 2 | Val loss: 11.9633 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 01:24:13 (running for 00:41:06.22)
Memory usage on this node: 24.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.202 |      0.129 |                   71 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.699 |      0.295 |                   41 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.049 |      0.336 |                   22 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.328 |                   23 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.14832089552238806
[2m[36m(func pid=83527)[0m top5: 0.5979477611940298
[2m[36m(func pid=83527)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=83527)[0m f1_macro: 0.12924121186454834
[2m[36m(func pid=83527)[0m f1_weighted: 0.15867379701419138
[2m[36m(func pid=83527)[0m f1_per_class: [0.147, 0.212, 0.182, 0.216, 0.012, 0.101, 0.116, 0.126, 0.101, 0.079]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.30970149253731344
[2m[36m(func pid=89821)[0m top5: 0.8652052238805971
[2m[36m(func pid=89821)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=89821)[0m f1_macro: 0.2941854820520515
[2m[36m(func pid=89821)[0m f1_weighted: 0.3305562956849242
[2m[36m(func pid=89821)[0m f1_per_class: [0.366, 0.344, 0.537, 0.479, 0.057, 0.147, 0.285, 0.261, 0.195, 0.27]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.0475 | Steps: 2 | Val loss: 2.0352 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=94182)[0m top1: 0.38013059701492535
[2m[36m(func pid=94182)[0m top5: 0.8661380597014925
[2m[36m(func pid=94182)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=94182)[0m f1_macro: 0.32386726181515424
[2m[36m(func pid=94182)[0m f1_weighted: 0.37236798244397057
[2m[36m(func pid=94182)[0m f1_per_class: [0.545, 0.452, 0.49, 0.437, 0.111, 0.032, 0.414, 0.38, 0.205, 0.173]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1146 | Steps: 2 | Val loss: 2.2689 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6727 | Steps: 2 | Val loss: 1.8438 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=94112)[0m top1: 0.324160447761194
[2m[36m(func pid=94112)[0m top5: 0.8652052238805971
[2m[36m(func pid=94112)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.3390088321817932
[2m[36m(func pid=94112)[0m f1_weighted: 0.3503594794436799
[2m[36m(func pid=94112)[0m f1_per_class: [0.459, 0.376, 0.71, 0.498, 0.107, 0.177, 0.296, 0.224, 0.203, 0.34]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0149 | Steps: 2 | Val loss: 12.0857 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 01:24:19 (running for 00:41:11.54)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.115 |      0.131 |                   72 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.613 |      0.294 |                   42 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.048 |      0.339 |                   23 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.324 |                   24 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.1501865671641791
[2m[36m(func pid=83527)[0m top5: 0.6007462686567164
[2m[36m(func pid=83527)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=83527)[0m f1_macro: 0.13147570175290113
[2m[36m(func pid=83527)[0m f1_weighted: 0.16077621759358743
[2m[36m(func pid=83527)[0m f1_per_class: [0.148, 0.213, 0.182, 0.216, 0.012, 0.102, 0.121, 0.128, 0.114, 0.08]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.3180970149253731
[2m[36m(func pid=89821)[0m top5: 0.871268656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=89821)[0m f1_macro: 0.3002614893760957
[2m[36m(func pid=89821)[0m f1_weighted: 0.3372723837925538
[2m[36m(func pid=89821)[0m f1_per_class: [0.365, 0.37, 0.537, 0.482, 0.059, 0.153, 0.287, 0.254, 0.208, 0.288]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.0445 | Steps: 2 | Val loss: 1.9940 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=94182)[0m top1: 0.3805970149253731
[2m[36m(func pid=94182)[0m top5: 0.8652052238805971
[2m[36m(func pid=94182)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=94182)[0m f1_macro: 0.32269420248773134
[2m[36m(func pid=94182)[0m f1_weighted: 0.37377542566247307
[2m[36m(func pid=94182)[0m f1_per_class: [0.55, 0.453, 0.462, 0.427, 0.105, 0.04, 0.424, 0.38, 0.214, 0.172]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2009 | Steps: 2 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5653 | Steps: 2 | Val loss: 1.8251 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=94112)[0m top1: 0.33348880597014924
[2m[36m(func pid=94112)[0m top5: 0.8726679104477612
[2m[36m(func pid=94112)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=94112)[0m f1_macro: 0.34131688230451657
[2m[36m(func pid=94112)[0m f1_weighted: 0.35997712638430474
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.37, 0.71, 0.508, 0.11, 0.176, 0.323, 0.226, 0.206, 0.333]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1118 | Steps: 2 | Val loss: 11.5588 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:24:24 (running for 00:41:16.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.201 |      0.132 |                   73 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.673 |      0.3   |                   43 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.044 |      0.341 |                   24 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.015 |      0.323 |                   25 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.1501865671641791
[2m[36m(func pid=83527)[0m top5: 0.6110074626865671
[2m[36m(func pid=83527)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=83527)[0m f1_macro: 0.13157413089425865
[2m[36m(func pid=83527)[0m f1_weighted: 0.16176153897779094
[2m[36m(func pid=83527)[0m f1_per_class: [0.142, 0.211, 0.208, 0.219, 0.012, 0.094, 0.127, 0.126, 0.099, 0.077]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.32369402985074625
[2m[36m(func pid=89821)[0m top5: 0.8773320895522388
[2m[36m(func pid=89821)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=89821)[0m f1_macro: 0.3041634528562286
[2m[36m(func pid=89821)[0m f1_weighted: 0.341485413084472
[2m[36m(func pid=89821)[0m f1_per_class: [0.361, 0.381, 0.55, 0.477, 0.06, 0.172, 0.293, 0.245, 0.212, 0.291]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.0333 | Steps: 2 | Val loss: 1.9614 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=94182)[0m top1: 0.3829291044776119
[2m[36m(func pid=94182)[0m top5: 0.8731343283582089
[2m[36m(func pid=94182)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=94182)[0m f1_macro: 0.31697109200346163
[2m[36m(func pid=94182)[0m f1_weighted: 0.38030739101488126
[2m[36m(func pid=94182)[0m f1_per_class: [0.545, 0.45, 0.407, 0.45, 0.114, 0.039, 0.43, 0.382, 0.193, 0.16]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0810 | Steps: 2 | Val loss: 2.2560 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5427 | Steps: 2 | Val loss: 1.8120 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=94112)[0m top1: 0.35027985074626866
[2m[36m(func pid=94112)[0m top5: 0.8736007462686567
[2m[36m(func pid=94112)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=94112)[0m f1_macro: 0.3447782005837746
[2m[36m(func pid=94112)[0m f1_weighted: 0.37811301690537447
[2m[36m(func pid=94112)[0m f1_per_class: [0.454, 0.381, 0.667, 0.519, 0.104, 0.193, 0.359, 0.236, 0.215, 0.32]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.4448 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:24:29 (running for 00:41:22.03)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.34675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00020 | RUNNING    | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.081 |      0.134 |                   74 |
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.565 |      0.304 |                   44 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.033 |      0.345 |                   25 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.112 |      0.317 |                   26 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=83527)[0m top1: 0.15298507462686567
[2m[36m(func pid=83527)[0m top5: 0.6147388059701493
[2m[36m(func pid=83527)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=83527)[0m f1_macro: 0.1340251592256135
[2m[36m(func pid=83527)[0m f1_weighted: 0.16548777399157039
[2m[36m(func pid=83527)[0m f1_per_class: [0.145, 0.211, 0.21, 0.223, 0.011, 0.097, 0.134, 0.125, 0.107, 0.077]
[2m[36m(func pid=83527)[0m 
[2m[36m(func pid=89821)[0m top1: 0.3292910447761194
[2m[36m(func pid=89821)[0m top5: 0.8791977611940298
[2m[36m(func pid=89821)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=89821)[0m f1_macro: 0.3060486929798764
[2m[36m(func pid=89821)[0m f1_weighted: 0.34574109806255404
[2m[36m(func pid=89821)[0m f1_per_class: [0.365, 0.39, 0.55, 0.477, 0.067, 0.162, 0.307, 0.244, 0.202, 0.297]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.0546 | Steps: 2 | Val loss: 1.9216 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=94182)[0m top1: 0.3829291044776119
[2m[36m(func pid=94182)[0m top5: 0.875
[2m[36m(func pid=94182)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=94182)[0m f1_macro: 0.31539296756978674
[2m[36m(func pid=94182)[0m f1_weighted: 0.3810640919390701
[2m[36m(func pid=94182)[0m f1_per_class: [0.541, 0.452, 0.387, 0.448, 0.112, 0.038, 0.433, 0.385, 0.2, 0.158]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=83527)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1557 | Steps: 2 | Val loss: 2.2481 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5501 | Steps: 2 | Val loss: 1.8126 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=94112)[0m top1: 0.36240671641791045
[2m[36m(func pid=94112)[0m top5: 0.8791977611940298
[2m[36m(func pid=94112)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=94112)[0m f1_macro: 0.3442602341957751
[2m[36m(func pid=94112)[0m f1_weighted: 0.39229001445125394
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.388, 0.611, 0.53, 0.096, 0.224, 0.385, 0.233, 0.196, 0.33]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0482 | Steps: 2 | Val loss: 10.9351 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=83527)[0m top1: 0.15671641791044777
[2m[36m(func pid=83527)[0m top5: 0.625
[2m[36m(func pid=83527)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=83527)[0m f1_macro: 0.1371968429312052
[2m[36m(func pid=83527)[0m f1_weighted: 0.17176768855599692
[2m[36m(func pid=83527)[0m f1_per_class: [0.148, 0.207, 0.225, 0.229, 0.011, 0.094, 0.153, 0.121, 0.108, 0.076]
== Status ==
Current time: 2024-01-07 01:24:35 (running for 00:41:27.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.543 |      0.306 |                   45 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.055 |      0.344 |                   26 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.315 |                   27 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.3302238805970149
[2m[36m(func pid=89821)[0m top5: 0.8763992537313433
[2m[36m(func pid=89821)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=89821)[0m f1_macro: 0.30674811037105265
[2m[36m(func pid=89821)[0m f1_weighted: 0.3454177249145306
[2m[36m(func pid=89821)[0m f1_per_class: [0.354, 0.389, 0.55, 0.479, 0.07, 0.163, 0.303, 0.246, 0.21, 0.303]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0261 | Steps: 2 | Val loss: 1.8855 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=94182)[0m top1: 0.38992537313432835
[2m[36m(func pid=94182)[0m top5: 0.8903917910447762
[2m[36m(func pid=94182)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=94182)[0m f1_macro: 0.3151302783605833
[2m[36m(func pid=94182)[0m f1_weighted: 0.3939633293576926
[2m[36m(func pid=94182)[0m f1_per_class: [0.527, 0.451, 0.348, 0.471, 0.107, 0.051, 0.452, 0.384, 0.206, 0.154]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5453 | Steps: 2 | Val loss: 1.7967 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=94112)[0m top1: 0.3712686567164179
[2m[36m(func pid=94112)[0m top5: 0.8833955223880597
[2m[36m(func pid=94112)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=94112)[0m f1_macro: 0.34609097657338195
[2m[36m(func pid=94112)[0m f1_weighted: 0.4007798430189707
[2m[36m(func pid=94112)[0m f1_per_class: [0.449, 0.39, 0.611, 0.536, 0.091, 0.236, 0.402, 0.234, 0.202, 0.311]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0001 | Steps: 2 | Val loss: 10.5903 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:24:40 (running for 00:41:33.11)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.545 |      0.309 |                   47 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.026 |      0.346 |                   27 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.048 |      0.315 |                   28 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.33861940298507465
[2m[36m(func pid=89821)[0m top5: 0.8801305970149254
[2m[36m(func pid=89821)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=89821)[0m f1_macro: 0.3092091810933272
[2m[36m(func pid=89821)[0m f1_weighted: 0.3548065012173244
[2m[36m(func pid=89821)[0m f1_per_class: [0.355, 0.387, 0.55, 0.495, 0.073, 0.174, 0.317, 0.249, 0.2, 0.291]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0300 | Steps: 2 | Val loss: 1.8566 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=94182)[0m top1: 0.386660447761194
[2m[36m(func pid=94182)[0m top5: 0.9006529850746269
[2m[36m(func pid=94182)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=94182)[0m f1_macro: 0.31226238090851555
[2m[36m(func pid=94182)[0m f1_weighted: 0.3966129588003318
[2m[36m(func pid=94182)[0m f1_per_class: [0.513, 0.442, 0.338, 0.471, 0.104, 0.092, 0.459, 0.346, 0.203, 0.154]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4547 | Steps: 2 | Val loss: 1.7922 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=94112)[0m top1: 0.3810634328358209
[2m[36m(func pid=94112)[0m top5: 0.8871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=94112)[0m f1_macro: 0.35099348541164976
[2m[36m(func pid=94112)[0m f1_weighted: 0.40835655019049594
[2m[36m(func pid=94112)[0m f1_per_class: [0.452, 0.39, 0.611, 0.541, 0.083, 0.237, 0.419, 0.243, 0.21, 0.324]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.3845 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:24:46 (running for 00:41:38.65)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.455 |      0.309 |                   48 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.03  |      0.351 |                   28 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.312 |                   29 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.34281716417910446
[2m[36m(func pid=89821)[0m top5: 0.8805970149253731
[2m[36m(func pid=89821)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=89821)[0m f1_macro: 0.30924405073924516
[2m[36m(func pid=89821)[0m f1_weighted: 0.3598446858732108
[2m[36m(func pid=89821)[0m f1_per_class: [0.359, 0.376, 0.55, 0.507, 0.077, 0.174, 0.33, 0.246, 0.2, 0.273]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.0265 | Steps: 2 | Val loss: 1.8585 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=94182)[0m top1: 0.38992537313432835
[2m[36m(func pid=94182)[0m top5: 0.9104477611940298
[2m[36m(func pid=94182)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=94182)[0m f1_macro: 0.3172072069023226
[2m[36m(func pid=94182)[0m f1_weighted: 0.40356335287410317
[2m[36m(func pid=94182)[0m f1_per_class: [0.518, 0.443, 0.329, 0.478, 0.102, 0.158, 0.451, 0.345, 0.192, 0.157]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5174 | Steps: 2 | Val loss: 1.7869 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=94112)[0m top1: 0.38152985074626866
[2m[36m(func pid=94112)[0m top5: 0.8894589552238806
[2m[36m(func pid=94112)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=94112)[0m f1_macro: 0.3479201673841229
[2m[36m(func pid=94112)[0m f1_weighted: 0.40864836183220643
[2m[36m(func pid=94112)[0m f1_per_class: [0.446, 0.379, 0.595, 0.548, 0.1, 0.233, 0.422, 0.249, 0.211, 0.298]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0003 | Steps: 2 | Val loss: 10.1911 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:24:51 (running for 00:41:44.14)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.517 |      0.309 |                   49 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.027 |      0.348 |                   29 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.317 |                   30 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.34281716417910446
[2m[36m(func pid=89821)[0m top5: 0.8810634328358209
[2m[36m(func pid=89821)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=89821)[0m f1_macro: 0.30878324643487787
[2m[36m(func pid=89821)[0m f1_weighted: 0.3594272349906236
[2m[36m(func pid=89821)[0m f1_per_class: [0.374, 0.363, 0.55, 0.512, 0.078, 0.175, 0.33, 0.254, 0.195, 0.256]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m top1: 0.386660447761194
[2m[36m(func pid=94182)[0m top5: 0.914179104477612
[2m[36m(func pid=94182)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=94182)[0m f1_macro: 0.3141100639978286
[2m[36m(func pid=94182)[0m f1_weighted: 0.4031137334481449
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.446, 0.316, 0.486, 0.099, 0.19, 0.435, 0.319, 0.177, 0.15]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.0275 | Steps: 2 | Val loss: 1.8566 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4960 | Steps: 2 | Val loss: 1.7963 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.1953 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=94112)[0m top1: 0.3824626865671642
[2m[36m(func pid=94112)[0m top5: 0.8871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=94112)[0m f1_macro: 0.3501955652255078
[2m[36m(func pid=94112)[0m f1_weighted: 0.40968557895220076
[2m[36m(func pid=94112)[0m f1_per_class: [0.446, 0.405, 0.611, 0.542, 0.079, 0.233, 0.415, 0.249, 0.218, 0.305]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:24:57 (running for 00:41:49.75)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.496 |      0.306 |                   50 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.027 |      0.35  |                   30 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.314 |                   31 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.33955223880597013
[2m[36m(func pid=89821)[0m top5: 0.8777985074626866
[2m[36m(func pid=89821)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=89821)[0m f1_macro: 0.3060617171431104
[2m[36m(func pid=89821)[0m f1_weighted: 0.35658940814186874
[2m[36m(func pid=89821)[0m f1_per_class: [0.368, 0.348, 0.55, 0.512, 0.078, 0.175, 0.329, 0.257, 0.207, 0.237]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m top1: 0.38199626865671643
[2m[36m(func pid=94182)[0m top5: 0.9188432835820896
[2m[36m(func pid=94182)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=94182)[0m f1_macro: 0.31101436262417714
[2m[36m(func pid=94182)[0m f1_weighted: 0.4013926325600025
[2m[36m(func pid=94182)[0m f1_per_class: [0.513, 0.441, 0.308, 0.488, 0.099, 0.203, 0.429, 0.303, 0.18, 0.146]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.0252 | Steps: 2 | Val loss: 1.8776 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4728 | Steps: 2 | Val loss: 1.7956 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.3107 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=94112)[0m top1: 0.3787313432835821
[2m[36m(func pid=94112)[0m top5: 0.8843283582089553
[2m[36m(func pid=94112)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=94112)[0m f1_macro: 0.3496702108789732
[2m[36m(func pid=94112)[0m f1_weighted: 0.4058422851299458
[2m[36m(func pid=94112)[0m f1_per_class: [0.453, 0.401, 0.611, 0.54, 0.087, 0.228, 0.407, 0.251, 0.218, 0.3]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:02 (running for 00:41:55.09)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.496 |      0.306 |                   50 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.025 |      0.35  |                   31 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.31  |                   33 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.37406716417910446
[2m[36m(func pid=94182)[0m top5: 0.9202425373134329
[2m[36m(func pid=94182)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=94182)[0m f1_macro: 0.30966406589145373
[2m[36m(func pid=94182)[0m f1_weighted: 0.3957286477657456
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.439, 0.312, 0.48, 0.097, 0.206, 0.415, 0.314, 0.172, 0.137]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34468283582089554
[2m[36m(func pid=89821)[0m top5: 0.8754664179104478
[2m[36m(func pid=89821)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=89821)[0m f1_macro: 0.3090615573261711
[2m[36m(func pid=89821)[0m f1_weighted: 0.3613377760944209
[2m[36m(func pid=89821)[0m f1_per_class: [0.385, 0.363, 0.55, 0.52, 0.077, 0.176, 0.327, 0.258, 0.211, 0.225]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.0289 | Steps: 2 | Val loss: 1.8833 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0019 | Steps: 2 | Val loss: 10.1989 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4974 | Steps: 2 | Val loss: 1.8003 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=94112)[0m top1: 0.3736007462686567
[2m[36m(func pid=94112)[0m top5: 0.8852611940298507
[2m[36m(func pid=94112)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=94112)[0m f1_macro: 0.35179143185363204
[2m[36m(func pid=94112)[0m f1_weighted: 0.40134073402571624
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.411, 0.649, 0.537, 0.075, 0.227, 0.39, 0.24, 0.218, 0.327]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:08 (running for 00:42:00.45)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.473 |      0.309 |                   51 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.029 |      0.352 |                   32 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.002 |      0.311 |                   34 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.38013059701492535
[2m[36m(func pid=94182)[0m top5: 0.9193097014925373
[2m[36m(func pid=94182)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=94182)[0m f1_macro: 0.31111354504980093
[2m[36m(func pid=94182)[0m f1_weighted: 0.40255093179625634
[2m[36m(func pid=94182)[0m f1_per_class: [0.514, 0.436, 0.312, 0.486, 0.095, 0.223, 0.432, 0.295, 0.172, 0.145]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34468283582089554
[2m[36m(func pid=89821)[0m top5: 0.8726679104477612
[2m[36m(func pid=89821)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=89821)[0m f1_macro: 0.30899705549539364
[2m[36m(func pid=89821)[0m f1_weighted: 0.3613001634819265
[2m[36m(func pid=89821)[0m f1_per_class: [0.398, 0.358, 0.55, 0.526, 0.076, 0.172, 0.323, 0.267, 0.214, 0.206]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0226 | Steps: 2 | Val loss: 1.8911 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0017 | Steps: 2 | Val loss: 10.3333 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5046 | Steps: 2 | Val loss: 1.7991 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=94112)[0m top1: 0.37080223880597013
[2m[36m(func pid=94112)[0m top5: 0.8871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=94112)[0m f1_macro: 0.3529690528158121
[2m[36m(func pid=94112)[0m f1_weighted: 0.3961691707034507
[2m[36m(func pid=94112)[0m f1_per_class: [0.456, 0.424, 0.649, 0.532, 0.08, 0.233, 0.367, 0.234, 0.224, 0.33]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:13 (running for 00:42:05.57)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.497 |      0.309 |                   52 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.023 |      0.353 |                   33 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.002 |      0.309 |                   35 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.37966417910447764
[2m[36m(func pid=94182)[0m top5: 0.917910447761194
[2m[36m(func pid=94182)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=94182)[0m f1_macro: 0.3088212091920616
[2m[36m(func pid=94182)[0m f1_weighted: 0.39995924158662144
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.448, 0.296, 0.482, 0.102, 0.235, 0.42, 0.284, 0.164, 0.149]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34048507462686567
[2m[36m(func pid=89821)[0m top5: 0.871268656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=89821)[0m f1_macro: 0.3079244757003677
[2m[36m(func pid=89821)[0m f1_weighted: 0.3578870919338736
[2m[36m(func pid=89821)[0m f1_per_class: [0.387, 0.353, 0.55, 0.515, 0.081, 0.172, 0.328, 0.251, 0.215, 0.228]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.0350 | Steps: 2 | Val loss: 1.9120 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4864 | Steps: 2 | Val loss: 1.7968 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=94112)[0m top1: 0.3689365671641791
[2m[36m(func pid=94112)[0m top5: 0.8810634328358209
[2m[36m(func pid=94112)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=94112)[0m f1_macro: 0.35045453286711903
[2m[36m(func pid=94112)[0m f1_weighted: 0.3919017040208367
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.432, 0.632, 0.534, 0.082, 0.223, 0.348, 0.249, 0.224, 0.336]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:18 (running for 00:42:10.76)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.505 |      0.308 |                   53 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.035 |      0.35  |                   34 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.316 |                   36 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.3829291044776119
[2m[36m(func pid=94182)[0m top5: 0.9188432835820896
[2m[36m(func pid=94182)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=94182)[0m f1_macro: 0.31557721936475386
[2m[36m(func pid=94182)[0m f1_weighted: 0.40340186970447317
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.448, 0.329, 0.485, 0.103, 0.254, 0.42, 0.277, 0.182, 0.15]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34328358208955223
[2m[36m(func pid=89821)[0m top5: 0.875
[2m[36m(func pid=89821)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=89821)[0m f1_macro: 0.31270878983693245
[2m[36m(func pid=89821)[0m f1_weighted: 0.3635957662975375
[2m[36m(func pid=89821)[0m f1_per_class: [0.395, 0.38, 0.55, 0.509, 0.073, 0.171, 0.335, 0.257, 0.224, 0.234]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0352 | Steps: 2 | Val loss: 1.9147 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0005 | Steps: 2 | Val loss: 10.3415 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5158 | Steps: 2 | Val loss: 1.8075 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=94112)[0m top1: 0.36800373134328357
[2m[36m(func pid=94112)[0m top5: 0.8833955223880597
[2m[36m(func pid=94112)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=94112)[0m f1_macro: 0.35007914240025845
[2m[36m(func pid=94112)[0m f1_weighted: 0.3908394752513647
[2m[36m(func pid=94112)[0m f1_per_class: [0.439, 0.43, 0.632, 0.535, 0.08, 0.215, 0.347, 0.249, 0.226, 0.346]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:23 (running for 00:42:16.20)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.486 |      0.313 |                   54 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.035 |      0.35  |                   35 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.001 |      0.316 |                   37 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.3810634328358209
[2m[36m(func pid=94182)[0m top5: 0.9188432835820896
[2m[36m(func pid=94182)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=94182)[0m f1_macro: 0.3160867177958664
[2m[36m(func pid=94182)[0m f1_weighted: 0.4014662807115072
[2m[36m(func pid=94182)[0m f1_per_class: [0.519, 0.451, 0.338, 0.48, 0.103, 0.258, 0.416, 0.27, 0.179, 0.149]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.33348880597014924
[2m[36m(func pid=89821)[0m top5: 0.871268656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=89821)[0m f1_macro: 0.30752899143372897
[2m[36m(func pid=89821)[0m f1_weighted: 0.35669631994053075
[2m[36m(func pid=89821)[0m f1_per_class: [0.393, 0.352, 0.564, 0.501, 0.067, 0.169, 0.337, 0.261, 0.213, 0.217]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.0209 | Steps: 2 | Val loss: 1.9285 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0012 | Steps: 2 | Val loss: 10.4212 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4294 | Steps: 2 | Val loss: 1.8049 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=94112)[0m top1: 0.365205223880597
[2m[36m(func pid=94112)[0m top5: 0.8805970149253731
[2m[36m(func pid=94112)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=94112)[0m f1_macro: 0.34689395385247446
[2m[36m(func pid=94112)[0m f1_weighted: 0.3870489791409246
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.43, 0.632, 0.533, 0.084, 0.215, 0.338, 0.248, 0.227, 0.313]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:25:29 (running for 00:42:21.29)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.516 |      0.308 |                   55 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.021 |      0.347 |                   36 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.001 |      0.316 |                   37 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.37919776119402987
[2m[36m(func pid=94182)[0m top5: 0.9202425373134329
[2m[36m(func pid=94182)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=94182)[0m f1_macro: 0.31495086804037126
[2m[36m(func pid=94182)[0m f1_weighted: 0.3993229451116404
[2m[36m(func pid=94182)[0m f1_per_class: [0.519, 0.451, 0.343, 0.478, 0.103, 0.252, 0.414, 0.261, 0.18, 0.149]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.3358208955223881
[2m[36m(func pid=89821)[0m top5: 0.871268656716418
[2m[36m(func pid=89821)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=89821)[0m f1_macro: 0.3118671101199764
[2m[36m(func pid=89821)[0m f1_weighted: 0.35827001334771474
[2m[36m(func pid=89821)[0m f1_per_class: [0.4, 0.363, 0.564, 0.498, 0.068, 0.171, 0.335, 0.266, 0.223, 0.231]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0040 | Steps: 2 | Val loss: 10.5225 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0306 | Steps: 2 | Val loss: 1.9498 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3837 | Steps: 2 | Val loss: 1.8014 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 01:25:34 (running for 00:42:26.58)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.429 |      0.312 |                   56 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.021 |      0.347 |                   36 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.004 |      0.315 |                   39 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.37919776119402987
[2m[36m(func pid=94182)[0m top5: 0.9193097014925373
[2m[36m(func pid=94182)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=94182)[0m f1_macro: 0.3149807538112254
[2m[36m(func pid=94182)[0m f1_weighted: 0.40045843243050716
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.45, 0.333, 0.478, 0.103, 0.255, 0.414, 0.279, 0.18, 0.148]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3628731343283582
[2m[36m(func pid=94112)[0m top5: 0.8768656716417911
[2m[36m(func pid=94112)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=94112)[0m f1_macro: 0.3471328321266026
[2m[36m(func pid=94112)[0m f1_weighted: 0.3845092472451577
[2m[36m(func pid=94112)[0m f1_per_class: [0.462, 0.424, 0.649, 0.53, 0.083, 0.2, 0.34, 0.251, 0.227, 0.308]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m top1: 0.33302238805970147
[2m[36m(func pid=89821)[0m top5: 0.8717350746268657
[2m[36m(func pid=89821)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=89821)[0m f1_macro: 0.3147680847963841
[2m[36m(func pid=89821)[0m f1_weighted: 0.35246512298002103
[2m[36m(func pid=89821)[0m f1_per_class: [0.414, 0.389, 0.564, 0.47, 0.072, 0.174, 0.323, 0.27, 0.218, 0.254]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.4704 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.0267 | Steps: 2 | Val loss: 1.9659 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4183 | Steps: 2 | Val loss: 1.7888 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 01:25:39 (running for 00:42:31.72)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.384 |      0.315 |                   57 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.031 |      0.347 |                   37 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.313 |                   40 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.37966417910447764
[2m[36m(func pid=94182)[0m top5: 0.9193097014925373
[2m[36m(func pid=94182)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=94182)[0m f1_macro: 0.31307120559917384
[2m[36m(func pid=94182)[0m f1_weighted: 0.40024602809584264
[2m[36m(func pid=94182)[0m f1_per_class: [0.5, 0.452, 0.324, 0.476, 0.103, 0.258, 0.416, 0.268, 0.179, 0.154]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35867537313432835
[2m[36m(func pid=94112)[0m top5: 0.8745335820895522
[2m[36m(func pid=94112)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=94112)[0m f1_macro: 0.3465906383611732
[2m[36m(func pid=94112)[0m f1_weighted: 0.37845982642605563
[2m[36m(func pid=94112)[0m f1_per_class: [0.468, 0.426, 0.649, 0.516, 0.078, 0.194, 0.331, 0.263, 0.219, 0.321]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m top1: 0.3400186567164179
[2m[36m(func pid=89821)[0m top5: 0.878731343283582
[2m[36m(func pid=89821)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=89821)[0m f1_macro: 0.3201910477264569
[2m[36m(func pid=89821)[0m f1_weighted: 0.36040185419800186
[2m[36m(func pid=89821)[0m f1_per_class: [0.417, 0.401, 0.564, 0.473, 0.064, 0.18, 0.336, 0.273, 0.222, 0.272]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0003 | Steps: 2 | Val loss: 10.4021 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.0201 | Steps: 2 | Val loss: 1.9658 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4223 | Steps: 2 | Val loss: 1.7908 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=94182)[0m top1: 0.3787313432835821
[2m[36m(func pid=94182)[0m top5: 0.9216417910447762
[2m[36m(func pid=94182)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=94182)[0m f1_macro: 0.30853462076377886
[2m[36m(func pid=94182)[0m f1_weighted: 0.3988108262571864
[2m[36m(func pid=94182)[0m f1_per_class: [0.486, 0.451, 0.304, 0.472, 0.109, 0.255, 0.419, 0.268, 0.165, 0.155]
== Status ==
Current time: 2024-01-07 01:25:44 (running for 00:42:36.79)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.418 |      0.32  |                   58 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.027 |      0.347 |                   38 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.309 |                   41 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.33348880597014924
[2m[36m(func pid=89821)[0m top5: 0.8782649253731343
[2m[36m(func pid=89821)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=89821)[0m f1_macro: 0.3144883383912472
[2m[36m(func pid=89821)[0m f1_weighted: 0.35220118609585854
[2m[36m(func pid=89821)[0m f1_per_class: [0.395, 0.4, 0.564, 0.474, 0.063, 0.176, 0.312, 0.272, 0.223, 0.267]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.353544776119403
[2m[36m(func pid=94112)[0m top5: 0.8759328358208955
[2m[36m(func pid=94112)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=94112)[0m f1_macro: 0.3446052954320457
[2m[36m(func pid=94112)[0m f1_weighted: 0.3735460615494219
[2m[36m(func pid=94112)[0m f1_per_class: [0.462, 0.422, 0.649, 0.496, 0.077, 0.194, 0.336, 0.258, 0.229, 0.324]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0262 | Steps: 2 | Val loss: 10.2438 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4382 | Steps: 2 | Val loss: 1.7871 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0247 | Steps: 2 | Val loss: 1.9623 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 01:25:49 (running for 00:42:41.83)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.422 |      0.314 |                   59 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.02  |      0.345 |                   39 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.026 |      0.312 |                   42 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.3880597014925373
[2m[36m(func pid=94182)[0m top5: 0.9202425373134329
[2m[36m(func pid=94182)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=94182)[0m f1_macro: 0.3116748107278272
[2m[36m(func pid=94182)[0m f1_weighted: 0.407065661159376
[2m[36m(func pid=94182)[0m f1_per_class: [0.486, 0.461, 0.308, 0.481, 0.107, 0.241, 0.439, 0.261, 0.165, 0.167]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.33861940298507465
[2m[36m(func pid=89821)[0m top5: 0.8805970149253731
[2m[36m(func pid=89821)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=89821)[0m f1_macro: 0.3184640900621942
[2m[36m(func pid=89821)[0m f1_weighted: 0.35957980717244614
[2m[36m(func pid=89821)[0m f1_per_class: [0.398, 0.407, 0.579, 0.475, 0.061, 0.183, 0.33, 0.266, 0.216, 0.27]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35447761194029853
[2m[36m(func pid=94112)[0m top5: 0.8759328358208955
[2m[36m(func pid=94112)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=94112)[0m f1_macro: 0.34258582895671047
[2m[36m(func pid=94112)[0m f1_weighted: 0.3754609357735877
[2m[36m(func pid=94112)[0m f1_per_class: [0.462, 0.422, 0.629, 0.498, 0.075, 0.188, 0.344, 0.254, 0.231, 0.323]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0743 | Steps: 2 | Val loss: 9.8176 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3957 | Steps: 2 | Val loss: 1.7873 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 01:25:54 (running for 00:42:47.15)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.438 |      0.318 |                   60 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.025 |      0.343 |                   40 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.074 |      0.317 |                   43 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4001865671641791
[2m[36m(func pid=94182)[0m top5: 0.9221082089552238
[2m[36m(func pid=94182)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=94182)[0m f1_macro: 0.3169953837972999
[2m[36m(func pid=94182)[0m f1_weighted: 0.415760079747434
[2m[36m(func pid=94182)[0m f1_per_class: [0.496, 0.459, 0.304, 0.484, 0.103, 0.242, 0.466, 0.247, 0.177, 0.193]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0196 | Steps: 2 | Val loss: 1.9704 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=89821)[0m top1: 0.3376865671641791
[2m[36m(func pid=89821)[0m top5: 0.8805970149253731
[2m[36m(func pid=89821)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=89821)[0m f1_macro: 0.32233129770929503
[2m[36m(func pid=89821)[0m f1_weighted: 0.36093899202659274
[2m[36m(func pid=89821)[0m f1_per_class: [0.405, 0.401, 0.611, 0.473, 0.065, 0.176, 0.342, 0.261, 0.217, 0.273]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0001 | Steps: 2 | Val loss: 9.5666 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=94112)[0m top1: 0.35447761194029853
[2m[36m(func pid=94112)[0m top5: 0.8736007462686567
[2m[36m(func pid=94112)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=94112)[0m f1_macro: 0.34422015657331484
[2m[36m(func pid=94112)[0m f1_weighted: 0.37689722557267563
[2m[36m(func pid=94112)[0m f1_per_class: [0.474, 0.426, 0.629, 0.498, 0.072, 0.188, 0.347, 0.245, 0.233, 0.33]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3857 | Steps: 2 | Val loss: 1.7898 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 01:26:00 (running for 00:42:52.44)
Memory usage on this node: 21.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.396 |      0.322 |                   61 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.02  |      0.344 |                   41 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.322 |                   44 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.408115671641791
[2m[36m(func pid=94182)[0m top5: 0.9230410447761194
[2m[36m(func pid=94182)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=94182)[0m f1_macro: 0.3222535332302073
[2m[36m(func pid=94182)[0m f1_weighted: 0.42108049666838193
[2m[36m(func pid=94182)[0m f1_per_class: [0.5, 0.465, 0.312, 0.487, 0.104, 0.207, 0.483, 0.281, 0.175, 0.207]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0258 | Steps: 2 | Val loss: 1.9615 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=89821)[0m top1: 0.3414179104477612
[2m[36m(func pid=89821)[0m top5: 0.8801305970149254
[2m[36m(func pid=89821)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=89821)[0m f1_macro: 0.32060926822190167
[2m[36m(func pid=89821)[0m f1_weighted: 0.36629215975345564
[2m[36m(func pid=89821)[0m f1_per_class: [0.398, 0.397, 0.595, 0.487, 0.062, 0.18, 0.348, 0.267, 0.209, 0.263]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.3773 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=94112)[0m top1: 0.35494402985074625
[2m[36m(func pid=94112)[0m top5: 0.875
[2m[36m(func pid=94112)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=94112)[0m f1_macro: 0.3432038061637257
[2m[36m(func pid=94112)[0m f1_weighted: 0.3785447632805631
[2m[36m(func pid=94112)[0m f1_per_class: [0.459, 0.426, 0.629, 0.501, 0.07, 0.191, 0.349, 0.247, 0.226, 0.333]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:26:05 (running for 00:42:57.49)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.386 |      0.321 |                   62 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.026 |      0.343 |                   42 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.325 |                   45 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.41511194029850745
[2m[36m(func pid=94182)[0m top5: 0.9207089552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=94182)[0m f1_macro: 0.3249921563011758
[2m[36m(func pid=94182)[0m f1_weighted: 0.42494358367360496
[2m[36m(func pid=94182)[0m f1_per_class: [0.5, 0.467, 0.304, 0.491, 0.103, 0.199, 0.491, 0.299, 0.175, 0.221]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3590 | Steps: 2 | Val loss: 1.7952 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0304 | Steps: 2 | Val loss: 1.9720 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0013 | Steps: 2 | Val loss: 9.3342 | Batch size: 32 | lr: 0.1 | Duration: 2.58s
[2m[36m(func pid=89821)[0m top1: 0.3381529850746269
[2m[36m(func pid=89821)[0m top5: 0.8763992537313433
[2m[36m(func pid=89821)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=89821)[0m f1_macro: 0.31712938913125355
[2m[36m(func pid=89821)[0m f1_weighted: 0.3635667641551624
[2m[36m(func pid=89821)[0m f1_per_class: [0.4, 0.392, 0.579, 0.487, 0.068, 0.165, 0.35, 0.259, 0.207, 0.265]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35261194029850745
[2m[36m(func pid=94112)[0m top5: 0.8745335820895522
[2m[36m(func pid=94112)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=94112)[0m f1_macro: 0.34306317503136485
[2m[36m(func pid=94112)[0m f1_weighted: 0.37491746015368255
[2m[36m(func pid=94112)[0m f1_per_class: [0.457, 0.423, 0.649, 0.496, 0.07, 0.189, 0.344, 0.257, 0.22, 0.327]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:26:10 (running for 00:43:02.50)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.359 |      0.317 |                   63 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.03  |      0.343 |                   43 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.001 |      0.329 |                   46 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42863805970149255
[2m[36m(func pid=94182)[0m top5: 0.9146455223880597
[2m[36m(func pid=94182)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=94182)[0m f1_macro: 0.32895228401918253
[2m[36m(func pid=94182)[0m f1_weighted: 0.4373041872929561
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.477, 0.304, 0.503, 0.096, 0.177, 0.521, 0.315, 0.174, 0.215]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3552 | Steps: 2 | Val loss: 1.8127 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0279 | Steps: 2 | Val loss: 1.9493 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.3312 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=89821)[0m top1: 0.33115671641791045
[2m[36m(func pid=89821)[0m top5: 0.8708022388059702
[2m[36m(func pid=89821)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=89821)[0m f1_macro: 0.3140634287916665
[2m[36m(func pid=89821)[0m f1_weighted: 0.35657936303016274
[2m[36m(func pid=89821)[0m f1_per_class: [0.393, 0.381, 0.579, 0.49, 0.057, 0.167, 0.33, 0.258, 0.202, 0.283]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3614738805970149
[2m[36m(func pid=94112)[0m top5: 0.8791977611940298
[2m[36m(func pid=94112)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=94112)[0m f1_macro: 0.346596870445287
[2m[36m(func pid=94112)[0m f1_weighted: 0.38311648482688326
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.43, 0.629, 0.519, 0.074, 0.206, 0.341, 0.251, 0.216, 0.352]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:26:15 (running for 00:43:07.57)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.355 |      0.314 |                   64 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.028 |      0.347 |                   44 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.328 |                   47 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4281716417910448
[2m[36m(func pid=94182)[0m top5: 0.9090485074626866
[2m[36m(func pid=94182)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=94182)[0m f1_macro: 0.32817172346126794
[2m[36m(func pid=94182)[0m f1_weighted: 0.43360734980789256
[2m[36m(func pid=94182)[0m f1_per_class: [0.522, 0.476, 0.304, 0.494, 0.104, 0.154, 0.525, 0.313, 0.174, 0.216]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3009 | Steps: 2 | Val loss: 1.8267 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0157 | Steps: 2 | Val loss: 1.9564 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.2900 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=89821)[0m top1: 0.3302238805970149
[2m[36m(func pid=89821)[0m top5: 0.8694029850746269
[2m[36m(func pid=89821)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=89821)[0m f1_macro: 0.30985938679471403
[2m[36m(func pid=89821)[0m f1_weighted: 0.353959468223989
[2m[36m(func pid=89821)[0m f1_per_class: [0.385, 0.375, 0.537, 0.493, 0.057, 0.17, 0.317, 0.281, 0.203, 0.281]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m top1: 0.36100746268656714
[2m[36m(func pid=94112)[0m top5: 0.8773320895522388
[2m[36m(func pid=94112)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=94112)[0m f1_macro: 0.3424772625148155
[2m[36m(func pid=94112)[0m f1_weighted: 0.38050501626770095
[2m[36m(func pid=94112)[0m f1_per_class: [0.435, 0.436, 0.611, 0.525, 0.066, 0.202, 0.324, 0.262, 0.212, 0.352]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:26:20 (running for 00:43:12.72)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.301 |      0.31  |                   65 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.342 |                   45 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.326 |                   48 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43236940298507465
[2m[36m(func pid=94182)[0m top5: 0.9057835820895522
[2m[36m(func pid=94182)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=94182)[0m f1_macro: 0.3256800376747116
[2m[36m(func pid=94182)[0m f1_weighted: 0.4344558440712675
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.482, 0.308, 0.497, 0.107, 0.121, 0.538, 0.296, 0.174, 0.225]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3484 | Steps: 2 | Val loss: 1.8397 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.3983 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0222 | Steps: 2 | Val loss: 1.9595 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=89821)[0m top1: 0.32276119402985076
[2m[36m(func pid=89821)[0m top5: 0.8675373134328358
[2m[36m(func pid=89821)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=89821)[0m f1_macro: 0.3042236819510148
[2m[36m(func pid=89821)[0m f1_weighted: 0.34339047446437787
[2m[36m(func pid=89821)[0m f1_per_class: [0.385, 0.371, 0.524, 0.495, 0.057, 0.168, 0.285, 0.278, 0.198, 0.281]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:26:25 (running for 00:43:17.93)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.348 |      0.304 |                   66 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.342 |                   45 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.33  |                   49 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43330223880597013
[2m[36m(func pid=94182)[0m top5: 0.9057835820895522
[2m[36m(func pid=94182)[0m f1_micro: 0.43330223880597013
[2m[36m(func pid=94182)[0m f1_macro: 0.3296996305826841
[2m[36m(func pid=94182)[0m f1_weighted: 0.43347409537696224
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.488, 0.312, 0.495, 0.11, 0.111, 0.532, 0.309, 0.174, 0.234]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3614738805970149
[2m[36m(func pid=94112)[0m top5: 0.8745335820895522
[2m[36m(func pid=94112)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=94112)[0m f1_macro: 0.34703078477112764
[2m[36m(func pid=94112)[0m f1_weighted: 0.3775750896328622
[2m[36m(func pid=94112)[0m f1_per_class: [0.433, 0.434, 0.649, 0.526, 0.07, 0.198, 0.312, 0.275, 0.211, 0.364]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.3494 | Steps: 2 | Val loss: 1.8404 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.4323 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0206 | Steps: 2 | Val loss: 1.9738 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=89821)[0m top1: 0.3255597014925373
[2m[36m(func pid=89821)[0m top5: 0.8647388059701493
[2m[36m(func pid=89821)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=89821)[0m f1_macro: 0.3053621414938802
[2m[36m(func pid=89821)[0m f1_weighted: 0.3435801286464935
[2m[36m(func pid=89821)[0m f1_per_class: [0.378, 0.373, 0.512, 0.503, 0.059, 0.184, 0.269, 0.283, 0.206, 0.288]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m top1: 0.4319029850746269
[2m[36m(func pid=94182)[0m top5: 0.9034514925373134
[2m[36m(func pid=94182)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=94182)[0m f1_macro: 0.32719007468061456
[2m[36m(func pid=94182)[0m f1_weighted: 0.43189739341124916
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.486, 0.304, 0.492, 0.111, 0.107, 0.535, 0.306, 0.171, 0.238]
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:26:31 (running for 00:43:23.34)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.349 |      0.305 |                   67 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.022 |      0.347 |                   46 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.327 |                   50 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.8740671641791045
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.3515313556975939
[2m[36m(func pid=94112)[0m f1_weighted: 0.3756825350989316
[2m[36m(func pid=94112)[0m f1_per_class: [0.425, 0.433, 0.706, 0.527, 0.071, 0.212, 0.301, 0.278, 0.196, 0.368]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2952 | Steps: 2 | Val loss: 1.8403 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.3372 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0165 | Steps: 2 | Val loss: 1.9688 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=89821)[0m top1: 0.32369402985074625
[2m[36m(func pid=89821)[0m top5: 0.8638059701492538
[2m[36m(func pid=89821)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=89821)[0m f1_macro: 0.30422939295659396
[2m[36m(func pid=89821)[0m f1_weighted: 0.3383842471128844
[2m[36m(func pid=89821)[0m f1_per_class: [0.383, 0.372, 0.512, 0.506, 0.059, 0.166, 0.254, 0.288, 0.214, 0.288]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:26:36 (running for 00:43:28.57)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.295 |      0.304 |                   68 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.021 |      0.352 |                   47 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.336 |                   51 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43796641791044777
[2m[36m(func pid=94182)[0m top5: 0.9043843283582089
[2m[36m(func pid=94182)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=94182)[0m f1_macro: 0.33606640201930976
[2m[36m(func pid=94182)[0m f1_weighted: 0.43558896428336985
[2m[36m(func pid=94182)[0m f1_per_class: [0.542, 0.491, 0.32, 0.499, 0.113, 0.083, 0.539, 0.32, 0.197, 0.257]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.363339552238806
[2m[36m(func pid=94112)[0m top5: 0.875
[2m[36m(func pid=94112)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=94112)[0m f1_macro: 0.35267072036440644
[2m[36m(func pid=94112)[0m f1_weighted: 0.3782241723595624
[2m[36m(func pid=94112)[0m f1_per_class: [0.413, 0.435, 0.686, 0.529, 0.085, 0.221, 0.302, 0.284, 0.2, 0.372]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3189 | Steps: 2 | Val loss: 1.8322 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.4270 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0233 | Steps: 2 | Val loss: 1.9829 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=89821)[0m top1: 0.32742537313432835
[2m[36m(func pid=89821)[0m top5: 0.8652052238805971
[2m[36m(func pid=89821)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=89821)[0m f1_macro: 0.30319292858482977
[2m[36m(func pid=89821)[0m f1_weighted: 0.3405556235414151
[2m[36m(func pid=89821)[0m f1_per_class: [0.389, 0.371, 0.512, 0.511, 0.066, 0.176, 0.254, 0.292, 0.207, 0.252]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:26:41 (running for 00:43:33.75)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.319 |      0.303 |                   69 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.017 |      0.353 |                   48 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.336 |                   52 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43796641791044777
[2m[36m(func pid=94182)[0m top5: 0.9043843283582089
[2m[36m(func pid=94182)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=94182)[0m f1_macro: 0.335668849830296
[2m[36m(func pid=94182)[0m f1_weighted: 0.4335277527648092
[2m[36m(func pid=94182)[0m f1_per_class: [0.537, 0.49, 0.324, 0.495, 0.124, 0.071, 0.541, 0.318, 0.194, 0.263]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3605410447761194
[2m[36m(func pid=94112)[0m top5: 0.8726679104477612
[2m[36m(func pid=94112)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.35228370388003927
[2m[36m(func pid=94112)[0m f1_weighted: 0.3739091262357644
[2m[36m(func pid=94112)[0m f1_per_class: [0.411, 0.425, 0.706, 0.526, 0.086, 0.227, 0.292, 0.291, 0.202, 0.357]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3004 | Steps: 2 | Val loss: 1.8253 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.4607 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0377 | Steps: 2 | Val loss: 1.9774 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=89821)[0m top1: 0.3302238805970149
[2m[36m(func pid=89821)[0m top5: 0.867070895522388
[2m[36m(func pid=89821)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=89821)[0m f1_macro: 0.30811141919439244
[2m[36m(func pid=89821)[0m f1_weighted: 0.34359385137025233
[2m[36m(func pid=89821)[0m f1_per_class: [0.4, 0.371, 0.512, 0.508, 0.069, 0.187, 0.262, 0.291, 0.207, 0.275]
[2m[36m(func pid=89821)[0m 
== Status ==
Current time: 2024-01-07 01:26:46 (running for 00:43:39.09)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.3   |      0.308 |                   70 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.023 |      0.352 |                   49 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.331 |                   53 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.435634328358209
[2m[36m(func pid=94182)[0m top5: 0.9029850746268657
[2m[36m(func pid=94182)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=94182)[0m f1_macro: 0.33062515835771816
[2m[36m(func pid=94182)[0m f1_weighted: 0.43240657630850354
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.488, 0.312, 0.505, 0.122, 0.064, 0.536, 0.308, 0.182, 0.257]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.363339552238806
[2m[36m(func pid=94112)[0m top5: 0.8722014925373134
[2m[36m(func pid=94112)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=94112)[0m f1_macro: 0.3510088303875334
[2m[36m(func pid=94112)[0m f1_weighted: 0.37573204957641215
[2m[36m(func pid=94112)[0m f1_per_class: [0.406, 0.427, 0.706, 0.536, 0.079, 0.228, 0.288, 0.293, 0.207, 0.341]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3358 | Steps: 2 | Val loss: 1.8184 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.5972 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=89821)[0m top1: 0.3306902985074627
[2m[36m(func pid=89821)[0m top5: 0.8703358208955224
[2m[36m(func pid=89821)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=89821)[0m f1_macro: 0.30643967177350306
[2m[36m(func pid=89821)[0m f1_weighted: 0.3451052152637597
[2m[36m(func pid=89821)[0m f1_per_class: [0.402, 0.371, 0.512, 0.508, 0.072, 0.187, 0.269, 0.284, 0.203, 0.257]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0189 | Steps: 2 | Val loss: 1.9955 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 01:26:51 (running for 00:43:44.13)
Memory usage on this node: 21.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.336 |      0.306 |                   71 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.038 |      0.351 |                   50 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.33  |                   54 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43097014925373134
[2m[36m(func pid=94182)[0m top5: 0.9020522388059702
[2m[36m(func pid=94182)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=94182)[0m f1_macro: 0.33037283845992443
[2m[36m(func pid=94182)[0m f1_weighted: 0.4252915047191035
[2m[36m(func pid=94182)[0m f1_per_class: [0.537, 0.49, 0.316, 0.485, 0.122, 0.051, 0.533, 0.311, 0.182, 0.277]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.3448481387376937
[2m[36m(func pid=94112)[0m f1_weighted: 0.37020075704218675
[2m[36m(func pid=94112)[0m f1_per_class: [0.42, 0.425, 0.629, 0.541, 0.09, 0.22, 0.27, 0.284, 0.208, 0.361]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3038 | Steps: 2 | Val loss: 1.8063 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.6670 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:26:57 (running for 00:43:49.36)
Memory usage on this node: 21.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.336 |      0.306 |                   71 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.019 |      0.345 |                   51 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.331 |                   55 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89821)[0m top1: 0.33488805970149255
[2m[36m(func pid=89821)[0m top5: 0.8745335820895522
[2m[36m(func pid=89821)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=89821)[0m f1_macro: 0.31039513641607525
[2m[36m(func pid=89821)[0m f1_weighted: 0.3506366319329943
[2m[36m(func pid=89821)[0m f1_per_class: [0.393, 0.373, 0.512, 0.508, 0.072, 0.186, 0.286, 0.284, 0.206, 0.283]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94182)[0m top1: 0.43283582089552236
[2m[36m(func pid=94182)[0m top5: 0.8987873134328358
[2m[36m(func pid=94182)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=94182)[0m f1_macro: 0.3306036609804649
[2m[36m(func pid=94182)[0m f1_weighted: 0.4264879305822771
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.498, 0.316, 0.485, 0.117, 0.051, 0.532, 0.319, 0.167, 0.29]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0171 | Steps: 2 | Val loss: 2.0250 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0001 | Steps: 2 | Val loss: 9.7552 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3360 | Steps: 2 | Val loss: 1.8009 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=94112)[0m top1: 0.353544776119403
[2m[36m(func pid=94112)[0m top5: 0.8666044776119403
[2m[36m(func pid=94112)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=94112)[0m f1_macro: 0.34217154149892953
[2m[36m(func pid=94112)[0m f1_weighted: 0.3660742964283487
[2m[36m(func pid=94112)[0m f1_per_class: [0.416, 0.416, 0.629, 0.537, 0.088, 0.204, 0.272, 0.275, 0.218, 0.368]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:27:02 (running for 00:43:54.72)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.304 |      0.31  |                   72 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.017 |      0.342 |                   52 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.331 |                   56 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4295708955223881
[2m[36m(func pid=94182)[0m top5: 0.8992537313432836
[2m[36m(func pid=94182)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.33050622170362565
[2m[36m(func pid=94182)[0m f1_weighted: 0.4239231998540191
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.486, 0.329, 0.482, 0.115, 0.044, 0.535, 0.312, 0.19, 0.279]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34095149253731344
[2m[36m(func pid=89821)[0m top5: 0.8726679104477612
[2m[36m(func pid=89821)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=89821)[0m f1_macro: 0.31929141507096687
[2m[36m(func pid=89821)[0m f1_weighted: 0.35398120222500384
[2m[36m(func pid=89821)[0m f1_per_class: [0.413, 0.383, 0.537, 0.511, 0.074, 0.189, 0.282, 0.295, 0.217, 0.291]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0243 | Steps: 2 | Val loss: 2.0473 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8086 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2833 | Steps: 2 | Val loss: 1.7990 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=94112)[0m top1: 0.35027985074626866
[2m[36m(func pid=94112)[0m top5: 0.8652052238805971
[2m[36m(func pid=94112)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=94112)[0m f1_macro: 0.3428330690914662
[2m[36m(func pid=94112)[0m f1_weighted: 0.3640062555456458
[2m[36m(func pid=94112)[0m f1_per_class: [0.425, 0.419, 0.647, 0.528, 0.086, 0.205, 0.271, 0.272, 0.219, 0.356]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:27:07 (running for 00:43:59.86)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.336 |      0.319 |                   73 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.024 |      0.343 |                   53 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.331 |                   57 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4291044776119403
[2m[36m(func pid=94182)[0m top5: 0.8969216417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=94182)[0m f1_macro: 0.33084827696302077
[2m[36m(func pid=94182)[0m f1_weighted: 0.42330927532474605
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.488, 0.329, 0.483, 0.114, 0.044, 0.53, 0.312, 0.192, 0.283]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34095149253731344
[2m[36m(func pid=89821)[0m top5: 0.8745335820895522
[2m[36m(func pid=89821)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=89821)[0m f1_macro: 0.3162054750842055
[2m[36m(func pid=89821)[0m f1_weighted: 0.3570486567027665
[2m[36m(func pid=89821)[0m f1_per_class: [0.42, 0.376, 0.512, 0.512, 0.073, 0.174, 0.302, 0.294, 0.212, 0.288]
[2m[36m(func pid=89821)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0191 | Steps: 2 | Val loss: 2.0380 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.7570 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=89821)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2835 | Steps: 2 | Val loss: 1.7914 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=94112)[0m top1: 0.35447761194029853
[2m[36m(func pid=94112)[0m top5: 0.8652052238805971
[2m[36m(func pid=94112)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=94112)[0m f1_macro: 0.34580175847745237
[2m[36m(func pid=94112)[0m f1_weighted: 0.36915982850919726
[2m[36m(func pid=94112)[0m f1_per_class: [0.433, 0.421, 0.667, 0.529, 0.089, 0.201, 0.287, 0.278, 0.213, 0.34]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:27:12 (running for 00:44:04.97)
Memory usage on this node: 22.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.344
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00021 | RUNNING    | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.283 |      0.316 |                   74 |
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.019 |      0.346 |                   54 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.332 |                   58 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43097014925373134
[2m[36m(func pid=94182)[0m top5: 0.898320895522388
[2m[36m(func pid=94182)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=94182)[0m f1_macro: 0.33188630220206583
[2m[36m(func pid=94182)[0m f1_weighted: 0.4248987172201743
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.493, 0.329, 0.486, 0.112, 0.044, 0.531, 0.311, 0.188, 0.303]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=89821)[0m top1: 0.34654850746268656
[2m[36m(func pid=89821)[0m top5: 0.8773320895522388
[2m[36m(func pid=89821)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=89821)[0m f1_macro: 0.31987216525934803
[2m[36m(func pid=89821)[0m f1_weighted: 0.3653210073178674
[2m[36m(func pid=89821)[0m f1_per_class: [0.42, 0.376, 0.524, 0.514, 0.069, 0.178, 0.328, 0.28, 0.216, 0.294]
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.0156 | Steps: 2 | Val loss: 2.0430 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8489 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=94112)[0m top1: 0.3530783582089552
[2m[36m(func pid=94112)[0m top5: 0.8680037313432836
[2m[36m(func pid=94112)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=94112)[0m f1_macro: 0.3446807079797113
[2m[36m(func pid=94112)[0m f1_weighted: 0.3703437620960793
[2m[36m(func pid=94112)[0m f1_per_class: [0.438, 0.422, 0.647, 0.526, 0.086, 0.188, 0.298, 0.271, 0.214, 0.356]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:27:18 (running for 00:44:10.35)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.345 |                   55 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.33  |                   59 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42723880597014924
[2m[36m(func pid=94182)[0m top5: 0.8936567164179104
[2m[36m(func pid=94182)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=94182)[0m f1_macro: 0.32982713440198463
[2m[36m(func pid=94182)[0m f1_weighted: 0.4219465528826353
[2m[36m(func pid=94182)[0m f1_per_class: [0.518, 0.489, 0.324, 0.482, 0.11, 0.044, 0.526, 0.32, 0.185, 0.3]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0160 | Steps: 2 | Val loss: 2.0348 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.7229 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=94112)[0m top1: 0.35401119402985076
[2m[36m(func pid=94112)[0m top5: 0.8666044776119403
[2m[36m(func pid=94112)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=94112)[0m f1_macro: 0.3440948277384035
[2m[36m(func pid=94112)[0m f1_weighted: 0.37290038400046477
[2m[36m(func pid=94112)[0m f1_per_class: [0.433, 0.423, 0.629, 0.522, 0.084, 0.2, 0.307, 0.268, 0.221, 0.356]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.4295708955223881
[2m[36m(func pid=94182)[0m top5: 0.8969216417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.3333223104790054
[2m[36m(func pid=94182)[0m f1_weighted: 0.4238752319977088
[2m[36m(func pid=94182)[0m f1_per_class: [0.518, 0.49, 0.343, 0.491, 0.117, 0.044, 0.522, 0.316, 0.189, 0.303]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.0222 | Steps: 2 | Val loss: 2.0123 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8056 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 01:27:28 (running for 00:44:20.35)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.344 |                   56 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.333 |                   60 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4295708955223881
[2m[36m(func pid=94182)[0m top5: 0.8950559701492538
[2m[36m(func pid=94182)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.3335858944803739
[2m[36m(func pid=94182)[0m f1_weighted: 0.4243727780870787
[2m[36m(func pid=94182)[0m f1_per_class: [0.526, 0.495, 0.338, 0.488, 0.109, 0.044, 0.523, 0.32, 0.189, 0.303]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.3450197232116078
[2m[36m(func pid=94112)[0m f1_weighted: 0.3806615670470099
[2m[36m(func pid=94112)[0m f1_per_class: [0.43, 0.422, 0.611, 0.525, 0.083, 0.2, 0.331, 0.267, 0.216, 0.364]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8593 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0274 | Steps: 2 | Val loss: 1.9907 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:27:33 (running for 00:44:25.60)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.022 |      0.345 |                   57 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.334 |                   61 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42677238805970147
[2m[36m(func pid=94182)[0m top5: 0.8969216417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=94182)[0m f1_macro: 0.3325579990033107
[2m[36m(func pid=94182)[0m f1_weighted: 0.4214751885033882
[2m[36m(func pid=94182)[0m f1_per_class: [0.531, 0.494, 0.329, 0.477, 0.106, 0.044, 0.524, 0.32, 0.19, 0.31]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35867537313432835
[2m[36m(func pid=94112)[0m top5: 0.8717350746268657
[2m[36m(func pid=94112)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=94112)[0m f1_macro: 0.3473474391222383
[2m[36m(func pid=94112)[0m f1_weighted: 0.379946654304406
[2m[36m(func pid=94112)[0m f1_per_class: [0.429, 0.425, 0.629, 0.518, 0.083, 0.197, 0.335, 0.259, 0.223, 0.376]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8701 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0254 | Steps: 2 | Val loss: 1.9827 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:27:38 (running for 00:44:31.00)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.027 |      0.347 |                   58 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.333 |                   62 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42677238805970147
[2m[36m(func pid=94182)[0m top5: 0.8973880597014925
[2m[36m(func pid=94182)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=94182)[0m f1_macro: 0.3327288645851495
[2m[36m(func pid=94182)[0m f1_weighted: 0.42089980245352515
[2m[36m(func pid=94182)[0m f1_per_class: [0.531, 0.493, 0.333, 0.475, 0.111, 0.044, 0.525, 0.321, 0.189, 0.305]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3582089552238806
[2m[36m(func pid=94112)[0m top5: 0.8722014925373134
[2m[36m(func pid=94112)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=94112)[0m f1_macro: 0.3478235402893147
[2m[36m(func pid=94112)[0m f1_weighted: 0.379417438938928
[2m[36m(func pid=94112)[0m f1_per_class: [0.436, 0.418, 0.629, 0.516, 0.082, 0.197, 0.339, 0.26, 0.221, 0.381]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8124 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0145 | Steps: 2 | Val loss: 1.9865 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 01:27:44 (running for 00:44:36.48)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.025 |      0.348 |                   59 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.333 |                   63 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42677238805970147
[2m[36m(func pid=94182)[0m top5: 0.894589552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=94182)[0m f1_macro: 0.330136044502619
[2m[36m(func pid=94182)[0m f1_weighted: 0.4206551022460726
[2m[36m(func pid=94182)[0m f1_per_class: [0.509, 0.488, 0.32, 0.476, 0.118, 0.044, 0.527, 0.326, 0.189, 0.305]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3572761194029851
[2m[36m(func pid=94112)[0m top5: 0.8722014925373134
[2m[36m(func pid=94112)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=94112)[0m f1_macro: 0.3468285365524992
[2m[36m(func pid=94112)[0m f1_weighted: 0.3795569626069966
[2m[36m(func pid=94112)[0m f1_per_class: [0.439, 0.421, 0.629, 0.516, 0.078, 0.199, 0.338, 0.252, 0.22, 0.376]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8432 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0149 | Steps: 2 | Val loss: 1.9868 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 01:27:49 (running for 00:44:42.01)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.014 |      0.347 |                   60 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.33  |                   64 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.427705223880597
[2m[36m(func pid=94182)[0m top5: 0.8959888059701493
[2m[36m(func pid=94182)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=94182)[0m f1_macro: 0.33177232511221577
[2m[36m(func pid=94182)[0m f1_weighted: 0.4213779697292496
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.491, 0.316, 0.477, 0.121, 0.044, 0.526, 0.321, 0.189, 0.31]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3605410447761194
[2m[36m(func pid=94112)[0m top5: 0.871268656716418
[2m[36m(func pid=94112)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.3463054813851251
[2m[36m(func pid=94112)[0m f1_weighted: 0.3819785216366059
[2m[36m(func pid=94112)[0m f1_per_class: [0.434, 0.429, 0.611, 0.517, 0.078, 0.209, 0.337, 0.26, 0.211, 0.376]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8113 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0237 | Steps: 2 | Val loss: 2.0025 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 01:27:55 (running for 00:44:47.45)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.346 |                   61 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.329 |                   66 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.427705223880597
[2m[36m(func pid=94182)[0m top5: 0.8955223880597015
[2m[36m(func pid=94182)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=94182)[0m f1_macro: 0.32884598084101535
[2m[36m(func pid=94182)[0m f1_weighted: 0.42146902767088945
[2m[36m(func pid=94182)[0m f1_per_class: [0.513, 0.491, 0.312, 0.477, 0.115, 0.044, 0.528, 0.32, 0.194, 0.295]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.36100746268656714
[2m[36m(func pid=94112)[0m top5: 0.8689365671641791
[2m[36m(func pid=94112)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=94112)[0m f1_macro: 0.3490197847884004
[2m[36m(func pid=94112)[0m f1_weighted: 0.3816580755979825
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.425, 0.629, 0.515, 0.076, 0.207, 0.338, 0.271, 0.225, 0.364]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8773 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0154 | Steps: 2 | Val loss: 2.0241 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 01:28:00 (running for 00:44:52.62)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.024 |      0.349 |                   62 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.33  |                   67 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4300373134328358
[2m[36m(func pid=94182)[0m top5: 0.8964552238805971
[2m[36m(func pid=94182)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=94182)[0m f1_macro: 0.3295141879686493
[2m[36m(func pid=94182)[0m f1_weighted: 0.4228966775627487
[2m[36m(func pid=94182)[0m f1_per_class: [0.518, 0.498, 0.308, 0.478, 0.118, 0.044, 0.528, 0.321, 0.183, 0.3]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35494402985074625
[2m[36m(func pid=94112)[0m top5: 0.8666044776119403
[2m[36m(func pid=94112)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=94112)[0m f1_macro: 0.3460131404258247
[2m[36m(func pid=94112)[0m f1_weighted: 0.37570492801614525
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.423, 0.629, 0.508, 0.07, 0.199, 0.329, 0.266, 0.221, 0.372]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8150 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0325 | Steps: 2 | Val loss: 2.0199 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 01:28:05 (running for 00:44:57.80)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.346 |                   63 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.327 |                   68 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.42863805970149255
[2m[36m(func pid=94182)[0m top5: 0.8969216417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=94182)[0m f1_macro: 0.3272844452073004
[2m[36m(func pid=94182)[0m f1_weighted: 0.4225230201969697
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.494, 0.304, 0.478, 0.111, 0.044, 0.532, 0.313, 0.163, 0.303]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3568097014925373
[2m[36m(func pid=94112)[0m top5: 0.8680037313432836
[2m[36m(func pid=94112)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=94112)[0m f1_macro: 0.3504121317235978
[2m[36m(func pid=94112)[0m f1_weighted: 0.3787677547924946
[2m[36m(func pid=94112)[0m f1_per_class: [0.452, 0.427, 0.647, 0.51, 0.068, 0.205, 0.331, 0.272, 0.211, 0.381]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8738 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0217 | Steps: 2 | Val loss: 2.0352 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 01:28:10 (running for 00:45:02.89)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.032 |      0.35  |                   64 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.326 |                   69 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4300373134328358
[2m[36m(func pid=94182)[0m top5: 0.8941231343283582
[2m[36m(func pid=94182)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=94182)[0m f1_macro: 0.3259948038278687
[2m[36m(func pid=94182)[0m f1_weighted: 0.42399838913774407
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.492, 0.296, 0.477, 0.109, 0.044, 0.539, 0.319, 0.167, 0.295]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35494402985074625
[2m[36m(func pid=94112)[0m top5: 0.867070895522388
[2m[36m(func pid=94112)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=94112)[0m f1_macro: 0.34442985591274683
[2m[36m(func pid=94112)[0m f1_weighted: 0.3748856160518207
[2m[36m(func pid=94112)[0m f1_per_class: [0.442, 0.437, 0.629, 0.522, 0.066, 0.188, 0.307, 0.287, 0.211, 0.356]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8405 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0169 | Steps: 2 | Val loss: 2.0420 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 01:28:15 (running for 00:45:08.00)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.022 |      0.344 |                   65 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.327 |                   70 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4295708955223881
[2m[36m(func pid=94182)[0m top5: 0.8936567164179104
[2m[36m(func pid=94182)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.32720542192046576
[2m[36m(func pid=94182)[0m f1_weighted: 0.42497328099393433
[2m[36m(func pid=94182)[0m f1_per_class: [0.527, 0.489, 0.3, 0.491, 0.108, 0.044, 0.529, 0.321, 0.177, 0.286]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3530783582089552
[2m[36m(func pid=94112)[0m top5: 0.8680037313432836
[2m[36m(func pid=94112)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=94112)[0m f1_macro: 0.3472974341669407
[2m[36m(func pid=94112)[0m f1_weighted: 0.3706102746831987
[2m[36m(func pid=94112)[0m f1_per_class: [0.438, 0.428, 0.667, 0.513, 0.067, 0.208, 0.297, 0.291, 0.212, 0.352]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.8107 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:28:20 (running for 00:45:13.04)
Memory usage on this node: 18.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.017 |      0.347 |                   66 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.329 |                   71 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0305 | Steps: 2 | Val loss: 2.0425 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=94182)[0m top1: 0.43236940298507465
[2m[36m(func pid=94182)[0m top5: 0.8931902985074627
[2m[36m(func pid=94182)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=94182)[0m f1_macro: 0.32866206801362574
[2m[36m(func pid=94182)[0m f1_weighted: 0.4269037685506211
[2m[36m(func pid=94182)[0m f1_per_class: [0.527, 0.493, 0.3, 0.489, 0.109, 0.045, 0.535, 0.32, 0.176, 0.293]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3516791044776119
[2m[36m(func pid=94112)[0m top5: 0.8680037313432836
[2m[36m(func pid=94112)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=94112)[0m f1_macro: 0.3436142793229528
[2m[36m(func pid=94112)[0m f1_weighted: 0.3680707250545243
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.432, 0.649, 0.517, 0.069, 0.191, 0.29, 0.286, 0.216, 0.343]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0862 | Steps: 2 | Val loss: 9.4952 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 01:28:26 (running for 00:45:18.26)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.031 |      0.344 |                   67 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.086 |      0.33  |                   72 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43656716417910446
[2m[36m(func pid=94182)[0m top5: 0.9011194029850746
[2m[36m(func pid=94182)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=94182)[0m f1_macro: 0.3298270830806772
[2m[36m(func pid=94182)[0m f1_weighted: 0.4310114297649373
[2m[36m(func pid=94182)[0m f1_per_class: [0.518, 0.49, 0.312, 0.494, 0.107, 0.051, 0.545, 0.312, 0.177, 0.293]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0149 | Steps: 2 | Val loss: 2.0477 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.0929 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m top1: 0.3521455223880597
[2m[36m(func pid=94112)[0m top5: 0.8652052238805971
[2m[36m(func pid=94112)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=94112)[0m f1_macro: 0.3425163367256273
[2m[36m(func pid=94112)[0m f1_weighted: 0.36645980633561903
[2m[36m(func pid=94112)[0m f1_per_class: [0.433, 0.434, 0.632, 0.513, 0.066, 0.197, 0.283, 0.298, 0.215, 0.354]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:28:31 (running for 00:45:23.49)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.343 |                   68 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.337 |                   73 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.44449626865671643
[2m[36m(func pid=94182)[0m top5: 0.9113805970149254
[2m[36m(func pid=94182)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=94182)[0m f1_macro: 0.33662983375154487
[2m[36m(func pid=94182)[0m f1_weighted: 0.44023264938948875
[2m[36m(func pid=94182)[0m f1_per_class: [0.532, 0.489, 0.316, 0.488, 0.112, 0.112, 0.561, 0.294, 0.176, 0.286]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.0179 | Steps: 2 | Val loss: 2.0521 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.7417 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=94112)[0m top1: 0.35634328358208955
[2m[36m(func pid=94112)[0m top5: 0.8638059701492538
[2m[36m(func pid=94112)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=94112)[0m f1_macro: 0.3415734307075705
[2m[36m(func pid=94112)[0m f1_weighted: 0.36917847854521874
[2m[36m(func pid=94112)[0m f1_per_class: [0.428, 0.442, 0.632, 0.525, 0.066, 0.206, 0.275, 0.299, 0.214, 0.33]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.44822761194029853
[2m[36m(func pid=94182)[0m top5: 0.9193097014925373
[2m[36m(func pid=94182)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=94182)[0m f1_macro: 0.3440252731410494
[2m[36m(func pid=94182)[0m f1_weighted: 0.4450320351126993
[2m[36m(func pid=94182)[0m f1_per_class: [0.537, 0.49, 0.338, 0.492, 0.115, 0.149, 0.56, 0.276, 0.188, 0.295]
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:28:36 (running for 00:45:28.79)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.34225
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.018 |      0.342 |                   69 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.344 |                   74 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0203 | Steps: 2 | Val loss: 2.0436 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.6644 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=94112)[0m top1: 0.36100746268656714
[2m[36m(func pid=94112)[0m top5: 0.8647388059701493
[2m[36m(func pid=94112)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=94112)[0m f1_macro: 0.348614081682506
[2m[36m(func pid=94112)[0m f1_weighted: 0.3726809706954572
[2m[36m(func pid=94112)[0m f1_per_class: [0.428, 0.439, 0.667, 0.531, 0.069, 0.204, 0.279, 0.315, 0.212, 0.343]
[2m[36m(func pid=94112)[0m 
== Status ==
Current time: 2024-01-07 01:28:41 (running for 00:45:33.80)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.02  |      0.349 |                   70 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.346 |                   75 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.45009328358208955
[2m[36m(func pid=94182)[0m top5: 0.9230410447761194
[2m[36m(func pid=94182)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=94182)[0m f1_macro: 0.3455787695559503
[2m[36m(func pid=94182)[0m f1_weighted: 0.4482631212610919
[2m[36m(func pid=94182)[0m f1_per_class: [0.531, 0.491, 0.348, 0.488, 0.115, 0.185, 0.566, 0.251, 0.186, 0.295]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0155 | Steps: 2 | Val loss: 2.0358 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.7832 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=94112)[0m top1: 0.36007462686567165
[2m[36m(func pid=94112)[0m top5: 0.863339552238806
[2m[36m(func pid=94112)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=94112)[0m f1_macro: 0.343125190707522
[2m[36m(func pid=94112)[0m f1_weighted: 0.37275008559043477
[2m[36m(func pid=94112)[0m f1_per_class: [0.423, 0.443, 0.632, 0.528, 0.067, 0.202, 0.283, 0.308, 0.209, 0.337]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.45009328358208955
[2m[36m(func pid=94182)[0m top5: 0.9249067164179104
[2m[36m(func pid=94182)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=94182)[0m f1_macro: 0.34722055795632006
[2m[36m(func pid=94182)[0m f1_weighted: 0.4513412391984386
[2m[36m(func pid=94182)[0m f1_per_class: [0.55, 0.491, 0.333, 0.483, 0.106, 0.223, 0.567, 0.239, 0.186, 0.293]
== Status ==
Current time: 2024-01-07 01:28:46 (running for 00:45:38.87)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.343 |                   71 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.347 |                   76 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0225 | Steps: 2 | Val loss: 2.0322 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.8225 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=94112)[0m top1: 0.36007462686567165
[2m[36m(func pid=94112)[0m top5: 0.8638059701492538
[2m[36m(func pid=94112)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=94112)[0m f1_macro: 0.3428775112648087
[2m[36m(func pid=94112)[0m f1_weighted: 0.3747784515661654
[2m[36m(func pid=94112)[0m f1_per_class: [0.411, 0.437, 0.649, 0.525, 0.078, 0.216, 0.297, 0.283, 0.206, 0.327]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.44216417910447764
[2m[36m(func pid=94182)[0m top5: 0.925839552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=94182)[0m f1_macro: 0.3426253659018782
[2m[36m(func pid=94182)[0m f1_weighted: 0.4451818419575113
[2m[36m(func pid=94182)[0m f1_per_class: [0.523, 0.486, 0.329, 0.476, 0.107, 0.23, 0.555, 0.244, 0.185, 0.293]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0229 | Steps: 2 | Val loss: 2.0222 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.8043 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 01:28:56 (running for 00:45:48.41)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.023 |      0.348 |                   73 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.343 |                   77 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3614738805970149
[2m[36m(func pid=94112)[0m top5: 0.8661380597014925
[2m[36m(func pid=94112)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=94112)[0m f1_macro: 0.34837880898472295
[2m[36m(func pid=94112)[0m f1_weighted: 0.3788421056469983
[2m[36m(func pid=94112)[0m f1_per_class: [0.418, 0.438, 0.667, 0.526, 0.077, 0.21, 0.31, 0.275, 0.216, 0.348]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.4435634328358209
[2m[36m(func pid=94182)[0m top5: 0.9267723880597015
[2m[36m(func pid=94182)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=94182)[0m f1_macro: 0.34263325619223794
[2m[36m(func pid=94182)[0m f1_weighted: 0.4461616502904403
[2m[36m(func pid=94182)[0m f1_per_class: [0.526, 0.489, 0.324, 0.481, 0.114, 0.223, 0.555, 0.24, 0.183, 0.291]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0112 | Steps: 2 | Val loss: 2.0170 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2047 | Steps: 2 | Val loss: 8.7938 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 01:29:01 (running for 00:45:54.16)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.011 |      0.343 |                   74 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.343 |                   78 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3614738805970149
[2m[36m(func pid=94112)[0m top5: 0.867070895522388
[2m[36m(func pid=94112)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=94112)[0m f1_macro: 0.34279783101844213
[2m[36m(func pid=94112)[0m f1_weighted: 0.3813348878677113
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.437, 0.611, 0.522, 0.076, 0.209, 0.327, 0.26, 0.213, 0.33]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.43423507462686567
[2m[36m(func pid=94182)[0m top5: 0.9239738805970149
[2m[36m(func pid=94182)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=94182)[0m f1_macro: 0.33833328496522735
[2m[36m(func pid=94182)[0m f1_weighted: 0.4410083014497703
[2m[36m(func pid=94182)[0m f1_per_class: [0.531, 0.475, 0.32, 0.491, 0.108, 0.224, 0.536, 0.241, 0.187, 0.27]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0211 | Steps: 2 | Val loss: 1.9968 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0000 | Steps: 2 | Val loss: 8.7847 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 01:29:07 (running for 00:45:59.55)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.345
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.011 |      0.343 |                   74 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.205 |      0.338 |                   79 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.36613805970149255
[2m[36m(func pid=94112)[0m top5: 0.8694029850746269
[2m[36m(func pid=94112)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=94112)[0m f1_macro: 0.3476945292025757
[2m[36m(func pid=94112)[0m f1_weighted: 0.387204353937508
[2m[36m(func pid=94112)[0m f1_per_class: [0.442, 0.438, 0.629, 0.528, 0.077, 0.21, 0.338, 0.26, 0.222, 0.333]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.4319029850746269
[2m[36m(func pid=94182)[0m top5: 0.917910447761194
[2m[36m(func pid=94182)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=94182)[0m f1_macro: 0.3305227550267421
[2m[36m(func pid=94182)[0m f1_weighted: 0.44289228605186076
[2m[36m(func pid=94182)[0m f1_per_class: [0.508, 0.435, 0.304, 0.53, 0.103, 0.228, 0.533, 0.242, 0.161, 0.262]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.1291 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.0182 | Steps: 2 | Val loss: 2.0001 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 01:29:12 (running for 00:46:04.93)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.021 |      0.348 |                   75 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.331 |                   80 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4281716417910448
[2m[36m(func pid=94182)[0m top5: 0.9081156716417911
[2m[36m(func pid=94182)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=94182)[0m f1_macro: 0.3323077558634097
[2m[36m(func pid=94182)[0m f1_weighted: 0.4410053661934887
[2m[36m(func pid=94182)[0m f1_per_class: [0.517, 0.401, 0.32, 0.544, 0.1, 0.228, 0.529, 0.236, 0.194, 0.254]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3675373134328358
[2m[36m(func pid=94112)[0m top5: 0.8689365671641791
[2m[36m(func pid=94112)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=94112)[0m f1_macro: 0.34622056732754924
[2m[36m(func pid=94112)[0m f1_weighted: 0.38942400555858375
[2m[36m(func pid=94112)[0m f1_per_class: [0.447, 0.437, 0.611, 0.525, 0.071, 0.213, 0.347, 0.269, 0.215, 0.327]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0000 | Steps: 2 | Val loss: 9.4383 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.0200 | Steps: 2 | Val loss: 2.0005 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:29:18 (running for 00:46:10.57)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.018 |      0.346 |                   76 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.337 |                   82 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.43236940298507465
[2m[36m(func pid=94182)[0m top5: 0.9020522388059702
[2m[36m(func pid=94182)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=94182)[0m f1_macro: 0.33697148058817716
[2m[36m(func pid=94182)[0m f1_weighted: 0.445360314545831
[2m[36m(func pid=94182)[0m f1_per_class: [0.504, 0.382, 0.32, 0.552, 0.131, 0.249, 0.539, 0.239, 0.198, 0.256]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3619402985074627
[2m[36m(func pid=94112)[0m top5: 0.8717350746268657
[2m[36m(func pid=94112)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=94112)[0m f1_macro: 0.3448352240527749
[2m[36m(func pid=94112)[0m f1_weighted: 0.38324876715700595
[2m[36m(func pid=94112)[0m f1_per_class: [0.442, 0.423, 0.649, 0.504, 0.073, 0.214, 0.354, 0.269, 0.22, 0.302]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0014 | Steps: 2 | Val loss: 9.8263 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.0241 | Steps: 2 | Val loss: 1.9989 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=94182)[0m top1: 0.4216417910447761
[2m[36m(func pid=94182)[0m top5: 0.8969216417910447
[2m[36m(func pid=94182)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=94182)[0m f1_macro: 0.3269325797503887
[2m[36m(func pid=94182)[0m f1_weighted: 0.4325488772536238
[2m[36m(func pid=94182)[0m f1_per_class: [0.5, 0.329, 0.312, 0.55, 0.137, 0.256, 0.532, 0.208, 0.205, 0.241]
== Status ==
Current time: 2024-01-07 01:29:23 (running for 00:46:15.91)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.02  |      0.345 |                   77 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.001 |      0.327 |                   83 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.36100746268656714
[2m[36m(func pid=94112)[0m top5: 0.8722014925373134
[2m[36m(func pid=94112)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=94112)[0m f1_macro: 0.33816503035438966
[2m[36m(func pid=94112)[0m f1_weighted: 0.3831507861964413
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.424, 0.611, 0.506, 0.075, 0.214, 0.353, 0.271, 0.214, 0.269]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.1016 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.0160 | Steps: 2 | Val loss: 2.0120 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 01:29:29 (running for 00:46:21.33)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.024 |      0.338 |                   78 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.322 |                   84 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.41324626865671643
[2m[36m(func pid=94182)[0m top5: 0.8899253731343284
[2m[36m(func pid=94182)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=94182)[0m f1_macro: 0.32202658106196663
[2m[36m(func pid=94182)[0m f1_weighted: 0.4239162500435169
[2m[36m(func pid=94182)[0m f1_per_class: [0.491, 0.305, 0.312, 0.543, 0.146, 0.258, 0.526, 0.196, 0.193, 0.25]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.36380597014925375
[2m[36m(func pid=94112)[0m top5: 0.8703358208955224
[2m[36m(func pid=94112)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=94112)[0m f1_macro: 0.34321517467710494
[2m[36m(func pid=94112)[0m f1_weighted: 0.3871027106769805
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.426, 0.629, 0.525, 0.081, 0.224, 0.344, 0.265, 0.215, 0.274]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.3303 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.0166 | Steps: 2 | Val loss: 2.0252 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=94182)[0m top1: 0.41091417910447764
[2m[36m(func pid=94182)[0m top5: 0.8871268656716418
[2m[36m(func pid=94182)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=94182)[0m f1_macro: 0.3189410877882981
[2m[36m(func pid=94182)[0m f1_weighted: 0.4225056590920169
[2m[36m(func pid=94182)[0m f1_per_class: [0.482, 0.289, 0.316, 0.545, 0.126, 0.253, 0.528, 0.218, 0.188, 0.244]
[2m[36m(func pid=94182)[0m 
== Status ==
Current time: 2024-01-07 01:29:36 (running for 00:46:28.33)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.017 |      0.341 |                   80 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.319 |                   85 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.8694029850746269
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.34102685432139956
[2m[36m(func pid=94112)[0m f1_weighted: 0.3836462216613868
[2m[36m(func pid=94112)[0m f1_per_class: [0.462, 0.413, 0.629, 0.523, 0.08, 0.237, 0.34, 0.247, 0.208, 0.271]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0000 | Steps: 2 | Val loss: 10.6456 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=94182)[0m top1: 0.408115671641791
[2m[36m(func pid=94182)[0m top5: 0.8819962686567164
[2m[36m(func pid=94182)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=94182)[0m f1_macro: 0.3150500050773267
[2m[36m(func pid=94182)[0m f1_weighted: 0.4189737649018097
[2m[36m(func pid=94182)[0m f1_per_class: [0.455, 0.278, 0.32, 0.546, 0.145, 0.26, 0.525, 0.196, 0.194, 0.233]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.0101 | Steps: 2 | Val loss: 2.0348 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 01:29:41 (running for 00:46:34.17)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.01  |      0.335 |                   81 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.315 |                   86 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3558768656716418
[2m[36m(func pid=94112)[0m top5: 0.867070895522388
[2m[36m(func pid=94112)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=94112)[0m f1_macro: 0.3351456137361787
[2m[36m(func pid=94112)[0m f1_weighted: 0.3778639859097667
[2m[36m(func pid=94112)[0m f1_per_class: [0.453, 0.414, 0.595, 0.523, 0.081, 0.244, 0.319, 0.247, 0.202, 0.274]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0017 | Steps: 2 | Val loss: 10.8046 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=94182)[0m top1: 0.40718283582089554
[2m[36m(func pid=94182)[0m top5: 0.8768656716417911
[2m[36m(func pid=94182)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=94182)[0m f1_macro: 0.31592763704329385
[2m[36m(func pid=94182)[0m f1_weighted: 0.4182627088306997
[2m[36m(func pid=94182)[0m f1_per_class: [0.464, 0.268, 0.324, 0.548, 0.142, 0.264, 0.524, 0.196, 0.195, 0.234]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0156 | Steps: 2 | Val loss: 2.0220 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.0646 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 01:29:47 (running for 00:46:40.08)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.337 |                   82 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0.002 |      0.316 |                   87 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3582089552238806
[2m[36m(func pid=94112)[0m top5: 0.8680037313432836
[2m[36m(func pid=94112)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=94112)[0m f1_macro: 0.33738888909136533
[2m[36m(func pid=94112)[0m f1_weighted: 0.3796343662517779
[2m[36m(func pid=94112)[0m f1_per_class: [0.45, 0.417, 0.6, 0.527, 0.073, 0.245, 0.319, 0.244, 0.211, 0.288]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.40904850746268656
[2m[36m(func pid=94182)[0m top5: 0.8722014925373134
[2m[36m(func pid=94182)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=94182)[0m f1_macro: 0.319364365435659
[2m[36m(func pid=94182)[0m f1_weighted: 0.42041821345980873
[2m[36m(func pid=94182)[0m f1_per_class: [0.464, 0.254, 0.329, 0.554, 0.142, 0.265, 0.526, 0.229, 0.196, 0.234]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.0132 | Steps: 2 | Val loss: 2.0205 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.1614 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=94112)[0m top1: 0.36007462686567165
[2m[36m(func pid=94112)[0m top5: 0.8694029850746269
[2m[36m(func pid=94112)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=94112)[0m f1_macro: 0.33689673967141276
[2m[36m(func pid=94112)[0m f1_weighted: 0.381124889053097
[2m[36m(func pid=94112)[0m f1_per_class: [0.447, 0.415, 0.585, 0.525, 0.077, 0.245, 0.324, 0.262, 0.21, 0.278]
== Status ==
Current time: 2024-01-07 01:29:53 (running for 00:46:45.74)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.013 |      0.337 |                   83 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.319 |                   88 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.40578358208955223
[2m[36m(func pid=94182)[0m top5: 0.8708022388059702
[2m[36m(func pid=94182)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=94182)[0m f1_macro: 0.31628167126728035
[2m[36m(func pid=94182)[0m f1_weighted: 0.4163511616568541
[2m[36m(func pid=94182)[0m f1_per_class: [0.464, 0.248, 0.333, 0.553, 0.143, 0.272, 0.519, 0.206, 0.193, 0.231]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.0216 | Steps: 2 | Val loss: 2.0037 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0001 | Steps: 2 | Val loss: 11.3014 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=94112)[0m top1: 0.36240671641791045
[2m[36m(func pid=94112)[0m top5: 0.8698694029850746
[2m[36m(func pid=94112)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=94112)[0m f1_macro: 0.33757878485821047
[2m[36m(func pid=94112)[0m f1_weighted: 0.38446812651792006
[2m[36m(func pid=94112)[0m f1_per_class: [0.44, 0.41, 0.6, 0.53, 0.079, 0.246, 0.334, 0.258, 0.211, 0.267]
== Status ==
Current time: 2024-01-07 01:29:59 (running for 00:46:51.34)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.022 |      0.338 |                   84 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.316 |                   89 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.40205223880597013
[2m[36m(func pid=94182)[0m top5: 0.8661380597014925
[2m[36m(func pid=94182)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=94182)[0m f1_macro: 0.30927760980229213
[2m[36m(func pid=94182)[0m f1_weighted: 0.4106385428550374
[2m[36m(func pid=94182)[0m f1_per_class: [0.45, 0.218, 0.324, 0.553, 0.133, 0.275, 0.519, 0.197, 0.195, 0.227]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.0115 | Steps: 2 | Val loss: 2.0232 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0001 | Steps: 2 | Val loss: 11.4174 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:30:04 (running for 00:46:56.88)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.011 |      0.337 |                   85 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.309 |                   90 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.35774253731343286
[2m[36m(func pid=94112)[0m top5: 0.8684701492537313
[2m[36m(func pid=94112)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=94112)[0m f1_macro: 0.33699405208571626
[2m[36m(func pid=94112)[0m f1_weighted: 0.37842745475905953
[2m[36m(func pid=94112)[0m f1_per_class: [0.431, 0.408, 0.615, 0.527, 0.078, 0.23, 0.321, 0.271, 0.215, 0.274]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.4001865671641791
[2m[36m(func pid=94182)[0m top5: 0.8624067164179104
[2m[36m(func pid=94182)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=94182)[0m f1_macro: 0.3093598092776887
[2m[36m(func pid=94182)[0m f1_weighted: 0.4088200722699593
[2m[36m(func pid=94182)[0m f1_per_class: [0.463, 0.208, 0.32, 0.555, 0.132, 0.275, 0.514, 0.209, 0.193, 0.224]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.0147 | Steps: 2 | Val loss: 2.0354 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.4153 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 01:30:10 (running for 00:47:02.39)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.338 |                   86 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.309 |                   91 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.8703358208955224
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.33825852635751485
[2m[36m(func pid=94112)[0m f1_weighted: 0.38011680351540333
[2m[36m(func pid=94112)[0m f1_per_class: [0.456, 0.406, 0.595, 0.528, 0.08, 0.253, 0.318, 0.274, 0.207, 0.267]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.40111940298507465
[2m[36m(func pid=94182)[0m top5: 0.8614738805970149
[2m[36m(func pid=94182)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=94182)[0m f1_macro: 0.3119603044353727
[2m[36m(func pid=94182)[0m f1_weighted: 0.40943573091670454
[2m[36m(func pid=94182)[0m f1_per_class: [0.468, 0.209, 0.333, 0.554, 0.144, 0.27, 0.518, 0.206, 0.192, 0.224]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.0105 | Steps: 2 | Val loss: 2.0616 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.5243 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 01:30:15 (running for 00:47:07.89)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.338 |                   86 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.312 |                   92 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.35261194029850745
[2m[36m(func pid=94112)[0m top5: 0.8661380597014925
[2m[36m(func pid=94112)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=94112)[0m f1_macro: 0.3309582346565275
[2m[36m(func pid=94112)[0m f1_weighted: 0.37339824024917206
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.406, 0.564, 0.523, 0.078, 0.235, 0.308, 0.278, 0.199, 0.274]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.3983208955223881
[2m[36m(func pid=94182)[0m top5: 0.8591417910447762
[2m[36m(func pid=94182)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.3097320959021338
[2m[36m(func pid=94182)[0m f1_weighted: 0.40722123207432226
[2m[36m(func pid=94182)[0m f1_per_class: [0.473, 0.21, 0.343, 0.554, 0.13, 0.267, 0.513, 0.205, 0.186, 0.217]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0251 | Steps: 2 | Val loss: 2.0863 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.4679 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 01:30:21 (running for 00:47:13.46)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.01  |      0.331 |                   87 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.31  |                   93 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.35401119402985076
[2m[36m(func pid=94112)[0m top5: 0.863339552238806
[2m[36m(func pid=94112)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=94112)[0m f1_macro: 0.3370828560742096
[2m[36m(func pid=94112)[0m f1_weighted: 0.3733010099878067
[2m[36m(func pid=94112)[0m f1_per_class: [0.44, 0.398, 0.615, 0.529, 0.077, 0.237, 0.303, 0.286, 0.206, 0.278]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m top1: 0.40298507462686567
[2m[36m(func pid=94182)[0m top5: 0.8619402985074627
[2m[36m(func pid=94182)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=94182)[0m f1_macro: 0.30845456872131943
[2m[36m(func pid=94182)[0m f1_weighted: 0.41130442084657926
[2m[36m(func pid=94182)[0m f1_per_class: [0.459, 0.21, 0.329, 0.56, 0.128, 0.268, 0.521, 0.206, 0.192, 0.212]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.5045 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.0156 | Steps: 2 | Val loss: 2.0711 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 01:30:26 (running for 00:47:18.78)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.025 |      0.337 |                   88 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.308 |                   94 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.4025186567164179
[2m[36m(func pid=94182)[0m top5: 0.8610074626865671
[2m[36m(func pid=94182)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=94182)[0m f1_macro: 0.31236769772919293
[2m[36m(func pid=94182)[0m f1_weighted: 0.41137120126776067
[2m[36m(func pid=94182)[0m f1_per_class: [0.468, 0.21, 0.333, 0.557, 0.131, 0.269, 0.519, 0.228, 0.187, 0.22]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3582089552238806
[2m[36m(func pid=94112)[0m top5: 0.8652052238805971
[2m[36m(func pid=94112)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=94112)[0m f1_macro: 0.34298569053183475
[2m[36m(func pid=94112)[0m f1_weighted: 0.37743274460828946
[2m[36m(func pid=94112)[0m f1_per_class: [0.438, 0.416, 0.632, 0.528, 0.077, 0.23, 0.307, 0.294, 0.21, 0.299]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.5378 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.0163 | Steps: 2 | Val loss: 2.0850 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 01:30:32 (running for 00:47:24.37)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.343 |                   89 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.309 |                   96 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.39925373134328357
[2m[36m(func pid=94182)[0m top5: 0.8610074626865671
[2m[36m(func pid=94182)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=94182)[0m f1_macro: 0.308815866873159
[2m[36m(func pid=94182)[0m f1_weighted: 0.4090869379611504
[2m[36m(func pid=94182)[0m f1_per_class: [0.467, 0.211, 0.32, 0.556, 0.13, 0.263, 0.515, 0.226, 0.186, 0.214]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3521455223880597
[2m[36m(func pid=94112)[0m top5: 0.8656716417910447
[2m[36m(func pid=94112)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=94112)[0m f1_macro: 0.33938474160259113
[2m[36m(func pid=94112)[0m f1_weighted: 0.37306096334412875
[2m[36m(func pid=94112)[0m f1_per_class: [0.433, 0.399, 0.632, 0.521, 0.074, 0.223, 0.313, 0.292, 0.204, 0.305]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.5475 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.0154 | Steps: 2 | Val loss: 2.0739 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 01:30:37 (running for 00:47:29.59)
Memory usage on this node: 19.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.016 |      0.339 |                   90 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.309 |                   97 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.40205223880597013
[2m[36m(func pid=94182)[0m top5: 0.8605410447761194
[2m[36m(func pid=94182)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=94182)[0m f1_macro: 0.3086648789413082
[2m[36m(func pid=94182)[0m f1_weighted: 0.4119861642319725
[2m[36m(func pid=94182)[0m f1_per_class: [0.463, 0.212, 0.312, 0.562, 0.128, 0.261, 0.518, 0.238, 0.185, 0.209]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3530783582089552
[2m[36m(func pid=94112)[0m top5: 0.8675373134328358
[2m[36m(func pid=94112)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=94112)[0m f1_macro: 0.33966206648197705
[2m[36m(func pid=94112)[0m f1_weighted: 0.37280445195031203
[2m[36m(func pid=94112)[0m f1_per_class: [0.438, 0.411, 0.611, 0.518, 0.074, 0.219, 0.308, 0.291, 0.209, 0.317]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.6806 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0115 | Steps: 2 | Val loss: 2.0842 | Batch size: 32 | lr: 0.01 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 01:30:42 (running for 00:47:34.80)
Memory usage on this node: 18.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.34  |                   91 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.31  |                   98 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.39972014925373134
[2m[36m(func pid=94182)[0m top5: 0.8586753731343284
[2m[36m(func pid=94182)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=94182)[0m f1_macro: 0.30986431361669786
[2m[36m(func pid=94182)[0m f1_weighted: 0.40885329336617665
[2m[36m(func pid=94182)[0m f1_per_class: [0.467, 0.204, 0.32, 0.558, 0.127, 0.266, 0.513, 0.238, 0.186, 0.22]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.35447761194029853
[2m[36m(func pid=94112)[0m top5: 0.863339552238806
[2m[36m(func pid=94112)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=94112)[0m f1_macro: 0.3427367169840731
[2m[36m(func pid=94112)[0m f1_weighted: 0.3734370204525466
[2m[36m(func pid=94112)[0m f1_per_class: [0.443, 0.413, 0.649, 0.52, 0.073, 0.216, 0.308, 0.292, 0.206, 0.308]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.7117 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0118 | Steps: 2 | Val loss: 2.0915 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:30:47 (running for 00:47:40.01)
Memory usage on this node: 18.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3465
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.011 |      0.343 |                   92 |
| train_57e67_00023 | RUNNING    | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.312 |                   99 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94182)[0m top1: 0.3987873134328358
[2m[36m(func pid=94182)[0m top5: 0.8582089552238806
[2m[36m(func pid=94182)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=94182)[0m f1_macro: 0.3115585545820713
[2m[36m(func pid=94182)[0m f1_weighted: 0.40838682219025013
[2m[36m(func pid=94182)[0m f1_per_class: [0.467, 0.2, 0.338, 0.561, 0.14, 0.262, 0.512, 0.238, 0.185, 0.214]
[2m[36m(func pid=94182)[0m 
[2m[36m(func pid=94112)[0m top1: 0.3516791044776119
[2m[36m(func pid=94112)[0m top5: 0.8628731343283582
[2m[36m(func pid=94112)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=94112)[0m f1_macro: 0.34314440199014734
[2m[36m(func pid=94112)[0m f1_weighted: 0.37105478660592545
[2m[36m(func pid=94112)[0m f1_per_class: [0.44, 0.416, 0.667, 0.511, 0.071, 0.214, 0.307, 0.292, 0.212, 0.302]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94182)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0000 | Steps: 2 | Val loss: 11.6990 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=94182)[0m top1: 0.3983208955223881
[2m[36m(func pid=94182)[0m top5: 0.8600746268656716
[2m[36m(func pid=94182)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=94182)[0m f1_macro: 0.3078105618568058
[2m[36m(func pid=94182)[0m f1_weighted: 0.4076481252376892
[2m[36m(func pid=94182)[0m f1_per_class: [0.472, 0.2, 0.316, 0.566, 0.125, 0.266, 0.505, 0.226, 0.188, 0.214]
== Status ==
Current time: 2024-01-07 01:30:53 (running for 00:47:45.49)
Memory usage on this node: 18.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.012 |      0.343 |                   93 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0193 | Steps: 2 | Val loss: 2.0785 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=94112)[0m top1: 0.35494402985074625
[2m[36m(func pid=94112)[0m top5: 0.8619402985074627
[2m[36m(func pid=94112)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=94112)[0m f1_macro: 0.34170849877621584
[2m[36m(func pid=94112)[0m f1_weighted: 0.37312631599899365
[2m[36m(func pid=94112)[0m f1_per_class: [0.447, 0.416, 0.629, 0.518, 0.083, 0.224, 0.304, 0.293, 0.211, 0.294]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.0125 | Steps: 2 | Val loss: 2.0634 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 01:31:01 (running for 00:47:53.29)
Memory usage on this node: 16.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.019 |      0.342 |                   94 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.8642723880597015
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.345138332805572
[2m[36m(func pid=94112)[0m f1_weighted: 0.3794163424821268
[2m[36m(func pid=94112)[0m f1_per_class: [0.444, 0.417, 0.629, 0.523, 0.082, 0.231, 0.318, 0.289, 0.209, 0.311]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.0099 | Steps: 2 | Val loss: 2.0463 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 01:31:06 (running for 00:47:58.88)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.012 |      0.345 |                   95 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3605410447761194
[2m[36m(func pid=94112)[0m top5: 0.8684701492537313
[2m[36m(func pid=94112)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=94112)[0m f1_macro: 0.3464452285529171
[2m[36m(func pid=94112)[0m f1_weighted: 0.3819513918348127
[2m[36m(func pid=94112)[0m f1_per_class: [0.439, 0.416, 0.629, 0.523, 0.08, 0.216, 0.333, 0.28, 0.217, 0.333]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.0233 | Steps: 2 | Val loss: 2.0398 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 01:31:12 (running for 00:48:04.31)
Memory usage on this node: 16.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.01  |      0.346 |                   96 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.36380597014925375
[2m[36m(func pid=94112)[0m top5: 0.8684701492537313
[2m[36m(func pid=94112)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=94112)[0m f1_macro: 0.34979677535697207
[2m[36m(func pid=94112)[0m f1_weighted: 0.3850806997601953
[2m[36m(func pid=94112)[0m f1_per_class: [0.434, 0.425, 0.629, 0.521, 0.079, 0.212, 0.339, 0.282, 0.216, 0.36]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.0153 | Steps: 2 | Val loss: 2.0466 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 01:31:17 (running for 00:48:09.98)
Memory usage on this node: 16.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.023 |      0.35  |                   97 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.36100746268656714
[2m[36m(func pid=94112)[0m top5: 0.8684701492537313
[2m[36m(func pid=94112)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=94112)[0m f1_macro: 0.3520626871689922
[2m[36m(func pid=94112)[0m f1_weighted: 0.3825329584516529
[2m[36m(func pid=94112)[0m f1_per_class: [0.431, 0.421, 0.649, 0.52, 0.076, 0.215, 0.335, 0.278, 0.206, 0.39]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0134 | Steps: 2 | Val loss: 2.0450 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 01:31:23 (running for 00:48:15.58)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.015 |      0.352 |                   98 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=94112)[0m top1: 0.3596082089552239
[2m[36m(func pid=94112)[0m top5: 0.8698694029850746
[2m[36m(func pid=94112)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=94112)[0m f1_macro: 0.35272828407234375
[2m[36m(func pid=94112)[0m f1_weighted: 0.3803178071375215
[2m[36m(func pid=94112)[0m f1_per_class: [0.431, 0.425, 0.649, 0.517, 0.073, 0.216, 0.327, 0.27, 0.215, 0.405]
[2m[36m(func pid=94112)[0m 
[2m[36m(func pid=94112)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.0116 | Steps: 2 | Val loss: 2.0611 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 01:31:29 (running for 00:48:21.33)
Memory usage on this node: 16.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3465
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00022 | RUNNING    | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.013 |      0.353 |                   99 |
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 01:31:29 (running for 00:48:21.87)
Memory usage on this node: 15.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.3465
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.28 GiB heap, 0.0/55.54 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_57e67_00000 | TERMINATED | 192.168.7.53:183140 | 0.0001 |       0.99 |         0      |  0.462 |      0.327 |                   75 |
| train_57e67_00001 | TERMINATED | 192.168.7.53:183515 | 0.001  |       0.99 |         0      |  0.003 |      0.363 |                  100 |
| train_57e67_00002 | TERMINATED | 192.168.7.53:183934 | 0.01   |       0.99 |         0      |  0     |      0.411 |                  100 |
| train_57e67_00003 | TERMINATED | 192.168.7.53:184353 | 0.1    |       0.99 |         0      |  0.368 |      0.29  |                   75 |
| train_57e67_00004 | TERMINATED | 192.168.7.53:12986  | 0.0001 |       0.9  |         0      |  2.109 |      0.141 |                   75 |
| train_57e67_00005 | TERMINATED | 192.168.7.53:13733  | 0.001  |       0.9  |         0      |  0.265 |      0.32  |                   75 |
| train_57e67_00006 | TERMINATED | 192.168.7.53:18554  | 0.01   |       0.9  |         0      |  0.027 |      0.344 |                   75 |
| train_57e67_00007 | TERMINATED | 192.168.7.53:19406  | 0.1    |       0.9  |         0      |  0     |      0.223 |                   75 |
| train_57e67_00008 | TERMINATED | 192.168.7.53:30734  | 0.0001 |       0.99 |         0.0001 |  0.539 |      0.334 |                   75 |
| train_57e67_00009 | TERMINATED | 192.168.7.53:31420  | 0.001  |       0.99 |         0.0001 |  0.002 |      0.381 |                  100 |
| train_57e67_00010 | TERMINATED | 192.168.7.53:36270  | 0.01   |       0.99 |         0.0001 |  0     |      0.407 |                  100 |
| train_57e67_00011 | TERMINATED | 192.168.7.53:36355  | 0.1    |       0.99 |         0.0001 |  0     |      0.284 |                   75 |
| train_57e67_00012 | TERMINATED | 192.168.7.53:48412  | 0.0001 |       0.9  |         0.0001 |  2.107 |      0.143 |                   75 |
| train_57e67_00013 | TERMINATED | 192.168.7.53:53465  | 0.001  |       0.9  |         0.0001 |  0.31  |      0.322 |                   75 |
| train_57e67_00014 | TERMINATED | 192.168.7.53:53984  | 0.01   |       0.9  |         0.0001 |  0.018 |      0.337 |                   75 |
| train_57e67_00015 | TERMINATED | 192.168.7.53:59563  | 0.1    |       0.9  |         0.0001 |  0     |      0.277 |                   75 |
| train_57e67_00016 | TERMINATED | 192.168.7.53:66105  | 0.0001 |       0.99 |         1e-05  |  0.526 |      0.322 |                   75 |
| train_57e67_00017 | TERMINATED | 192.168.7.53:70967  | 0.001  |       0.99 |         1e-05  |  0.003 |      0.383 |                  100 |
| train_57e67_00018 | TERMINATED | 192.168.7.53:72016  | 0.01   |       0.99 |         1e-05  |  0     |      0.288 |                   75 |
| train_57e67_00019 | TERMINATED | 192.168.7.53:76915  | 0.1    |       0.99 |         1e-05  |  2.098 |      0.276 |                   75 |
| train_57e67_00020 | TERMINATED | 192.168.7.53:83527  | 0.0001 |       0.9  |         1e-05  |  2.156 |      0.137 |                   75 |
| train_57e67_00021 | TERMINATED | 192.168.7.53:89821  | 0.001  |       0.9  |         1e-05  |  0.284 |      0.32  |                   75 |
| train_57e67_00022 | TERMINATED | 192.168.7.53:94112  | 0.01   |       0.9  |         1e-05  |  0.012 |      0.349 |                  100 |
| train_57e67_00023 | TERMINATED | 192.168.7.53:94182  | 0.1    |       0.9  |         1e-05  |  0     |      0.308 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 01:31:29,623	INFO tune.py:798 -- Total run time: 2902.75 seconds (2901.85 seconds for the tuning loop).
[2m[36m(func pid=94112)[0m top1: 0.35027985074626866
[2m[36m(func pid=94112)[0m top5: 0.8656716417910447
[2m[36m(func pid=94112)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=94112)[0m f1_macro: 0.3487553023873588
[2m[36m(func pid=94112)[0m f1_weighted: 0.3695958989042018
[2m[36m(func pid=94112)[0m f1_per_class: [0.431, 0.417, 0.649, 0.494, 0.076, 0.201, 0.322, 0.271, 0.222, 0.405]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341334.1 ON aap04 CANCELLED AT 2024-01-07T01:31:37 ***
srun: error: aap04: task 0: Exited with exit code 1

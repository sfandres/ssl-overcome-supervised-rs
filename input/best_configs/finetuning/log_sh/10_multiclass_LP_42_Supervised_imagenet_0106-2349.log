IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 11:06:55,357	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 11:06:55,357	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 11:06:57,889	SUCC scripts.py:747 -- --------------------
2024-01-07 11:06:57,889	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 11:06:57,889	SUCC scripts.py:749 -- --------------------
2024-01-07 11:06:57,889	INFO scripts.py:751 -- Next steps
2024-01-07 11:06:57,889	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 11:06:57,889	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 11:06:57,890	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 11:06:57,890	INFO scripts.py:773 -- import ray
2024-01-07 11:06:57,890	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 11:06:57,890	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 11:06:57,890	INFO scripts.py:791 --   ray status
2024-01-07 11:06:57,890	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 11:06:57,890	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 11:06:57,890	INFO scripts.py:810 --   ray stop
2024-01-07 11:06:57,891	INFO scripts.py:891 -- --block
2024-01-07 11:06:57,891	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 11:06:57,891	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              6492264528998816908
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7fd7718d7190>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         imagenet
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.91
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!
Using ImageNet weights

Supervised model resnet18 with imagenet weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 11:07:41,523	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 11:07:41,533	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 11:08:04,493	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 11:08:04 (running for 00:00:22.10)
Memory usage on this node: 13.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |
| train_98a10_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_98a10_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_98a10_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=83673)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=83673)[0m Configuration completed!
[2m[36m(func pid=83673)[0m New optimizer parameters:
[2m[36m(func pid=83673)[0m SGD (
[2m[36m(func pid=83673)[0m Parameter Group 0
[2m[36m(func pid=83673)[0m     dampening: 0
[2m[36m(func pid=83673)[0m     differentiable: False
[2m[36m(func pid=83673)[0m     foreach: None
[2m[36m(func pid=83673)[0m     lr: 0.0001
[2m[36m(func pid=83673)[0m     maximize: False
[2m[36m(func pid=83673)[0m     momentum: 0.99
[2m[36m(func pid=83673)[0m     nesterov: False
[2m[36m(func pid=83673)[0m     weight_decay: 0
[2m[36m(func pid=83673)[0m )
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0346 | Steps: 4 | Val loss: 2.5387 | Batch size: 32 | lr: 0.0001 | Duration: 4.84s
[2m[36m(func pid=83673)[0m top1: 0.060167910447761194
[2m[36m(func pid=83673)[0m top5: 0.4832089552238806
[2m[36m(func pid=83673)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=83673)[0m f1_macro: 0.028803948082707892
[2m[36m(func pid=83673)[0m f1_weighted: 0.03365056320044726
[2m[36m(func pid=83673)[0m f1_per_class: [0.0, 0.015, 0.0, 0.079, 0.0, 0.019, 0.0, 0.098, 0.022, 0.056]
== Status ==
Current time: 2024-01-07 11:08:15 (running for 00:00:32.75)
Memory usage on this node: 15.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |
| train_98a10_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_98a10_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84053)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84053)[0m Configuration completed!
[2m[36m(func pid=84053)[0m New optimizer parameters:
[2m[36m(func pid=84053)[0m SGD (
[2m[36m(func pid=84053)[0m Parameter Group 0
[2m[36m(func pid=84053)[0m     dampening: 0
[2m[36m(func pid=84053)[0m     differentiable: False
[2m[36m(func pid=84053)[0m     foreach: None
[2m[36m(func pid=84053)[0m     lr: 0.001
[2m[36m(func pid=84053)[0m     maximize: False
[2m[36m(func pid=84053)[0m     momentum: 0.99
[2m[36m(func pid=84053)[0m     nesterov: False
[2m[36m(func pid=84053)[0m     weight_decay: 0
[2m[36m(func pid=84053)[0m )
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0868 | Steps: 4 | Val loss: 2.4525 | Batch size: 32 | lr: 0.001 | Duration: 4.95s
[2m[36m(func pid=84053)[0m top1: 0.06949626865671642
[2m[36m(func pid=84053)[0m top5: 0.4878731343283582
[2m[36m(func pid=84053)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=84053)[0m f1_macro: 0.04865895669653454
[2m[36m(func pid=84053)[0m f1_weighted: 0.05057109972109999
[2m[36m(func pid=84053)[0m f1_per_class: [0.115, 0.033, 0.0, 0.116, 0.0, 0.017, 0.0, 0.097, 0.059, 0.049]
== Status ==
Current time: 2024-01-07 11:08:24 (running for 00:00:41.76)
Memory usage on this node: 18.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |
| train_98a10_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84475)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84475)[0m Configuration completed!
[2m[36m(func pid=84475)[0m New optimizer parameters:
[2m[36m(func pid=84475)[0m SGD (
[2m[36m(func pid=84475)[0m Parameter Group 0
[2m[36m(func pid=84475)[0m     dampening: 0
[2m[36m(func pid=84475)[0m     differentiable: False
[2m[36m(func pid=84475)[0m     foreach: None
[2m[36m(func pid=84475)[0m     lr: 0.01
[2m[36m(func pid=84475)[0m     maximize: False
[2m[36m(func pid=84475)[0m     momentum: 0.99
[2m[36m(func pid=84475)[0m     nesterov: False
[2m[36m(func pid=84475)[0m     weight_decay: 0
[2m[36m(func pid=84475)[0m )
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0057 | Steps: 4 | Val loss: 2.5870 | Batch size: 32 | lr: 0.01 | Duration: 4.60s
[2m[36m(func pid=84475)[0m top1: 0.05970149253731343
[2m[36m(func pid=84475)[0m top5: 0.3894589552238806
[2m[36m(func pid=84475)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=84475)[0m f1_macro: 0.0573441735304832
[2m[36m(func pid=84475)[0m f1_weighted: 0.048715937068222706
[2m[36m(func pid=84475)[0m f1_per_class: [0.137, 0.0, 0.133, 0.0, 0.025, 0.013, 0.133, 0.0, 0.111, 0.022]
== Status ==
Current time: 2024-01-07 11:08:32 (running for 00:00:50.30)
Memory usage on this node: 20.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 11:08:41 (running for 00:00:58.55)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |        |            |                      |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.006 |      0.057 |                    1 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |        |            |                      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84902)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84902)[0m Configuration completed!
[2m[36m(func pid=84902)[0m New optimizer parameters:
[2m[36m(func pid=84902)[0m SGD (
[2m[36m(func pid=84902)[0m Parameter Group 0
[2m[36m(func pid=84902)[0m     dampening: 0
[2m[36m(func pid=84902)[0m     differentiable: False
[2m[36m(func pid=84902)[0m     foreach: None
[2m[36m(func pid=84902)[0m     lr: 0.1
[2m[36m(func pid=84902)[0m     maximize: False
[2m[36m(func pid=84902)[0m     momentum: 0.99
[2m[36m(func pid=84902)[0m     nesterov: False
[2m[36m(func pid=84902)[0m     weight_decay: 0
[2m[36m(func pid=84902)[0m )
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7544 | Steps: 4 | Val loss: 2.1315 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1171 | Steps: 4 | Val loss: 2.5591 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9840 | Steps: 4 | Val loss: 2.4012 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.2958 | Steps: 4 | Val loss: 12.6686 | Batch size: 32 | lr: 0.1 | Duration: 4.62s
== Status ==
Current time: 2024-01-07 11:08:46 (running for 00:01:03.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  3.035 |      0.029 |                    1 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  3.087 |      0.049 |                    1 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.006 |      0.057 |                    1 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |        |            |                      |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.17537313432835822
[2m[36m(func pid=84475)[0m top5: 0.7290111940298507
[2m[36m(func pid=84475)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=84475)[0m f1_macro: 0.1429765143397686
[2m[36m(func pid=84475)[0m f1_weighted: 0.16976146974381212
[2m[36m(func pid=84475)[0m f1_per_class: [0.447, 0.13, 0.117, 0.045, 0.057, 0.122, 0.362, 0.0, 0.037, 0.111]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.05830223880597015
[2m[36m(func pid=83673)[0m top5: 0.4743470149253731
[2m[36m(func pid=83673)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=83673)[0m f1_macro: 0.032254973331053004
[2m[36m(func pid=83673)[0m f1_weighted: 0.03913811932046012
[2m[36m(func pid=83673)[0m f1_per_class: [0.052, 0.038, 0.0, 0.085, 0.0, 0.019, 0.0, 0.089, 0.0, 0.039]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m top1: 0.07509328358208955
[2m[36m(func pid=84053)[0m top5: 0.47294776119402987
[2m[36m(func pid=84053)[0m f1_micro: 0.07509328358208955
[2m[36m(func pid=84053)[0m f1_macro: 0.063143790738758
[2m[36m(func pid=84053)[0m f1_weighted: 0.06882404980491101
[2m[36m(func pid=84053)[0m f1_per_class: [0.12, 0.122, 0.06, 0.054, 0.0, 0.026, 0.065, 0.097, 0.045, 0.042]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.03311567164179104
[2m[36m(func pid=84902)[0m top5: 0.6385261194029851
[2m[36m(func pid=84902)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=84902)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=84902)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=84902)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3069 | Steps: 4 | Val loss: 1.7870 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7089 | Steps: 4 | Val loss: 2.2913 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0327 | Steps: 4 | Val loss: 2.5288 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 30.8531 | Steps: 4 | Val loss: 37.3678 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:08:51 (running for 00:01:09.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  3.117 |      0.032 |                    2 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.984 |      0.063 |                    2 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.307 |      0.297 |                    3 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  5.296 |      0.006 |                    1 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.37593283582089554
[2m[36m(func pid=84475)[0m top5: 0.8465485074626866
[2m[36m(func pid=84475)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=84475)[0m f1_macro: 0.29651252244788745
[2m[36m(func pid=84475)[0m f1_weighted: 0.30749629756643193
[2m[36m(func pid=84475)[0m f1_per_class: [0.611, 0.551, 0.595, 0.536, 0.114, 0.128, 0.028, 0.403, 0.0, 0.0]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.1599813432835821
[2m[36m(func pid=84053)[0m top5: 0.6063432835820896
[2m[36m(func pid=84053)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=84053)[0m f1_macro: 0.07349060832102337
[2m[36m(func pid=84053)[0m f1_weighted: 0.13719183392106854
[2m[36m(func pid=84053)[0m f1_per_class: [0.08, 0.227, 0.041, 0.007, 0.0, 0.014, 0.304, 0.0, 0.061, 0.0]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.0708955223880597
[2m[36m(func pid=83673)[0m top5: 0.470615671641791
[2m[36m(func pid=83673)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=83673)[0m f1_macro: 0.0398812225539935
[2m[36m(func pid=83673)[0m f1_weighted: 0.06400103953183543
[2m[36m(func pid=83673)[0m f1_per_class: [0.042, 0.093, 0.0, 0.145, 0.0, 0.013, 0.0, 0.084, 0.0, 0.022]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.05830223880597015
[2m[36m(func pid=84902)[0m top5: 0.3805970149253731
[2m[36m(func pid=84902)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=84902)[0m f1_macro: 0.028091151099990337
[2m[36m(func pid=84902)[0m f1_weighted: 0.014690973810244038
[2m[36m(func pid=84902)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.247, 0.0, 0.034]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2979 | Steps: 4 | Val loss: 2.9549 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8906 | Steps: 4 | Val loss: 2.2681 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9983 | Steps: 4 | Val loss: 2.4755 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 28.1058 | Steps: 4 | Val loss: 26.8793 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:08:57 (running for 00:01:14.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  3.033 |      0.04  |                    3 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.709 |      0.073 |                    3 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.298 |      0.225 |                    4 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 30.853 |      0.028 |                    2 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.19029850746268656
[2m[36m(func pid=84475)[0m top5: 0.6618470149253731
[2m[36m(func pid=84475)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=84475)[0m f1_macro: 0.2246642434865715
[2m[36m(func pid=84475)[0m f1_weighted: 0.17216854646538804
[2m[36m(func pid=84475)[0m f1_per_class: [0.313, 0.11, 0.632, 0.411, 0.059, 0.008, 0.009, 0.302, 0.104, 0.3]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.18003731343283583
[2m[36m(func pid=84053)[0m top5: 0.6403917910447762
[2m[36m(func pid=84053)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=84053)[0m f1_macro: 0.07241307965918474
[2m[36m(func pid=84053)[0m f1_weighted: 0.1257219549727729
[2m[36m(func pid=84053)[0m f1_per_class: [0.107, 0.293, 0.06, 0.007, 0.0, 0.0, 0.235, 0.0, 0.022, 0.0]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.0960820895522388
[2m[36m(func pid=83673)[0m top5: 0.4766791044776119
[2m[36m(func pid=83673)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=83673)[0m f1_macro: 0.05773229606392964
[2m[36m(func pid=83673)[0m f1_weighted: 0.0924047958656673
[2m[36m(func pid=83673)[0m f1_per_class: [0.039, 0.14, 0.0, 0.209, 0.0, 0.018, 0.0, 0.084, 0.059, 0.029]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2416044776119403
[2m[36m(func pid=84902)[0m top5: 0.5111940298507462
[2m[36m(func pid=84902)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=84902)[0m f1_macro: 0.12135174394798513
[2m[36m(func pid=84902)[0m f1_weighted: 0.16772322125699393
[2m[36m(func pid=84902)[0m f1_per_class: [0.333, 0.0, 0.044, 0.47, 0.0, 0.235, 0.0, 0.0, 0.055, 0.077]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.2386 | Steps: 4 | Val loss: 3.0468 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8027 | Steps: 4 | Val loss: 2.1936 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9821 | Steps: 4 | Val loss: 2.4325 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 24.7576 | Steps: 4 | Val loss: 36.4320 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:09:02 (running for 00:01:19.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.998 |      0.058 |                    4 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.891 |      0.072 |                    4 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.239 |      0.193 |                    5 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 28.106 |      0.121 |                    3 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.197294776119403
[2m[36m(func pid=84475)[0m top5: 0.7845149253731343
[2m[36m(func pid=84475)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=84475)[0m f1_macro: 0.1934094366677618
[2m[36m(func pid=84475)[0m f1_weighted: 0.19981978535135725
[2m[36m(func pid=84475)[0m f1_per_class: [0.557, 0.173, 0.197, 0.141, 0.182, 0.239, 0.288, 0.0, 0.087, 0.07]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.177705223880597
[2m[36m(func pid=84053)[0m top5: 0.6986940298507462
[2m[36m(func pid=84053)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=84053)[0m f1_macro: 0.13147239267680294
[2m[36m(func pid=84053)[0m f1_weighted: 0.15176559256734537
[2m[36m(func pid=84053)[0m f1_per_class: [0.278, 0.301, 0.313, 0.016, 0.0, 0.054, 0.264, 0.0, 0.088, 0.0]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.11707089552238806
[2m[36m(func pid=83673)[0m top5: 0.4878731343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=83673)[0m f1_macro: 0.06443569801046098
[2m[36m(func pid=83673)[0m f1_weighted: 0.10996993221388081
[2m[36m(func pid=83673)[0m f1_per_class: [0.045, 0.183, 0.0, 0.238, 0.0, 0.028, 0.006, 0.083, 0.033, 0.029]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.12220149253731344
[2m[36m(func pid=84902)[0m top5: 0.617070895522388
[2m[36m(func pid=84902)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=84902)[0m f1_macro: 0.22754216471132857
[2m[36m(func pid=84902)[0m f1_weighted: 0.1620680097819356
[2m[36m(func pid=84902)[0m f1_per_class: [0.415, 0.0, 0.815, 0.324, 0.02, 0.0, 0.161, 0.0, 0.173, 0.367]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.2027 | Steps: 4 | Val loss: 3.4693 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.1481 | Steps: 4 | Val loss: 2.1587 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8973 | Steps: 4 | Val loss: 2.4103 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 36.5617 | Steps: 4 | Val loss: 26.9208 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:09:07 (running for 00:01:25.14)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.982 |      0.064 |                    5 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.803 |      0.131 |                    5 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.203 |      0.275 |                    6 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 24.758 |      0.228 |                    4 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31949626865671643
[2m[36m(func pid=84475)[0m top5: 0.8610074626865671
[2m[36m(func pid=84475)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=84475)[0m f1_macro: 0.27495046295796305
[2m[36m(func pid=84475)[0m f1_weighted: 0.27833053832085264
[2m[36m(func pid=84475)[0m f1_per_class: [0.611, 0.591, 0.126, 0.222, 0.308, 0.294, 0.205, 0.0, 0.0, 0.394]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.15625
[2m[36m(func pid=84053)[0m top5: 0.7602611940298507
[2m[36m(func pid=84053)[0m f1_micro: 0.15625
[2m[36m(func pid=84053)[0m f1_macro: 0.19986429705174805
[2m[36m(func pid=84053)[0m f1_weighted: 0.13323130556314708
[2m[36m(func pid=84053)[0m f1_per_class: [0.228, 0.071, 0.818, 0.156, 0.087, 0.222, 0.114, 0.046, 0.112, 0.145]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.11707089552238806
[2m[36m(func pid=83673)[0m top5: 0.48367537313432835
[2m[36m(func pid=83673)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=83673)[0m f1_macro: 0.06646010117607161
[2m[36m(func pid=83673)[0m f1_weighted: 0.10861938102693683
[2m[36m(func pid=83673)[0m f1_per_class: [0.05, 0.198, 0.0, 0.218, 0.0, 0.035, 0.006, 0.088, 0.04, 0.029]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.27098880597014924
[2m[36m(func pid=84902)[0m top5: 0.8269589552238806
[2m[36m(func pid=84902)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=84902)[0m f1_macro: 0.28032463459705503
[2m[36m(func pid=84902)[0m f1_weighted: 0.2246505783078728
[2m[36m(func pid=84902)[0m f1_per_class: [0.393, 0.343, 0.815, 0.111, 0.146, 0.06, 0.336, 0.0, 0.314, 0.286]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4462 | Steps: 4 | Val loss: 2.9364 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.1259 | Steps: 4 | Val loss: 2.1928 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8271 | Steps: 4 | Val loss: 2.3654 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=84475)[0m top1: 0.41697761194029853
[2m[36m(func pid=84475)[0m top5: 0.8717350746268657
[2m[36m(func pid=84475)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=84475)[0m f1_macro: 0.3427703059989677
[2m[36m(func pid=84475)[0m f1_weighted: 0.3817181476970115
[2m[36m(func pid=84475)[0m f1_per_class: [0.658, 0.607, 0.206, 0.526, 0.188, 0.188, 0.256, 0.076, 0.177, 0.545]
== Status ==
Current time: 2024-01-07 11:09:12 (running for 00:01:30.35)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.897 |      0.066 |                    6 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.148 |      0.2   |                    6 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.446 |      0.343 |                    7 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 36.562 |      0.28  |                    5 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 22.6089 | Steps: 4 | Val loss: 31.4518 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84053)[0m top1: 0.17537313432835822
[2m[36m(func pid=84053)[0m top5: 0.7131529850746269
[2m[36m(func pid=84053)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=84053)[0m f1_macro: 0.19363557675090545
[2m[36m(func pid=84053)[0m f1_weighted: 0.13331077137865283
[2m[36m(func pid=84053)[0m f1_per_class: [0.288, 0.0, 0.556, 0.272, 0.175, 0.24, 0.012, 0.184, 0.128, 0.081]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.1259328358208955
[2m[36m(func pid=83673)[0m top5: 0.5009328358208955
[2m[36m(func pid=83673)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=83673)[0m f1_macro: 0.06341397788645718
[2m[36m(func pid=83673)[0m f1_weighted: 0.1062528499767025
[2m[36m(func pid=83673)[0m f1_per_class: [0.04, 0.247, 0.025, 0.172, 0.0, 0.025, 0.024, 0.057, 0.044, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.31296641791044777
[2m[36m(func pid=84902)[0m top5: 0.8628731343283582
[2m[36m(func pid=84902)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=84902)[0m f1_macro: 0.25626118882973997
[2m[36m(func pid=84902)[0m f1_weighted: 0.28637701186611814
[2m[36m(func pid=84902)[0m f1_per_class: [0.586, 0.329, 0.255, 0.553, 0.32, 0.291, 0.07, 0.0, 0.157, 0.0]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.1171 | Steps: 4 | Val loss: 4.6542 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.0120 | Steps: 4 | Val loss: 2.2079 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7747 | Steps: 4 | Val loss: 2.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:09:18 (running for 00:01:35.50)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.827 |      0.063 |                    7 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.126 |      0.194 |                    7 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.117 |      0.288 |                    8 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 22.609 |      0.256 |                    6 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.25513059701492535
[2m[36m(func pid=84475)[0m top5: 0.8456156716417911
[2m[36m(func pid=84475)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=84475)[0m f1_macro: 0.28822261161180884
[2m[36m(func pid=84475)[0m f1_weighted: 0.2679958536648052
[2m[36m(func pid=84475)[0m f1_per_class: [0.675, 0.026, 0.423, 0.534, 0.174, 0.104, 0.187, 0.4, 0.1, 0.26]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 46.4503 | Steps: 4 | Val loss: 34.7824 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=84053)[0m top1: 0.20988805970149255
[2m[36m(func pid=84053)[0m top5: 0.6875
[2m[36m(func pid=84053)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=84053)[0m f1_macro: 0.24443179275453972
[2m[36m(func pid=84053)[0m f1_weighted: 0.19252116705282707
[2m[36m(func pid=84053)[0m f1_per_class: [0.354, 0.0, 0.818, 0.46, 0.102, 0.197, 0.015, 0.365, 0.071, 0.062]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.1357276119402985
[2m[36m(func pid=83673)[0m top5: 0.5256529850746269
[2m[36m(func pid=83673)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=83673)[0m f1_macro: 0.07198653050781725
[2m[36m(func pid=83673)[0m f1_weighted: 0.09718612113830026
[2m[36m(func pid=83673)[0m f1_per_class: [0.065, 0.266, 0.087, 0.082, 0.0, 0.029, 0.059, 0.052, 0.079, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3572761194029851
[2m[36m(func pid=84902)[0m top5: 0.847481343283582
[2m[36m(func pid=84902)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=84902)[0m f1_macro: 0.24288952780513157
[2m[36m(func pid=84902)[0m f1_weighted: 0.3052860626483566
[2m[36m(func pid=84902)[0m f1_per_class: [0.478, 0.005, 0.094, 0.598, 0.308, 0.303, 0.221, 0.422, 0.0, 0.0]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.2648 | Steps: 4 | Val loss: 8.5142 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.7319 | Steps: 4 | Val loss: 2.1797 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7694 | Steps: 4 | Val loss: 2.3303 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:09:23 (running for 00:01:40.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.775 |      0.072 |                    8 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  2.012 |      0.244 |                    8 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.265 |      0.201 |                    9 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 46.45  |      0.243 |                    7 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.10914179104477612
[2m[36m(func pid=84475)[0m top5: 0.6823694029850746
[2m[36m(func pid=84475)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=84475)[0m f1_macro: 0.2008718514355095
[2m[36m(func pid=84475)[0m f1_weighted: 0.07817462021166849
[2m[36m(func pid=84475)[0m f1_per_class: [0.619, 0.021, 0.571, 0.049, 0.128, 0.066, 0.045, 0.274, 0.194, 0.042]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 23.8234 | Steps: 4 | Val loss: 60.3163 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84053)[0m top1: 0.23367537313432835
[2m[36m(func pid=84053)[0m top5: 0.6968283582089553
[2m[36m(func pid=84053)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=84053)[0m f1_macro: 0.21883073295477257
[2m[36m(func pid=84053)[0m f1_weighted: 0.18951219474354475
[2m[36m(func pid=84053)[0m f1_per_class: [0.253, 0.011, 0.786, 0.522, 0.082, 0.045, 0.018, 0.3, 0.071, 0.1]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.14039179104477612
[2m[36m(func pid=83673)[0m top5: 0.5443097014925373
[2m[36m(func pid=83673)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=83673)[0m f1_macro: 0.07212652228876168
[2m[36m(func pid=83673)[0m f1_weighted: 0.09186574587501213
[2m[36m(func pid=83673)[0m f1_per_class: [0.078, 0.274, 0.109, 0.025, 0.0, 0.034, 0.092, 0.028, 0.083, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.0984 | Steps: 4 | Val loss: 8.8189 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=84902)[0m top1: 0.26492537313432835
[2m[36m(func pid=84902)[0m top5: 0.7565298507462687
[2m[36m(func pid=84902)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=84902)[0m f1_macro: 0.17941080725900657
[2m[36m(func pid=84902)[0m f1_weighted: 0.2289280473557331
[2m[36m(func pid=84902)[0m f1_per_class: [0.327, 0.011, 0.202, 0.595, 0.087, 0.0, 0.112, 0.223, 0.161, 0.077]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.5302 | Steps: 4 | Val loss: 2.1106 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6993 | Steps: 4 | Val loss: 2.3035 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84475)[0m top1: 0.14832089552238806
[2m[36m(func pid=84475)[0m top5: 0.7131529850746269
[2m[36m(func pid=84475)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=84475)[0m f1_macro: 0.19941297369688074
[2m[36m(func pid=84475)[0m f1_weighted: 0.10132662587323148
[2m[36m(func pid=84475)[0m f1_per_class: [0.486, 0.234, 0.585, 0.013, 0.119, 0.175, 0.031, 0.196, 0.028, 0.127]
== Status ==
Current time: 2024-01-07 11:09:28 (running for 00:01:45.64)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.769 |      0.072 |                    9 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.732 |      0.219 |                    9 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.098 |      0.199 |                   10 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 23.823 |      0.179 |                    8 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 43.9916 | Steps: 4 | Val loss: 88.9960 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84053)[0m top1: 0.24347014925373134
[2m[36m(func pid=84053)[0m top5: 0.7705223880597015
[2m[36m(func pid=84053)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=84053)[0m f1_macro: 0.2057066489017351
[2m[36m(func pid=84053)[0m f1_weighted: 0.20718920911828093
[2m[36m(func pid=84053)[0m f1_per_class: [0.36, 0.176, 0.436, 0.531, 0.097, 0.0, 0.006, 0.208, 0.056, 0.185]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.14225746268656717
[2m[36m(func pid=83673)[0m top5: 0.5657649253731343
[2m[36m(func pid=83673)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=83673)[0m f1_macro: 0.07426734097392897
[2m[36m(func pid=83673)[0m f1_weighted: 0.10012068879569619
[2m[36m(func pid=83673)[0m f1_per_class: [0.118, 0.274, 0.059, 0.013, 0.0, 0.043, 0.126, 0.015, 0.095, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.9589 | Steps: 4 | Val loss: 5.8491 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=84902)[0m top1: 0.1623134328358209
[2m[36m(func pid=84902)[0m top5: 0.8199626865671642
[2m[36m(func pid=84902)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=84902)[0m f1_macro: 0.2565435462415891
[2m[36m(func pid=84902)[0m f1_weighted: 0.15456476407638184
[2m[36m(func pid=84902)[0m f1_per_class: [0.507, 0.241, 0.545, 0.201, 0.349, 0.0, 0.078, 0.153, 0.126, 0.364]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.5815 | Steps: 4 | Val loss: 2.0013 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6653 | Steps: 4 | Val loss: 2.2868 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:09:33 (running for 00:01:50.88)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.699 |      0.074 |                   10 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.53  |      0.206 |                   10 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.959 |      0.289 |                   11 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 43.992 |      0.257 |                    9 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31529850746268656
[2m[36m(func pid=84475)[0m top5: 0.8754664179104478
[2m[36m(func pid=84475)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=84475)[0m f1_macro: 0.28887766756087324
[2m[36m(func pid=84475)[0m f1_weighted: 0.26690926227610984
[2m[36m(func pid=84475)[0m f1_per_class: [0.587, 0.562, 0.667, 0.187, 0.173, 0.224, 0.194, 0.294, 0.0, 0.0]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 38.2702 | Steps: 4 | Val loss: 82.0504 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=84053)[0m top1: 0.2751865671641791
[2m[36m(func pid=84053)[0m top5: 0.8073694029850746
[2m[36m(func pid=84053)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=84053)[0m f1_macro: 0.2518264952272455
[2m[36m(func pid=84053)[0m f1_weighted: 0.26410269746806875
[2m[36m(func pid=84053)[0m f1_per_class: [0.545, 0.403, 0.222, 0.498, 0.088, 0.0, 0.068, 0.246, 0.14, 0.308]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.15065298507462688
[2m[36m(func pid=83673)[0m top5: 0.5830223880597015
[2m[36m(func pid=83673)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=83673)[0m f1_macro: 0.08693466665519876
[2m[36m(func pid=83673)[0m f1_weighted: 0.12013266142332263
[2m[36m(func pid=83673)[0m f1_per_class: [0.146, 0.275, 0.09, 0.01, 0.0, 0.089, 0.181, 0.0, 0.08, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3805970149253731
[2m[36m(func pid=84902)[0m top5: 0.7504664179104478
[2m[36m(func pid=84902)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=84902)[0m f1_macro: 0.31779791254687095
[2m[36m(func pid=84902)[0m f1_weighted: 0.31882957556278135
[2m[36m(func pid=84902)[0m f1_per_class: [0.289, 0.395, 0.786, 0.068, 0.225, 0.0, 0.633, 0.374, 0.209, 0.2]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.0822 | Steps: 4 | Val loss: 3.6683 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0005 | Steps: 4 | Val loss: 1.8529 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:09:38 (running for 00:01:56.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.665 |      0.087 |                   11 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.581 |      0.252 |                   11 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.082 |      0.376 |                   12 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 38.27  |      0.318 |                   10 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.5149253731343284
[2m[36m(func pid=84475)[0m top5: 0.9435634328358209
[2m[36m(func pid=84475)[0m f1_micro: 0.5149253731343284
[2m[36m(func pid=84475)[0m f1_macro: 0.3759399764297698
[2m[36m(func pid=84475)[0m f1_weighted: 0.49809391657555285
[2m[36m(func pid=84475)[0m f1_per_class: [0.556, 0.617, 0.606, 0.547, 0.186, 0.207, 0.59, 0.35, 0.101, 0.0]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7464 | Steps: 4 | Val loss: 2.2778 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 50.0125 | Steps: 4 | Val loss: 105.9680 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84053)[0m top1: 0.34701492537313433
[2m[36m(func pid=84053)[0m top5: 0.8535447761194029
[2m[36m(func pid=84053)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=84053)[0m f1_macro: 0.2851382641731265
[2m[36m(func pid=84053)[0m f1_weighted: 0.37926018780279136
[2m[36m(func pid=84053)[0m f1_per_class: [0.523, 0.415, 0.233, 0.398, 0.126, 0.031, 0.52, 0.336, 0.122, 0.148]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.15625
[2m[36m(func pid=83673)[0m top5: 0.5918843283582089
[2m[36m(func pid=83673)[0m f1_micro: 0.15625
[2m[36m(func pid=83673)[0m f1_macro: 0.09546705401837177
[2m[36m(func pid=83673)[0m f1_weighted: 0.13874650753483905
[2m[36m(func pid=83673)[0m f1_per_class: [0.164, 0.271, 0.094, 0.006, 0.0, 0.101, 0.243, 0.0, 0.076, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.22527985074626866
[2m[36m(func pid=84902)[0m top5: 0.6935634328358209
[2m[36m(func pid=84902)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=84902)[0m f1_macro: 0.2002055094046154
[2m[36m(func pid=84902)[0m f1_weighted: 0.23256535481827018
[2m[36m(func pid=84902)[0m f1_per_class: [0.086, 0.377, 0.267, 0.029, 0.176, 0.0, 0.433, 0.303, 0.204, 0.128]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.3501 | Steps: 4 | Val loss: 5.5917 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.1468 | Steps: 4 | Val loss: 1.9137 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 11:09:44 (running for 00:02:01.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.746 |      0.095 |                   12 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.001 |      0.285 |                   12 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.35  |      0.324 |                   13 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 50.013 |      0.2   |                   11 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.44402985074626866
[2m[36m(func pid=84475)[0m top5: 0.9151119402985075
[2m[36m(func pid=84475)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=84475)[0m f1_macro: 0.3238989113796905
[2m[36m(func pid=84475)[0m f1_weighted: 0.3912409088722505
[2m[36m(func pid=84475)[0m f1_per_class: [0.591, 0.068, 0.511, 0.606, 0.3, 0.067, 0.536, 0.352, 0.134, 0.074]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6065 | Steps: 4 | Val loss: 2.2659 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 75.2254 | Steps: 4 | Val loss: 96.6749 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84053)[0m top1: 0.34468283582089554
[2m[36m(func pid=84053)[0m top5: 0.8722014925373134
[2m[36m(func pid=84053)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=84053)[0m f1_macro: 0.2531758310494824
[2m[36m(func pid=84053)[0m f1_weighted: 0.3534890150875814
[2m[36m(func pid=84053)[0m f1_per_class: [0.448, 0.409, 0.25, 0.25, 0.157, 0.031, 0.599, 0.268, 0.119, 0.0]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m top1: 0.15904850746268656
[2m[36m(func pid=83673)[0m top5: 0.6026119402985075
[2m[36m(func pid=83673)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=83673)[0m f1_macro: 0.10478476534991725
[2m[36m(func pid=83673)[0m f1_weighted: 0.1470682933180537
[2m[36m(func pid=83673)[0m f1_per_class: [0.19, 0.291, 0.113, 0.01, 0.0, 0.134, 0.242, 0.0, 0.067, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.3842 | Steps: 4 | Val loss: 10.4326 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=84902)[0m top1: 0.3306902985074627
[2m[36m(func pid=84902)[0m top5: 0.7010261194029851
[2m[36m(func pid=84902)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=84902)[0m f1_macro: 0.29160463285756555
[2m[36m(func pid=84902)[0m f1_weighted: 0.3059626861262388
[2m[36m(func pid=84902)[0m f1_per_class: [0.33, 0.436, 0.741, 0.493, 0.123, 0.0, 0.186, 0.33, 0.158, 0.119]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.0523 | Steps: 4 | Val loss: 1.7831 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:09:49 (running for 00:02:06.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.606 |      0.105 |                   13 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.147 |      0.253 |                   13 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  4.384 |      0.305 |                   14 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 75.225 |      0.292 |                   12 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3260261194029851
[2m[36m(func pid=84475)[0m top5: 0.8092350746268657
[2m[36m(func pid=84475)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=84475)[0m f1_macro: 0.3047124755550603
[2m[36m(func pid=84475)[0m f1_weighted: 0.253696142832602
[2m[36m(func pid=84475)[0m f1_per_class: [0.621, 0.0, 0.421, 0.524, 0.327, 0.058, 0.179, 0.357, 0.127, 0.435]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6270 | Steps: 4 | Val loss: 2.2963 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 43.4513 | Steps: 4 | Val loss: 117.1447 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=84053)[0m top1: 0.3987873134328358
[2m[36m(func pid=84053)[0m top5: 0.8908582089552238
[2m[36m(func pid=84053)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=84053)[0m f1_macro: 0.30870144323598464
[2m[36m(func pid=84053)[0m f1_weighted: 0.4058731326277182
[2m[36m(func pid=84053)[0m f1_per_class: [0.467, 0.496, 0.21, 0.328, 0.115, 0.157, 0.604, 0.166, 0.158, 0.387]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.8927 | Steps: 4 | Val loss: 11.8111 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=83673)[0m top1: 0.14085820895522388
[2m[36m(func pid=83673)[0m top5: 0.601679104477612
[2m[36m(func pid=83673)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=83673)[0m f1_macro: 0.10567135794912423
[2m[36m(func pid=83673)[0m f1_weighted: 0.1385339640563315
[2m[36m(func pid=83673)[0m f1_per_class: [0.155, 0.239, 0.18, 0.01, 0.019, 0.143, 0.24, 0.0, 0.071, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.34468283582089554
[2m[36m(func pid=84902)[0m top5: 0.7332089552238806
[2m[36m(func pid=84902)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=84902)[0m f1_macro: 0.3062997093687394
[2m[36m(func pid=84902)[0m f1_weighted: 0.2677697984960361
[2m[36m(func pid=84902)[0m f1_per_class: [0.563, 0.349, 0.667, 0.535, 0.065, 0.0, 0.04, 0.389, 0.125, 0.33]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.1421 | Steps: 4 | Val loss: 1.8049 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:09:54 (running for 00:02:11.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.627 |      0.106 |                   14 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.052 |      0.309 |                   14 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.893 |      0.219 |                   15 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 43.451 |      0.306 |                   13 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2733208955223881
[2m[36m(func pid=84475)[0m top5: 0.6268656716417911
[2m[36m(func pid=84475)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=84475)[0m f1_macro: 0.2188128491481917
[2m[36m(func pid=84475)[0m f1_weighted: 0.22508345368368937
[2m[36m(func pid=84475)[0m f1_per_class: [0.552, 0.0, 0.126, 0.533, 0.143, 0.088, 0.079, 0.41, 0.134, 0.123]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 33.7349 | Steps: 4 | Val loss: 99.2495 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5464 | Steps: 4 | Val loss: 2.2997 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84053)[0m top1: 0.35867537313432835
[2m[36m(func pid=84053)[0m top5: 0.8913246268656716
[2m[36m(func pid=84053)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=84053)[0m f1_macro: 0.3258567747964091
[2m[36m(func pid=84053)[0m f1_weighted: 0.37665684051937176
[2m[36m(func pid=84053)[0m f1_per_class: [0.59, 0.533, 0.159, 0.374, 0.133, 0.237, 0.385, 0.234, 0.185, 0.429]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3431 | Steps: 4 | Val loss: 14.7122 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84902)[0m top1: 0.2947761194029851
[2m[36m(func pid=84902)[0m top5: 0.6902985074626866
[2m[36m(func pid=84902)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=84902)[0m f1_macro: 0.2477440541851282
[2m[36m(func pid=84902)[0m f1_weighted: 0.26541798970822467
[2m[36m(func pid=84902)[0m f1_per_class: [0.429, 0.231, 0.093, 0.586, 0.056, 0.04, 0.045, 0.429, 0.138, 0.431]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.12779850746268656
[2m[36m(func pid=83673)[0m top5: 0.6124067164179104
[2m[36m(func pid=83673)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=83673)[0m f1_macro: 0.10161254456070172
[2m[36m(func pid=83673)[0m f1_weighted: 0.13126407076012356
[2m[36m(func pid=83673)[0m f1_per_class: [0.155, 0.205, 0.186, 0.016, 0.013, 0.13, 0.234, 0.0, 0.077, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7372 | Steps: 4 | Val loss: 1.8731 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:09:59 (running for 00:02:17.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.546 |      0.102 |                   15 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.142 |      0.326 |                   15 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.343 |      0.189 |                   16 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 33.735 |      0.248 |                   14 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.146455223880597
[2m[36m(func pid=84475)[0m top5: 0.613339552238806
[2m[36m(func pid=84475)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=84475)[0m f1_macro: 0.18878885651621508
[2m[36m(func pid=84475)[0m f1_weighted: 0.1687543748942551
[2m[36m(func pid=84475)[0m f1_per_class: [0.561, 0.042, 0.04, 0.377, 0.056, 0.045, 0.022, 0.402, 0.25, 0.094]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 16.3990 | Steps: 4 | Val loss: 133.7343 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.4397 | Steps: 4 | Val loss: 2.2753 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=84053)[0m top1: 0.3460820895522388
[2m[36m(func pid=84053)[0m top5: 0.8955223880597015
[2m[36m(func pid=84053)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=84053)[0m f1_macro: 0.31068148508019966
[2m[36m(func pid=84053)[0m f1_weighted: 0.3644529243685645
[2m[36m(func pid=84053)[0m f1_per_class: [0.624, 0.529, 0.191, 0.442, 0.146, 0.26, 0.276, 0.28, 0.127, 0.231]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.8639 | Steps: 4 | Val loss: 14.6571 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=84902)[0m top1: 0.125
[2m[36m(func pid=84902)[0m top5: 0.5918843283582089
[2m[36m(func pid=84902)[0m f1_micro: 0.125
[2m[36m(func pid=84902)[0m f1_macro: 0.18861927560385552
[2m[36m(func pid=84902)[0m f1_weighted: 0.12112408814103563
[2m[36m(func pid=84902)[0m f1_per_class: [0.375, 0.207, 0.035, 0.061, 0.122, 0.214, 0.012, 0.421, 0.096, 0.343]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.12686567164179105
[2m[36m(func pid=83673)[0m top5: 0.6338619402985075
[2m[36m(func pid=83673)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=83673)[0m f1_macro: 0.10392231377515752
[2m[36m(func pid=83673)[0m f1_weighted: 0.13036341553500702
[2m[36m(func pid=83673)[0m f1_per_class: [0.187, 0.151, 0.18, 0.029, 0.041, 0.13, 0.247, 0.0, 0.074, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7209 | Steps: 4 | Val loss: 1.8708 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=84475)[0m top1: 0.17490671641791045
[2m[36m(func pid=84475)[0m top5: 0.6347947761194029
[2m[36m(func pid=84475)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=84475)[0m f1_macro: 0.21687763051416611
[2m[36m(func pid=84475)[0m f1_weighted: 0.21104497550769072
[2m[36m(func pid=84475)[0m f1_per_class: [0.517, 0.384, 0.061, 0.307, 0.036, 0.058, 0.031, 0.429, 0.156, 0.19]
== Status ==
Current time: 2024-01-07 11:10:04 (running for 00:02:22.35)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.44  |      0.104 |                   16 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.737 |      0.311 |                   16 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.864 |      0.217 |                   17 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 16.399 |      0.189 |                   15 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 26.1009 | Steps: 4 | Val loss: 168.2299 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4160 | Steps: 4 | Val loss: 2.2440 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=84053)[0m top1: 0.3474813432835821
[2m[36m(func pid=84053)[0m top5: 0.9053171641791045
[2m[36m(func pid=84053)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=84053)[0m f1_macro: 0.30048786877555944
[2m[36m(func pid=84053)[0m f1_weighted: 0.3680669344996475
[2m[36m(func pid=84053)[0m f1_per_class: [0.493, 0.459, 0.239, 0.517, 0.162, 0.263, 0.25, 0.408, 0.064, 0.151]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7600 | Steps: 4 | Val loss: 12.7068 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=84902)[0m top1: 0.15951492537313433
[2m[36m(func pid=84902)[0m top5: 0.613339552238806
[2m[36m(func pid=84902)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=84902)[0m f1_macro: 0.2143792375227976
[2m[36m(func pid=84902)[0m f1_weighted: 0.12012371967191388
[2m[36m(func pid=84902)[0m f1_per_class: [0.421, 0.264, 0.094, 0.007, 0.239, 0.24, 0.018, 0.361, 0.099, 0.4]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.13526119402985073
[2m[36m(func pid=83673)[0m top5: 0.659981343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=83673)[0m f1_macro: 0.11052255647603179
[2m[36m(func pid=83673)[0m f1_weighted: 0.14772902336132593
[2m[36m(func pid=83673)[0m f1_per_class: [0.22, 0.143, 0.156, 0.089, 0.048, 0.125, 0.254, 0.0, 0.07, 0.0]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.7168 | Steps: 4 | Val loss: 1.9096 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:10:10 (running for 00:02:27.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.416 |      0.111 |                   17 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.721 |      0.3   |                   17 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.76  |      0.249 |                   18 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 26.101 |      0.214 |                   16 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2513992537313433
[2m[36m(func pid=84475)[0m top5: 0.6902985074626866
[2m[36m(func pid=84475)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=84475)[0m f1_macro: 0.2488507131870342
[2m[36m(func pid=84475)[0m f1_weighted: 0.23111367820280498
[2m[36m(func pid=84475)[0m f1_per_class: [0.393, 0.512, 0.162, 0.233, 0.054, 0.113, 0.071, 0.406, 0.222, 0.322]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 33.5910 | Steps: 4 | Val loss: 149.2733 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2642 | Steps: 4 | Val loss: 2.1967 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84053)[0m top1: 0.35074626865671643
[2m[36m(func pid=84053)[0m top5: 0.898320895522388
[2m[36m(func pid=84053)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=84053)[0m f1_macro: 0.2919048778621044
[2m[36m(func pid=84053)[0m f1_weighted: 0.35437817988358566
[2m[36m(func pid=84053)[0m f1_per_class: [0.446, 0.326, 0.286, 0.579, 0.182, 0.269, 0.226, 0.377, 0.095, 0.134]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8907 | Steps: 4 | Val loss: 11.6408 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=84902)[0m top1: 0.21641791044776118
[2m[36m(func pid=84902)[0m top5: 0.6693097014925373
[2m[36m(func pid=84902)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=84902)[0m f1_macro: 0.28055410152237475
[2m[36m(func pid=84902)[0m f1_weighted: 0.17865323949270598
[2m[36m(func pid=84902)[0m f1_per_class: [0.553, 0.389, 0.276, 0.01, 0.258, 0.24, 0.124, 0.336, 0.142, 0.478]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.16044776119402984
[2m[36m(func pid=83673)[0m top5: 0.6833022388059702
[2m[36m(func pid=83673)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=83673)[0m f1_macro: 0.12452112097962717
[2m[36m(func pid=83673)[0m f1_weighted: 0.1771774907859759
[2m[36m(func pid=83673)[0m f1_per_class: [0.202, 0.126, 0.153, 0.156, 0.046, 0.139, 0.294, 0.0, 0.073, 0.057]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:15 (running for 00:02:32.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.264 |      0.125 |                   18 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.717 |      0.292 |                   18 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.891 |      0.298 |                   19 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 33.591 |      0.281 |                   17 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3765 | Steps: 4 | Val loss: 1.9062 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=84475)[0m top1: 0.30363805970149255
[2m[36m(func pid=84475)[0m top5: 0.8092350746268657
[2m[36m(func pid=84475)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=84475)[0m f1_macro: 0.2975894596213779
[2m[36m(func pid=84475)[0m f1_weighted: 0.25364262383683706
[2m[36m(func pid=84475)[0m f1_per_class: [0.448, 0.451, 0.386, 0.271, 0.089, 0.185, 0.103, 0.423, 0.22, 0.4]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 23.7656 | Steps: 4 | Val loss: 108.6582 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1847 | Steps: 4 | Val loss: 2.1620 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84053)[0m top1: 0.36380597014925375
[2m[36m(func pid=84053)[0m top5: 0.8959888059701493
[2m[36m(func pid=84053)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=84053)[0m f1_macro: 0.3209414264685474
[2m[36m(func pid=84053)[0m f1_weighted: 0.36231577721887165
[2m[36m(func pid=84053)[0m f1_per_class: [0.48, 0.342, 0.462, 0.598, 0.188, 0.239, 0.227, 0.345, 0.175, 0.154]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4749 | Steps: 4 | Val loss: 9.8876 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=84902)[0m top1: 0.28078358208955223
[2m[36m(func pid=84902)[0m top5: 0.7248134328358209
[2m[36m(func pid=84902)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=84902)[0m f1_macro: 0.3166951912842633
[2m[36m(func pid=84902)[0m f1_weighted: 0.24944271289080922
[2m[36m(func pid=84902)[0m f1_per_class: [0.537, 0.494, 0.489, 0.046, 0.214, 0.252, 0.27, 0.284, 0.175, 0.407]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.18330223880597016
[2m[36m(func pid=83673)[0m top5: 0.7047574626865671
[2m[36m(func pid=83673)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=83673)[0m f1_macro: 0.1397219066693949
[2m[36m(func pid=83673)[0m f1_weighted: 0.2064536179387936
[2m[36m(func pid=83673)[0m f1_per_class: [0.219, 0.104, 0.14, 0.255, 0.048, 0.133, 0.311, 0.0, 0.076, 0.111]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:20 (running for 00:02:37.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.185 |      0.14  |                   19 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.377 |      0.321 |                   19 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.475 |      0.325 |                   20 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 23.766 |      0.317 |                   18 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3493470149253731
[2m[36m(func pid=84475)[0m top5: 0.8857276119402985
[2m[36m(func pid=84475)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=84475)[0m f1_macro: 0.3252863310397118
[2m[36m(func pid=84475)[0m f1_weighted: 0.3183276860322291
[2m[36m(func pid=84475)[0m f1_per_class: [0.433, 0.461, 0.595, 0.346, 0.096, 0.25, 0.239, 0.335, 0.205, 0.294]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3303 | Steps: 4 | Val loss: 2.1141 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 13.4184 | Steps: 4 | Val loss: 102.2348 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2234 | Steps: 4 | Val loss: 2.1372 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=84053)[0m top1: 0.3278917910447761
[2m[36m(func pid=84053)[0m top5: 0.882929104477612
[2m[36m(func pid=84053)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=84053)[0m f1_macro: 0.32008978316907805
[2m[36m(func pid=84053)[0m f1_weighted: 0.3189920392345755
[2m[36m(func pid=84053)[0m f1_per_class: [0.471, 0.359, 0.6, 0.568, 0.229, 0.164, 0.138, 0.269, 0.162, 0.24]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6555 | Steps: 4 | Val loss: 6.4680 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=84902)[0m top1: 0.33115671641791045
[2m[36m(func pid=84902)[0m top5: 0.746268656716418
[2m[36m(func pid=84902)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=84902)[0m f1_macro: 0.3544995328985253
[2m[36m(func pid=84902)[0m f1_weighted: 0.3056086671575481
[2m[36m(func pid=84902)[0m f1_per_class: [0.552, 0.517, 0.72, 0.112, 0.278, 0.105, 0.437, 0.232, 0.23, 0.361]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.19962686567164178
[2m[36m(func pid=83673)[0m top5: 0.7285447761194029
[2m[36m(func pid=83673)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=83673)[0m f1_macro: 0.15312052398476156
[2m[36m(func pid=83673)[0m f1_weighted: 0.22735918284882706
[2m[36m(func pid=83673)[0m f1_per_class: [0.243, 0.103, 0.175, 0.329, 0.045, 0.12, 0.311, 0.015, 0.092, 0.098]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:25 (running for 00:02:43.27)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.223 |      0.153 |                   20 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.33  |      0.32  |                   20 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.656 |      0.395 |                   21 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 13.418 |      0.354 |                   19 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.4533582089552239
[2m[36m(func pid=84475)[0m top5: 0.9393656716417911
[2m[36m(func pid=84475)[0m f1_micro: 0.4533582089552239
[2m[36m(func pid=84475)[0m f1_macro: 0.3953325401520335
[2m[36m(func pid=84475)[0m f1_weighted: 0.46422084929730695
[2m[36m(func pid=84475)[0m f1_per_class: [0.347, 0.568, 0.643, 0.554, 0.215, 0.276, 0.47, 0.281, 0.199, 0.4]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.6285 | Steps: 4 | Val loss: 2.2267 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 17.8097 | Steps: 4 | Val loss: 121.4955 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0551 | Steps: 4 | Val loss: 2.1323 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84053)[0m top1: 0.32369402985074625
[2m[36m(func pid=84053)[0m top5: 0.8903917910447762
[2m[36m(func pid=84053)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=84053)[0m f1_macro: 0.3450420556057135
[2m[36m(func pid=84053)[0m f1_weighted: 0.31714372885282177
[2m[36m(func pid=84053)[0m f1_per_class: [0.481, 0.358, 0.667, 0.559, 0.273, 0.108, 0.153, 0.262, 0.164, 0.425]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7898 | Steps: 4 | Val loss: 7.6841 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84902)[0m top1: 0.30783582089552236
[2m[36m(func pid=84902)[0m top5: 0.7873134328358209
[2m[36m(func pid=84902)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=84902)[0m f1_macro: 0.33845223700400473
[2m[36m(func pid=84902)[0m f1_weighted: 0.30761981270256367
[2m[36m(func pid=84902)[0m f1_per_class: [0.586, 0.561, 0.6, 0.264, 0.275, 0.008, 0.321, 0.191, 0.261, 0.319]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.20055970149253732
[2m[36m(func pid=83673)[0m top5: 0.7164179104477612
[2m[36m(func pid=83673)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=83673)[0m f1_macro: 0.15593457689586443
[2m[36m(func pid=83673)[0m f1_weighted: 0.23338932066774382
[2m[36m(func pid=83673)[0m f1_per_class: [0.21, 0.094, 0.155, 0.372, 0.041, 0.088, 0.288, 0.14, 0.09, 0.081]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:31 (running for 00:02:48.65)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.055 |      0.156 |                   21 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.629 |      0.345 |                   21 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.79  |      0.378 |                   22 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 17.81  |      0.338 |                   20 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.4239738805970149
[2m[36m(func pid=84475)[0m top5: 0.917910447761194
[2m[36m(func pid=84475)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=84475)[0m f1_macro: 0.37770996598329687
[2m[36m(func pid=84475)[0m f1_weighted: 0.42855232757624634
[2m[36m(func pid=84475)[0m f1_per_class: [0.364, 0.246, 0.667, 0.592, 0.261, 0.279, 0.495, 0.303, 0.195, 0.376]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5488 | Steps: 4 | Val loss: 2.2728 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 23.3668 | Steps: 4 | Val loss: 123.6174 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.9518 | Steps: 4 | Val loss: 2.1049 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=84053)[0m top1: 0.32322761194029853
[2m[36m(func pid=84053)[0m top5: 0.8875932835820896
[2m[36m(func pid=84053)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=84053)[0m f1_macro: 0.3544468467942573
[2m[36m(func pid=84053)[0m f1_weighted: 0.3382132236801846
[2m[36m(func pid=84053)[0m f1_per_class: [0.486, 0.392, 0.667, 0.495, 0.231, 0.092, 0.266, 0.284, 0.161, 0.471]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3879 | Steps: 4 | Val loss: 11.0286 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=84902)[0m top1: 0.33348880597014924
[2m[36m(func pid=84902)[0m top5: 0.8185634328358209
[2m[36m(func pid=84902)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=84902)[0m f1_macro: 0.3202875161386776
[2m[36m(func pid=84902)[0m f1_weighted: 0.35261737369133395
[2m[36m(func pid=84902)[0m f1_per_class: [0.571, 0.56, 0.444, 0.428, 0.214, 0.008, 0.334, 0.195, 0.178, 0.27]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.22667910447761194
[2m[36m(func pid=83673)[0m top5: 0.7350746268656716
[2m[36m(func pid=83673)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=83673)[0m f1_macro: 0.17498268849048412
[2m[36m(func pid=83673)[0m f1_weighted: 0.24899343159123602
[2m[36m(func pid=83673)[0m f1_per_class: [0.248, 0.083, 0.222, 0.444, 0.046, 0.055, 0.271, 0.237, 0.07, 0.074]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:36 (running for 00:02:54.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.952 |      0.175 |                   22 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.549 |      0.354 |                   22 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.388 |      0.336 |                   23 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 23.367 |      0.32  |                   21 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.36427238805970147
[2m[36m(func pid=84475)[0m top5: 0.8558768656716418
[2m[36m(func pid=84475)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=84475)[0m f1_macro: 0.33605437921101433
[2m[36m(func pid=84475)[0m f1_weighted: 0.3498483347235133
[2m[36m(func pid=84475)[0m f1_per_class: [0.503, 0.032, 0.571, 0.59, 0.276, 0.254, 0.367, 0.279, 0.177, 0.311]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4328 | Steps: 4 | Val loss: 2.1379 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 39.1577 | Steps: 4 | Val loss: 108.7887 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0023 | Steps: 4 | Val loss: 2.0652 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=84053)[0m top1: 0.37033582089552236
[2m[36m(func pid=84053)[0m top5: 0.8913246268656716
[2m[36m(func pid=84053)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=84053)[0m f1_macro: 0.38337901927403567
[2m[36m(func pid=84053)[0m f1_weighted: 0.39273108381969085
[2m[36m(func pid=84053)[0m f1_per_class: [0.529, 0.49, 0.647, 0.466, 0.142, 0.099, 0.405, 0.307, 0.184, 0.564]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.9336 | Steps: 4 | Val loss: 14.2042 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=84902)[0m top1: 0.3917910447761194
[2m[36m(func pid=84902)[0m top5: 0.8213619402985075
[2m[36m(func pid=84902)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=84902)[0m f1_macro: 0.35984827092581917
[2m[36m(func pid=84902)[0m f1_weighted: 0.4061065425265631
[2m[36m(func pid=84902)[0m f1_per_class: [0.597, 0.536, 0.636, 0.49, 0.226, 0.016, 0.455, 0.226, 0.184, 0.232]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.24860074626865672
[2m[36m(func pid=83673)[0m top5: 0.7672574626865671
[2m[36m(func pid=83673)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=83673)[0m f1_macro: 0.19282772244199642
[2m[36m(func pid=83673)[0m f1_weighted: 0.2602951199874672
[2m[36m(func pid=83673)[0m f1_per_class: [0.292, 0.083, 0.289, 0.503, 0.055, 0.059, 0.24, 0.289, 0.048, 0.071]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:41 (running for 00:02:59.21)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.002 |      0.193 |                   23 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.433 |      0.383 |                   23 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.934 |      0.293 |                   24 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 39.158 |      0.36  |                   22 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31576492537313433
[2m[36m(func pid=84475)[0m top5: 0.804570895522388
[2m[36m(func pid=84475)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=84475)[0m f1_macro: 0.2932832192580518
[2m[36m(func pid=84475)[0m f1_weighted: 0.2997117796418018
[2m[36m(func pid=84475)[0m f1_per_class: [0.555, 0.005, 0.333, 0.587, 0.276, 0.176, 0.256, 0.238, 0.174, 0.333]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4176 | Steps: 4 | Val loss: 2.0840 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 13.4967 | Steps: 4 | Val loss: 91.7816 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0363 | Steps: 4 | Val loss: 2.0622 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=84053)[0m top1: 0.39225746268656714
[2m[36m(func pid=84053)[0m top5: 0.898320895522388
[2m[36m(func pid=84053)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=84053)[0m f1_macro: 0.3873116025213554
[2m[36m(func pid=84053)[0m f1_weighted: 0.4179848419235175
[2m[36m(func pid=84053)[0m f1_per_class: [0.583, 0.515, 0.615, 0.441, 0.104, 0.122, 0.486, 0.324, 0.195, 0.486]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 5.6310 | Steps: 4 | Val loss: 14.4815 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=84902)[0m top1: 0.42257462686567165
[2m[36m(func pid=84902)[0m top5: 0.8283582089552238
[2m[36m(func pid=84902)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=84902)[0m f1_macro: 0.3837878625207293
[2m[36m(func pid=84902)[0m f1_weighted: 0.4323108859489575
[2m[36m(func pid=84902)[0m f1_per_class: [0.527, 0.511, 0.72, 0.554, 0.194, 0.11, 0.443, 0.293, 0.286, 0.201]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.24486940298507462
[2m[36m(func pid=83673)[0m top5: 0.7644589552238806
[2m[36m(func pid=83673)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=83673)[0m f1_macro: 0.1832200568661853
[2m[36m(func pid=83673)[0m f1_weighted: 0.24734568765161213
[2m[36m(func pid=83673)[0m f1_per_class: [0.326, 0.097, 0.247, 0.5, 0.066, 0.032, 0.207, 0.266, 0.025, 0.065]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:10:46 (running for 00:03:04.31)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.036 |      0.183 |                   24 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.418 |      0.387 |                   24 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  5.631 |      0.304 |                   25 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 13.497 |      0.384 |                   23 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2891791044776119
[2m[36m(func pid=84475)[0m top5: 0.8194962686567164
[2m[36m(func pid=84475)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=84475)[0m f1_macro: 0.3042563531048009
[2m[36m(func pid=84475)[0m f1_weighted: 0.2781316363391994
[2m[36m(func pid=84475)[0m f1_per_class: [0.604, 0.027, 0.522, 0.569, 0.375, 0.158, 0.195, 0.213, 0.173, 0.208]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1558 | Steps: 4 | Val loss: 2.1265 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8333 | Steps: 4 | Val loss: 89.5297 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.7699 | Steps: 4 | Val loss: 2.0065 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7909 | Steps: 4 | Val loss: 11.8115 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84053)[0m top1: 0.41277985074626866
[2m[36m(func pid=84053)[0m top5: 0.8959888059701493
[2m[36m(func pid=84053)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=84053)[0m f1_macro: 0.39431207820980385
[2m[36m(func pid=84053)[0m f1_weighted: 0.42579228212389075
[2m[36m(func pid=84053)[0m f1_per_class: [0.596, 0.534, 0.632, 0.39, 0.096, 0.129, 0.538, 0.349, 0.239, 0.44]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.4197761194029851
[2m[36m(func pid=84902)[0m top5: 0.8222947761194029
[2m[36m(func pid=84902)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=84902)[0m f1_macro: 0.3666481515346161
[2m[36m(func pid=84902)[0m f1_weighted: 0.4361126726548411
[2m[36m(func pid=84902)[0m f1_per_class: [0.308, 0.486, 0.667, 0.56, 0.168, 0.211, 0.438, 0.307, 0.286, 0.235]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:10:52 (running for 00:03:09.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  2.036 |      0.183 |                   24 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.156 |      0.394 |                   25 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.791 |      0.336 |                   26 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  1.833 |      0.367 |                   24 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m top1: 0.25419776119402987
[2m[36m(func pid=83673)[0m top5: 0.7919776119402985
[2m[36m(func pid=83673)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=83673)[0m f1_macro: 0.19259195638549842
[2m[36m(func pid=83673)[0m f1_weighted: 0.2380232985408002
[2m[36m(func pid=83673)[0m f1_per_class: [0.372, 0.107, 0.32, 0.532, 0.093, 0.046, 0.136, 0.246, 0.0, 0.073]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.31949626865671643
[2m[36m(func pid=84475)[0m top5: 0.8838619402985075
[2m[36m(func pid=84475)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=84475)[0m f1_macro: 0.3360394044382162
[2m[36m(func pid=84475)[0m f1_weighted: 0.34038749603314855
[2m[36m(func pid=84475)[0m f1_per_class: [0.585, 0.315, 0.625, 0.553, 0.286, 0.155, 0.252, 0.21, 0.214, 0.166]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2681 | Steps: 4 | Val loss: 2.2387 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 21.4403 | Steps: 4 | Val loss: 108.5094 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.0635 | Steps: 4 | Val loss: 10.8850 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.9178 | Steps: 4 | Val loss: 1.9896 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=84053)[0m top1: 0.4076492537313433
[2m[36m(func pid=84053)[0m top5: 0.8889925373134329
[2m[36m(func pid=84053)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=84053)[0m f1_macro: 0.4005773674867962
[2m[36m(func pid=84053)[0m f1_weighted: 0.4182852096303691
[2m[36m(func pid=84053)[0m f1_per_class: [0.61, 0.534, 0.649, 0.346, 0.079, 0.126, 0.549, 0.348, 0.275, 0.491]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.37406716417910446
[2m[36m(func pid=84902)[0m top5: 0.7938432835820896
[2m[36m(func pid=84902)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=84902)[0m f1_macro: 0.3250549460359671
[2m[36m(func pid=84902)[0m f1_weighted: 0.39812165592691057
[2m[36m(func pid=84902)[0m f1_per_class: [0.195, 0.456, 0.585, 0.537, 0.152, 0.241, 0.367, 0.247, 0.214, 0.256]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:10:57 (running for 00:03:14.63)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.77  |      0.193 |                   25 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.268 |      0.401 |                   26 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.064 |      0.361 |                   27 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 21.44  |      0.325 |                   25 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.36427238805970147
[2m[36m(func pid=84475)[0m top5: 0.8955223880597015
[2m[36m(func pid=84475)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=84475)[0m f1_macro: 0.3608541402193775
[2m[36m(func pid=84475)[0m f1_weighted: 0.3896766890193582
[2m[36m(func pid=84475)[0m f1_per_class: [0.674, 0.544, 0.611, 0.49, 0.18, 0.121, 0.345, 0.243, 0.239, 0.161]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.2523320895522388
[2m[36m(func pid=83673)[0m top5: 0.808768656716418
[2m[36m(func pid=83673)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=83673)[0m f1_macro: 0.2118288171213417
[2m[36m(func pid=83673)[0m f1_weighted: 0.22955260627528598
[2m[36m(func pid=83673)[0m f1_per_class: [0.409, 0.112, 0.48, 0.532, 0.133, 0.035, 0.103, 0.238, 0.0, 0.075]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3705 | Steps: 4 | Val loss: 2.2201 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6739 | Steps: 4 | Val loss: 117.1225 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.4095 | Steps: 4 | Val loss: 12.0926 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8999 | Steps: 4 | Val loss: 1.9879 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84053)[0m top1: 0.40298507462686567
[2m[36m(func pid=84053)[0m top5: 0.8931902985074627
[2m[36m(func pid=84053)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=84053)[0m f1_macro: 0.38334590197417845
[2m[36m(func pid=84053)[0m f1_weighted: 0.42320861755202094
[2m[36m(func pid=84053)[0m f1_per_class: [0.615, 0.542, 0.511, 0.384, 0.074, 0.149, 0.523, 0.352, 0.245, 0.438]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.35774253731343286
[2m[36m(func pid=84902)[0m top5: 0.7742537313432836
[2m[36m(func pid=84902)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=84902)[0m f1_macro: 0.2991240855120135
[2m[36m(func pid=84902)[0m f1_weighted: 0.39089097426750363
[2m[36m(func pid=84902)[0m f1_per_class: [0.163, 0.414, 0.429, 0.543, 0.195, 0.255, 0.379, 0.166, 0.188, 0.261]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:11:02 (running for 00:03:19.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.918 |      0.212 |                   26 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.37  |      0.383 |                   27 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.41  |      0.333 |                   28 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  1.674 |      0.299 |                   26 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3694029850746269
[2m[36m(func pid=84475)[0m top5: 0.8805970149253731
[2m[36m(func pid=84475)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=84475)[0m f1_macro: 0.3326328055513584
[2m[36m(func pid=84475)[0m f1_weighted: 0.3670819474094418
[2m[36m(func pid=84475)[0m f1_per_class: [0.623, 0.568, 0.541, 0.343, 0.109, 0.087, 0.414, 0.269, 0.154, 0.217]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.2513992537313433
[2m[36m(func pid=83673)[0m top5: 0.8069029850746269
[2m[36m(func pid=83673)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.20279302102840382
[2m[36m(func pid=83673)[0m f1_weighted: 0.21869141129249017
[2m[36m(func pid=83673)[0m f1_per_class: [0.443, 0.151, 0.353, 0.541, 0.175, 0.022, 0.042, 0.233, 0.0, 0.068]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2572 | Steps: 4 | Val loss: 2.4891 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 15.5905 | Steps: 4 | Val loss: 127.0113 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3126 | Steps: 4 | Val loss: 13.4753 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.7581 | Steps: 4 | Val loss: 1.9779 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=84053)[0m top1: 0.36473880597014924
[2m[36m(func pid=84053)[0m top5: 0.8819962686567164
[2m[36m(func pid=84053)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=84053)[0m f1_macro: 0.36047165170642387
[2m[36m(func pid=84053)[0m f1_weighted: 0.3777426053407798
[2m[36m(func pid=84053)[0m f1_per_class: [0.641, 0.544, 0.5, 0.337, 0.076, 0.173, 0.41, 0.326, 0.261, 0.337]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3302238805970149
[2m[36m(func pid=84902)[0m top5: 0.7546641791044776
[2m[36m(func pid=84902)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=84902)[0m f1_macro: 0.2683655298191802
[2m[36m(func pid=84902)[0m f1_weighted: 0.36536317013331043
[2m[36m(func pid=84902)[0m f1_per_class: [0.148, 0.343, 0.292, 0.501, 0.21, 0.293, 0.381, 0.072, 0.178, 0.265]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:11:07 (running for 00:03:25.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.9   |      0.203 |                   27 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.257 |      0.36  |                   28 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.313 |      0.295 |                   29 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 15.59  |      0.268 |                   27 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.376865671641791
[2m[36m(func pid=84475)[0m top5: 0.8703358208955224
[2m[36m(func pid=84475)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=84475)[0m f1_macro: 0.29497560788437394
[2m[36m(func pid=84475)[0m f1_weighted: 0.36272464864422077
[2m[36m(func pid=84475)[0m f1_per_class: [0.485, 0.532, 0.465, 0.253, 0.067, 0.078, 0.528, 0.285, 0.081, 0.177]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.2583955223880597
[2m[36m(func pid=83673)[0m top5: 0.8134328358208955
[2m[36m(func pid=83673)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=83673)[0m f1_macro: 0.21042429052877587
[2m[36m(func pid=83673)[0m f1_weighted: 0.22901881880457975
[2m[36m(func pid=83673)[0m f1_per_class: [0.464, 0.198, 0.338, 0.548, 0.152, 0.035, 0.033, 0.236, 0.026, 0.074]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3610 | Steps: 4 | Val loss: 2.6002 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.3509 | Steps: 4 | Val loss: 131.6791 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.5838 | Steps: 4 | Val loss: 14.5988 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84053)[0m top1: 0.35261194029850745
[2m[36m(func pid=84053)[0m top5: 0.8726679104477612
[2m[36m(func pid=84053)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=84053)[0m f1_macro: 0.3366390382160308
[2m[36m(func pid=84053)[0m f1_weighted: 0.36033410197563587
[2m[36m(func pid=84053)[0m f1_per_class: [0.66, 0.549, 0.448, 0.36, 0.096, 0.218, 0.332, 0.255, 0.214, 0.234]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5657 | Steps: 4 | Val loss: 1.9449 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=84902)[0m top1: 0.32276119402985076
[2m[36m(func pid=84902)[0m top5: 0.7551305970149254
[2m[36m(func pid=84902)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=84902)[0m f1_macro: 0.264797221279981
[2m[36m(func pid=84902)[0m f1_weighted: 0.35560842186852165
[2m[36m(func pid=84902)[0m f1_per_class: [0.192, 0.329, 0.217, 0.495, 0.212, 0.291, 0.36, 0.086, 0.147, 0.319]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.355410447761194
[2m[36m(func pid=84475)[0m top5: 0.855410447761194
[2m[36m(func pid=84475)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=84475)[0m f1_macro: 0.2758620727503286
[2m[36m(func pid=84475)[0m f1_weighted: 0.34753302968941496
[2m[36m(func pid=84475)[0m f1_per_class: [0.444, 0.535, 0.367, 0.229, 0.067, 0.084, 0.5, 0.271, 0.105, 0.156]
[2m[36m(func pid=84475)[0m 
== Status ==
Current time: 2024-01-07 11:11:12 (running for 00:03:30.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.758 |      0.21  |                   28 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.361 |      0.337 |                   29 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.584 |      0.276 |                   30 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  4.351 |      0.265 |                   28 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m top1: 0.27425373134328357
[2m[36m(func pid=83673)[0m top5: 0.8166977611940298
[2m[36m(func pid=83673)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=83673)[0m f1_macro: 0.22395452550593062
[2m[36m(func pid=83673)[0m f1_weighted: 0.24541139430324113
[2m[36m(func pid=83673)[0m f1_per_class: [0.472, 0.287, 0.316, 0.558, 0.19, 0.047, 0.021, 0.24, 0.025, 0.084]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2522 | Steps: 4 | Val loss: 2.5832 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 14.2725 | Steps: 4 | Val loss: 142.2400 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.6648 | Steps: 4 | Val loss: 14.8800 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84053)[0m top1: 0.3558768656716418
[2m[36m(func pid=84053)[0m top5: 0.8754664179104478
[2m[36m(func pid=84053)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=84053)[0m f1_macro: 0.3463946514055584
[2m[36m(func pid=84053)[0m f1_weighted: 0.36741117098462184
[2m[36m(func pid=84053)[0m f1_per_class: [0.64, 0.551, 0.4, 0.366, 0.125, 0.247, 0.316, 0.362, 0.248, 0.209]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4934 | Steps: 4 | Val loss: 1.9055 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=84902)[0m top1: 0.3208955223880597
[2m[36m(func pid=84902)[0m top5: 0.7546641791044776
[2m[36m(func pid=84902)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=84902)[0m f1_macro: 0.2702841144734634
[2m[36m(func pid=84902)[0m f1_weighted: 0.35240946271691487
[2m[36m(func pid=84902)[0m f1_per_class: [0.286, 0.413, 0.12, 0.48, 0.218, 0.303, 0.3, 0.123, 0.13, 0.329]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:11:17 (running for 00:03:35.42)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.566 |      0.224 |                   29 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.252 |      0.346 |                   30 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.665 |      0.282 |                   31 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 14.272 |      0.27  |                   29 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3180970149253731
[2m[36m(func pid=84475)[0m top5: 0.8498134328358209
[2m[36m(func pid=84475)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=84475)[0m f1_macro: 0.2819625553305817
[2m[36m(func pid=84475)[0m f1_weighted: 0.3381893337609208
[2m[36m(func pid=84475)[0m f1_per_class: [0.529, 0.546, 0.253, 0.311, 0.063, 0.105, 0.356, 0.334, 0.169, 0.154]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3003731343283582
[2m[36m(func pid=83673)[0m top5: 0.8218283582089553
[2m[36m(func pid=83673)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=83673)[0m f1_macro: 0.2498011407783402
[2m[36m(func pid=83673)[0m f1_weighted: 0.27295958558829553
[2m[36m(func pid=83673)[0m f1_per_class: [0.513, 0.412, 0.32, 0.554, 0.165, 0.099, 0.015, 0.248, 0.068, 0.103]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1234 | Steps: 4 | Val loss: 2.5953 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 39.9940 | Steps: 4 | Val loss: 138.7193 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3124 | Steps: 4 | Val loss: 15.2870 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=84053)[0m top1: 0.36613805970149255
[2m[36m(func pid=84053)[0m top5: 0.8791977611940298
[2m[36m(func pid=84053)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=84053)[0m f1_macro: 0.34936004166711687
[2m[36m(func pid=84053)[0m f1_weighted: 0.37732891763734533
[2m[36m(func pid=84053)[0m f1_per_class: [0.612, 0.549, 0.366, 0.429, 0.141, 0.261, 0.281, 0.406, 0.237, 0.212]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.5806 | Steps: 4 | Val loss: 1.8776 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84902)[0m top1: 0.35867537313432835
[2m[36m(func pid=84902)[0m top5: 0.7649253731343284
[2m[36m(func pid=84902)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=84902)[0m f1_macro: 0.3063094506307464
[2m[36m(func pid=84902)[0m f1_weighted: 0.36877811965858553
[2m[36m(func pid=84902)[0m f1_per_class: [0.504, 0.533, 0.135, 0.432, 0.179, 0.298, 0.31, 0.136, 0.158, 0.378]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.2943097014925373
[2m[36m(func pid=84475)[0m top5: 0.8227611940298507
[2m[36m(func pid=84475)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=84475)[0m f1_macro: 0.2899088694651551
[2m[36m(func pid=84475)[0m f1_weighted: 0.3299480493144622
[2m[36m(func pid=84475)[0m f1_per_class: [0.556, 0.523, 0.19, 0.425, 0.063, 0.146, 0.207, 0.348, 0.222, 0.22]
[2m[36m(func pid=84475)[0m 
== Status ==
Current time: 2024-01-07 11:11:23 (running for 00:03:40.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.493 |      0.25  |                   30 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.123 |      0.349 |                   31 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.312 |      0.29  |                   32 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 39.994 |      0.306 |                   30 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m top1: 0.32322761194029853
[2m[36m(func pid=83673)[0m top5: 0.8260261194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=83673)[0m f1_macro: 0.26863615703997534
[2m[36m(func pid=83673)[0m f1_weighted: 0.2893094321970998
[2m[36m(func pid=83673)[0m f1_per_class: [0.539, 0.515, 0.312, 0.515, 0.203, 0.151, 0.025, 0.263, 0.042, 0.123]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5886 | Steps: 4 | Val loss: 2.5341 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 16.3163 | Steps: 4 | Val loss: 163.0774 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0378 | Steps: 4 | Val loss: 17.1424 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84053)[0m top1: 0.373134328358209
[2m[36m(func pid=84053)[0m top5: 0.8824626865671642
[2m[36m(func pid=84053)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=84053)[0m f1_macro: 0.3610674699144497
[2m[36m(func pid=84053)[0m f1_weighted: 0.38201316600879287
[2m[36m(func pid=84053)[0m f1_per_class: [0.645, 0.548, 0.448, 0.458, 0.157, 0.271, 0.268, 0.379, 0.235, 0.202]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.4402 | Steps: 4 | Val loss: 1.8238 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84902)[0m top1: 0.34095149253731344
[2m[36m(func pid=84902)[0m top5: 0.7439365671641791
[2m[36m(func pid=84902)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=84902)[0m f1_macro: 0.3111931284400528
[2m[36m(func pid=84902)[0m f1_weighted: 0.32925004516685347
[2m[36m(func pid=84902)[0m f1_per_class: [0.553, 0.514, 0.102, 0.433, 0.128, 0.294, 0.165, 0.193, 0.212, 0.519]
[2m[36m(func pid=84902)[0m 
== Status ==
Current time: 2024-01-07 11:11:28 (running for 00:03:45.71)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.581 |      0.269 |                   31 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.589 |      0.361 |                   32 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.038 |      0.265 |                   33 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 16.316 |      0.311 |                   31 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.26072761194029853
[2m[36m(func pid=84475)[0m top5: 0.7971082089552238
[2m[36m(func pid=84475)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=84475)[0m f1_macro: 0.2649956150311491
[2m[36m(func pid=84475)[0m f1_weighted: 0.2892975920839545
[2m[36m(func pid=84475)[0m f1_per_class: [0.528, 0.256, 0.179, 0.474, 0.067, 0.192, 0.167, 0.36, 0.141, 0.286]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3423507462686567
[2m[36m(func pid=83673)[0m top5: 0.8526119402985075
[2m[36m(func pid=83673)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=83673)[0m f1_macro: 0.2980067427585314
[2m[36m(func pid=83673)[0m f1_weighted: 0.30274528433919806
[2m[36m(func pid=83673)[0m f1_per_class: [0.566, 0.551, 0.364, 0.493, 0.182, 0.17, 0.045, 0.273, 0.124, 0.212]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1612 | Steps: 4 | Val loss: 2.4596 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 9.4570 | Steps: 4 | Val loss: 204.3551 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2575 | Steps: 4 | Val loss: 22.3520 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=84053)[0m top1: 0.3871268656716418
[2m[36m(func pid=84053)[0m top5: 0.8875932835820896
[2m[36m(func pid=84053)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=84053)[0m f1_macro: 0.356396356904563
[2m[36m(func pid=84053)[0m f1_weighted: 0.3956352498694213
[2m[36m(func pid=84053)[0m f1_per_class: [0.62, 0.542, 0.377, 0.532, 0.164, 0.267, 0.252, 0.393, 0.206, 0.212]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.33488805970149255
[2m[36m(func pid=84902)[0m top5: 0.6986940298507462
[2m[36m(func pid=84902)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=84902)[0m f1_macro: 0.28233999375991936
[2m[36m(func pid=84902)[0m f1_weighted: 0.3052997636406977
[2m[36m(func pid=84902)[0m f1_per_class: [0.333, 0.473, 0.101, 0.488, 0.116, 0.273, 0.065, 0.286, 0.2, 0.488]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2976 | Steps: 4 | Val loss: 1.7915 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:11:33 (running for 00:03:50.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.44  |      0.298 |                   32 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.161 |      0.356 |                   33 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.257 |      0.24  |                   34 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  9.457 |      0.282 |                   32 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2140858208955224
[2m[36m(func pid=84475)[0m top5: 0.7458022388059702
[2m[36m(func pid=84475)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=84475)[0m f1_macro: 0.24043556975467464
[2m[36m(func pid=84475)[0m f1_weighted: 0.2222904932840953
[2m[36m(func pid=84475)[0m f1_per_class: [0.507, 0.042, 0.173, 0.432, 0.086, 0.184, 0.103, 0.39, 0.108, 0.377]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2589 | Steps: 4 | Val loss: 2.4441 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=83673)[0m top1: 0.34468283582089554
[2m[36m(func pid=83673)[0m top5: 0.867070895522388
[2m[36m(func pid=83673)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=83673)[0m f1_macro: 0.31442974752668074
[2m[36m(func pid=83673)[0m f1_weighted: 0.30517552319972147
[2m[36m(func pid=83673)[0m f1_per_class: [0.579, 0.553, 0.375, 0.446, 0.212, 0.185, 0.076, 0.309, 0.159, 0.25]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 56.8811 | Steps: 4 | Val loss: 246.9099 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5068 | Steps: 4 | Val loss: 24.5726 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=84053)[0m top1: 0.39365671641791045
[2m[36m(func pid=84053)[0m top5: 0.8903917910447762
[2m[36m(func pid=84053)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=84053)[0m f1_macro: 0.35521493494985984
[2m[36m(func pid=84053)[0m f1_weighted: 0.39238105182062216
[2m[36m(func pid=84053)[0m f1_per_class: [0.63, 0.543, 0.371, 0.557, 0.17, 0.251, 0.225, 0.379, 0.211, 0.215]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.34095149253731344
[2m[36m(func pid=84902)[0m top5: 0.6898320895522388
[2m[36m(func pid=84902)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=84902)[0m f1_macro: 0.27768385706430726
[2m[36m(func pid=84902)[0m f1_weighted: 0.2954565805850299
[2m[36m(func pid=84902)[0m f1_per_class: [0.333, 0.459, 0.112, 0.523, 0.115, 0.219, 0.018, 0.347, 0.189, 0.462]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.2103544776119403
[2m[36m(func pid=84475)[0m top5: 0.7173507462686567
[2m[36m(func pid=84475)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=84475)[0m f1_macro: 0.23018261392475586
[2m[36m(func pid=84475)[0m f1_weighted: 0.21464033868717647
[2m[36m(func pid=84475)[0m f1_per_class: [0.551, 0.005, 0.174, 0.433, 0.095, 0.195, 0.098, 0.377, 0.102, 0.27]
== Status ==
Current time: 2024-01-07 11:11:38 (running for 00:03:56.05)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.298 |      0.314 |                   33 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.259 |      0.355 |                   34 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.507 |      0.23  |                   35 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 56.881 |      0.278 |                   33 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5539 | Steps: 4 | Val loss: 1.7990 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0703 | Steps: 4 | Val loss: 2.5173 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=83673)[0m top1: 0.34328358208955223
[2m[36m(func pid=83673)[0m top5: 0.863339552238806
[2m[36m(func pid=83673)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=83673)[0m f1_macro: 0.32548238838624083
[2m[36m(func pid=83673)[0m f1_weighted: 0.3011199672250466
[2m[36m(func pid=83673)[0m f1_per_class: [0.569, 0.542, 0.369, 0.387, 0.173, 0.233, 0.095, 0.333, 0.187, 0.367]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 33.3972 | Steps: 4 | Val loss: 253.4349 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 4.5303 | Steps: 4 | Val loss: 23.7994 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=84053)[0m top1: 0.404384328358209
[2m[36m(func pid=84053)[0m top5: 0.8913246268656716
[2m[36m(func pid=84053)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=84053)[0m f1_macro: 0.3592596714787116
[2m[36m(func pid=84053)[0m f1_weighted: 0.3889985381335684
[2m[36m(func pid=84053)[0m f1_per_class: [0.63, 0.537, 0.347, 0.579, 0.179, 0.237, 0.198, 0.394, 0.186, 0.305]
[2m[36m(func pid=84053)[0m 
== Status ==
Current time: 2024-01-07 11:11:43 (running for 00:04:01.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.554 |      0.325 |                   34 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.07  |      0.359 |                   35 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.507 |      0.23  |                   35 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 33.397 |      0.303 |                   34 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84902)[0m top1: 0.37919776119402987
[2m[36m(func pid=84902)[0m top5: 0.7425373134328358
[2m[36m(func pid=84902)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=84902)[0m f1_macro: 0.3034648934315325
[2m[36m(func pid=84902)[0m f1_weighted: 0.3170734005745286
[2m[36m(func pid=84902)[0m f1_per_class: [0.414, 0.492, 0.21, 0.561, 0.126, 0.28, 0.006, 0.366, 0.163, 0.417]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.2555970149253731
[2m[36m(func pid=84475)[0m top5: 0.7112873134328358
[2m[36m(func pid=84475)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=84475)[0m f1_macro: 0.2340700352739225
[2m[36m(func pid=84475)[0m f1_weighted: 0.24699073533950924
[2m[36m(func pid=84475)[0m f1_per_class: [0.554, 0.005, 0.165, 0.51, 0.097, 0.222, 0.127, 0.372, 0.118, 0.171]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2680 | Steps: 4 | Val loss: 1.7564 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4218 | Steps: 4 | Val loss: 2.7082 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 23.1220 | Steps: 4 | Val loss: 236.3355 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=83673)[0m top1: 0.353544776119403
[2m[36m(func pid=83673)[0m top5: 0.8908582089552238
[2m[36m(func pid=83673)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=83673)[0m f1_macro: 0.3442950027889946
[2m[36m(func pid=83673)[0m f1_weighted: 0.32158749541303144
[2m[36m(func pid=83673)[0m f1_per_class: [0.566, 0.548, 0.414, 0.361, 0.171, 0.249, 0.169, 0.34, 0.25, 0.375]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6528 | Steps: 4 | Val loss: 22.6568 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:11:48 (running for 00:04:06.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.268 |      0.344 |                   35 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.422 |      0.351 |                   36 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  4.53  |      0.234 |                   36 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 33.397 |      0.303 |                   34 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.396455223880597
[2m[36m(func pid=84053)[0m top5: 0.8740671641791045
[2m[36m(func pid=84053)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=84053)[0m f1_macro: 0.3510197359989652
[2m[36m(func pid=84053)[0m f1_weighted: 0.3744895356278735
[2m[36m(func pid=84053)[0m f1_per_class: [0.614, 0.502, 0.289, 0.585, 0.197, 0.241, 0.166, 0.386, 0.169, 0.362]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.4039179104477612
[2m[36m(func pid=84902)[0m top5: 0.8013059701492538
[2m[36m(func pid=84902)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=84902)[0m f1_macro: 0.3587357531889207
[2m[36m(func pid=84902)[0m f1_weighted: 0.3440416517017604
[2m[36m(func pid=84902)[0m f1_per_class: [0.554, 0.534, 0.481, 0.602, 0.167, 0.313, 0.0, 0.394, 0.174, 0.368]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.283115671641791
[2m[36m(func pid=84475)[0m top5: 0.7140858208955224
[2m[36m(func pid=84475)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=84475)[0m f1_macro: 0.24787994198684923
[2m[36m(func pid=84475)[0m f1_weighted: 0.2553761859302192
[2m[36m(func pid=84475)[0m f1_per_class: [0.335, 0.005, 0.268, 0.554, 0.088, 0.203, 0.124, 0.366, 0.146, 0.389]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.1289 | Steps: 4 | Val loss: 1.7126 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2019 | Steps: 4 | Val loss: 2.8843 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 31.3461 | Steps: 4 | Val loss: 251.7067 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=83673)[0m top1: 0.36613805970149255
[2m[36m(func pid=83673)[0m top5: 0.90625
[2m[36m(func pid=83673)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=83673)[0m f1_macro: 0.36487860654026105
[2m[36m(func pid=83673)[0m f1_weighted: 0.34762083726519394
[2m[36m(func pid=83673)[0m f1_per_class: [0.587, 0.547, 0.511, 0.336, 0.159, 0.254, 0.276, 0.325, 0.264, 0.39]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5801 | Steps: 4 | Val loss: 19.7242 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:11:53 (running for 00:04:11.36)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.129 |      0.365 |                   36 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.202 |      0.338 |                   37 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.653 |      0.248 |                   37 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 23.122 |      0.359 |                   35 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.37453358208955223
[2m[36m(func pid=84053)[0m top5: 0.863339552238806
[2m[36m(func pid=84053)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=84053)[0m f1_macro: 0.3384958734099413
[2m[36m(func pid=84053)[0m f1_weighted: 0.3582106668187451
[2m[36m(func pid=84053)[0m f1_per_class: [0.583, 0.461, 0.255, 0.579, 0.206, 0.23, 0.153, 0.364, 0.14, 0.414]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.373134328358209
[2m[36m(func pid=84902)[0m top5: 0.789179104477612
[2m[36m(func pid=84902)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=84902)[0m f1_macro: 0.3535784479106716
[2m[36m(func pid=84902)[0m f1_weighted: 0.32184008869208497
[2m[36m(func pid=84902)[0m f1_per_class: [0.593, 0.387, 0.667, 0.617, 0.169, 0.304, 0.0, 0.385, 0.183, 0.232]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.2957089552238806
[2m[36m(func pid=84475)[0m top5: 0.7831156716417911
[2m[36m(func pid=84475)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=84475)[0m f1_macro: 0.25814771024860017
[2m[36m(func pid=84475)[0m f1_weighted: 0.2760358687395117
[2m[36m(func pid=84475)[0m f1_per_class: [0.246, 0.042, 0.394, 0.56, 0.082, 0.186, 0.177, 0.355, 0.171, 0.368]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.1583 | Steps: 4 | Val loss: 1.6906 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6568 | Steps: 4 | Val loss: 2.9738 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 9.9433 | Steps: 4 | Val loss: 254.9350 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=83673)[0m top1: 0.3787313432835821
[2m[36m(func pid=83673)[0m top5: 0.9123134328358209
[2m[36m(func pid=83673)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=83673)[0m f1_macro: 0.3870205466411236
[2m[36m(func pid=83673)[0m f1_weighted: 0.3774915848377319
[2m[36m(func pid=83673)[0m f1_per_class: [0.615, 0.526, 0.564, 0.335, 0.154, 0.25, 0.383, 0.362, 0.219, 0.462]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5531 | Steps: 4 | Val loss: 18.3957 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 11:11:59 (running for 00:04:16.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.158 |      0.387 |                   37 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.657 |      0.342 |                   38 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.58  |      0.258 |                   38 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 31.346 |      0.354 |                   36 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.373134328358209
[2m[36m(func pid=84053)[0m top5: 0.8666044776119403
[2m[36m(func pid=84053)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=84053)[0m f1_macro: 0.3420774161301363
[2m[36m(func pid=84053)[0m f1_weighted: 0.3548804095026568
[2m[36m(func pid=84053)[0m f1_per_class: [0.566, 0.457, 0.325, 0.581, 0.208, 0.206, 0.153, 0.354, 0.13, 0.441]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.33302238805970147
[2m[36m(func pid=84902)[0m top5: 0.7448694029850746
[2m[36m(func pid=84902)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=84902)[0m f1_macro: 0.3074587026753243
[2m[36m(func pid=84902)[0m f1_weighted: 0.27364064721259673
[2m[36m(func pid=84902)[0m f1_per_class: [0.365, 0.137, 0.786, 0.612, 0.144, 0.287, 0.015, 0.354, 0.174, 0.2]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.3255597014925373
[2m[36m(func pid=84475)[0m top5: 0.8148320895522388
[2m[36m(func pid=84475)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=84475)[0m f1_macro: 0.28559106897735076
[2m[36m(func pid=84475)[0m f1_weighted: 0.30936792382238776
[2m[36m(func pid=84475)[0m f1_per_class: [0.242, 0.221, 0.49, 0.574, 0.081, 0.169, 0.176, 0.353, 0.195, 0.356]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1465 | Steps: 4 | Val loss: 1.6792 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3835 | Steps: 4 | Val loss: 2.8140 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.9479 | Steps: 4 | Val loss: 16.4012 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 11.2398 | Steps: 4 | Val loss: 258.0476 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=83673)[0m top1: 0.38619402985074625
[2m[36m(func pid=83673)[0m top5: 0.9085820895522388
[2m[36m(func pid=83673)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=83673)[0m f1_macro: 0.3807307040858935
[2m[36m(func pid=83673)[0m f1_weighted: 0.38474745812728967
[2m[36m(func pid=83673)[0m f1_per_class: [0.581, 0.535, 0.524, 0.345, 0.18, 0.258, 0.407, 0.28, 0.224, 0.474]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:04 (running for 00:04:22.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.146 |      0.381 |                   38 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.384 |      0.344 |                   39 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.553 |      0.286 |                   39 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  9.943 |      0.307 |                   37 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.3824626865671642
[2m[36m(func pid=84053)[0m top5: 0.8815298507462687
[2m[36m(func pid=84053)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=84053)[0m f1_macro: 0.34357372470555536
[2m[36m(func pid=84053)[0m f1_weighted: 0.3748047663154812
[2m[36m(func pid=84053)[0m f1_per_class: [0.559, 0.441, 0.252, 0.587, 0.204, 0.199, 0.227, 0.347, 0.137, 0.483]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.292910447761194
[2m[36m(func pid=84902)[0m top5: 0.6823694029850746
[2m[36m(func pid=84902)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=84902)[0m f1_macro: 0.24946151544091358
[2m[36m(func pid=84902)[0m f1_weighted: 0.25041363253055005
[2m[36m(func pid=84902)[0m f1_per_class: [0.184, 0.021, 0.526, 0.599, 0.1, 0.258, 0.045, 0.352, 0.186, 0.224]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.3666044776119403
[2m[36m(func pid=84475)[0m top5: 0.851679104477612
[2m[36m(func pid=84475)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=84475)[0m f1_macro: 0.33009445705538715
[2m[36m(func pid=84475)[0m f1_weighted: 0.36231249597024284
[2m[36m(func pid=84475)[0m f1_per_class: [0.289, 0.434, 0.615, 0.583, 0.093, 0.177, 0.215, 0.347, 0.175, 0.373]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.1133 | Steps: 4 | Val loss: 1.6485 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5939 | Steps: 4 | Val loss: 2.7951 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 18.5475 | Steps: 4 | Val loss: 259.6934 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=83673)[0m top1: 0.3908582089552239
[2m[36m(func pid=83673)[0m top5: 0.9197761194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=83673)[0m f1_macro: 0.3702771625171422
[2m[36m(func pid=83673)[0m f1_weighted: 0.4031434367923126
[2m[36m(func pid=83673)[0m f1_per_class: [0.574, 0.532, 0.537, 0.412, 0.187, 0.27, 0.433, 0.161, 0.197, 0.4]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1414 | Steps: 4 | Val loss: 14.5408 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:12:09 (running for 00:04:27.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.113 |      0.37  |                   39 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.594 |      0.334 |                   40 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.948 |      0.33  |                   40 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 11.24  |      0.249 |                   38 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.3805970149253731
[2m[36m(func pid=84053)[0m top5: 0.8959888059701493
[2m[36m(func pid=84053)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=84053)[0m f1_macro: 0.3342973692340431
[2m[36m(func pid=84053)[0m f1_weighted: 0.38725001167938433
[2m[36m(func pid=84053)[0m f1_per_class: [0.559, 0.427, 0.241, 0.58, 0.242, 0.161, 0.305, 0.339, 0.127, 0.364]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2579291044776119
[2m[36m(func pid=84902)[0m top5: 0.6464552238805971
[2m[36m(func pid=84902)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=84902)[0m f1_macro: 0.22330661380524294
[2m[36m(func pid=84902)[0m f1_weighted: 0.25023116232469866
[2m[36m(func pid=84902)[0m f1_per_class: [0.127, 0.005, 0.353, 0.563, 0.088, 0.205, 0.117, 0.345, 0.163, 0.267]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m top1: 0.404384328358209
[2m[36m(func pid=84475)[0m top5: 0.8777985074626866
[2m[36m(func pid=84475)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=84475)[0m f1_macro: 0.36975683948298566
[2m[36m(func pid=84475)[0m f1_weighted: 0.3957478789654577
[2m[36m(func pid=84475)[0m f1_per_class: [0.528, 0.545, 0.629, 0.594, 0.111, 0.18, 0.239, 0.318, 0.198, 0.357]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.3259 | Steps: 4 | Val loss: 1.6401 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6910 | Steps: 4 | Val loss: 2.8517 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 87.7445 | Steps: 4 | Val loss: 242.8921 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9242 | Steps: 4 | Val loss: 14.6585 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=83673)[0m top1: 0.3829291044776119
[2m[36m(func pid=83673)[0m top5: 0.9221082089552238
[2m[36m(func pid=83673)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=83673)[0m f1_macro: 0.35859503917907365
[2m[36m(func pid=83673)[0m f1_weighted: 0.40372786655747406
[2m[36m(func pid=83673)[0m f1_per_class: [0.6, 0.505, 0.55, 0.425, 0.185, 0.266, 0.457, 0.086, 0.169, 0.343]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:14 (running for 00:04:32.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.326 |      0.359 |                   40 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.691 |      0.338 |                   41 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.141 |      0.37  |                   41 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 18.547 |      0.223 |                   39 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.37220149253731344
[2m[36m(func pid=84053)[0m top5: 0.9001865671641791
[2m[36m(func pid=84053)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=84053)[0m f1_macro: 0.33761661230253565
[2m[36m(func pid=84053)[0m f1_weighted: 0.39428647330886973
[2m[36m(func pid=84053)[0m f1_per_class: [0.484, 0.427, 0.274, 0.554, 0.232, 0.164, 0.353, 0.337, 0.135, 0.417]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.41091417910447764
[2m[36m(func pid=84475)[0m top5: 0.8880597014925373
[2m[36m(func pid=84475)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=84475)[0m f1_macro: 0.3834208452611932
[2m[36m(func pid=84475)[0m f1_weighted: 0.39970853062006484
[2m[36m(func pid=84475)[0m f1_per_class: [0.647, 0.623, 0.611, 0.571, 0.1, 0.157, 0.23, 0.295, 0.229, 0.37]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m top1: 0.26399253731343286
[2m[36m(func pid=84902)[0m top5: 0.648320895522388
[2m[36m(func pid=84902)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=84902)[0m f1_macro: 0.21820660244602852
[2m[36m(func pid=84902)[0m f1_weighted: 0.2808848066894925
[2m[36m(func pid=84902)[0m f1_per_class: [0.129, 0.005, 0.25, 0.54, 0.073, 0.179, 0.258, 0.32, 0.17, 0.258]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.1565 | Steps: 4 | Val loss: 1.6098 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6292 | Steps: 4 | Val loss: 2.6137 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0001 | Steps: 4 | Val loss: 15.4059 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 29.2036 | Steps: 4 | Val loss: 199.5520 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=83673)[0m top1: 0.3908582089552239
[2m[36m(func pid=83673)[0m top5: 0.9305037313432836
[2m[36m(func pid=83673)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=83673)[0m f1_macro: 0.36611626333330066
[2m[36m(func pid=83673)[0m f1_weighted: 0.4198567242952391
[2m[36m(func pid=83673)[0m f1_per_class: [0.592, 0.451, 0.564, 0.452, 0.179, 0.265, 0.5, 0.185, 0.17, 0.303]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:20 (running for 00:04:37.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.157 |      0.366 |                   41 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.629 |      0.366 |                   42 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.924 |      0.383 |                   42 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 87.745 |      0.218 |                   40 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.4221082089552239
[2m[36m(func pid=84053)[0m top5: 0.9151119402985075
[2m[36m(func pid=84053)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=84053)[0m f1_macro: 0.3656739089022759
[2m[36m(func pid=84053)[0m f1_weighted: 0.4379741390049687
[2m[36m(func pid=84053)[0m f1_per_class: [0.493, 0.45, 0.371, 0.569, 0.224, 0.149, 0.473, 0.344, 0.138, 0.444]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.38992537313432835
[2m[36m(func pid=84475)[0m top5: 0.8927238805970149
[2m[36m(func pid=84475)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=84475)[0m f1_macro: 0.3820374425930813
[2m[36m(func pid=84475)[0m f1_weighted: 0.3789750939186981
[2m[36m(func pid=84475)[0m f1_per_class: [0.623, 0.603, 0.692, 0.512, 0.104, 0.194, 0.217, 0.28, 0.218, 0.376]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m top1: 0.29617537313432835
[2m[36m(func pid=84902)[0m top5: 0.7131529850746269
[2m[36m(func pid=84902)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=84902)[0m f1_macro: 0.24778306130138752
[2m[36m(func pid=84902)[0m f1_weighted: 0.33455464695985115
[2m[36m(func pid=84902)[0m f1_per_class: [0.206, 0.184, 0.353, 0.521, 0.07, 0.165, 0.367, 0.251, 0.161, 0.198]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.3818 | Steps: 4 | Val loss: 1.6377 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.1707 | Steps: 4 | Val loss: 2.5064 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4356 | Steps: 4 | Val loss: 16.9653 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 28.6504 | Steps: 4 | Val loss: 166.3541 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=83673)[0m top1: 0.37826492537313433
[2m[36m(func pid=83673)[0m top5: 0.9239738805970149
[2m[36m(func pid=83673)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.3668637604124634
[2m[36m(func pid=83673)[0m f1_weighted: 0.41274631261183675
[2m[36m(func pid=83673)[0m f1_per_class: [0.58, 0.377, 0.579, 0.446, 0.172, 0.26, 0.519, 0.231, 0.156, 0.35]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4528917910447761
[2m[36m(func pid=84053)[0m top5: 0.9211753731343284
[2m[36m(func pid=84053)[0m f1_micro: 0.4528917910447761
[2m[36m(func pid=84053)[0m f1_macro: 0.379486913000955
[2m[36m(func pid=84053)[0m f1_weighted: 0.4630405914198571
[2m[36m(func pid=84053)[0m f1_per_class: [0.479, 0.453, 0.453, 0.574, 0.208, 0.125, 0.56, 0.333, 0.142, 0.467]
== Status ==
Current time: 2024-01-07 11:12:25 (running for 00:04:43.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.382 |      0.367 |                   42 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.171 |      0.379 |                   43 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0     |      0.382 |                   43 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 29.204 |      0.248 |                   41 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.363339552238806
[2m[36m(func pid=84475)[0m top5: 0.8889925373134329
[2m[36m(func pid=84475)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=84475)[0m f1_macro: 0.3565570424524885
[2m[36m(func pid=84475)[0m f1_weighted: 0.34364073256383304
[2m[36m(func pid=84475)[0m f1_per_class: [0.444, 0.578, 0.72, 0.412, 0.141, 0.248, 0.198, 0.27, 0.248, 0.306]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3689365671641791
[2m[36m(func pid=84902)[0m top5: 0.753731343283582
[2m[36m(func pid=84902)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=84902)[0m f1_macro: 0.3230328769415583
[2m[36m(func pid=84902)[0m f1_weighted: 0.4064847228672329
[2m[36m(func pid=84902)[0m f1_per_class: [0.455, 0.392, 0.636, 0.478, 0.105, 0.186, 0.526, 0.09, 0.189, 0.172]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0933 | Steps: 4 | Val loss: 1.6329 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1029 | Steps: 4 | Val loss: 2.4202 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.8259 | Steps: 4 | Val loss: 17.9933 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 3.7643 | Steps: 4 | Val loss: 149.4132 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=83673)[0m top1: 0.37779850746268656
[2m[36m(func pid=83673)[0m top5: 0.9230410447761194
[2m[36m(func pid=83673)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=83673)[0m f1_macro: 0.35593542451471805
[2m[36m(func pid=83673)[0m f1_weighted: 0.4093475021009621
[2m[36m(func pid=83673)[0m f1_per_class: [0.589, 0.32, 0.595, 0.518, 0.153, 0.236, 0.503, 0.114, 0.15, 0.381]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:31 (running for 00:04:48.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.093 |      0.356 |                   43 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.103 |      0.383 |                   44 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.436 |      0.357 |                   44 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 28.65  |      0.323 |                   42 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.4748134328358209
[2m[36m(func pid=84053)[0m top5: 0.9244402985074627
[2m[36m(func pid=84053)[0m f1_micro: 0.4748134328358209
[2m[36m(func pid=84053)[0m f1_macro: 0.38252567355948097
[2m[36m(func pid=84053)[0m f1_weighted: 0.4771952716073354
[2m[36m(func pid=84053)[0m f1_per_class: [0.497, 0.476, 0.473, 0.579, 0.189, 0.124, 0.6, 0.279, 0.148, 0.462]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.36100746268656714
[2m[36m(func pid=84475)[0m top5: 0.8852611940298507
[2m[36m(func pid=84475)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=84475)[0m f1_macro: 0.34892450370418426
[2m[36m(func pid=84475)[0m f1_weighted: 0.33790337998107417
[2m[36m(func pid=84475)[0m f1_per_class: [0.413, 0.575, 0.667, 0.409, 0.194, 0.266, 0.182, 0.269, 0.218, 0.297]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m top1: 0.44263059701492535
[2m[36m(func pid=84902)[0m top5: 0.789179104477612
[2m[36m(func pid=84902)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=84902)[0m f1_macro: 0.3716474011092325
[2m[36m(func pid=84902)[0m f1_weighted: 0.4518850055303223
[2m[36m(func pid=84902)[0m f1_per_class: [0.622, 0.489, 0.696, 0.502, 0.157, 0.174, 0.596, 0.044, 0.214, 0.224]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.1402 | Steps: 4 | Val loss: 1.6355 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2965 | Steps: 4 | Val loss: 2.3346 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.1991 | Steps: 4 | Val loss: 17.3939 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 35.4026 | Steps: 4 | Val loss: 158.3719 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=83673)[0m top1: 0.38572761194029853
[2m[36m(func pid=83673)[0m top5: 0.9235074626865671
[2m[36m(func pid=83673)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=83673)[0m f1_macro: 0.3428974586540016
[2m[36m(func pid=83673)[0m f1_weighted: 0.40786776328393565
[2m[36m(func pid=83673)[0m f1_per_class: [0.507, 0.294, 0.55, 0.567, 0.145, 0.25, 0.474, 0.074, 0.16, 0.408]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:36 (running for 00:04:54.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.14  |      0.343 |                   44 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.297 |      0.402 |                   45 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.826 |      0.349 |                   45 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  3.764 |      0.372 |                   43 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.4962686567164179
[2m[36m(func pid=84053)[0m top5: 0.9267723880597015
[2m[36m(func pid=84053)[0m f1_micro: 0.4962686567164179
[2m[36m(func pid=84053)[0m f1_macro: 0.4016096928707927
[2m[36m(func pid=84053)[0m f1_weighted: 0.4905986986672507
[2m[36m(func pid=84053)[0m f1_per_class: [0.55, 0.483, 0.55, 0.596, 0.179, 0.115, 0.616, 0.294, 0.185, 0.447]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.35261194029850745
[2m[36m(func pid=84475)[0m top5: 0.8843283582089553
[2m[36m(func pid=84475)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=84475)[0m f1_macro: 0.3413187895775543
[2m[36m(func pid=84475)[0m f1_weighted: 0.34874871309337024
[2m[36m(func pid=84475)[0m f1_per_class: [0.438, 0.589, 0.526, 0.43, 0.241, 0.269, 0.195, 0.247, 0.225, 0.254]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m top1: 0.45242537313432835
[2m[36m(func pid=84902)[0m top5: 0.7868470149253731
[2m[36m(func pid=84902)[0m f1_micro: 0.45242537313432835
[2m[36m(func pid=84902)[0m f1_macro: 0.34418503888254676
[2m[36m(func pid=84902)[0m f1_weighted: 0.4439346961919241
[2m[36m(func pid=84902)[0m f1_per_class: [0.327, 0.526, 0.692, 0.488, 0.133, 0.137, 0.592, 0.044, 0.22, 0.281]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.0386 | Steps: 4 | Val loss: 1.6325 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0819 | Steps: 4 | Val loss: 18.9823 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2336 | Steps: 4 | Val loss: 2.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 40.5382 | Steps: 4 | Val loss: 172.6379 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=83673)[0m top1: 0.3941231343283582
[2m[36m(func pid=83673)[0m top5: 0.9249067164179104
[2m[36m(func pid=83673)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=83673)[0m f1_macro: 0.3299029454307305
[2m[36m(func pid=83673)[0m f1_weighted: 0.41014732251426594
[2m[36m(func pid=83673)[0m f1_per_class: [0.471, 0.26, 0.5, 0.597, 0.144, 0.239, 0.473, 0.139, 0.155, 0.323]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:41 (running for 00:04:59.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.039 |      0.33  |                   45 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.297 |      0.402 |                   45 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.082 |      0.32  |                   47 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 35.403 |      0.344 |                   44 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3125
[2m[36m(func pid=84475)[0m top5: 0.8717350746268657
[2m[36m(func pid=84475)[0m f1_micro: 0.3125
[2m[36m(func pid=84475)[0m f1_macro: 0.32032433920236403
[2m[36m(func pid=84475)[0m f1_weighted: 0.3224495376634682
[2m[36m(func pid=84475)[0m f1_per_class: [0.469, 0.527, 0.421, 0.382, 0.255, 0.285, 0.184, 0.244, 0.205, 0.23]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.49533582089552236
[2m[36m(func pid=84053)[0m top5: 0.9333022388059702
[2m[36m(func pid=84053)[0m f1_micro: 0.49533582089552236
[2m[36m(func pid=84053)[0m f1_macro: 0.4055725667053654
[2m[36m(func pid=84053)[0m f1_weighted: 0.49202458106855607
[2m[36m(func pid=84053)[0m f1_per_class: [0.576, 0.486, 0.595, 0.597, 0.163, 0.127, 0.61, 0.311, 0.186, 0.404]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.39738805970149255
[2m[36m(func pid=84902)[0m top5: 0.7588619402985075
[2m[36m(func pid=84902)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=84902)[0m f1_macro: 0.3281386926423032
[2m[36m(func pid=84902)[0m f1_weighted: 0.40388980165640176
[2m[36m(func pid=84902)[0m f1_per_class: [0.167, 0.528, 0.692, 0.461, 0.125, 0.149, 0.472, 0.133, 0.182, 0.373]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4086 | Steps: 4 | Val loss: 1.5980 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 7.1343 | Steps: 4 | Val loss: 21.5846 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0806 | Steps: 4 | Val loss: 2.3188 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 29.5670 | Steps: 4 | Val loss: 192.3551 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=83673)[0m top1: 0.4123134328358209
[2m[36m(func pid=83673)[0m top5: 0.9211753731343284
[2m[36m(func pid=83673)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=83673)[0m f1_macro: 0.34175273162055475
[2m[36m(func pid=83673)[0m f1_weighted: 0.42175584962692636
[2m[36m(func pid=83673)[0m f1_per_class: [0.47, 0.232, 0.579, 0.612, 0.149, 0.239, 0.505, 0.184, 0.163, 0.286]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:47 (running for 00:05:04.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.409 |      0.342 |                   46 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.234 |      0.406 |                   46 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  7.134 |      0.291 |                   48 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 40.538 |      0.328 |                   45 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2728544776119403
[2m[36m(func pid=84475)[0m top5: 0.8736007462686567
[2m[36m(func pid=84475)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=84475)[0m f1_macro: 0.2913464754245066
[2m[36m(func pid=84475)[0m f1_weighted: 0.2851097359301568
[2m[36m(func pid=84475)[0m f1_per_class: [0.522, 0.361, 0.235, 0.356, 0.278, 0.29, 0.179, 0.257, 0.145, 0.291]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4822761194029851
[2m[36m(func pid=84053)[0m top5: 0.9328358208955224
[2m[36m(func pid=84053)[0m f1_micro: 0.4822761194029851
[2m[36m(func pid=84053)[0m f1_macro: 0.40487148115659677
[2m[36m(func pid=84053)[0m f1_weighted: 0.48682855150940496
[2m[36m(func pid=84053)[0m f1_per_class: [0.562, 0.487, 0.629, 0.587, 0.157, 0.165, 0.589, 0.308, 0.181, 0.384]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3596082089552239
[2m[36m(func pid=84902)[0m top5: 0.761660447761194
[2m[36m(func pid=84902)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=84902)[0m f1_macro: 0.3168595155610695
[2m[36m(func pid=84902)[0m f1_weighted: 0.35999544837600617
[2m[36m(func pid=84902)[0m f1_per_class: [0.044, 0.529, 0.741, 0.473, 0.138, 0.135, 0.301, 0.269, 0.17, 0.368]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.9362 | Steps: 4 | Val loss: 1.6171 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7673 | Steps: 4 | Val loss: 22.5886 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1211 | Steps: 4 | Val loss: 2.3393 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 29.2184 | Steps: 4 | Val loss: 206.6596 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=83673)[0m top1: 0.40671641791044777
[2m[36m(func pid=83673)[0m top5: 0.9127798507462687
[2m[36m(func pid=83673)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=83673)[0m f1_macro: 0.33981228971953154
[2m[36m(func pid=83673)[0m f1_weighted: 0.41586686258091865
[2m[36m(func pid=83673)[0m f1_per_class: [0.474, 0.231, 0.49, 0.613, 0.133, 0.257, 0.457, 0.292, 0.18, 0.273]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:12:52 (running for 00:05:09.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.936 |      0.34  |                   47 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.081 |      0.405 |                   47 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.767 |      0.305 |                   49 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 29.567 |      0.317 |                   46 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.269589552238806
[2m[36m(func pid=84475)[0m top5: 0.8549440298507462
[2m[36m(func pid=84475)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=84475)[0m f1_macro: 0.3045061731651309
[2m[36m(func pid=84475)[0m f1_weighted: 0.2803933636449009
[2m[36m(func pid=84475)[0m f1_per_class: [0.598, 0.242, 0.333, 0.37, 0.326, 0.307, 0.205, 0.253, 0.146, 0.265]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4664179104477612
[2m[36m(func pid=84053)[0m top5: 0.9305037313432836
[2m[36m(func pid=84053)[0m f1_micro: 0.4664179104477612
[2m[36m(func pid=84053)[0m f1_macro: 0.4054797474838333
[2m[36m(func pid=84053)[0m f1_weighted: 0.4808698406129651
[2m[36m(func pid=84053)[0m f1_per_class: [0.551, 0.515, 0.667, 0.59, 0.126, 0.168, 0.541, 0.347, 0.204, 0.346]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.36473880597014924
[2m[36m(func pid=84902)[0m top5: 0.7747201492537313
[2m[36m(func pid=84902)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=84902)[0m f1_macro: 0.31681140318546824
[2m[36m(func pid=84902)[0m f1_weighted: 0.3331663854671861
[2m[36m(func pid=84902)[0m f1_per_class: [0.128, 0.52, 0.645, 0.508, 0.074, 0.215, 0.139, 0.288, 0.217, 0.435]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0415 | Steps: 4 | Val loss: 1.6906 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3853 | Steps: 4 | Val loss: 20.4762 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.1447 | Steps: 4 | Val loss: 2.4383 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 49.9453 | Steps: 4 | Val loss: 236.9808 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=83673)[0m top1: 0.3763992537313433
[2m[36m(func pid=83673)[0m top5: 0.9006529850746269
[2m[36m(func pid=83673)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.3069580900182658
[2m[36m(func pid=83673)[0m f1_weighted: 0.3805040976203438
[2m[36m(func pid=83673)[0m f1_per_class: [0.457, 0.2, 0.353, 0.609, 0.109, 0.227, 0.365, 0.369, 0.159, 0.222]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.2989738805970149
[2m[36m(func pid=84475)[0m top5: 0.8479477611940298
[2m[36m(func pid=84475)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=84475)[0m f1_macro: 0.31112736352067444
[2m[36m(func pid=84475)[0m f1_weighted: 0.31144666820723965
[2m[36m(func pid=84475)[0m f1_per_class: [0.507, 0.268, 0.421, 0.446, 0.319, 0.289, 0.234, 0.278, 0.129, 0.22]
== Status ==
Current time: 2024-01-07 11:12:57 (running for 00:05:15.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  1.042 |      0.307 |                   48 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.121 |      0.405 |                   48 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.385 |      0.311 |                   50 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 29.218 |      0.317 |                   47 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4449626865671642
[2m[36m(func pid=84053)[0m top5: 0.9244402985074627
[2m[36m(func pid=84053)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=84053)[0m f1_macro: 0.411689665336481
[2m[36m(func pid=84053)[0m f1_weighted: 0.4671427190495836
[2m[36m(func pid=84053)[0m f1_per_class: [0.6, 0.517, 0.759, 0.584, 0.115, 0.188, 0.486, 0.349, 0.223, 0.295]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.37033582089552236
[2m[36m(func pid=84902)[0m top5: 0.7947761194029851
[2m[36m(func pid=84902)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=84902)[0m f1_macro: 0.3163207312436
[2m[36m(func pid=84902)[0m f1_weighted: 0.31955657200726373
[2m[36m(func pid=84902)[0m f1_per_class: [0.204, 0.545, 0.579, 0.54, 0.125, 0.236, 0.036, 0.291, 0.224, 0.383]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9483 | Steps: 4 | Val loss: 1.7246 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0353 | Steps: 4 | Val loss: 17.8534 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0825 | Steps: 4 | Val loss: 2.6742 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 70.5013 | Steps: 4 | Val loss: 267.9191 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=83673)[0m top1: 0.3689365671641791
[2m[36m(func pid=83673)[0m top5: 0.8847947761194029
[2m[36m(func pid=83673)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=83673)[0m f1_macro: 0.2911407465576238
[2m[36m(func pid=83673)[0m f1_weighted: 0.3751409488659636
[2m[36m(func pid=83673)[0m f1_per_class: [0.432, 0.164, 0.27, 0.609, 0.111, 0.224, 0.372, 0.364, 0.188, 0.178]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:13:03 (running for 00:05:20.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.948 |      0.291 |                   49 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.145 |      0.412 |                   49 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.035 |      0.331 |                   51 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 49.945 |      0.316 |                   48 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.355410447761194
[2m[36m(func pid=84475)[0m top5: 0.863339552238806
[2m[36m(func pid=84475)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=84475)[0m f1_macro: 0.33101134446572067
[2m[36m(func pid=84475)[0m f1_weighted: 0.36200716827946766
[2m[36m(func pid=84475)[0m f1_per_class: [0.439, 0.243, 0.571, 0.568, 0.237, 0.272, 0.311, 0.276, 0.145, 0.248]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4155783582089552
[2m[36m(func pid=84053)[0m top5: 0.9183768656716418
[2m[36m(func pid=84053)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=84053)[0m f1_macro: 0.40813918523640413
[2m[36m(func pid=84053)[0m f1_weighted: 0.43446304710325717
[2m[36m(func pid=84053)[0m f1_per_class: [0.596, 0.533, 0.815, 0.585, 0.115, 0.201, 0.363, 0.329, 0.237, 0.308]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.36986940298507465
[2m[36m(func pid=84902)[0m top5: 0.8078358208955224
[2m[36m(func pid=84902)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=84902)[0m f1_macro: 0.3239405304429094
[2m[36m(func pid=84902)[0m f1_weighted: 0.326799452048173
[2m[36m(func pid=84902)[0m f1_per_class: [0.357, 0.541, 0.473, 0.58, 0.185, 0.257, 0.012, 0.266, 0.247, 0.321]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.9200 | Steps: 4 | Val loss: 1.7432 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7067 | Steps: 4 | Val loss: 17.8031 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0489 | Steps: 4 | Val loss: 3.0545 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 10.7276 | Steps: 4 | Val loss: 278.2479 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=83673)[0m top1: 0.3591417910447761
[2m[36m(func pid=83673)[0m top5: 0.878731343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=83673)[0m f1_macro: 0.28409527500969795
[2m[36m(func pid=83673)[0m f1_weighted: 0.36530766940939685
[2m[36m(func pid=83673)[0m f1_per_class: [0.446, 0.175, 0.236, 0.594, 0.111, 0.203, 0.352, 0.38, 0.185, 0.158]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:13:08 (running for 00:05:25.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.92  |      0.284 |                   50 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.082 |      0.408 |                   50 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.707 |      0.319 |                   52 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 70.501 |      0.324 |                   49 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.38199626865671643
[2m[36m(func pid=84475)[0m top5: 0.8614738805970149
[2m[36m(func pid=84475)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=84475)[0m f1_macro: 0.3186916576804088
[2m[36m(func pid=84475)[0m f1_weighted: 0.37276068664702683
[2m[36m(func pid=84475)[0m f1_per_class: [0.343, 0.192, 0.545, 0.6, 0.174, 0.268, 0.353, 0.291, 0.145, 0.275]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.37453358208955223
[2m[36m(func pid=84053)[0m top5: 0.9015858208955224
[2m[36m(func pid=84053)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=84053)[0m f1_macro: 0.3819115907977234
[2m[36m(func pid=84053)[0m f1_weighted: 0.3851273021437813
[2m[36m(func pid=84053)[0m f1_per_class: [0.593, 0.529, 0.759, 0.557, 0.109, 0.188, 0.234, 0.318, 0.242, 0.29]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3400186567164179
[2m[36m(func pid=84902)[0m top5: 0.8059701492537313
[2m[36m(func pid=84902)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=84902)[0m f1_macro: 0.3065751059988166
[2m[36m(func pid=84902)[0m f1_weighted: 0.3168963969155756
[2m[36m(func pid=84902)[0m f1_per_class: [0.475, 0.523, 0.321, 0.578, 0.195, 0.207, 0.015, 0.236, 0.213, 0.304]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7090 | Steps: 4 | Val loss: 1.7915 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6894 | Steps: 4 | Val loss: 19.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2887 | Steps: 4 | Val loss: 3.4749 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 50.1399 | Steps: 4 | Val loss: 282.9141 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=83673)[0m top1: 0.3451492537313433
[2m[36m(func pid=83673)[0m top5: 0.8675373134328358
[2m[36m(func pid=83673)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.2727123641464206
[2m[36m(func pid=83673)[0m f1_weighted: 0.3548641783743595
[2m[36m(func pid=83673)[0m f1_per_class: [0.427, 0.181, 0.206, 0.589, 0.121, 0.195, 0.329, 0.352, 0.199, 0.13]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:13:13 (running for 00:05:31.01)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.709 |      0.273 |                   51 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.049 |      0.382 |                   51 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.689 |      0.31  |                   53 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 10.728 |      0.307 |                   50 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3931902985074627
[2m[36m(func pid=84475)[0m top5: 0.8493470149253731
[2m[36m(func pid=84475)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=84475)[0m f1_macro: 0.3101912707724101
[2m[36m(func pid=84475)[0m f1_weighted: 0.3637907764936995
[2m[36m(func pid=84475)[0m f1_per_class: [0.328, 0.111, 0.609, 0.617, 0.151, 0.203, 0.381, 0.276, 0.167, 0.261]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.353544776119403
[2m[36m(func pid=84053)[0m top5: 0.8843283582089553
[2m[36m(func pid=84053)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=84053)[0m f1_macro: 0.3652635180207213
[2m[36m(func pid=84053)[0m f1_weighted: 0.3568215640928827
[2m[36m(func pid=84053)[0m f1_per_class: [0.581, 0.511, 0.759, 0.558, 0.104, 0.201, 0.153, 0.299, 0.211, 0.276]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.30177238805970147
[2m[36m(func pid=84902)[0m top5: 0.7747201492537313
[2m[36m(func pid=84902)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=84902)[0m f1_macro: 0.27776426184462244
[2m[36m(func pid=84902)[0m f1_weighted: 0.29441572437129604
[2m[36m(func pid=84902)[0m f1_per_class: [0.433, 0.423, 0.274, 0.56, 0.106, 0.198, 0.022, 0.226, 0.264, 0.272]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7287 | Steps: 4 | Val loss: 1.8178 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2123 | Steps: 4 | Val loss: 19.9281 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1444 | Steps: 4 | Val loss: 3.7593 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 7.1701 | Steps: 4 | Val loss: 284.8532 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=83673)[0m top1: 0.33722014925373134
[2m[36m(func pid=83673)[0m top5: 0.8596082089552238
[2m[36m(func pid=83673)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=83673)[0m f1_macro: 0.2727306299591309
[2m[36m(func pid=83673)[0m f1_weighted: 0.34111367392025654
[2m[36m(func pid=83673)[0m f1_per_class: [0.49, 0.18, 0.197, 0.582, 0.124, 0.187, 0.287, 0.367, 0.182, 0.132]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.40345149253731344
[2m[36m(func pid=84475)[0m top5: 0.8460820895522388
[2m[36m(func pid=84475)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=84475)[0m f1_macro: 0.31833031791861643
[2m[36m(func pid=84475)[0m f1_weighted: 0.36637516347746474
[2m[36m(func pid=84475)[0m f1_per_class: [0.343, 0.097, 0.636, 0.608, 0.162, 0.131, 0.427, 0.269, 0.2, 0.31]
== Status ==
Current time: 2024-01-07 11:13:18 (running for 00:05:36.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.729 |      0.273 |                   52 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.289 |      0.365 |                   52 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.212 |      0.318 |                   54 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 50.14  |      0.278 |                   51 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.34375
[2m[36m(func pid=84053)[0m top5: 0.8647388059701493
[2m[36m(func pid=84053)[0m f1_micro: 0.34375
[2m[36m(func pid=84053)[0m f1_macro: 0.362696998962189
[2m[36m(func pid=84053)[0m f1_weighted: 0.34241567411136403
[2m[36m(func pid=84053)[0m f1_per_class: [0.602, 0.506, 0.774, 0.553, 0.098, 0.225, 0.102, 0.306, 0.199, 0.263]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2635261194029851
[2m[36m(func pid=84902)[0m top5: 0.730410447761194
[2m[36m(func pid=84902)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=84902)[0m f1_macro: 0.261584854463043
[2m[36m(func pid=84902)[0m f1_weighted: 0.2720639986792489
[2m[36m(func pid=84902)[0m f1_per_class: [0.515, 0.358, 0.232, 0.498, 0.067, 0.201, 0.045, 0.234, 0.173, 0.293]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.8271 | Steps: 4 | Val loss: 1.9046 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2352 | Steps: 4 | Val loss: 20.6914 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1159 | Steps: 4 | Val loss: 4.0091 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 27.5382 | Steps: 4 | Val loss: 279.6488 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=83673)[0m top1: 0.31576492537313433
[2m[36m(func pid=83673)[0m top5: 0.8260261194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.2570115846348745
[2m[36m(func pid=83673)[0m f1_weighted: 0.31474018071199733
[2m[36m(func pid=83673)[0m f1_per_class: [0.461, 0.224, 0.177, 0.568, 0.109, 0.167, 0.199, 0.356, 0.177, 0.132]
== Status ==
Current time: 2024-01-07 11:13:24 (running for 00:05:41.48)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.827 |      0.257 |                   53 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.144 |      0.363 |                   53 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.212 |      0.318 |                   54 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  7.17  |      0.262 |                   52 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.4146455223880597
[2m[36m(func pid=84475)[0m top5: 0.8386194029850746
[2m[36m(func pid=84475)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=84475)[0m f1_macro: 0.3227118725916151
[2m[36m(func pid=84475)[0m f1_weighted: 0.3694503528344609
[2m[36m(func pid=84475)[0m f1_per_class: [0.386, 0.073, 0.667, 0.612, 0.188, 0.08, 0.468, 0.248, 0.192, 0.314]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.32369402985074625
[2m[36m(func pid=84053)[0m top5: 0.8479477611940298
[2m[36m(func pid=84053)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=84053)[0m f1_macro: 0.350650725766528
[2m[36m(func pid=84053)[0m f1_weighted: 0.32559091032610193
[2m[36m(func pid=84053)[0m f1_per_class: [0.595, 0.492, 0.759, 0.519, 0.099, 0.229, 0.085, 0.308, 0.199, 0.222]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2355410447761194
[2m[36m(func pid=84902)[0m top5: 0.6870335820895522
[2m[36m(func pid=84902)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=84902)[0m f1_macro: 0.24438137032950813
[2m[36m(func pid=84902)[0m f1_weighted: 0.25922963239281427
[2m[36m(func pid=84902)[0m f1_per_class: [0.571, 0.256, 0.171, 0.39, 0.057, 0.206, 0.162, 0.232, 0.148, 0.25]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7243 | Steps: 4 | Val loss: 1.8754 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 4.5410 | Steps: 4 | Val loss: 20.2584 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.2634 | Steps: 4 | Val loss: 4.1141 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 62.5641 | Steps: 4 | Val loss: 265.9574 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:13:29 (running for 00:05:46.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.724 |      0.263 |                   54 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.116 |      0.351 |                   54 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.235 |      0.323 |                   55 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 27.538 |      0.244 |                   53 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m top1: 0.324160447761194
[2m[36m(func pid=83673)[0m top5: 0.8414179104477612
[2m[36m(func pid=83673)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=83673)[0m f1_macro: 0.2630612662361439
[2m[36m(func pid=83673)[0m f1_weighted: 0.32180660757215956
[2m[36m(func pid=83673)[0m f1_per_class: [0.446, 0.221, 0.186, 0.58, 0.121, 0.185, 0.205, 0.359, 0.187, 0.141]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.4244402985074627
[2m[36m(func pid=84475)[0m top5: 0.8348880597014925
[2m[36m(func pid=84475)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=84475)[0m f1_macro: 0.33301775959817814
[2m[36m(func pid=84475)[0m f1_weighted: 0.38855742175479513
[2m[36m(func pid=84475)[0m f1_per_class: [0.4, 0.165, 0.667, 0.611, 0.18, 0.059, 0.489, 0.241, 0.178, 0.34]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m top1: 0.3344216417910448
[2m[36m(func pid=84053)[0m top5: 0.8386194029850746
[2m[36m(func pid=84053)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=84053)[0m f1_macro: 0.3603506891323907
[2m[36m(func pid=84053)[0m f1_weighted: 0.32656643245613304
[2m[36m(func pid=84053)[0m f1_per_class: [0.61, 0.535, 0.759, 0.501, 0.107, 0.234, 0.071, 0.333, 0.199, 0.255]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.25279850746268656
[2m[36m(func pid=84902)[0m top5: 0.6893656716417911
[2m[36m(func pid=84902)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=84902)[0m f1_macro: 0.23634421701951255
[2m[36m(func pid=84902)[0m f1_weighted: 0.28217631595560366
[2m[36m(func pid=84902)[0m f1_per_class: [0.372, 0.186, 0.188, 0.437, 0.053, 0.209, 0.25, 0.234, 0.105, 0.329]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6689 | Steps: 4 | Val loss: 18.9978 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9126 | Steps: 4 | Val loss: 1.8885 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7572 | Steps: 4 | Val loss: 3.7766 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 59.9873 | Steps: 4 | Val loss: 254.9479 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:13:34 (running for 00:05:52.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.724 |      0.263 |                   54 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  1.263 |      0.36  |                   55 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.669 |      0.352 |                   57 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 62.564 |      0.236 |                   54 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.4183768656716418
[2m[36m(func pid=84475)[0m top5: 0.8418843283582089
[2m[36m(func pid=84475)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=84475)[0m f1_macro: 0.35171336851861645
[2m[36m(func pid=84475)[0m f1_weighted: 0.39395794487900265
[2m[36m(func pid=84475)[0m f1_per_class: [0.433, 0.216, 0.786, 0.614, 0.178, 0.051, 0.475, 0.22, 0.195, 0.349]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.31902985074626866
[2m[36m(func pid=83673)[0m top5: 0.8414179104477612
[2m[36m(func pid=83673)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=83673)[0m f1_macro: 0.2612516861354505
[2m[36m(func pid=83673)[0m f1_weighted: 0.31085066281405926
[2m[36m(func pid=83673)[0m f1_per_class: [0.449, 0.228, 0.22, 0.584, 0.116, 0.171, 0.168, 0.35, 0.174, 0.153]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m top1: 0.33955223880597013
[2m[36m(func pid=84053)[0m top5: 0.8694029850746269
[2m[36m(func pid=84053)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=84053)[0m f1_macro: 0.3719469133882901
[2m[36m(func pid=84053)[0m f1_weighted: 0.3389884009059348
[2m[36m(func pid=84053)[0m f1_per_class: [0.653, 0.533, 0.759, 0.471, 0.105, 0.243, 0.13, 0.358, 0.203, 0.265]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2835820895522388
[2m[36m(func pid=84902)[0m top5: 0.6777052238805971
[2m[36m(func pid=84902)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=84902)[0m f1_macro: 0.24666181394822456
[2m[36m(func pid=84902)[0m f1_weighted: 0.3223112990720639
[2m[36m(func pid=84902)[0m f1_per_class: [0.235, 0.118, 0.206, 0.449, 0.059, 0.203, 0.414, 0.241, 0.161, 0.381]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0767 | Steps: 4 | Val loss: 18.9955 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7832 | Steps: 4 | Val loss: 1.9423 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5379 | Steps: 4 | Val loss: 3.4093 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 6.5060 | Steps: 4 | Val loss: 250.3456 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:13:39 (running for 00:05:57.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.913 |      0.261 |                   55 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.757 |      0.372 |                   56 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.077 |      0.327 |                   58 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 59.987 |      0.247 |                   55 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3983208955223881
[2m[36m(func pid=84475)[0m top5: 0.8512126865671642
[2m[36m(func pid=84475)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=84475)[0m f1_macro: 0.3274965932294895
[2m[36m(func pid=84475)[0m f1_weighted: 0.39190542195530687
[2m[36m(func pid=84475)[0m f1_per_class: [0.434, 0.281, 0.579, 0.598, 0.157, 0.051, 0.452, 0.226, 0.172, 0.324]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3041044776119403
[2m[36m(func pid=83673)[0m top5: 0.8250932835820896
[2m[36m(func pid=83673)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=83673)[0m f1_macro: 0.25638347995755684
[2m[36m(func pid=83673)[0m f1_weighted: 0.30453089629916225
[2m[36m(func pid=83673)[0m f1_per_class: [0.449, 0.316, 0.183, 0.545, 0.108, 0.15, 0.143, 0.344, 0.165, 0.161]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3101679104477612
[2m[36m(func pid=84902)[0m top5: 0.6707089552238806
[2m[36m(func pid=84902)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=84902)[0m f1_macro: 0.25148031037286406
[2m[36m(func pid=84902)[0m f1_weighted: 0.34234249956381807
[2m[36m(func pid=84902)[0m f1_per_class: [0.156, 0.067, 0.282, 0.459, 0.073, 0.195, 0.511, 0.213, 0.177, 0.381]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.36007462686567165
[2m[36m(func pid=84053)[0m top5: 0.8885261194029851
[2m[36m(func pid=84053)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=84053)[0m f1_macro: 0.3908726282582412
[2m[36m(func pid=84053)[0m f1_weighted: 0.3709664932794445
[2m[36m(func pid=84053)[0m f1_per_class: [0.614, 0.54, 0.759, 0.477, 0.112, 0.264, 0.212, 0.388, 0.232, 0.312]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1882 | Steps: 4 | Val loss: 18.8989 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7314 | Steps: 4 | Val loss: 1.9388 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 95.4164 | Steps: 4 | Val loss: 265.9615 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0750 | Steps: 4 | Val loss: 2.9794 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:13:45 (running for 00:06:02.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.783 |      0.256 |                   56 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.538 |      0.391 |                   57 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.188 |      0.307 |                   59 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  6.506 |      0.251 |                   56 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3787313432835821
[2m[36m(func pid=84475)[0m top5: 0.8470149253731343
[2m[36m(func pid=84475)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=84475)[0m f1_macro: 0.3074130613056004
[2m[36m(func pid=84475)[0m f1_weighted: 0.39292371054762226
[2m[36m(func pid=84475)[0m f1_per_class: [0.45, 0.349, 0.426, 0.572, 0.147, 0.043, 0.452, 0.219, 0.149, 0.267]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.31763059701492535
[2m[36m(func pid=83673)[0m top5: 0.8180970149253731
[2m[36m(func pid=83673)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=83673)[0m f1_macro: 0.2711644722783204
[2m[36m(func pid=83673)[0m f1_weighted: 0.3164594239854694
[2m[36m(func pid=83673)[0m f1_per_class: [0.448, 0.426, 0.198, 0.507, 0.123, 0.144, 0.153, 0.349, 0.189, 0.175]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m top1: 0.39365671641791045
[2m[36m(func pid=84053)[0m top5: 0.9137126865671642
[2m[36m(func pid=84053)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=84053)[0m f1_macro: 0.41667045251686374
[2m[36m(func pid=84053)[0m f1_weighted: 0.41186152659970376
[2m[36m(func pid=84053)[0m f1_per_class: [0.608, 0.549, 0.786, 0.535, 0.119, 0.261, 0.288, 0.378, 0.234, 0.409]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3148320895522388
[2m[36m(func pid=84902)[0m top5: 0.6637126865671642
[2m[36m(func pid=84902)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=84902)[0m f1_macro: 0.2540876990426902
[2m[36m(func pid=84902)[0m f1_weighted: 0.33584857918143873
[2m[36m(func pid=84902)[0m f1_per_class: [0.132, 0.037, 0.393, 0.446, 0.105, 0.21, 0.53, 0.122, 0.166, 0.4]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6789 | Steps: 4 | Val loss: 19.5592 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7445 | Steps: 4 | Val loss: 1.8809 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6618 | Steps: 4 | Val loss: 2.8612 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 17.4663 | Steps: 4 | Val loss: 260.1621 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:13:50 (running for 00:06:08.05)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.731 |      0.271 |                   57 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.075 |      0.417 |                   58 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.679 |      0.298 |                   60 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 95.416 |      0.254 |                   57 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3358208955223881
[2m[36m(func pid=84475)[0m top5: 0.8362873134328358
[2m[36m(func pid=84475)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=84475)[0m f1_macro: 0.2981173671010945
[2m[36m(func pid=84475)[0m f1_weighted: 0.3662784756254747
[2m[36m(func pid=84475)[0m f1_per_class: [0.508, 0.441, 0.342, 0.479, 0.131, 0.056, 0.391, 0.201, 0.167, 0.265]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.33302238805970147
[2m[36m(func pid=83673)[0m top5: 0.8353544776119403
[2m[36m(func pid=83673)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=83673)[0m f1_macro: 0.28715448210537947
[2m[36m(func pid=83673)[0m f1_weighted: 0.3335096376717875
[2m[36m(func pid=83673)[0m f1_per_class: [0.483, 0.488, 0.257, 0.507, 0.106, 0.129, 0.178, 0.323, 0.208, 0.191]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.33255597014925375
[2m[36m(func pid=84902)[0m top5: 0.6711753731343284
[2m[36m(func pid=84902)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=84902)[0m f1_macro: 0.2698463756456224
[2m[36m(func pid=84902)[0m f1_weighted: 0.35414954925201425
[2m[36m(func pid=84902)[0m f1_per_class: [0.138, 0.082, 0.435, 0.457, 0.153, 0.255, 0.545, 0.077, 0.164, 0.393]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4085820895522388
[2m[36m(func pid=84053)[0m top5: 0.9193097014925373
[2m[36m(func pid=84053)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=84053)[0m f1_macro: 0.4274302055858171
[2m[36m(func pid=84053)[0m f1_weighted: 0.43617184065981346
[2m[36m(func pid=84053)[0m f1_per_class: [0.625, 0.543, 0.786, 0.515, 0.114, 0.242, 0.402, 0.336, 0.262, 0.45]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8821 | Steps: 4 | Val loss: 22.8939 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7398 | Steps: 4 | Val loss: 1.8185 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 20.8695 | Steps: 4 | Val loss: 249.5559 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0710 | Steps: 4 | Val loss: 2.6696 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:13:55 (running for 00:06:13.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.744 |      0.287 |                   58 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.662 |      0.427 |                   59 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.882 |      0.263 |                   61 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 17.466 |      0.27  |                   58 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.28404850746268656
[2m[36m(func pid=84475)[0m top5: 0.7938432835820896
[2m[36m(func pid=84475)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=84475)[0m f1_macro: 0.2628257292866224
[2m[36m(func pid=84475)[0m f1_weighted: 0.3189833885114844
[2m[36m(func pid=84475)[0m f1_per_class: [0.525, 0.407, 0.22, 0.383, 0.106, 0.062, 0.344, 0.215, 0.135, 0.232]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.34701492537313433
[2m[36m(func pid=83673)[0m top5: 0.8605410447761194
[2m[36m(func pid=83673)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=83673)[0m f1_macro: 0.30658247898133734
[2m[36m(func pid=83673)[0m f1_weighted: 0.3508757377565094
[2m[36m(func pid=83673)[0m f1_per_class: [0.526, 0.523, 0.304, 0.466, 0.125, 0.137, 0.246, 0.321, 0.225, 0.193]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.322294776119403
[2m[36m(func pid=84902)[0m top5: 0.675839552238806
[2m[36m(func pid=84902)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=84902)[0m f1_macro: 0.26759495241177805
[2m[36m(func pid=84902)[0m f1_weighted: 0.3398809562489082
[2m[36m(func pid=84902)[0m f1_per_class: [0.215, 0.095, 0.541, 0.416, 0.186, 0.26, 0.534, 0.042, 0.116, 0.27]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.43423507462686567
[2m[36m(func pid=84053)[0m top5: 0.9309701492537313
[2m[36m(func pid=84053)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=84053)[0m f1_macro: 0.4294528207512093
[2m[36m(func pid=84053)[0m f1_weighted: 0.46367390059481806
[2m[36m(func pid=84053)[0m f1_per_class: [0.632, 0.532, 0.786, 0.555, 0.11, 0.216, 0.475, 0.356, 0.216, 0.418]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.7036 | Steps: 4 | Val loss: 24.8875 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7008 | Steps: 4 | Val loss: 1.7717 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 67.6937 | Steps: 4 | Val loss: 261.1039 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0752 | Steps: 4 | Val loss: 2.6524 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:14:00 (running for 00:06:18.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.74  |      0.307 |                   59 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.071 |      0.429 |                   60 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.704 |      0.255 |                   62 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 20.87  |      0.268 |                   59 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.25699626865671643
[2m[36m(func pid=84475)[0m top5: 0.78125
[2m[36m(func pid=84475)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=84475)[0m f1_macro: 0.2554675201250031
[2m[36m(func pid=84475)[0m f1_weighted: 0.2816208178398678
[2m[36m(func pid=84475)[0m f1_per_class: [0.537, 0.465, 0.265, 0.27, 0.086, 0.071, 0.284, 0.233, 0.133, 0.211]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3605410447761194
[2m[36m(func pid=83673)[0m top5: 0.878731343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=83673)[0m f1_macro: 0.320084696121284
[2m[36m(func pid=83673)[0m f1_weighted: 0.36463148051396993
[2m[36m(func pid=83673)[0m f1_per_class: [0.559, 0.55, 0.317, 0.447, 0.125, 0.133, 0.293, 0.31, 0.239, 0.227]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3041044776119403
[2m[36m(func pid=84902)[0m top5: 0.6627798507462687
[2m[36m(func pid=84902)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=84902)[0m f1_macro: 0.2825883308046902
[2m[36m(func pid=84902)[0m f1_weighted: 0.33375325580262877
[2m[36m(func pid=84902)[0m f1_per_class: [0.31, 0.127, 0.581, 0.395, 0.18, 0.269, 0.504, 0.043, 0.108, 0.31]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.44449626865671643
[2m[36m(func pid=84053)[0m top5: 0.9314365671641791
[2m[36m(func pid=84053)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=84053)[0m f1_macro: 0.4282141804451968
[2m[36m(func pid=84053)[0m f1_weighted: 0.46775032045187576
[2m[36m(func pid=84053)[0m f1_per_class: [0.652, 0.532, 0.759, 0.551, 0.108, 0.183, 0.508, 0.33, 0.219, 0.441]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0060 | Steps: 4 | Val loss: 29.2096 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7145 | Steps: 4 | Val loss: 1.7304 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 6.4213 | Steps: 4 | Val loss: 253.8099 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7781 | Steps: 4 | Val loss: 2.5712 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:14:06 (running for 00:06:23.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.701 |      0.32  |                   60 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.075 |      0.428 |                   61 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.006 |      0.231 |                   63 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 67.694 |      0.283 |                   60 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.23274253731343283
[2m[36m(func pid=84475)[0m top5: 0.7168843283582089
[2m[36m(func pid=84475)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=84475)[0m f1_macro: 0.23062327960333118
[2m[36m(func pid=84475)[0m f1_weighted: 0.23815896147895046
[2m[36m(func pid=84475)[0m f1_per_class: [0.517, 0.496, 0.168, 0.14, 0.07, 0.096, 0.23, 0.269, 0.12, 0.2]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3843283582089552
[2m[36m(func pid=83673)[0m top5: 0.8810634328358209
[2m[36m(func pid=83673)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=83673)[0m f1_macro: 0.339738867800616
[2m[36m(func pid=83673)[0m f1_weighted: 0.38426728255302856
[2m[36m(func pid=83673)[0m f1_per_class: [0.584, 0.57, 0.338, 0.447, 0.161, 0.155, 0.333, 0.318, 0.241, 0.25]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3148320895522388
[2m[36m(func pid=84902)[0m top5: 0.7010261194029851
[2m[36m(func pid=84902)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=84902)[0m f1_macro: 0.31321913568773524
[2m[36m(func pid=84902)[0m f1_weighted: 0.34277467815072904
[2m[36m(func pid=84902)[0m f1_per_class: [0.424, 0.158, 0.741, 0.466, 0.228, 0.278, 0.432, 0.055, 0.132, 0.219]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4743470149253731
[2m[36m(func pid=84053)[0m top5: 0.9430970149253731
[2m[36m(func pid=84053)[0m f1_micro: 0.4743470149253731
[2m[36m(func pid=84053)[0m f1_macro: 0.4342029271940645
[2m[36m(func pid=84053)[0m f1_weighted: 0.4895772658694565
[2m[36m(func pid=84053)[0m f1_per_class: [0.652, 0.524, 0.71, 0.57, 0.121, 0.168, 0.565, 0.378, 0.214, 0.44]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2793 | Steps: 4 | Val loss: 32.9535 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7697 | Steps: 4 | Val loss: 1.7238 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 18.0162 | Steps: 4 | Val loss: 242.4137 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4001 | Steps: 4 | Val loss: 2.6224 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:14:11 (running for 00:06:28.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.715 |      0.34  |                   61 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.778 |      0.434 |                   62 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.279 |      0.225 |                   64 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  6.421 |      0.313 |                   61 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.22527985074626866
[2m[36m(func pid=84475)[0m top5: 0.6683768656716418
[2m[36m(func pid=84475)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=84475)[0m f1_macro: 0.22495141253321785
[2m[36m(func pid=84475)[0m f1_weighted: 0.20849664402299967
[2m[36m(func pid=84475)[0m f1_per_class: [0.493, 0.505, 0.16, 0.058, 0.079, 0.153, 0.178, 0.287, 0.123, 0.213]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.39552238805970147
[2m[36m(func pid=83673)[0m top5: 0.8824626865671642
[2m[36m(func pid=83673)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=83673)[0m f1_macro: 0.34908730057319237
[2m[36m(func pid=83673)[0m f1_weighted: 0.3960191392623778
[2m[36m(func pid=83673)[0m f1_per_class: [0.6, 0.558, 0.364, 0.444, 0.157, 0.153, 0.379, 0.316, 0.251, 0.268]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.34841417910447764
[2m[36m(func pid=84902)[0m top5: 0.7527985074626866
[2m[36m(func pid=84902)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=84902)[0m f1_macro: 0.33573948964882894
[2m[36m(func pid=84902)[0m f1_weighted: 0.35765371728684997
[2m[36m(func pid=84902)[0m f1_per_class: [0.504, 0.297, 0.692, 0.54, 0.239, 0.285, 0.305, 0.144, 0.168, 0.183]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4710820895522388
[2m[36m(func pid=84053)[0m top5: 0.9398320895522388
[2m[36m(func pid=84053)[0m f1_micro: 0.4710820895522388
[2m[36m(func pid=84053)[0m f1_macro: 0.4267571352114806
[2m[36m(func pid=84053)[0m f1_weighted: 0.48414093095598265
[2m[36m(func pid=84053)[0m f1_per_class: [0.624, 0.523, 0.71, 0.565, 0.127, 0.156, 0.566, 0.341, 0.211, 0.444]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7375 | Steps: 4 | Val loss: 35.4404 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4315 | Steps: 4 | Val loss: 1.6658 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 37.1339 | Steps: 4 | Val loss: 268.8031 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:14:16 (running for 00:06:34.06)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.77  |      0.349 |                   62 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.4   |      0.427 |                   63 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.737 |      0.216 |                   65 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 18.016 |      0.336 |                   62 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8591 | Steps: 4 | Val loss: 2.7512 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=84475)[0m top1: 0.22014925373134328
[2m[36m(func pid=84475)[0m top5: 0.6357276119402985
[2m[36m(func pid=84475)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=84475)[0m f1_macro: 0.2160379891530923
[2m[36m(func pid=84475)[0m f1_weighted: 0.19488287968664703
[2m[36m(func pid=84475)[0m f1_per_class: [0.455, 0.507, 0.162, 0.07, 0.081, 0.177, 0.116, 0.272, 0.122, 0.195]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.41044776119402987
[2m[36m(func pid=83673)[0m top5: 0.8922574626865671
[2m[36m(func pid=83673)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=83673)[0m f1_macro: 0.36265260224624835
[2m[36m(func pid=83673)[0m f1_weighted: 0.41023947136106464
[2m[36m(func pid=83673)[0m f1_per_class: [0.606, 0.56, 0.375, 0.46, 0.161, 0.18, 0.394, 0.328, 0.283, 0.28]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.34794776119402987
[2m[36m(func pid=84902)[0m top5: 0.7709888059701493
[2m[36m(func pid=84902)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=84902)[0m f1_macro: 0.32250764757267925
[2m[36m(func pid=84902)[0m f1_weighted: 0.33229840602169713
[2m[36m(func pid=84902)[0m f1_per_class: [0.536, 0.32, 0.667, 0.551, 0.231, 0.274, 0.202, 0.134, 0.184, 0.127]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m top1: 0.4612873134328358
[2m[36m(func pid=84053)[0m top5: 0.9253731343283582
[2m[36m(func pid=84053)[0m f1_micro: 0.4612873134328358
[2m[36m(func pid=84053)[0m f1_macro: 0.40980713283395226
[2m[36m(func pid=84053)[0m f1_weighted: 0.4744011895893511
[2m[36m(func pid=84053)[0m f1_per_class: [0.612, 0.525, 0.647, 0.543, 0.117, 0.126, 0.572, 0.32, 0.199, 0.436]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.1454 | Steps: 4 | Val loss: 38.4200 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7208 | Steps: 4 | Val loss: 1.6193 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 24.4261 | Steps: 4 | Val loss: 327.0741 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:14:21 (running for 00:06:39.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.431 |      0.363 |                   63 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.859 |      0.41  |                   64 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.145 |      0.221 |                   66 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 37.134 |      0.323 |                   63 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.23787313432835822
[2m[36m(func pid=84475)[0m top5: 0.6110074626865671
[2m[36m(func pid=84475)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=84475)[0m f1_macro: 0.22136178122562744
[2m[36m(func pid=84475)[0m f1_weighted: 0.1830879618824368
[2m[36m(func pid=84475)[0m f1_per_class: [0.457, 0.521, 0.203, 0.055, 0.072, 0.191, 0.071, 0.29, 0.148, 0.206]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.1819 | Steps: 4 | Val loss: 2.9303 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=83673)[0m top1: 0.4239738805970149
[2m[36m(func pid=83673)[0m top5: 0.9057835820895522
[2m[36m(func pid=83673)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=83673)[0m f1_macro: 0.38417684408487024
[2m[36m(func pid=83673)[0m f1_weighted: 0.4236684089302036
[2m[36m(func pid=83673)[0m f1_per_class: [0.625, 0.563, 0.436, 0.445, 0.19, 0.193, 0.44, 0.334, 0.277, 0.337]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.33255597014925375
[2m[36m(func pid=84902)[0m top5: 0.7742537313432836
[2m[36m(func pid=84902)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=84902)[0m f1_macro: 0.3277024677632321
[2m[36m(func pid=84902)[0m f1_weighted: 0.295782346554339
[2m[36m(func pid=84902)[0m f1_per_class: [0.619, 0.32, 0.714, 0.518, 0.227, 0.19, 0.11, 0.253, 0.22, 0.106]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.7026 | Steps: 4 | Val loss: 37.3217 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=84053)[0m top1: 0.435634328358209
[2m[36m(func pid=84053)[0m top5: 0.9127798507462687
[2m[36m(func pid=84053)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=84053)[0m f1_macro: 0.39855950123752565
[2m[36m(func pid=84053)[0m f1_weighted: 0.4520460262937982
[2m[36m(func pid=84053)[0m f1_per_class: [0.564, 0.546, 0.647, 0.522, 0.115, 0.086, 0.519, 0.345, 0.204, 0.438]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7288 | Steps: 4 | Val loss: 1.6114 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:14:26 (running for 00:06:44.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.721 |      0.384 |                   64 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.182 |      0.399 |                   65 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.703 |      0.214 |                   67 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 24.426 |      0.328 |                   64 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.23600746268656717
[2m[36m(func pid=84475)[0m top5: 0.6352611940298507
[2m[36m(func pid=84475)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=84475)[0m f1_macro: 0.21430110840311958
[2m[36m(func pid=84475)[0m f1_weighted: 0.18140495175603522
[2m[36m(func pid=84475)[0m f1_per_class: [0.428, 0.508, 0.186, 0.076, 0.067, 0.183, 0.062, 0.278, 0.129, 0.227]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 65.0823 | Steps: 4 | Val loss: 372.7244 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1147 | Steps: 4 | Val loss: 2.9429 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=83673)[0m top1: 0.427705223880597
[2m[36m(func pid=83673)[0m top5: 0.9109141791044776
[2m[36m(func pid=83673)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=83673)[0m f1_macro: 0.39802389013838135
[2m[36m(func pid=83673)[0m f1_weighted: 0.4275941045569647
[2m[36m(func pid=83673)[0m f1_per_class: [0.653, 0.571, 0.522, 0.46, 0.185, 0.19, 0.432, 0.335, 0.274, 0.359]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.324160447761194
[2m[36m(func pid=84902)[0m top5: 0.7761194029850746
[2m[36m(func pid=84902)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=84902)[0m f1_macro: 0.3093303688163611
[2m[36m(func pid=84902)[0m f1_weighted: 0.28530711383423224
[2m[36m(func pid=84902)[0m f1_per_class: [0.563, 0.364, 0.625, 0.513, 0.242, 0.226, 0.051, 0.269, 0.141, 0.099]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6565 | Steps: 4 | Val loss: 37.3271 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=84053)[0m top1: 0.43703358208955223
[2m[36m(func pid=84053)[0m top5: 0.9113805970149254
[2m[36m(func pid=84053)[0m f1_micro: 0.43703358208955223
[2m[36m(func pid=84053)[0m f1_macro: 0.3862446346810293
[2m[36m(func pid=84053)[0m f1_weighted: 0.4521267653383586
[2m[36m(func pid=84053)[0m f1_per_class: [0.55, 0.549, 0.512, 0.506, 0.12, 0.101, 0.528, 0.346, 0.211, 0.439]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5613 | Steps: 4 | Val loss: 1.5946 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 11:14:32 (running for 00:06:49.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.729 |      0.398 |                   65 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.115 |      0.386 |                   66 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  2.656 |      0.223 |                   68 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 65.082 |      0.309 |                   65 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=84475)[0m top1: 0.24207089552238806

[2m[36m(func pid=84475)[0m top5: 0.6403917910447762
[2m[36m(func pid=84475)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=84475)[0m f1_macro: 0.22314329207402261
[2m[36m(func pid=84475)[0m f1_weighted: 0.1926805013503407
[2m[36m(func pid=84475)[0m f1_per_class: [0.454, 0.509, 0.239, 0.121, 0.068, 0.161, 0.057, 0.315, 0.132, 0.176]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 88.5945 | Steps: 4 | Val loss: 362.0901 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0291 | Steps: 4 | Val loss: 3.1111 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=83673)[0m top1: 0.43097014925373134
[2m[36m(func pid=83673)[0m top5: 0.909981343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=83673)[0m f1_macro: 0.39447293865919225
[2m[36m(func pid=83673)[0m f1_weighted: 0.4368136570173412
[2m[36m(func pid=83673)[0m f1_per_class: [0.614, 0.567, 0.48, 0.481, 0.189, 0.212, 0.438, 0.352, 0.261, 0.35]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3316231343283582
[2m[36m(func pid=84902)[0m top5: 0.7527985074626866
[2m[36m(func pid=84902)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=84902)[0m f1_macro: 0.31541602763647775
[2m[36m(func pid=84902)[0m f1_weighted: 0.3005754572700513
[2m[36m(func pid=84902)[0m f1_per_class: [0.576, 0.437, 0.55, 0.505, 0.238, 0.279, 0.045, 0.292, 0.127, 0.106]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4212 | Steps: 4 | Val loss: 33.3997 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=84053)[0m top1: 0.417910447761194
[2m[36m(func pid=84053)[0m top5: 0.894589552238806
[2m[36m(func pid=84053)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=84053)[0m f1_macro: 0.3624161025689431
[2m[36m(func pid=84053)[0m f1_weighted: 0.4313398274152096
[2m[36m(func pid=84053)[0m f1_per_class: [0.514, 0.56, 0.413, 0.474, 0.129, 0.119, 0.482, 0.355, 0.19, 0.388]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5236 | Steps: 4 | Val loss: 1.5646 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:14:37 (running for 00:06:55.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.561 |      0.394 |                   66 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.029 |      0.362 |                   67 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.421 |      0.242 |                   69 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 88.594 |      0.315 |                   66 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2667910447761194
[2m[36m(func pid=84475)[0m top5: 0.6944962686567164
[2m[36m(func pid=84475)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=84475)[0m f1_macro: 0.24229387479201417
[2m[36m(func pid=84475)[0m f1_weighted: 0.23183475575311693
[2m[36m(func pid=84475)[0m f1_per_class: [0.466, 0.513, 0.299, 0.243, 0.078, 0.173, 0.068, 0.308, 0.131, 0.144]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 22.5703 | Steps: 4 | Val loss: 362.2494 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0895 | Steps: 4 | Val loss: 3.4492 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=83673)[0m top1: 0.43423507462686567
[2m[36m(func pid=83673)[0m top5: 0.9197761194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=83673)[0m f1_macro: 0.38723554767923996
[2m[36m(func pid=83673)[0m f1_weighted: 0.4445124537036084
[2m[36m(func pid=83673)[0m f1_per_class: [0.612, 0.573, 0.381, 0.509, 0.154, 0.202, 0.441, 0.355, 0.245, 0.4]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3246268656716418
[2m[36m(func pid=84902)[0m top5: 0.7378731343283582
[2m[36m(func pid=84902)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=84902)[0m f1_macro: 0.3115604676688075
[2m[36m(func pid=84902)[0m f1_weighted: 0.28240728370705126
[2m[36m(func pid=84902)[0m f1_per_class: [0.567, 0.442, 0.533, 0.428, 0.186, 0.304, 0.039, 0.301, 0.15, 0.165]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1164 | Steps: 4 | Val loss: 31.7190 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=84053)[0m top1: 0.384794776119403
[2m[36m(func pid=84053)[0m top5: 0.8703358208955224
[2m[36m(func pid=84053)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=84053)[0m f1_macro: 0.3298614591072838
[2m[36m(func pid=84053)[0m f1_weighted: 0.39036545380084264
[2m[36m(func pid=84053)[0m f1_per_class: [0.497, 0.54, 0.329, 0.448, 0.141, 0.126, 0.388, 0.341, 0.181, 0.308]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9353 | Steps: 4 | Val loss: 1.5463 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=84475)[0m top1: 0.28218283582089554
[2m[36m(func pid=84475)[0m top5: 0.7140858208955224
[2m[36m(func pid=84475)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=84475)[0m f1_macro: 0.25258148715288276
[2m[36m(func pid=84475)[0m f1_weighted: 0.2579634004331513
[2m[36m(func pid=84475)[0m f1_per_class: [0.446, 0.511, 0.369, 0.354, 0.08, 0.162, 0.065, 0.261, 0.14, 0.138]
[2m[36m(func pid=84475)[0m 
== Status ==
Current time: 2024-01-07 11:14:42 (running for 00:07:00.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.524 |      0.387 |                   67 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.09  |      0.33  |                   68 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.116 |      0.253 |                   70 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 22.57  |      0.312 |                   67 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 10.9486 | Steps: 4 | Val loss: 388.2224 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1156 | Steps: 4 | Val loss: 3.8714 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=83673)[0m top1: 0.44029850746268656
[2m[36m(func pid=83673)[0m top5: 0.9235074626865671
[2m[36m(func pid=83673)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=83673)[0m f1_macro: 0.40089312082896084
[2m[36m(func pid=83673)[0m f1_weighted: 0.45497536822404355
[2m[36m(func pid=83673)[0m f1_per_class: [0.667, 0.56, 0.381, 0.518, 0.155, 0.221, 0.463, 0.356, 0.244, 0.444]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.3069029850746269
[2m[36m(func pid=84902)[0m top5: 0.7024253731343284
[2m[36m(func pid=84902)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=84902)[0m f1_macro: 0.2979920427655654
[2m[36m(func pid=84902)[0m f1_weighted: 0.2484579903119148
[2m[36m(func pid=84902)[0m f1_per_class: [0.54, 0.419, 0.453, 0.312, 0.193, 0.312, 0.045, 0.314, 0.105, 0.288]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3934 | Steps: 4 | Val loss: 29.9953 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=84053)[0m top1: 0.3512126865671642
[2m[36m(func pid=84053)[0m top5: 0.8334888059701493
[2m[36m(func pid=84053)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=84053)[0m f1_macro: 0.2989986703130561
[2m[36m(func pid=84053)[0m f1_weighted: 0.3509269988310427
[2m[36m(func pid=84053)[0m f1_per_class: [0.478, 0.531, 0.234, 0.41, 0.13, 0.118, 0.305, 0.331, 0.189, 0.262]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6417 | Steps: 4 | Val loss: 1.5625 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:14:48 (running for 00:07:05.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.935 |      0.401 |                   68 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.116 |      0.299 |                   69 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.393 |      0.278 |                   71 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 10.949 |      0.298 |                   68 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31669776119402987
[2m[36m(func pid=84475)[0m top5: 0.7308768656716418
[2m[36m(func pid=84475)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=84475)[0m f1_macro: 0.2784744107186721
[2m[36m(func pid=84475)[0m f1_weighted: 0.290240202865118
[2m[36m(func pid=84475)[0m f1_per_class: [0.443, 0.527, 0.436, 0.442, 0.088, 0.142, 0.071, 0.349, 0.139, 0.148]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 78.6010 | Steps: 4 | Val loss: 414.9576 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0413 | Steps: 4 | Val loss: 3.9860 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=83673)[0m top1: 0.43656716417910446
[2m[36m(func pid=83673)[0m top5: 0.9249067164179104
[2m[36m(func pid=83673)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=83673)[0m f1_macro: 0.39616628082273364
[2m[36m(func pid=83673)[0m f1_weighted: 0.452615748700186
[2m[36m(func pid=83673)[0m f1_per_class: [0.637, 0.561, 0.364, 0.511, 0.148, 0.251, 0.45, 0.374, 0.228, 0.438]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.2943097014925373
[2m[36m(func pid=84902)[0m top5: 0.6819029850746269
[2m[36m(func pid=84902)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=84902)[0m f1_macro: 0.29191343482536464
[2m[36m(func pid=84902)[0m f1_weighted: 0.22964137486027433
[2m[36m(func pid=84902)[0m f1_per_class: [0.441, 0.415, 0.444, 0.248, 0.155, 0.294, 0.048, 0.31, 0.173, 0.391]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.0198 | Steps: 4 | Val loss: 27.5434 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=84053)[0m top1: 0.33488805970149255
[2m[36m(func pid=84053)[0m top5: 0.8208955223880597
[2m[36m(func pid=84053)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=84053)[0m f1_macro: 0.2825764808167695
[2m[36m(func pid=84053)[0m f1_weighted: 0.32936280152927705
[2m[36m(func pid=84053)[0m f1_per_class: [0.411, 0.534, 0.234, 0.422, 0.16, 0.119, 0.23, 0.312, 0.18, 0.223]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4624 | Steps: 4 | Val loss: 1.5793 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84475)[0m top1: 0.34888059701492535
[2m[36m(func pid=84475)[0m top5: 0.769589552238806
[2m[36m(func pid=84475)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=84475)[0m f1_macro: 0.31276544493723474
[2m[36m(func pid=84475)[0m f1_weighted: 0.31896752052175137
[2m[36m(func pid=84475)[0m f1_per_class: [0.544, 0.526, 0.571, 0.508, 0.104, 0.095, 0.111, 0.359, 0.142, 0.167]
[2m[36m(func pid=84475)[0m 
== Status ==
Current time: 2024-01-07 11:14:53 (running for 00:07:11.02)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.642 |      0.396 |                   69 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.041 |      0.283 |                   70 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  3.02  |      0.313 |                   72 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 78.601 |      0.292 |                   69 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 58.3069 | Steps: 4 | Val loss: 397.4123 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0626 | Steps: 4 | Val loss: 4.3656 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=83673)[0m top1: 0.4291044776119403
[2m[36m(func pid=83673)[0m top5: 0.9225746268656716
[2m[36m(func pid=83673)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=83673)[0m f1_macro: 0.39437251249769717
[2m[36m(func pid=83673)[0m f1_weighted: 0.4498289981642003
[2m[36m(func pid=83673)[0m f1_per_class: [0.592, 0.551, 0.393, 0.51, 0.136, 0.235, 0.454, 0.38, 0.225, 0.467]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.28404850746268656
[2m[36m(func pid=84902)[0m top5: 0.6772388059701493
[2m[36m(func pid=84902)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=84902)[0m f1_macro: 0.2736077237633925
[2m[36m(func pid=84902)[0m f1_weighted: 0.22515592093224482
[2m[36m(func pid=84902)[0m f1_per_class: [0.414, 0.436, 0.292, 0.227, 0.115, 0.256, 0.048, 0.361, 0.202, 0.386]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6337 | Steps: 4 | Val loss: 27.0090 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=84053)[0m top1: 0.3162313432835821
[2m[36m(func pid=84053)[0m top5: 0.7901119402985075
[2m[36m(func pid=84053)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=84053)[0m f1_macro: 0.2660312633000935
[2m[36m(func pid=84053)[0m f1_weighted: 0.3061928735942395
[2m[36m(func pid=84053)[0m f1_per_class: [0.433, 0.522, 0.17, 0.441, 0.171, 0.113, 0.145, 0.305, 0.192, 0.168]
[2m[36m(func pid=84053)[0m 
== Status ==
Current time: 2024-01-07 11:14:58 (running for 00:07:16.13)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.462 |      0.394 |                   70 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.063 |      0.266 |                   71 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.634 |      0.32  |                   73 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 58.307 |      0.274 |                   70 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.36007462686567165
[2m[36m(func pid=84475)[0m top5: 0.7793843283582089
[2m[36m(func pid=84475)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=84475)[0m f1_macro: 0.32017072566570043
[2m[36m(func pid=84475)[0m f1_weighted: 0.3235418313976301
[2m[36m(func pid=84475)[0m f1_per_class: [0.569, 0.525, 0.667, 0.519, 0.113, 0.054, 0.139, 0.31, 0.131, 0.175]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4554 | Steps: 4 | Val loss: 1.6060 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2499 | Steps: 4 | Val loss: 372.8307 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.8300 | Steps: 4 | Val loss: 4.8032 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=83673)[0m top1: 0.41884328358208955
[2m[36m(func pid=83673)[0m top5: 0.9221082089552238
[2m[36m(func pid=83673)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=83673)[0m f1_macro: 0.3891955896704057
[2m[36m(func pid=83673)[0m f1_weighted: 0.44212832664385493
[2m[36m(func pid=83673)[0m f1_per_class: [0.592, 0.532, 0.364, 0.521, 0.158, 0.263, 0.421, 0.377, 0.209, 0.455]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84902)[0m top1: 0.27052238805970147
[2m[36m(func pid=84902)[0m top5: 0.6847014925373134
[2m[36m(func pid=84902)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=84902)[0m f1_macro: 0.24758208255118652
[2m[36m(func pid=84902)[0m f1_weighted: 0.2341987994424797
[2m[36m(func pid=84902)[0m f1_per_class: [0.351, 0.493, 0.178, 0.268, 0.084, 0.215, 0.046, 0.312, 0.148, 0.38]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0096 | Steps: 4 | Val loss: 27.0342 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=84053)[0m top1: 0.28824626865671643
[2m[36m(func pid=84053)[0m top5: 0.7495335820895522
[2m[36m(func pid=84053)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=84053)[0m f1_macro: 0.24180769446974945
[2m[36m(func pid=84053)[0m f1_weighted: 0.2741571189972961
[2m[36m(func pid=84053)[0m f1_per_class: [0.402, 0.506, 0.128, 0.408, 0.156, 0.096, 0.09, 0.313, 0.162, 0.157]
[2m[36m(func pid=84053)[0m 
== Status ==
Current time: 2024-01-07 11:15:03 (running for 00:07:21.40)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.455 |      0.389 |                   71 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.83  |      0.242 |                   72 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  1.01  |      0.322 |                   74 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      |  1.25  |      0.248 |                   71 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3605410447761194
[2m[36m(func pid=84475)[0m top5: 0.7873134328358209
[2m[36m(func pid=84475)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=84475)[0m f1_macro: 0.3215910301804092
[2m[36m(func pid=84475)[0m f1_weighted: 0.33223700698060993
[2m[36m(func pid=84475)[0m f1_per_class: [0.566, 0.52, 0.714, 0.521, 0.098, 0.03, 0.184, 0.283, 0.13, 0.171]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 57.8171 | Steps: 4 | Val loss: 409.6435 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6742 | Steps: 4 | Val loss: 1.6053 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9081 | Steps: 4 | Val loss: 5.1370 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5451 | Steps: 4 | Val loss: 26.2791 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=84902)[0m top1: 0.21828358208955223
[2m[36m(func pid=84902)[0m top5: 0.6637126865671642
[2m[36m(func pid=84902)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=84902)[0m f1_macro: 0.22112552421777262
[2m[36m(func pid=84902)[0m f1_weighted: 0.20526367545774937
[2m[36m(func pid=84902)[0m f1_per_class: [0.296, 0.469, 0.104, 0.246, 0.081, 0.168, 0.012, 0.284, 0.131, 0.418]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=83673)[0m top1: 0.41744402985074625
[2m[36m(func pid=83673)[0m top5: 0.9169776119402985
[2m[36m(func pid=83673)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=83673)[0m f1_macro: 0.3858562778285658
[2m[36m(func pid=83673)[0m f1_weighted: 0.4435590556933029
[2m[36m(func pid=83673)[0m f1_per_class: [0.586, 0.502, 0.329, 0.55, 0.164, 0.27, 0.416, 0.37, 0.202, 0.469]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84053)[0m top1: 0.28638059701492535
[2m[36m(func pid=84053)[0m top5: 0.7210820895522388
[2m[36m(func pid=84053)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=84053)[0m f1_macro: 0.23414846126218417
[2m[36m(func pid=84053)[0m f1_weighted: 0.27174975994786116
[2m[36m(func pid=84053)[0m f1_per_class: [0.389, 0.485, 0.115, 0.46, 0.145, 0.07, 0.057, 0.315, 0.159, 0.146]
[2m[36m(func pid=84053)[0m 
== Status ==
Current time: 2024-01-07 11:15:09 (running for 00:07:26.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.31
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.674 |      0.386 |                   72 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.908 |      0.234 |                   73 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.545 |      0.31  |                   75 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 57.817 |      0.221 |                   72 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3689365671641791
[2m[36m(func pid=84475)[0m top5: 0.7933768656716418
[2m[36m(func pid=84475)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=84475)[0m f1_macro: 0.30987947034322383
[2m[36m(func pid=84475)[0m f1_weighted: 0.3437501893674227
[2m[36m(func pid=84475)[0m f1_per_class: [0.545, 0.51, 0.609, 0.532, 0.091, 0.022, 0.232, 0.235, 0.14, 0.182]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 53.4631 | Steps: 4 | Val loss: 470.7584 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4720 | Steps: 4 | Val loss: 1.6110 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.8532 | Steps: 4 | Val loss: 5.3657 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=84902)[0m top1: 0.15811567164179105
[2m[36m(func pid=84902)[0m top5: 0.6375932835820896
[2m[36m(func pid=84902)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=84902)[0m f1_macro: 0.18639094561911523
[2m[36m(func pid=84902)[0m f1_weighted: 0.15701949512807856
[2m[36m(func pid=84902)[0m f1_per_class: [0.286, 0.277, 0.115, 0.248, 0.081, 0.094, 0.0, 0.245, 0.093, 0.426]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2558 | Steps: 4 | Val loss: 27.5601 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=83673)[0m top1: 0.41277985074626866
[2m[36m(func pid=83673)[0m top5: 0.9202425373134329
[2m[36m(func pid=83673)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=83673)[0m f1_macro: 0.3788039654998138
[2m[36m(func pid=83673)[0m f1_weighted: 0.4370269987149231
[2m[36m(func pid=83673)[0m f1_per_class: [0.581, 0.49, 0.343, 0.554, 0.165, 0.278, 0.398, 0.366, 0.201, 0.413]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:15:14 (running for 00:07:31.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.31
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING  | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |  0.472 |      0.379 |                   73 |
| train_98a10_00001 | RUNNING  | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |  0.853 |      0.233 |                   74 |
| train_98a10_00002 | RUNNING  | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |  0.545 |      0.31  |                   75 |
| train_98a10_00003 | RUNNING  | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 53.463 |      0.186 |                   73 |
| train_98a10_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.27845149253731344
[2m[36m(func pid=84053)[0m top5: 0.7103544776119403
[2m[36m(func pid=84053)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=84053)[0m f1_macro: 0.2334085967636875
[2m[36m(func pid=84053)[0m f1_weighted: 0.2646526632861143
[2m[36m(func pid=84053)[0m f1_per_class: [0.429, 0.451, 0.107, 0.478, 0.154, 0.063, 0.039, 0.292, 0.166, 0.156]
[2m[36m(func pid=84053)[0m 
[2m[36m(func pid=84475)[0m top1: 0.37033582089552236
[2m[36m(func pid=84475)[0m top5: 0.7877798507462687
[2m[36m(func pid=84475)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=84475)[0m f1_macro: 0.3130044879286305
[2m[36m(func pid=84475)[0m f1_weighted: 0.34262684183992687
[2m[36m(func pid=84475)[0m f1_per_class: [0.552, 0.526, 0.583, 0.531, 0.083, 0.015, 0.211, 0.289, 0.151, 0.189]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 112.8523 | Steps: 4 | Val loss: 550.1373 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4604 | Steps: 4 | Val loss: 1.6142 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84902)[0m top1: 0.12406716417910447
[2m[36m(func pid=84902)[0m top5: 0.59375
[2m[36m(func pid=84902)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=84902)[0m f1_macro: 0.13647359084128877
[2m[36m(func pid=84902)[0m f1_weighted: 0.10945789516730779
[2m[36m(func pid=84902)[0m f1_per_class: [0.316, 0.105, 0.1, 0.214, 0.102, 0.05, 0.0, 0.237, 0.079, 0.162]
[2m[36m(func pid=84902)[0m 
[2m[36m(func pid=84053)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7090 | Steps: 4 | Val loss: 5.3958 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3860 | Steps: 4 | Val loss: 25.5310 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=83673)[0m top1: 0.40904850746268656
[2m[36m(func pid=83673)[0m top5: 0.9193097014925373
[2m[36m(func pid=83673)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=83673)[0m f1_macro: 0.37906089282936445
[2m[36m(func pid=83673)[0m f1_weighted: 0.4337142445608585
[2m[36m(func pid=83673)[0m f1_per_class: [0.571, 0.46, 0.393, 0.574, 0.167, 0.269, 0.389, 0.359, 0.201, 0.406]
[2m[36m(func pid=83673)[0m 
== Status ==
Current time: 2024-01-07 11:15:19 (running for 00:07:37.08)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.29475
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 3 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+---------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673 | 0.0001 |       0.99 |         0      |   0.46  |      0.379 |                   74 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475 | 0.01   |       0.99 |         0      |   0.256 |      0.313 |                   76 |
| train_98a10_00003 | RUNNING    | 192.168.7.53:84902 | 0.1    |       0.99 |         0      | 112.852 |      0.136 |                   74 |
| train_98a10_00004 | PENDING    |                    | 0.0001 |       0.9  |         0      |         |            |                      |
| train_98a10_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |         |            |                      |
| train_98a10_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |         |            |                      |
| train_98a10_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |         |            |                      |
| train_98a10_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_98a10_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_98a10_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_98a10_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_98a10_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053 | 0.001  |       0.99 |         0      |   0.709 |      0.249 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84053)[0m top1: 0.29617537313432835
[2m[36m(func pid=84053)[0m top5: 0.7159514925373134
[2m[36m(func pid=84053)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=84053)[0m f1_macro: 0.248528921425907
[2m[36m(func pid=84053)[0m f1_weighted: 0.2761759174487526
[2m[36m(func pid=84053)[0m f1_per_class: [0.474, 0.47, 0.113, 0.499, 0.164, 0.094, 0.031, 0.293, 0.168, 0.179]
[2m[36m(func pid=84475)[0m top1: 0.37220149253731344
[2m[36m(func pid=84475)[0m top5: 0.804570895522388
[2m[36m(func pid=84475)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=84475)[0m f1_macro: 0.32136779420169925
[2m[36m(func pid=84475)[0m f1_weighted: 0.3608501414619383
[2m[36m(func pid=84475)[0m f1_per_class: [0.552, 0.493, 0.609, 0.542, 0.096, 0.015, 0.277, 0.311, 0.146, 0.173]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=84902)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 13.7285 | Steps: 4 | Val loss: 554.3273 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7864 | Steps: 4 | Val loss: 1.6797 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84902)[0m top1: 0.12453358208955224
[2m[36m(func pid=84902)[0m top5: 0.5895522388059702
[2m[36m(func pid=84902)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=84902)[0m f1_macro: 0.1400869192918463
[2m[36m(func pid=84902)[0m f1_weighted: 0.0993536952886513
[2m[36m(func pid=84902)[0m f1_per_class: [0.426, 0.067, 0.084, 0.192, 0.148, 0.045, 0.006, 0.207, 0.099, 0.125]
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 2.0512 | Steps: 4 | Val loss: 25.0326 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=83673)[0m top1: 0.3871268656716418
[2m[36m(func pid=83673)[0m top5: 0.9127798507462687
[2m[36m(func pid=83673)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=83673)[0m f1_macro: 0.36280273493620674
[2m[36m(func pid=83673)[0m f1_weighted: 0.41000196363745645
[2m[36m(func pid=83673)[0m f1_per_class: [0.543, 0.421, 0.358, 0.564, 0.182, 0.273, 0.344, 0.375, 0.162, 0.406]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.3810634328358209
[2m[36m(func pid=84475)[0m top5: 0.8190298507462687
[2m[36m(func pid=84475)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=84475)[0m f1_macro: 0.32105253438968057
[2m[36m(func pid=84475)[0m f1_weighted: 0.38235729825948905
[2m[36m(func pid=84475)[0m f1_per_class: [0.489, 0.494, 0.609, 0.549, 0.081, 0.022, 0.344, 0.309, 0.146, 0.168]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.4487 | Steps: 4 | Val loss: 1.7361 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 2.7172 | Steps: 4 | Val loss: 22.8188 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=83673)[0m top1: 0.37033582089552236
[2m[36m(func pid=83673)[0m top5: 0.9015858208955224
[2m[36m(func pid=83673)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=83673)[0m f1_macro: 0.34325699983861097
[2m[36m(func pid=83673)[0m f1_weighted: 0.3888116652729419
[2m[36m(func pid=83673)[0m f1_per_class: [0.473, 0.382, 0.387, 0.571, 0.166, 0.27, 0.3, 0.364, 0.154, 0.366]
[2m[36m(func pid=102433)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102433)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=102433)[0m Configuration completed!
[2m[36m(func pid=102433)[0m New optimizer parameters:
[2m[36m(func pid=102433)[0m SGD (
[2m[36m(func pid=102433)[0m Parameter Group 0
[2m[36m(func pid=102433)[0m     dampening: 0
[2m[36m(func pid=102433)[0m     differentiable: False
[2m[36m(func pid=102433)[0m     foreach: None
[2m[36m(func pid=102433)[0m     lr: 0.0001
[2m[36m(func pid=102433)[0m     maximize: False
[2m[36m(func pid=102433)[0m     momentum: 0.9
[2m[36m(func pid=102433)[0m     nesterov: False
[2m[36m(func pid=102433)[0m     weight_decay: 0
[2m[36m(func pid=102433)[0m )
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=84475)[0m top1: 0.39738805970149255
[2m[36m(func pid=84475)[0m top5: 0.8390858208955224
[2m[36m(func pid=84475)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=84475)[0m f1_macro: 0.328235734163188
[2m[36m(func pid=84475)[0m f1_weighted: 0.4031359012321669
[2m[36m(func pid=84475)[0m f1_per_class: [0.446, 0.504, 0.609, 0.565, 0.084, 0.035, 0.389, 0.31, 0.156, 0.184]
== Status ==
Current time: 2024-01-07 11:15:25 (running for 00:07:42.51)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.786 |      0.363 |                   75 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  2.051 |      0.321 |                   78 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 11:15:30 (running for 00:07:48.30)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.786 |      0.363 |                   75 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  2.717 |      0.328 |                   79 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102477)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102477)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=102477)[0m Configuration completed!
[2m[36m(func pid=102477)[0m New optimizer parameters:
[2m[36m(func pid=102477)[0m SGD (
[2m[36m(func pid=102477)[0m Parameter Group 0
[2m[36m(func pid=102477)[0m     dampening: 0
[2m[36m(func pid=102477)[0m     differentiable: False
[2m[36m(func pid=102477)[0m     foreach: None
[2m[36m(func pid=102477)[0m     lr: 0.001
[2m[36m(func pid=102477)[0m     maximize: False
[2m[36m(func pid=102477)[0m     momentum: 0.9
[2m[36m(func pid=102477)[0m     nesterov: False
[2m[36m(func pid=102477)[0m     weight_decay: 0
[2m[36m(func pid=102477)[0m )
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1768 | Steps: 4 | Val loss: 2.5459 | Batch size: 32 | lr: 0.0001 | Duration: 4.52s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4073 | Steps: 4 | Val loss: 1.7200 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4183 | Steps: 4 | Val loss: 19.4725 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9505 | Steps: 4 | Val loss: 2.4784 | Batch size: 32 | lr: 0.001 | Duration: 4.43s
[2m[36m(func pid=102433)[0m top1: 0.06296641791044776
[2m[36m(func pid=102433)[0m top5: 0.478544776119403
[2m[36m(func pid=102433)[0m f1_micro: 0.06296641791044776
[2m[36m(func pid=102433)[0m f1_macro: 0.03743564852726017
[2m[36m(func pid=102433)[0m f1_weighted: 0.03546897340868278
[2m[36m(func pid=102433)[0m f1_per_class: [0.084, 0.015, 0.0, 0.076, 0.0, 0.025, 0.0, 0.101, 0.022, 0.052]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=83673)[0m top1: 0.37546641791044777
[2m[36m(func pid=83673)[0m top5: 0.9053171641791045
[2m[36m(func pid=83673)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=83673)[0m f1_macro: 0.34267905248738634
[2m[36m(func pid=83673)[0m f1_weighted: 0.38879246487281965
[2m[36m(func pid=83673)[0m f1_per_class: [0.463, 0.34, 0.387, 0.587, 0.156, 0.248, 0.314, 0.368, 0.169, 0.394]
== Status ==
Current time: 2024-01-07 11:15:36 (running for 00:07:53.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.407 |      0.343 |                   77 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  2.717 |      0.328 |                   79 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  3.177 |      0.037 |                    1 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |        |            |                      |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.4216417910447761
[2m[36m(func pid=84475)[0m top5: 0.8745335820895522
[2m[36m(func pid=84475)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=84475)[0m f1_macro: 0.364863516796629
[2m[36m(func pid=84475)[0m f1_weighted: 0.43978648196114273
[2m[36m(func pid=84475)[0m f1_per_class: [0.486, 0.515, 0.696, 0.567, 0.101, 0.118, 0.468, 0.287, 0.181, 0.231]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102477)[0m top1: 0.06529850746268656
[2m[36m(func pid=102477)[0m top5: 0.48740671641791045
[2m[36m(func pid=102477)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=102477)[0m f1_macro: 0.04047716316078577
[2m[36m(func pid=102477)[0m f1_weighted: 0.046956263330769465
[2m[36m(func pid=102477)[0m f1_per_class: [0.081, 0.019, 0.0, 0.116, 0.0, 0.02, 0.0, 0.096, 0.04, 0.032]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9533 | Steps: 4 | Val loss: 2.5625 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.5559 | Steps: 4 | Val loss: 1.7610 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.0986 | Steps: 4 | Val loss: 18.5302 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0722 | Steps: 4 | Val loss: 2.4104 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=102433)[0m top1: 0.0625
[2m[36m(func pid=102433)[0m top5: 0.458955223880597
[2m[36m(func pid=102433)[0m f1_micro: 0.0625
[2m[36m(func pid=102433)[0m f1_macro: 0.034682157243311806
[2m[36m(func pid=102433)[0m f1_weighted: 0.043149473775926234
[2m[36m(func pid=102433)[0m f1_per_class: [0.048, 0.048, 0.0, 0.092, 0.0, 0.02, 0.0, 0.093, 0.0, 0.045]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:15:41 (running for 00:07:59.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.556 |      0.338 |                   78 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.418 |      0.365 |                   80 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.953 |      0.035 |                    2 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.95  |      0.04  |                    1 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m top1: 0.3670708955223881
[2m[36m(func pid=83673)[0m top5: 0.9001865671641791
[2m[36m(func pid=83673)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=83673)[0m f1_macro: 0.3381733516909039
[2m[36m(func pid=83673)[0m f1_weighted: 0.38610112137499963
[2m[36m(func pid=83673)[0m f1_per_class: [0.463, 0.365, 0.429, 0.577, 0.137, 0.238, 0.308, 0.357, 0.171, 0.337]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m top1: 0.41744402985074625
[2m[36m(func pid=84475)[0m top5: 0.886660447761194
[2m[36m(func pid=84475)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=84475)[0m f1_macro: 0.37504256258741947
[2m[36m(func pid=84475)[0m f1_weighted: 0.44169111394824767
[2m[36m(func pid=84475)[0m f1_per_class: [0.393, 0.539, 0.75, 0.533, 0.102, 0.218, 0.455, 0.296, 0.178, 0.286]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102477)[0m top1: 0.07695895522388059
[2m[36m(func pid=102477)[0m top5: 0.49580223880597013
[2m[36m(func pid=102477)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=102477)[0m f1_macro: 0.05203197470820653
[2m[36m(func pid=102477)[0m f1_weighted: 0.07463799045103799
[2m[36m(func pid=102477)[0m f1_per_class: [0.055, 0.103, 0.0, 0.142, 0.0, 0.041, 0.015, 0.091, 0.036, 0.037]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1482 | Steps: 4 | Val loss: 2.5655 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1596 | Steps: 4 | Val loss: 20.5025 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4520 | Steps: 4 | Val loss: 1.7535 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8602 | Steps: 4 | Val loss: 2.3054 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=102433)[0m top1: 0.05970149253731343
[2m[36m(func pid=102433)[0m top5: 0.4552238805970149
[2m[36m(func pid=102433)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=102433)[0m f1_macro: 0.03404482991480388
[2m[36m(func pid=102433)[0m f1_weighted: 0.04615754348434834
[2m[36m(func pid=102433)[0m f1_per_class: [0.048, 0.054, 0.0, 0.106, 0.0, 0.007, 0.0, 0.087, 0.0, 0.038]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:15:46 (running for 00:08:04.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.556 |      0.338 |                   78 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.16  |      0.359 |                   82 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  3.148 |      0.034 |                    3 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  3.072 |      0.052 |                    2 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.38526119402985076
[2m[36m(func pid=84475)[0m top5: 0.8880597014925373
[2m[36m(func pid=84475)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=84475)[0m f1_macro: 0.3587464518653008
[2m[36m(func pid=84475)[0m f1_weighted: 0.4103658543871895
[2m[36m(func pid=84475)[0m f1_per_class: [0.378, 0.543, 0.696, 0.454, 0.095, 0.241, 0.42, 0.273, 0.148, 0.338]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3656716417910448
[2m[36m(func pid=83673)[0m top5: 0.9006529850746269
[2m[36m(func pid=83673)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=83673)[0m f1_macro: 0.3350588145443006
[2m[36m(func pid=83673)[0m f1_weighted: 0.3774655696632149
[2m[36m(func pid=83673)[0m f1_per_class: [0.481, 0.32, 0.414, 0.594, 0.136, 0.237, 0.289, 0.353, 0.163, 0.364]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.12220149253731344
[2m[36m(func pid=102477)[0m top5: 0.5648320895522388
[2m[36m(func pid=102477)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=102477)[0m f1_macro: 0.09019168451449593
[2m[36m(func pid=102477)[0m f1_weighted: 0.13028825516394066
[2m[36m(func pid=102477)[0m f1_per_class: [0.106, 0.23, 0.071, 0.063, 0.0, 0.049, 0.191, 0.087, 0.083, 0.021]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0603 | Steps: 4 | Val loss: 2.5532 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.1529 | Steps: 4 | Val loss: 23.9227 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4479 | Steps: 4 | Val loss: 1.7784 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8008 | Steps: 4 | Val loss: 2.2627 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=102433)[0m top1: 0.0625
[2m[36m(func pid=102433)[0m top5: 0.44029850746268656
[2m[36m(func pid=102433)[0m f1_micro: 0.0625
[2m[36m(func pid=102433)[0m f1_macro: 0.03936040734920721
[2m[36m(func pid=102433)[0m f1_weighted: 0.054558664537862504
[2m[36m(func pid=102433)[0m f1_per_class: [0.05, 0.052, 0.0, 0.131, 0.0, 0.013, 0.0, 0.093, 0.022, 0.032]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:15:51 (running for 00:08:09.38)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.452 |      0.335 |                   79 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.153 |      0.323 |                   83 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  3.06  |      0.039 |                    4 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.86  |      0.09  |                    3 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3414179104477612
[2m[36m(func pid=84475)[0m top5: 0.8647388059701493
[2m[36m(func pid=84475)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=84475)[0m f1_macro: 0.3232868624416347
[2m[36m(func pid=84475)[0m f1_weighted: 0.36652273417147013
[2m[36m(func pid=84475)[0m f1_per_class: [0.347, 0.5, 0.636, 0.384, 0.114, 0.238, 0.38, 0.262, 0.072, 0.298]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.35867537313432835
[2m[36m(func pid=83673)[0m top5: 0.8950559701492538
[2m[36m(func pid=83673)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=83673)[0m f1_macro: 0.33323854575885425
[2m[36m(func pid=83673)[0m f1_weighted: 0.3721988513281138
[2m[36m(func pid=83673)[0m f1_per_class: [0.497, 0.369, 0.429, 0.585, 0.104, 0.205, 0.26, 0.364, 0.183, 0.337]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.23041044776119404
[2m[36m(func pid=102477)[0m top5: 0.5965485074626866
[2m[36m(func pid=102477)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=102477)[0m f1_macro: 0.12264779244248591
[2m[36m(func pid=102477)[0m f1_weighted: 0.1780450587939558
[2m[36m(func pid=102477)[0m f1_per_class: [0.177, 0.258, 0.156, 0.007, 0.0, 0.04, 0.391, 0.048, 0.077, 0.073]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0845 | Steps: 4 | Val loss: 2.5179 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 3.8308 | Steps: 4 | Val loss: 27.6506 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4709 | Steps: 4 | Val loss: 1.8035 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7488 | Steps: 4 | Val loss: 2.3085 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=102433)[0m top1: 0.06809701492537314
[2m[36m(func pid=102433)[0m top5: 0.4388992537313433
[2m[36m(func pid=102433)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=102433)[0m f1_macro: 0.04719746737992713
[2m[36m(func pid=102433)[0m f1_weighted: 0.06607656640707578
[2m[36m(func pid=102433)[0m f1_per_class: [0.054, 0.088, 0.0, 0.147, 0.0, 0.019, 0.0, 0.08, 0.051, 0.033]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:15:57 (running for 00:08:14.77)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.448 |      0.333 |                   80 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.831 |      0.29  |                   84 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  3.084 |      0.047 |                    5 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.801 |      0.123 |                    4 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3064365671641791
[2m[36m(func pid=84475)[0m top5: 0.8404850746268657
[2m[36m(func pid=84475)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=84475)[0m f1_macro: 0.2895374221669066
[2m[36m(func pid=84475)[0m f1_weighted: 0.3229946444285478
[2m[36m(func pid=84475)[0m f1_per_class: [0.326, 0.469, 0.571, 0.278, 0.121, 0.238, 0.359, 0.259, 0.051, 0.222]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3530783582089552
[2m[36m(func pid=83673)[0m top5: 0.8917910447761194
[2m[36m(func pid=83673)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=83673)[0m f1_macro: 0.31758200370006195
[2m[36m(func pid=83673)[0m f1_weighted: 0.36461111632584176
[2m[36m(func pid=83673)[0m f1_per_class: [0.439, 0.366, 0.364, 0.591, 0.097, 0.2, 0.243, 0.349, 0.165, 0.364]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2196828358208955
[2m[36m(func pid=102477)[0m top5: 0.5489738805970149
[2m[36m(func pid=102477)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=102477)[0m f1_macro: 0.11463021448312141
[2m[36m(func pid=102477)[0m f1_weighted: 0.17275737055186102
[2m[36m(func pid=102477)[0m f1_per_class: [0.184, 0.167, 0.127, 0.007, 0.024, 0.025, 0.437, 0.0, 0.11, 0.066]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9306 | Steps: 4 | Val loss: 2.4902 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 2.8736 | Steps: 4 | Val loss: 30.8878 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.5595 | Steps: 4 | Val loss: 1.8798 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.4950 | Steps: 4 | Val loss: 2.4530 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=102433)[0m top1: 0.07602611940298508
[2m[36m(func pid=102433)[0m top5: 0.42490671641791045
[2m[36m(func pid=102433)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=102433)[0m f1_macro: 0.05126246762167207
[2m[36m(func pid=102433)[0m f1_weighted: 0.07736248223590732
[2m[36m(func pid=102433)[0m f1_per_class: [0.051, 0.08, 0.0, 0.182, 0.0, 0.03, 0.006, 0.075, 0.057, 0.031]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:02 (running for 00:08:20.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.471 |      0.318 |                   81 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  2.874 |      0.275 |                   85 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.931 |      0.051 |                    6 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.749 |      0.115 |                    5 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.2775186567164179
[2m[36m(func pid=84475)[0m top5: 0.8264925373134329
[2m[36m(func pid=84475)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=84475)[0m f1_macro: 0.2753466862521073
[2m[36m(func pid=84475)[0m f1_weighted: 0.28675608230555083
[2m[36m(func pid=84475)[0m f1_per_class: [0.361, 0.472, 0.571, 0.25, 0.124, 0.224, 0.268, 0.269, 0.026, 0.188]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.33861940298507465
[2m[36m(func pid=83673)[0m top5: 0.8791977611940298
[2m[36m(func pid=83673)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=83673)[0m f1_macro: 0.3037076304619709
[2m[36m(func pid=83673)[0m f1_weighted: 0.3536608479832968
[2m[36m(func pid=83673)[0m f1_per_class: [0.398, 0.396, 0.358, 0.566, 0.096, 0.171, 0.224, 0.352, 0.198, 0.278]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.06763059701492537
[2m[36m(func pid=102477)[0m top5: 0.4916044776119403
[2m[36m(func pid=102477)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=102477)[0m f1_macro: 0.10530293268729873
[2m[36m(func pid=102477)[0m f1_weighted: 0.07317404830534986
[2m[36m(func pid=102477)[0m f1_per_class: [0.217, 0.034, 0.462, 0.007, 0.022, 0.007, 0.18, 0.015, 0.076, 0.035]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8835 | Steps: 4 | Val loss: 2.4759 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.5323 | Steps: 4 | Val loss: 31.2234 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4228 | Steps: 4 | Val loss: 1.8602 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6249 | Steps: 4 | Val loss: 2.4697 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=102433)[0m top1: 0.08162313432835822
[2m[36m(func pid=102433)[0m top5: 0.4291044776119403
[2m[36m(func pid=102433)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=102433)[0m f1_macro: 0.05356003422363037
[2m[36m(func pid=102433)[0m f1_weighted: 0.08346854034942741
[2m[36m(func pid=102433)[0m f1_per_class: [0.053, 0.107, 0.0, 0.183, 0.0, 0.024, 0.012, 0.076, 0.053, 0.027]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:07 (running for 00:08:25.41)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.56  |      0.304 |                   82 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  1.532 |      0.262 |                   86 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.884 |      0.054 |                    7 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.495 |      0.105 |                    6 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.27472014925373134
[2m[36m(func pid=84475)[0m top5: 0.8199626865671642
[2m[36m(func pid=84475)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=84475)[0m f1_macro: 0.2622888380670765
[2m[36m(func pid=84475)[0m f1_weighted: 0.28416716747050486
[2m[36m(func pid=84475)[0m f1_per_class: [0.276, 0.502, 0.571, 0.26, 0.104, 0.227, 0.238, 0.264, 0.051, 0.129]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.34468283582089554
[2m[36m(func pid=83673)[0m top5: 0.8815298507462687
[2m[36m(func pid=83673)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=83673)[0m f1_macro: 0.29938974052040407
[2m[36m(func pid=83673)[0m f1_weighted: 0.3564760442343544
[2m[36m(func pid=83673)[0m f1_per_class: [0.398, 0.38, 0.342, 0.566, 0.093, 0.191, 0.236, 0.361, 0.196, 0.231]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.05550373134328358
[2m[36m(func pid=102477)[0m top5: 0.5228544776119403
[2m[36m(func pid=102477)[0m f1_micro: 0.05550373134328358
[2m[36m(func pid=102477)[0m f1_macro: 0.11315698223023123
[2m[36m(func pid=102477)[0m f1_weighted: 0.048024307628408425
[2m[36m(func pid=102477)[0m f1_per_class: [0.242, 0.021, 0.476, 0.048, 0.037, 0.014, 0.034, 0.146, 0.068, 0.045]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9563 | Steps: 4 | Val loss: 2.4601 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 3.4224 | Steps: 4 | Val loss: 30.7877 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3964 | Steps: 4 | Val loss: 1.8203 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3789 | Steps: 4 | Val loss: 2.1931 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=102433)[0m top1: 0.08069029850746269
[2m[36m(func pid=102433)[0m top5: 0.43330223880597013
[2m[36m(func pid=102433)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=102433)[0m f1_macro: 0.05382694321781835
[2m[36m(func pid=102433)[0m f1_weighted: 0.08505424188627153
[2m[36m(func pid=102433)[0m f1_per_class: [0.052, 0.126, 0.0, 0.174, 0.0, 0.029, 0.015, 0.066, 0.056, 0.02]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:13 (running for 00:08:30.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.423 |      0.299 |                   83 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.422 |      0.258 |                   87 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.956 |      0.054 |                    8 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.625 |      0.113 |                    7 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.27798507462686567
[2m[36m(func pid=84475)[0m top5: 0.8120335820895522
[2m[36m(func pid=84475)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=84475)[0m f1_macro: 0.2581030582532654
[2m[36m(func pid=84475)[0m f1_weighted: 0.28684922203383606
[2m[36m(func pid=84475)[0m f1_per_class: [0.261, 0.519, 0.5, 0.223, 0.082, 0.216, 0.272, 0.275, 0.095, 0.138]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3591417910447761
[2m[36m(func pid=83673)[0m top5: 0.8885261194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=83673)[0m f1_macro: 0.3148925510948643
[2m[36m(func pid=83673)[0m f1_weighted: 0.3694023563601355
[2m[36m(func pid=83673)[0m f1_per_class: [0.444, 0.42, 0.377, 0.575, 0.103, 0.196, 0.244, 0.344, 0.212, 0.234]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.166044776119403
[2m[36m(func pid=102477)[0m top5: 0.6730410447761194
[2m[36m(func pid=102477)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=102477)[0m f1_macro: 0.20595920637946946
[2m[36m(func pid=102477)[0m f1_weighted: 0.15936369943827083
[2m[36m(func pid=102477)[0m f1_per_class: [0.328, 0.29, 0.588, 0.174, 0.052, 0.092, 0.069, 0.235, 0.122, 0.108]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8791 | Steps: 4 | Val loss: 2.4521 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 5.8404 | Steps: 4 | Val loss: 29.7699 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4229 | Steps: 4 | Val loss: 1.8532 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.1690 | Steps: 4 | Val loss: 2.0516 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=102433)[0m top1: 0.07602611940298508
[2m[36m(func pid=102433)[0m top5: 0.4314365671641791
[2m[36m(func pid=102433)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=102433)[0m f1_macro: 0.05292694255237149
[2m[36m(func pid=102433)[0m f1_weighted: 0.08258454710193447
[2m[36m(func pid=102433)[0m f1_per_class: [0.047, 0.132, 0.0, 0.153, 0.0, 0.043, 0.021, 0.048, 0.061, 0.024]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:18 (running for 00:08:35.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.396 |      0.315 |                   84 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  5.84  |      0.271 |                   88 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.879 |      0.053 |                    9 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.379 |      0.206 |                    8 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.28171641791044777
[2m[36m(func pid=84475)[0m top5: 0.8083022388059702
[2m[36m(func pid=84475)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=84475)[0m f1_macro: 0.2708639542545582
[2m[36m(func pid=84475)[0m f1_weighted: 0.2926323865804442
[2m[36m(func pid=84475)[0m f1_per_class: [0.248, 0.522, 0.6, 0.215, 0.072, 0.193, 0.298, 0.285, 0.138, 0.138]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.353544776119403
[2m[36m(func pid=83673)[0m top5: 0.8861940298507462
[2m[36m(func pid=83673)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=83673)[0m f1_macro: 0.31663378598110553
[2m[36m(func pid=83673)[0m f1_weighted: 0.36529995589384273
[2m[36m(func pid=83673)[0m f1_per_class: [0.427, 0.428, 0.433, 0.567, 0.101, 0.19, 0.234, 0.339, 0.23, 0.216]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.24813432835820895
[2m[36m(func pid=102477)[0m top5: 0.7915111940298507
[2m[36m(func pid=102477)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=102477)[0m f1_macro: 0.2247120568693576
[2m[36m(func pid=102477)[0m f1_weighted: 0.20673711259211808
[2m[36m(func pid=102477)[0m f1_per_class: [0.438, 0.443, 0.324, 0.24, 0.118, 0.183, 0.045, 0.23, 0.076, 0.151]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9091 | Steps: 4 | Val loss: 2.4424 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2069 | Steps: 4 | Val loss: 28.2459 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4590 | Steps: 4 | Val loss: 1.8708 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2392 | Steps: 4 | Val loss: 1.9556 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=102433)[0m top1: 0.07416044776119403
[2m[36m(func pid=102433)[0m top5: 0.43843283582089554
[2m[36m(func pid=102433)[0m f1_micro: 0.07416044776119403
[2m[36m(func pid=102433)[0m f1_macro: 0.053663948330826904
[2m[36m(func pid=102433)[0m f1_weighted: 0.08116131481932588
[2m[36m(func pid=102433)[0m f1_per_class: [0.058, 0.13, 0.0, 0.143, 0.0, 0.032, 0.029, 0.052, 0.065, 0.028]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:23 (running for 00:08:41.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.423 |      0.317 |                   85 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.207 |      0.299 |                   89 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.909 |      0.054 |                   10 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.169 |      0.225 |                    9 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.302705223880597
[2m[36m(func pid=84475)[0m top5: 0.8083022388059702
[2m[36m(func pid=84475)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=84475)[0m f1_macro: 0.2988086616480271
[2m[36m(func pid=84475)[0m f1_weighted: 0.31589648038587714
[2m[36m(func pid=84475)[0m f1_per_class: [0.255, 0.52, 0.667, 0.202, 0.072, 0.19, 0.382, 0.286, 0.157, 0.258]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3582089552238806
[2m[36m(func pid=83673)[0m top5: 0.8740671641791045
[2m[36m(func pid=83673)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=83673)[0m f1_macro: 0.31607009058445434
[2m[36m(func pid=83673)[0m f1_weighted: 0.36427034788799234
[2m[36m(func pid=83673)[0m f1_per_class: [0.429, 0.47, 0.433, 0.567, 0.1, 0.194, 0.206, 0.348, 0.213, 0.2]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2887126865671642
[2m[36m(func pid=102477)[0m top5: 0.8101679104477612
[2m[36m(func pid=102477)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=102477)[0m f1_macro: 0.24120028265461527
[2m[36m(func pid=102477)[0m f1_weighted: 0.24729658179081856
[2m[36m(func pid=102477)[0m f1_per_class: [0.473, 0.438, 0.304, 0.389, 0.118, 0.159, 0.047, 0.263, 0.046, 0.175]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7878 | Steps: 4 | Val loss: 2.4367 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.4282 | Steps: 4 | Val loss: 27.4434 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.5002 | Steps: 4 | Val loss: 1.9290 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.9127 | Steps: 4 | Val loss: 1.8599 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=102433)[0m top1: 0.07602611940298508
[2m[36m(func pid=102433)[0m top5: 0.4412313432835821
[2m[36m(func pid=102433)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=102433)[0m f1_macro: 0.05470218746840185
[2m[36m(func pid=102433)[0m f1_weighted: 0.08479116806862805
[2m[36m(func pid=102433)[0m f1_per_class: [0.053, 0.123, 0.0, 0.149, 0.0, 0.038, 0.038, 0.05, 0.067, 0.03]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:29 (running for 00:08:46.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.459 |      0.316 |                   86 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  1.428 |      0.3   |                   90 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.788 |      0.055 |                   11 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  2.239 |      0.241 |                   10 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31763059701492535
[2m[36m(func pid=84475)[0m top5: 0.8194962686567164
[2m[36m(func pid=84475)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=84475)[0m f1_macro: 0.29958991646944344
[2m[36m(func pid=84475)[0m f1_weighted: 0.33161788256804586
[2m[36m(func pid=84475)[0m f1_per_class: [0.269, 0.513, 0.714, 0.227, 0.066, 0.137, 0.438, 0.273, 0.158, 0.2]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.33955223880597013
[2m[36m(func pid=83673)[0m top5: 0.8656716417910447
[2m[36m(func pid=83673)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=83673)[0m f1_macro: 0.30021604637436744
[2m[36m(func pid=83673)[0m f1_weighted: 0.34600348114717017
[2m[36m(func pid=83673)[0m f1_per_class: [0.422, 0.457, 0.371, 0.545, 0.104, 0.179, 0.184, 0.324, 0.214, 0.201]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3302238805970149
[2m[36m(func pid=102477)[0m top5: 0.8558768656716418
[2m[36m(func pid=102477)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=102477)[0m f1_macro: 0.26643420840833076
[2m[36m(func pid=102477)[0m f1_weighted: 0.28710922312784254
[2m[36m(func pid=102477)[0m f1_per_class: [0.469, 0.358, 0.429, 0.516, 0.19, 0.111, 0.122, 0.269, 0.043, 0.157]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8640 | Steps: 4 | Val loss: 2.4434 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2003 | Steps: 4 | Val loss: 27.1931 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.5650 | Steps: 4 | Val loss: 1.8829 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.8252 | Steps: 4 | Val loss: 1.8293 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=102433)[0m top1: 0.07276119402985075
[2m[36m(func pid=102433)[0m top5: 0.43796641791044777
[2m[36m(func pid=102433)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=102433)[0m f1_macro: 0.05441542518285075
[2m[36m(func pid=102433)[0m f1_weighted: 0.08143897616099315
[2m[36m(func pid=102433)[0m f1_per_class: [0.053, 0.102, 0.0, 0.148, 0.0, 0.047, 0.032, 0.07, 0.062, 0.029]
[2m[36m(func pid=102433)[0m 
== Status ==
Current time: 2024-01-07 11:16:34 (running for 00:08:51.76)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.5   |      0.3   |                   87 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.2   |      0.31  |                   91 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.864 |      0.054 |                   12 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.913 |      0.266 |                   11 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.33722014925373134
[2m[36m(func pid=84475)[0m top5: 0.8176305970149254
[2m[36m(func pid=84475)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=84475)[0m f1_macro: 0.30954711066554197
[2m[36m(func pid=84475)[0m f1_weighted: 0.34251803678818554
[2m[36m(func pid=84475)[0m f1_per_class: [0.294, 0.528, 0.733, 0.227, 0.07, 0.094, 0.479, 0.273, 0.155, 0.242]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.35261194029850745
[2m[36m(func pid=83673)[0m top5: 0.8768656716417911
[2m[36m(func pid=83673)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=83673)[0m f1_macro: 0.31125545409087924
[2m[36m(func pid=83673)[0m f1_weighted: 0.36033669082962305
[2m[36m(func pid=83673)[0m f1_per_class: [0.49, 0.462, 0.377, 0.568, 0.104, 0.17, 0.21, 0.312, 0.205, 0.214]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.38386194029850745
[2m[36m(func pid=102477)[0m top5: 0.8432835820895522
[2m[36m(func pid=102477)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=102477)[0m f1_macro: 0.26089570096207054
[2m[36m(func pid=102477)[0m f1_weighted: 0.348149560626574
[2m[36m(func pid=102477)[0m f1_per_class: [0.436, 0.203, 0.258, 0.542, 0.177, 0.022, 0.421, 0.321, 0.058, 0.171]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8109 | Steps: 4 | Val loss: 2.4404 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.0024 | Steps: 4 | Val loss: 26.7792 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2512 | Steps: 4 | Val loss: 1.8252 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7433 | Steps: 4 | Val loss: 1.8057 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:16:39 (running for 00:08:56.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.565 |      0.311 |                   88 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.2   |      0.31  |                   91 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.811 |      0.056 |                   13 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.825 |      0.261 |                   12 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.0732276119402985
[2m[36m(func pid=102433)[0m top5: 0.44029850746268656
[2m[36m(func pid=102433)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=102433)[0m f1_macro: 0.05589773085147528
[2m[36m(func pid=102433)[0m f1_weighted: 0.08379471847791259
[2m[36m(func pid=102433)[0m f1_per_class: [0.06, 0.106, 0.0, 0.142, 0.0, 0.053, 0.041, 0.069, 0.06, 0.027]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=84475)[0m top1: 0.3460820895522388
[2m[36m(func pid=84475)[0m top5: 0.8278917910447762
[2m[36m(func pid=84475)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=84475)[0m f1_macro: 0.3142839502763425
[2m[36m(func pid=84475)[0m f1_weighted: 0.3510351435277915
[2m[36m(func pid=84475)[0m f1_per_class: [0.357, 0.52, 0.688, 0.262, 0.062, 0.071, 0.485, 0.269, 0.161, 0.27]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=83673)[0m top1: 0.365205223880597
[2m[36m(func pid=83673)[0m top5: 0.8843283582089553
[2m[36m(func pid=83673)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=83673)[0m f1_macro: 0.33047212591551595
[2m[36m(func pid=83673)[0m f1_weighted: 0.37242987732744604
[2m[36m(func pid=83673)[0m f1_per_class: [0.532, 0.487, 0.448, 0.558, 0.116, 0.192, 0.228, 0.329, 0.222, 0.193]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.42350746268656714
[2m[36m(func pid=102477)[0m top5: 0.8339552238805971
[2m[36m(func pid=102477)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=102477)[0m f1_macro: 0.2794946084622958
[2m[36m(func pid=102477)[0m f1_weighted: 0.3778422152520752
[2m[36m(func pid=102477)[0m f1_per_class: [0.491, 0.117, 0.343, 0.56, 0.105, 0.016, 0.552, 0.303, 0.063, 0.247]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7648 | Steps: 4 | Val loss: 2.4391 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0029 | Steps: 4 | Val loss: 28.0054 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3136 | Steps: 4 | Val loss: 1.8438 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7483 | Steps: 4 | Val loss: 1.9363 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 11:16:44 (running for 00:09:02.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.251 |      0.33  |                   89 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.002 |      0.314 |                   92 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.765 |      0.057 |                   14 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.743 |      0.279 |                   13 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.06996268656716417
[2m[36m(func pid=102433)[0m top5: 0.4351679104477612
[2m[36m(func pid=102433)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=102433)[0m f1_macro: 0.05686332230686721
[2m[36m(func pid=102433)[0m f1_weighted: 0.0811061506791115
[2m[36m(func pid=102433)[0m f1_per_class: [0.075, 0.11, 0.0, 0.131, 0.0, 0.049, 0.04, 0.071, 0.067, 0.026]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=84475)[0m top1: 0.37080223880597013
[2m[36m(func pid=84475)[0m top5: 0.8185634328358209
[2m[36m(func pid=84475)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=84475)[0m f1_macro: 0.30769317653523165
[2m[36m(func pid=84475)[0m f1_weighted: 0.36523523525608015
[2m[36m(func pid=84475)[0m f1_per_class: [0.377, 0.527, 0.537, 0.27, 0.07, 0.051, 0.525, 0.293, 0.157, 0.27]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3596082089552239
[2m[36m(func pid=102477)[0m top5: 0.7994402985074627
[2m[36m(func pid=102477)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=102477)[0m f1_macro: 0.24739380414805243
[2m[36m(func pid=102477)[0m f1_weighted: 0.3638363382690994
[2m[36m(func pid=102477)[0m f1_per_class: [0.316, 0.127, 0.255, 0.494, 0.063, 0.04, 0.558, 0.332, 0.105, 0.183]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3736007462686567
[2m[36m(func pid=83673)[0m top5: 0.8763992537313433
[2m[36m(func pid=83673)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=83673)[0m f1_macro: 0.3292016651710656
[2m[36m(func pid=83673)[0m f1_weighted: 0.37624560623402975
[2m[36m(func pid=83673)[0m f1_per_class: [0.514, 0.524, 0.406, 0.563, 0.124, 0.185, 0.219, 0.336, 0.209, 0.212]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.9111 | Steps: 4 | Val loss: 29.3387 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8316 | Steps: 4 | Val loss: 2.4377 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6239 | Steps: 4 | Val loss: 2.1223 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3083 | Steps: 4 | Val loss: 1.8146 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:16:49 (running for 00:09:07.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.314 |      0.329 |                   90 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.911 |      0.292 |                   94 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.765 |      0.057 |                   14 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.748 |      0.247 |                   14 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3596082089552239
[2m[36m(func pid=84475)[0m top5: 0.8055037313432836
[2m[36m(func pid=84475)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=84475)[0m f1_macro: 0.29232817097294417
[2m[36m(func pid=84475)[0m f1_weighted: 0.35575575820725847
[2m[36m(func pid=84475)[0m f1_per_class: [0.388, 0.507, 0.458, 0.275, 0.07, 0.046, 0.507, 0.285, 0.15, 0.238]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102433)[0m top1: 0.07555970149253731
[2m[36m(func pid=102433)[0m top5: 0.4430970149253731
[2m[36m(func pid=102433)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=102433)[0m f1_macro: 0.062122084841610026
[2m[36m(func pid=102433)[0m f1_weighted: 0.08932377280977483
[2m[36m(func pid=102433)[0m f1_per_class: [0.09, 0.104, 0.0, 0.14, 0.0, 0.045, 0.062, 0.071, 0.074, 0.036]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.20662313432835822
[2m[36m(func pid=102477)[0m top5: 0.7336753731343284
[2m[36m(func pid=102477)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=102477)[0m f1_macro: 0.1915342315281958
[2m[36m(func pid=102477)[0m f1_weighted: 0.24012411933859482
[2m[36m(func pid=102477)[0m f1_per_class: [0.249, 0.145, 0.183, 0.337, 0.052, 0.081, 0.27, 0.34, 0.108, 0.151]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3712686567164179
[2m[36m(func pid=83673)[0m top5: 0.8824626865671642
[2m[36m(func pid=83673)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=83673)[0m f1_macro: 0.3323017983038052
[2m[36m(func pid=83673)[0m f1_weighted: 0.37708851452022607
[2m[36m(func pid=83673)[0m f1_per_class: [0.537, 0.545, 0.4, 0.525, 0.136, 0.198, 0.24, 0.311, 0.239, 0.192]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7621 | Steps: 4 | Val loss: 2.4398 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.4241 | Steps: 4 | Val loss: 30.2686 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5398 | Steps: 4 | Val loss: 2.1342 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.4498 | Steps: 4 | Val loss: 1.7829 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:16:55 (running for 00:09:12.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.308 |      0.332 |                   91 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.911 |      0.292 |                   94 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.762 |      0.059 |                   16 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.624 |      0.192 |                   15 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.06949626865671642
[2m[36m(func pid=102433)[0m top5: 0.43656716417910446
[2m[36m(func pid=102433)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=102433)[0m f1_macro: 0.0585141575097199
[2m[36m(func pid=102433)[0m f1_weighted: 0.08253932863520735
[2m[36m(func pid=102433)[0m f1_per_class: [0.088, 0.094, 0.0, 0.12, 0.0, 0.028, 0.067, 0.088, 0.068, 0.03]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=84475)[0m top1: 0.33675373134328357
[2m[36m(func pid=84475)[0m top5: 0.7999067164179104
[2m[36m(func pid=84475)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=84475)[0m f1_macro: 0.2971249059210976
[2m[36m(func pid=84475)[0m f1_weighted: 0.35205533814799345
[2m[36m(func pid=84475)[0m f1_per_class: [0.4, 0.491, 0.421, 0.319, 0.073, 0.039, 0.462, 0.291, 0.123, 0.351]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102477)[0m top1: 0.19682835820895522
[2m[36m(func pid=102477)[0m top5: 0.75
[2m[36m(func pid=102477)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=102477)[0m f1_macro: 0.21148763278500277
[2m[36m(func pid=102477)[0m f1_weighted: 0.2113694677989639
[2m[36m(func pid=102477)[0m f1_per_class: [0.308, 0.202, 0.282, 0.277, 0.081, 0.156, 0.156, 0.356, 0.117, 0.18]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.38619402985074625
[2m[36m(func pid=83673)[0m top5: 0.8880597014925373
[2m[36m(func pid=83673)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=83673)[0m f1_macro: 0.34770811372767807
[2m[36m(func pid=83673)[0m f1_weighted: 0.39642926749555185
[2m[36m(func pid=83673)[0m f1_per_class: [0.579, 0.553, 0.406, 0.528, 0.119, 0.203, 0.285, 0.325, 0.282, 0.197]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 3.1049 | Steps: 4 | Val loss: 30.8390 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6918 | Steps: 4 | Val loss: 2.4373 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5620 | Steps: 4 | Val loss: 2.0635 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3101 | Steps: 4 | Val loss: 1.7895 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 11:17:00 (running for 00:09:18.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.45  |      0.348 |                   92 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.105 |      0.295 |                   96 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.762 |      0.059 |                   16 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.54  |      0.211 |                   16 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31669776119402987
[2m[36m(func pid=84475)[0m top5: 0.800839552238806
[2m[36m(func pid=84475)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=84475)[0m f1_macro: 0.2950170122811937
[2m[36m(func pid=84475)[0m f1_weighted: 0.33959026591365593
[2m[36m(func pid=84475)[0m f1_per_class: [0.411, 0.488, 0.381, 0.37, 0.062, 0.039, 0.37, 0.316, 0.106, 0.407]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102433)[0m top1: 0.07462686567164178
[2m[36m(func pid=102433)[0m top5: 0.43843283582089554
[2m[36m(func pid=102433)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=102433)[0m f1_macro: 0.06517851271889973
[2m[36m(func pid=102433)[0m f1_weighted: 0.08759536892446092
[2m[36m(func pid=102433)[0m f1_per_class: [0.101, 0.104, 0.037, 0.124, 0.0, 0.024, 0.075, 0.089, 0.073, 0.025]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2271455223880597
[2m[36m(func pid=102477)[0m top5: 0.7709888059701493
[2m[36m(func pid=102477)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=102477)[0m f1_macro: 0.2745469893686509
[2m[36m(func pid=102477)[0m f1_weighted: 0.20790430501459348
[2m[36m(func pid=102477)[0m f1_per_class: [0.439, 0.374, 0.5, 0.217, 0.189, 0.199, 0.065, 0.339, 0.137, 0.286]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3824626865671642
[2m[36m(func pid=83673)[0m top5: 0.8899253731343284
[2m[36m(func pid=83673)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=83673)[0m f1_macro: 0.3466994328957623
[2m[36m(func pid=83673)[0m f1_weighted: 0.3896425536786861
[2m[36m(func pid=83673)[0m f1_per_class: [0.55, 0.55, 0.393, 0.502, 0.14, 0.209, 0.287, 0.324, 0.279, 0.233]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 2.8698 | Steps: 4 | Val loss: 32.7831 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7397 | Steps: 4 | Val loss: 2.4199 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7448 | Steps: 4 | Val loss: 2.0013 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.4032 | Steps: 4 | Val loss: 1.8092 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:17:06 (running for 00:09:23.50)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.31  |      0.347 |                   93 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  2.87  |      0.286 |                   97 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.692 |      0.065 |                   17 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.562 |      0.275 |                   17 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=84475)[0m top1: 0.3050373134328358

[2m[36m(func pid=84475)[0m top5: 0.7975746268656716
[2m[36m(func pid=84475)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=84475)[0m f1_macro: 0.28638153875658434
[2m[36m(func pid=84475)[0m f1_weighted: 0.3331797056072514
[2m[36m(func pid=84475)[0m f1_per_class: [0.414, 0.464, 0.316, 0.464, 0.073, 0.031, 0.273, 0.349, 0.099, 0.381]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102433)[0m top1: 0.07462686567164178
[2m[36m(func pid=102433)[0m top5: 0.45382462686567165
[2m[36m(func pid=102433)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=102433)[0m f1_macro: 0.06308219752928365
[2m[36m(func pid=102433)[0m f1_weighted: 0.09022185499405004
[2m[36m(func pid=102433)[0m f1_per_class: [0.111, 0.098, 0.018, 0.13, 0.0, 0.022, 0.084, 0.083, 0.06, 0.024]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2574626865671642
[2m[36m(func pid=102477)[0m top5: 0.7985074626865671
[2m[36m(func pid=102477)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=102477)[0m f1_macro: 0.30228918415735173
[2m[36m(func pid=102477)[0m f1_weighted: 0.2407180021029316
[2m[36m(func pid=102477)[0m f1_per_class: [0.54, 0.412, 0.407, 0.292, 0.209, 0.216, 0.065, 0.338, 0.151, 0.392]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3805970149253731
[2m[36m(func pid=83673)[0m top5: 0.8796641791044776
[2m[36m(func pid=83673)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=83673)[0m f1_macro: 0.3464848645904172
[2m[36m(func pid=83673)[0m f1_weighted: 0.3872164298206948
[2m[36m(func pid=83673)[0m f1_per_class: [0.557, 0.546, 0.429, 0.501, 0.124, 0.192, 0.288, 0.336, 0.261, 0.231]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1018 | Steps: 4 | Val loss: 33.1390 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7373 | Steps: 4 | Val loss: 2.4200 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.2942 | Steps: 4 | Val loss: 1.9349 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3143 | Steps: 4 | Val loss: 1.8335 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:17:11 (running for 00:09:28.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.403 |      0.346 |                   94 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  0.102 |      0.278 |                   98 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.74  |      0.063 |                   18 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.745 |      0.302 |                   18 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.3050373134328358
[2m[36m(func pid=84475)[0m top5: 0.7835820895522388
[2m[36m(func pid=84475)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=84475)[0m f1_macro: 0.2780796617289733
[2m[36m(func pid=84475)[0m f1_weighted: 0.3258565057406994
[2m[36m(func pid=84475)[0m f1_per_class: [0.479, 0.447, 0.26, 0.51, 0.076, 0.038, 0.217, 0.318, 0.095, 0.34]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102433)[0m top1: 0.07695895522388059
[2m[36m(func pid=102433)[0m top5: 0.44449626865671643
[2m[36m(func pid=102433)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=102433)[0m f1_macro: 0.07173394317915266
[2m[36m(func pid=102433)[0m f1_weighted: 0.08940233665926982
[2m[36m(func pid=102433)[0m f1_per_class: [0.088, 0.102, 0.083, 0.123, 0.0, 0.029, 0.075, 0.116, 0.077, 0.025]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3041044776119403
[2m[36m(func pid=102477)[0m top5: 0.8274253731343284
[2m[36m(func pid=102477)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=102477)[0m f1_macro: 0.30830691932160836
[2m[36m(func pid=102477)[0m f1_weighted: 0.2800717960062607
[2m[36m(func pid=102477)[0m f1_per_class: [0.554, 0.474, 0.333, 0.409, 0.198, 0.276, 0.03, 0.346, 0.169, 0.294]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.38199626865671643
[2m[36m(func pid=83673)[0m top5: 0.8759328358208955
[2m[36m(func pid=83673)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=83673)[0m f1_macro: 0.34405943348130397
[2m[36m(func pid=83673)[0m f1_weighted: 0.3866942797758976
[2m[36m(func pid=83673)[0m f1_per_class: [0.522, 0.554, 0.444, 0.481, 0.133, 0.181, 0.31, 0.322, 0.253, 0.24]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.5554 | Steps: 4 | Val loss: 32.9270 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8037 | Steps: 4 | Val loss: 2.4082 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4220 | Steps: 4 | Val loss: 1.9274 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4037 | Steps: 4 | Val loss: 1.7961 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:17:16 (running for 00:09:33.96)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.314 |      0.344 |                   95 |
| train_98a10_00002 | RUNNING    | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  1.555 |      0.284 |                   99 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.737 |      0.072 |                   19 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.294 |      0.308 |                   19 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.31203358208955223
[2m[36m(func pid=84475)[0m top5: 0.7803171641791045
[2m[36m(func pid=84475)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=84475)[0m f1_macro: 0.2844604163407146
[2m[36m(func pid=84475)[0m f1_weighted: 0.330572010062071
[2m[36m(func pid=84475)[0m f1_per_class: [0.593, 0.432, 0.2, 0.534, 0.086, 0.038, 0.209, 0.346, 0.088, 0.319]
[2m[36m(func pid=84475)[0m 
[2m[36m(func pid=102433)[0m top1: 0.07929104477611941
[2m[36m(func pid=102433)[0m top5: 0.4556902985074627
[2m[36m(func pid=102433)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=102433)[0m f1_macro: 0.07319321012482381
[2m[36m(func pid=102433)[0m f1_weighted: 0.09159283423026897
[2m[36m(func pid=102433)[0m f1_per_class: [0.114, 0.108, 0.078, 0.137, 0.0, 0.024, 0.07, 0.105, 0.068, 0.029]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2957089552238806
[2m[36m(func pid=102477)[0m top5: 0.8367537313432836
[2m[36m(func pid=102477)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=102477)[0m f1_macro: 0.2710863318877245
[2m[36m(func pid=102477)[0m f1_weighted: 0.2843044663127288
[2m[36m(func pid=102477)[0m f1_per_class: [0.504, 0.424, 0.324, 0.494, 0.103, 0.241, 0.033, 0.304, 0.091, 0.191]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3885261194029851
[2m[36m(func pid=83673)[0m top5: 0.8838619402985075
[2m[36m(func pid=83673)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=83673)[0m f1_macro: 0.34907034931781455
[2m[36m(func pid=83673)[0m f1_weighted: 0.39252200454157715
[2m[36m(func pid=83673)[0m f1_per_class: [0.563, 0.556, 0.429, 0.491, 0.133, 0.169, 0.32, 0.328, 0.245, 0.258]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=84475)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 3.4076 | Steps: 4 | Val loss: 34.4765 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6907 | Steps: 4 | Val loss: 2.3870 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3842 | Steps: 4 | Val loss: 1.8846 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:17:21 (running for 00:09:39.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32325
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 3 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.404 |      0.349 |                   96 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.804 |      0.073 |                   20 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.422 |      0.271 |                   20 |
| train_98a10_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=84475)[0m top1: 0.30597014925373134
[2m[36m(func pid=84475)[0m top5: 0.7667910447761194
[2m[36m(func pid=84475)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=84475)[0m f1_macro: 0.2714943623203188
[2m[36m(func pid=84475)[0m f1_weighted: 0.3138996860392848
[2m[36m(func pid=84475)[0m f1_per_class: [0.513, 0.405, 0.17, 0.543, 0.103, 0.071, 0.15, 0.368, 0.089, 0.304]
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.4697 | Steps: 4 | Val loss: 1.8253 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102433)[0m top1: 0.09188432835820895
[2m[36m(func pid=102433)[0m top5: 0.4808768656716418
[2m[36m(func pid=102433)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=102433)[0m f1_macro: 0.08183201437699125
[2m[36m(func pid=102433)[0m f1_weighted: 0.10260648824455719
[2m[36m(func pid=102433)[0m f1_per_class: [0.127, 0.14, 0.074, 0.146, 0.0, 0.035, 0.071, 0.121, 0.07, 0.035]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2966417910447761
[2m[36m(func pid=102477)[0m top5: 0.8572761194029851
[2m[36m(func pid=102477)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=102477)[0m f1_macro: 0.27573728013465304
[2m[36m(func pid=102477)[0m f1_weighted: 0.29374661164415933
[2m[36m(func pid=102477)[0m f1_per_class: [0.5, 0.302, 0.387, 0.537, 0.095, 0.208, 0.1, 0.331, 0.119, 0.178]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=83673)[0m top1: 0.3787313432835821
[2m[36m(func pid=83673)[0m top5: 0.878731343283582
[2m[36m(func pid=83673)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=83673)[0m f1_macro: 0.3446003796923084
[2m[36m(func pid=83673)[0m f1_weighted: 0.3807541631115912
[2m[36m(func pid=83673)[0m f1_per_class: [0.576, 0.553, 0.407, 0.485, 0.13, 0.183, 0.282, 0.328, 0.249, 0.254]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6303 | Steps: 4 | Val loss: 2.3613 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.1470 | Steps: 4 | Val loss: 1.7414 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3760 | Steps: 4 | Val loss: 1.8555 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=102433)[0m top1: 0.10820895522388059
[2m[36m(func pid=102433)[0m top5: 0.5107276119402985
[2m[36m(func pid=102433)[0m f1_micro: 0.10820895522388059
[2m[36m(func pid=102433)[0m f1_macro: 0.08892535366682972
[2m[36m(func pid=102433)[0m f1_weighted: 0.11874296316252114
[2m[36m(func pid=102433)[0m f1_per_class: [0.137, 0.177, 0.076, 0.166, 0.0, 0.025, 0.088, 0.118, 0.069, 0.033]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.35774253731343286
[2m[36m(func pid=102477)[0m top5: 0.8959888059701493
[2m[36m(func pid=102477)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=102477)[0m f1_macro: 0.3112336999809507
[2m[36m(func pid=102477)[0m f1_weighted: 0.36223530662081066
[2m[36m(func pid=102477)[0m f1_per_class: [0.5, 0.37, 0.5, 0.572, 0.111, 0.149, 0.279, 0.305, 0.15, 0.176]
[2m[36m(func pid=83673)[0m top1: 0.3805970149253731
[2m[36m(func pid=83673)[0m top5: 0.8740671641791045
[2m[36m(func pid=83673)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=83673)[0m f1_macro: 0.34494541544770535
[2m[36m(func pid=83673)[0m f1_weighted: 0.3863328039592563
[2m[36m(func pid=83673)[0m f1_per_class: [0.576, 0.562, 0.407, 0.477, 0.117, 0.18, 0.304, 0.333, 0.239, 0.254]
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6841 | Steps: 4 | Val loss: 2.3485 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:17:27 (running for 00:09:44.87)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.47  |      0.345 |                   97 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.63  |      0.089 |                   22 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.384 |      0.276 |                   21 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=108399)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=108399)[0m Configuration completed!
[2m[36m(func pid=108399)[0m New optimizer parameters:
[2m[36m(func pid=108399)[0m SGD (
[2m[36m(func pid=108399)[0m Parameter Group 0
[2m[36m(func pid=108399)[0m     dampening: 0
[2m[36m(func pid=108399)[0m     differentiable: False
[2m[36m(func pid=108399)[0m     foreach: None
[2m[36m(func pid=108399)[0m     lr: 0.01
[2m[36m(func pid=108399)[0m     maximize: False
[2m[36m(func pid=108399)[0m     momentum: 0.9
[2m[36m(func pid=108399)[0m     nesterov: False
[2m[36m(func pid=108399)[0m     weight_decay: 0
[2m[36m(func pid=108399)[0m )
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:17:32 (running for 00:09:50.35)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.376 |      0.345 |                   98 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.684 |      0.093 |                   23 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.147 |      0.311 |                   22 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |        |            |                      |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.11287313432835822
[2m[36m(func pid=102433)[0m top5: 0.5270522388059702
[2m[36m(func pid=102433)[0m f1_micro: 0.11287313432835822
[2m[36m(func pid=102433)[0m f1_macro: 0.0932997349733479
[2m[36m(func pid=102433)[0m f1_weighted: 0.12217756518323475
[2m[36m(func pid=102433)[0m f1_per_class: [0.109, 0.192, 0.135, 0.156, 0.0, 0.027, 0.103, 0.113, 0.062, 0.037]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3326 | Steps: 4 | Val loss: 1.8367 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1983 | Steps: 4 | Val loss: 1.6679 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0896 | Steps: 4 | Val loss: 2.1867 | Batch size: 32 | lr: 0.01 | Duration: 4.45s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6391 | Steps: 4 | Val loss: 2.3417 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=83673)[0m top1: 0.37919776119402987
[2m[36m(func pid=83673)[0m top5: 0.8885261194029851
[2m[36m(func pid=83673)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=83673)[0m f1_macro: 0.3534477123057028
[2m[36m(func pid=83673)[0m f1_weighted: 0.3886088671569134
[2m[36m(func pid=83673)[0m f1_per_class: [0.582, 0.545, 0.407, 0.485, 0.154, 0.18, 0.313, 0.33, 0.215, 0.323]
[2m[36m(func pid=83673)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3894589552238806
[2m[36m(func pid=102477)[0m top5: 0.9015858208955224
[2m[36m(func pid=102477)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=102477)[0m f1_macro: 0.337645181577296
[2m[36m(func pid=102477)[0m f1_weighted: 0.3817330549372103
[2m[36m(func pid=102477)[0m f1_per_class: [0.59, 0.345, 0.6, 0.583, 0.174, 0.105, 0.355, 0.315, 0.146, 0.163]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.16184701492537312
[2m[36m(func pid=108399)[0m top5: 0.7234141791044776
[2m[36m(func pid=108399)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=108399)[0m f1_macro: 0.0536152285076746
[2m[36m(func pid=108399)[0m f1_weighted: 0.07960125629621218
[2m[36m(func pid=108399)[0m f1_per_class: [0.038, 0.294, 0.0, 0.038, 0.0, 0.007, 0.044, 0.0, 0.116, 0.0]
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:17:38 (running for 00:09:55.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | RUNNING    | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.333 |      0.353 |                   99 |
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.639 |      0.102 |                   24 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.198 |      0.338 |                   23 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  3.09  |      0.054 |                    1 |
| train_98a10_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.12080223880597014
[2m[36m(func pid=102433)[0m top5: 0.5340485074626866
[2m[36m(func pid=102433)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=102433)[0m f1_macro: 0.10183681247262688
[2m[36m(func pid=102433)[0m f1_weighted: 0.12637328227126032
[2m[36m(func pid=102433)[0m f1_per_class: [0.128, 0.193, 0.161, 0.151, 0.0, 0.026, 0.113, 0.138, 0.067, 0.04]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=83673)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.4236 | Steps: 4 | Val loss: 1.9200 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.1721 | Steps: 4 | Val loss: 1.6504 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9672 | Steps: 4 | Val loss: 2.1289 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6101 | Steps: 4 | Val loss: 2.3443 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=83673)[0m top1: 0.36473880597014924
[2m[36m(func pid=83673)[0m top5: 0.8759328358208955
[2m[36m(func pid=83673)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=83673)[0m f1_macro: 0.34761973100052135
[2m[36m(func pid=83673)[0m f1_weighted: 0.3697687576060316
[2m[36m(func pid=83673)[0m f1_per_class: [0.579, 0.536, 0.444, 0.444, 0.156, 0.182, 0.292, 0.342, 0.211, 0.291]
[2m[36m(func pid=102477)[0m top1: 0.39505597014925375
[2m[36m(func pid=102477)[0m top5: 0.8997201492537313
[2m[36m(func pid=102477)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=102477)[0m f1_macro: 0.3415865631147099
[2m[36m(func pid=102477)[0m f1_weighted: 0.3683319419568145
[2m[36m(func pid=102477)[0m f1_per_class: [0.59, 0.313, 0.571, 0.567, 0.206, 0.078, 0.341, 0.353, 0.175, 0.221]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.2994402985074627
[2m[36m(func pid=108399)[0m top5: 0.7271455223880597
[2m[36m(func pid=108399)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=108399)[0m f1_macro: 0.17902010056339457
[2m[36m(func pid=108399)[0m f1_weighted: 0.2268687141290388
[2m[36m(func pid=108399)[0m f1_per_class: [0.492, 0.032, 0.393, 0.193, 0.058, 0.008, 0.512, 0.0, 0.0, 0.103]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.12173507462686567
[2m[36m(func pid=102433)[0m top5: 0.5363805970149254
[2m[36m(func pid=102433)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=102433)[0m f1_macro: 0.10104624645923713
[2m[36m(func pid=102433)[0m f1_weighted: 0.12460201842802598
[2m[36m(func pid=102433)[0m f1_per_class: [0.158, 0.199, 0.135, 0.139, 0.0, 0.02, 0.116, 0.14, 0.07, 0.034]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.4254 | Steps: 4 | Val loss: 1.6281 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1465 | Steps: 4 | Val loss: 2.0189 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6248 | Steps: 4 | Val loss: 2.3285 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=102477)[0m top1: 0.41697761194029853
[2m[36m(func pid=102477)[0m top5: 0.9104477611940298
[2m[36m(func pid=102477)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=102477)[0m f1_macro: 0.37011365756416736
[2m[36m(func pid=102477)[0m f1_weighted: 0.41380013202028415
[2m[36m(func pid=102477)[0m f1_per_class: [0.604, 0.427, 0.533, 0.563, 0.217, 0.106, 0.412, 0.38, 0.183, 0.276]
[2m[36m(func pid=108399)[0m top1: 0.2691231343283582
[2m[36m(func pid=108399)[0m top5: 0.7756529850746269
[2m[36m(func pid=108399)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=108399)[0m f1_macro: 0.18560414772841186
[2m[36m(func pid=108399)[0m f1_weighted: 0.2696862185095944
[2m[36m(func pid=108399)[0m f1_per_class: [0.211, 0.397, 0.176, 0.141, 0.083, 0.152, 0.451, 0.031, 0.0, 0.214]
[2m[36m(func pid=102433)[0m top1: 0.12779850746268656
[2m[36m(func pid=102433)[0m top5: 0.542910447761194
[2m[36m(func pid=102433)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=102433)[0m f1_macro: 0.10619930707579417
[2m[36m(func pid=102433)[0m f1_weighted: 0.1310433172329906
[2m[36m(func pid=102433)[0m f1_per_class: [0.152, 0.206, 0.156, 0.113, 0.0, 0.019, 0.156, 0.14, 0.079, 0.04]
== Status ==
Current time: 2024-01-07 11:17:43 (running for 00:10:01.08)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.61  |      0.101 |                   25 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.172 |      0.342 |                   24 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  2.967 |      0.179 |                    2 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 11:17:50 (running for 00:10:08.32)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.61  |      0.101 |                   25 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.425 |      0.37  |                   25 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  2.967 |      0.179 |                    2 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=109465)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=109465)[0m Configuration completed!
[2m[36m(func pid=109465)[0m New optimizer parameters:
[2m[36m(func pid=109465)[0m SGD (
[2m[36m(func pid=109465)[0m Parameter Group 0
[2m[36m(func pid=109465)[0m     dampening: 0
[2m[36m(func pid=109465)[0m     differentiable: False
[2m[36m(func pid=109465)[0m     foreach: None
[2m[36m(func pid=109465)[0m     lr: 0.1
[2m[36m(func pid=109465)[0m     maximize: False
[2m[36m(func pid=109465)[0m     momentum: 0.9
[2m[36m(func pid=109465)[0m     nesterov: False
[2m[36m(func pid=109465)[0m     weight_decay: 0
[2m[36m(func pid=109465)[0m )
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.6371 | Steps: 4 | Val loss: 1.6649 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.2896 | Steps: 4 | Val loss: 1.7209 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.6043 | Steps: 4 | Val loss: 2.3199 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.3060 | Steps: 4 | Val loss: 23.1842 | Batch size: 32 | lr: 0.1 | Duration: 4.45s
== Status ==
Current time: 2024-01-07 11:17:55 (running for 00:10:13.36)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.625 |      0.106 |                   26 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.425 |      0.37  |                   25 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  2.147 |      0.186 |                    3 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |        |            |                      |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.373134328358209
[2m[36m(func pid=102477)[0m top5: 0.8969216417910447
[2m[36m(func pid=102477)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=102477)[0m f1_macro: 0.3523011398925897
[2m[36m(func pid=102477)[0m f1_weighted: 0.374682654380669
[2m[36m(func pid=102477)[0m f1_per_class: [0.589, 0.473, 0.338, 0.534, 0.23, 0.28, 0.22, 0.37, 0.184, 0.306]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.35634328358208955
[2m[36m(func pid=108399)[0m top5: 0.9011194029850746
[2m[36m(func pid=108399)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=108399)[0m f1_macro: 0.3061056811551705
[2m[36m(func pid=108399)[0m f1_weighted: 0.34082530302194064
[2m[36m(func pid=108399)[0m f1_per_class: [0.667, 0.473, 0.343, 0.562, 0.213, 0.0, 0.215, 0.24, 0.148, 0.2]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.13386194029850745
[2m[36m(func pid=102433)[0m top5: 0.5522388059701493
[2m[36m(func pid=102433)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=102433)[0m f1_macro: 0.1071848761203809
[2m[36m(func pid=102433)[0m f1_weighted: 0.13777574165701795
[2m[36m(func pid=102433)[0m f1_per_class: [0.163, 0.22, 0.129, 0.116, 0.0, 0.019, 0.17, 0.129, 0.078, 0.047]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.17210820895522388
[2m[36m(func pid=109465)[0m top5: 0.6030783582089553
[2m[36m(func pid=109465)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=109465)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=109465)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=109465)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5069 | Steps: 4 | Val loss: 2.3137 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0876 | Steps: 4 | Val loss: 1.8586 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.5506 | Steps: 4 | Val loss: 1.7894 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 26.1788 | Steps: 4 | Val loss: 26.7425 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:18:01 (running for 00:10:19.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.604 |      0.107 |                   27 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.088 |      0.311 |                   27 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.637 |      0.306 |                    4 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  5.306 |      0.029 |                    1 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.13852611940298507
[2m[36m(func pid=102433)[0m top5: 0.5517723880597015
[2m[36m(func pid=102433)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=102433)[0m f1_macro: 0.10954064180606596
[2m[36m(func pid=102433)[0m f1_weighted: 0.14262753549739995
[2m[36m(func pid=102433)[0m f1_per_class: [0.167, 0.23, 0.119, 0.098, 0.0, 0.025, 0.193, 0.133, 0.083, 0.047]
[2m[36m(func pid=102477)[0m top1: 0.31203358208955223
[2m[36m(func pid=102477)[0m top5: 0.8521455223880597
[2m[36m(func pid=102477)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=102477)[0m f1_macro: 0.31065768043356823
[2m[36m(func pid=102477)[0m f1_weighted: 0.3119827343154076
[2m[36m(func pid=102477)[0m f1_per_class: [0.569, 0.492, 0.304, 0.404, 0.126, 0.247, 0.143, 0.342, 0.192, 0.289]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3208955223880597
[2m[36m(func pid=108399)[0m top5: 0.8936567164179104
[2m[36m(func pid=108399)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=108399)[0m f1_macro: 0.2959971523786693
[2m[36m(func pid=108399)[0m f1_weighted: 0.35474105982616294
[2m[36m(func pid=108399)[0m f1_per_class: [0.554, 0.388, 0.393, 0.528, 0.119, 0.225, 0.283, 0.15, 0.166, 0.154]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.22621268656716417
[2m[36m(func pid=109465)[0m top5: 0.6315298507462687
[2m[36m(func pid=109465)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=109465)[0m f1_macro: 0.10813761010801412
[2m[36m(func pid=109465)[0m f1_weighted: 0.16367148760957814
[2m[36m(func pid=109465)[0m f1_per_class: [0.269, 0.0, 0.0, 0.545, 0.0, 0.0, 0.0, 0.016, 0.108, 0.143]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6326 | Steps: 4 | Val loss: 2.3024 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2488 | Steps: 4 | Val loss: 2.0344 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3232 | Steps: 4 | Val loss: 1.8538 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 20.6868 | Steps: 4 | Val loss: 34.2514 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 11:18:06 (running for 00:10:24.35)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.507 |      0.11  |                   28 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.088 |      0.311 |                   27 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.323 |      0.292 |                    6 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 26.179 |      0.108 |                    2 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.34701492537313433
[2m[36m(func pid=108399)[0m top5: 0.8684701492537313
[2m[36m(func pid=108399)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=108399)[0m f1_macro: 0.2921313713270505
[2m[36m(func pid=108399)[0m f1_weighted: 0.371948155438073
[2m[36m(func pid=108399)[0m f1_per_class: [0.313, 0.178, 0.615, 0.54, 0.073, 0.08, 0.497, 0.265, 0.153, 0.207]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.14272388059701493
[2m[36m(func pid=102433)[0m top5: 0.5694962686567164
[2m[36m(func pid=102433)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=102433)[0m f1_macro: 0.11075868472523753
[2m[36m(func pid=102433)[0m f1_weighted: 0.1457113079674916
[2m[36m(func pid=102433)[0m f1_per_class: [0.162, 0.239, 0.117, 0.087, 0.0, 0.026, 0.209, 0.136, 0.081, 0.052]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2555970149253731
[2m[36m(func pid=102477)[0m top5: 0.7835820895522388
[2m[36m(func pid=102477)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=102477)[0m f1_macro: 0.2559817613190779
[2m[36m(func pid=102477)[0m f1_weighted: 0.25463591966314447
[2m[36m(func pid=102477)[0m f1_per_class: [0.515, 0.423, 0.27, 0.338, 0.09, 0.186, 0.089, 0.325, 0.187, 0.135]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.2042910447761194
[2m[36m(func pid=109465)[0m top5: 0.47341417910447764
[2m[36m(func pid=109465)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=109465)[0m f1_macro: 0.16139843746098576
[2m[36m(func pid=109465)[0m f1_weighted: 0.08565946884461952
[2m[36m(func pid=109465)[0m f1_per_class: [0.0, 0.323, 0.769, 0.0, 0.068, 0.0, 0.0, 0.399, 0.055, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.6531 | Steps: 4 | Val loss: 2.0319 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0796 | Steps: 4 | Val loss: 2.1298 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5699 | Steps: 4 | Val loss: 2.2769 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 27.3818 | Steps: 4 | Val loss: 33.0453 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:18:12 (running for 00:10:29.52)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.633 |      0.111 |                   29 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.249 |      0.256 |                   28 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.653 |      0.34  |                    7 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 20.687 |      0.161 |                    3 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3824626865671642
[2m[36m(func pid=108399)[0m top5: 0.875
[2m[36m(func pid=108399)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=108399)[0m f1_macro: 0.3404802373182877
[2m[36m(func pid=108399)[0m f1_weighted: 0.3016465395227995
[2m[36m(func pid=108399)[0m f1_per_class: [0.442, 0.398, 0.774, 0.558, 0.261, 0.008, 0.105, 0.34, 0.185, 0.333]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.20848880597014927
[2m[36m(func pid=102477)[0m top5: 0.7625932835820896
[2m[36m(func pid=102477)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=102477)[0m f1_macro: 0.21118288921568854
[2m[36m(func pid=102477)[0m f1_weighted: 0.19867606551171266
[2m[36m(func pid=102477)[0m f1_per_class: [0.42, 0.365, 0.245, 0.282, 0.113, 0.122, 0.042, 0.211, 0.181, 0.131]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.1553171641791045
[2m[36m(func pid=102433)[0m top5: 0.5918843283582089
[2m[36m(func pid=102433)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=102433)[0m f1_macro: 0.11998531200190014
[2m[36m(func pid=102433)[0m f1_weighted: 0.1638192005686183
[2m[36m(func pid=102433)[0m f1_per_class: [0.173, 0.226, 0.133, 0.123, 0.0, 0.026, 0.241, 0.136, 0.089, 0.053]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.07602611940298508
[2m[36m(func pid=109465)[0m top5: 0.39225746268656714
[2m[36m(func pid=109465)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=109465)[0m f1_macro: 0.09298160616828348
[2m[36m(func pid=109465)[0m f1_weighted: 0.048272723236338966
[2m[36m(func pid=109465)[0m f1_per_class: [0.044, 0.0, 0.057, 0.0, 0.244, 0.0, 0.066, 0.377, 0.103, 0.039]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9621 | Steps: 4 | Val loss: 2.5339 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0836 | Steps: 4 | Val loss: 1.9643 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5038 | Steps: 4 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 16.9100 | Steps: 4 | Val loss: 13.4047 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:18:17 (running for 00:10:34.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.57  |      0.12  |                   30 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.08  |      0.211 |                   29 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.962 |      0.306 |                    8 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 27.382 |      0.093 |                    4 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.36100746268656714
[2m[36m(func pid=108399)[0m top5: 0.722481343283582
[2m[36m(func pid=108399)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=108399)[0m f1_macro: 0.3062722340165137
[2m[36m(func pid=108399)[0m f1_weighted: 0.3141170837290821
[2m[36m(func pid=108399)[0m f1_per_class: [0.087, 0.558, 0.5, 0.529, 0.149, 0.245, 0.006, 0.389, 0.249, 0.352]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.16511194029850745
[2m[36m(func pid=102433)[0m top5: 0.6100746268656716
[2m[36m(func pid=102433)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=102433)[0m f1_macro: 0.12357329220243514
[2m[36m(func pid=102433)[0m f1_weighted: 0.17242577148927277
[2m[36m(func pid=102433)[0m f1_per_class: [0.177, 0.231, 0.126, 0.114, 0.0, 0.031, 0.271, 0.148, 0.096, 0.043]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.2756529850746269
[2m[36m(func pid=102477)[0m top5: 0.8120335820895522
[2m[36m(func pid=102477)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=102477)[0m f1_macro: 0.24880942099038877
[2m[36m(func pid=102477)[0m f1_weighted: 0.27154237600091224
[2m[36m(func pid=102477)[0m f1_per_class: [0.409, 0.387, 0.264, 0.472, 0.129, 0.151, 0.074, 0.25, 0.199, 0.152]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.4039179104477612
[2m[36m(func pid=109465)[0m top5: 0.8446828358208955
[2m[36m(func pid=109465)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=109465)[0m f1_macro: 0.31591591114175854
[2m[36m(func pid=109465)[0m f1_weighted: 0.31746982325947887
[2m[36m(func pid=109465)[0m f1_per_class: [0.637, 0.011, 0.71, 0.6, 0.312, 0.386, 0.236, 0.213, 0.054, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0695 | Steps: 4 | Val loss: 1.7375 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.0401 | Steps: 4 | Val loss: 2.2666 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4472 | Steps: 4 | Val loss: 2.2654 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 11.6911 | Steps: 4 | Val loss: 16.7790 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:18:22 (running for 00:10:40.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.504 |      0.124 |                   31 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.084 |      0.249 |                   30 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.04  |      0.309 |                    9 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 16.91  |      0.316 |                    5 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.2905783582089552
[2m[36m(func pid=108399)[0m top5: 0.8050373134328358
[2m[36m(func pid=108399)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=108399)[0m f1_macro: 0.3086116091814963
[2m[36m(func pid=108399)[0m f1_weighted: 0.2958501589644663
[2m[36m(func pid=108399)[0m f1_per_class: [0.632, 0.509, 0.218, 0.362, 0.076, 0.198, 0.136, 0.301, 0.203, 0.452]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3726679104477612
[2m[36m(func pid=102477)[0m top5: 0.8871268656716418
[2m[36m(func pid=102477)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=102477)[0m f1_macro: 0.3349791958224447
[2m[36m(func pid=102477)[0m f1_weighted: 0.35450742163442306
[2m[36m(func pid=102477)[0m f1_per_class: [0.527, 0.482, 0.381, 0.592, 0.179, 0.246, 0.116, 0.337, 0.19, 0.3]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.16417910447761194
[2m[36m(func pid=102433)[0m top5: 0.6058768656716418
[2m[36m(func pid=102433)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=102433)[0m f1_macro: 0.12244240172825678
[2m[36m(func pid=102433)[0m f1_weighted: 0.17659452277917498
[2m[36m(func pid=102433)[0m f1_per_class: [0.169, 0.237, 0.111, 0.119, 0.0, 0.037, 0.278, 0.14, 0.079, 0.054]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.26632462686567165
[2m[36m(func pid=109465)[0m top5: 0.8288246268656716
[2m[36m(func pid=109465)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=109465)[0m f1_macro: 0.32744276089423396
[2m[36m(func pid=109465)[0m f1_weighted: 0.22795054609012042
[2m[36m(func pid=109465)[0m f1_per_class: [0.734, 0.527, 0.815, 0.033, 0.195, 0.241, 0.177, 0.321, 0.231, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8242 | Steps: 4 | Val loss: 1.6671 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4148 | Steps: 4 | Val loss: 2.4947 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5074 | Steps: 4 | Val loss: 2.2660 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 13.9476 | Steps: 4 | Val loss: 10.0990 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=102477)[0m top1: 0.39738805970149255
[2m[36m(func pid=102477)[0m top5: 0.9048507462686567
[2m[36m(func pid=102477)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=102477)[0m f1_macro: 0.3661291063764116
[2m[36m(func pid=102477)[0m f1_weighted: 0.3899570748435846
[2m[36m(func pid=102477)[0m f1_per_class: [0.507, 0.471, 0.48, 0.588, 0.194, 0.256, 0.227, 0.39, 0.199, 0.349]
[2m[36m(func pid=102477)[0m 
== Status ==
Current time: 2024-01-07 11:18:28 (running for 00:10:45.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.447 |      0.122 |                   32 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.824 |      0.366 |                   32 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.04  |      0.309 |                    9 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 11.691 |      0.327 |                    6 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.1553171641791045
[2m[36m(func pid=102433)[0m top5: 0.6044776119402985
[2m[36m(func pid=102433)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=102433)[0m f1_macro: 0.12112089288635856
[2m[36m(func pid=102433)[0m f1_weighted: 0.16504719450560157
[2m[36m(func pid=102433)[0m f1_per_class: [0.187, 0.204, 0.113, 0.115, 0.0, 0.036, 0.259, 0.145, 0.089, 0.063]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.32276119402985076
[2m[36m(func pid=108399)[0m top5: 0.8106343283582089
[2m[36m(func pid=108399)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=108399)[0m f1_macro: 0.2710199479933342
[2m[36m(func pid=108399)[0m f1_weighted: 0.32701841270392695
[2m[36m(func pid=108399)[0m f1_per_class: [0.209, 0.182, 0.489, 0.301, 0.192, 0.031, 0.604, 0.194, 0.118, 0.391]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.435634328358209
[2m[36m(func pid=109465)[0m top5: 0.9510261194029851
[2m[36m(func pid=109465)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=109465)[0m f1_macro: 0.3940032501296535
[2m[36m(func pid=109465)[0m f1_weighted: 0.409717579823164
[2m[36m(func pid=109465)[0m f1_per_class: [0.771, 0.588, 0.706, 0.52, 0.087, 0.37, 0.23, 0.394, 0.274, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2662 | Steps: 4 | Val loss: 1.6524 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.3621 | Steps: 4 | Val loss: 2.1637 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5783 | Steps: 4 | Val loss: 2.2608 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 10.5669 | Steps: 4 | Val loss: 37.0272 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:18:33 (running for 00:10:50.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.507 |      0.121 |                   33 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.824 |      0.366 |                   32 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.362 |      0.311 |                   11 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 13.948 |      0.394 |                    7 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.40205223880597013
[2m[36m(func pid=102477)[0m top5: 0.9123134328358209
[2m[36m(func pid=102477)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=102477)[0m f1_macro: 0.36357804043759356
[2m[36m(func pid=102477)[0m f1_weighted: 0.4009155246801778
[2m[36m(func pid=102477)[0m f1_per_class: [0.5, 0.499, 0.407, 0.558, 0.171, 0.249, 0.292, 0.324, 0.178, 0.458]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3138992537313433
[2m[36m(func pid=108399)[0m top5: 0.875
[2m[36m(func pid=108399)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=108399)[0m f1_macro: 0.3114134606900425
[2m[36m(func pid=108399)[0m f1_weighted: 0.32790104415561655
[2m[36m(func pid=108399)[0m f1_per_class: [0.299, 0.395, 0.55, 0.48, 0.244, 0.21, 0.22, 0.29, 0.152, 0.273]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.15764925373134328
[2m[36m(func pid=102433)[0m top5: 0.613339552238806
[2m[36m(func pid=102433)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=102433)[0m f1_macro: 0.12346707597957027
[2m[36m(func pid=102433)[0m f1_weighted: 0.17066419034710445
[2m[36m(func pid=102433)[0m f1_per_class: [0.186, 0.216, 0.12, 0.148, 0.0, 0.031, 0.244, 0.143, 0.078, 0.07]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.21128731343283583
[2m[36m(func pid=109465)[0m top5: 0.5597014925373134
[2m[36m(func pid=109465)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=109465)[0m f1_macro: 0.2568107976435248
[2m[36m(func pid=109465)[0m f1_weighted: 0.1873451914425252
[2m[36m(func pid=109465)[0m f1_per_class: [0.682, 0.096, 0.426, 0.475, 0.435, 0.008, 0.0, 0.212, 0.121, 0.113]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9130 | Steps: 4 | Val loss: 1.6699 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7379 | Steps: 4 | Val loss: 2.3959 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5676 | Steps: 4 | Val loss: 2.2540 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 15.3197 | Steps: 4 | Val loss: 34.8359 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:18:38 (running for 00:10:56.18)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.578 |      0.123 |                   34 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.913 |      0.344 |                   34 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.362 |      0.311 |                   11 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 10.567 |      0.257 |                    8 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3880597014925373
[2m[36m(func pid=102477)[0m top5: 0.9151119402985075
[2m[36m(func pid=102477)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=102477)[0m f1_macro: 0.3439642710545744
[2m[36m(func pid=102477)[0m f1_weighted: 0.394328420401464
[2m[36m(func pid=102477)[0m f1_per_class: [0.486, 0.518, 0.358, 0.552, 0.14, 0.277, 0.257, 0.36, 0.147, 0.344]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.39552238805970147
[2m[36m(func pid=108399)[0m top5: 0.832089552238806
[2m[36m(func pid=108399)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=108399)[0m f1_macro: 0.37405558289686713
[2m[36m(func pid=108399)[0m f1_weighted: 0.33172356960501465
[2m[36m(func pid=108399)[0m f1_per_class: [0.653, 0.56, 0.629, 0.491, 0.267, 0.357, 0.064, 0.129, 0.22, 0.37]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.16277985074626866
[2m[36m(func pid=102433)[0m top5: 0.6152052238805971
[2m[36m(func pid=102433)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=102433)[0m f1_macro: 0.12887197681564794
[2m[36m(func pid=102433)[0m f1_weighted: 0.17340696444675094
[2m[36m(func pid=102433)[0m f1_per_class: [0.189, 0.217, 0.123, 0.133, 0.0, 0.048, 0.255, 0.158, 0.086, 0.08]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.13899253731343283
[2m[36m(func pid=109465)[0m top5: 0.4552238805970149
[2m[36m(func pid=109465)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=109465)[0m f1_macro: 0.1800345984339657
[2m[36m(func pid=109465)[0m f1_weighted: 0.10804151336497635
[2m[36m(func pid=109465)[0m f1_per_class: [0.576, 0.272, 0.057, 0.0, 0.086, 0.132, 0.022, 0.365, 0.108, 0.182]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6434 | Steps: 4 | Val loss: 2.3090 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.2033 | Steps: 4 | Val loss: 1.7362 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4487 | Steps: 4 | Val loss: 2.2641 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 7.3107 | Steps: 4 | Val loss: 19.6391 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:18:44 (running for 00:11:01.46)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.568 |      0.129 |                   35 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.913 |      0.344 |                   34 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.643 |      0.352 |                   13 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 15.32  |      0.18  |                    9 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.353544776119403
[2m[36m(func pid=108399)[0m top5: 0.8264925373134329
[2m[36m(func pid=108399)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=108399)[0m f1_macro: 0.35224705425090386
[2m[36m(func pid=108399)[0m f1_weighted: 0.3299563850602443
[2m[36m(func pid=108399)[0m f1_per_class: [0.658, 0.417, 0.512, 0.541, 0.22, 0.256, 0.104, 0.348, 0.147, 0.32]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3619402985074627
[2m[36m(func pid=102477)[0m top5: 0.8927238805970149
[2m[36m(func pid=102477)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=102477)[0m f1_macro: 0.32911846902363695
[2m[36m(func pid=102477)[0m f1_weighted: 0.3790638570357778
[2m[36m(func pid=102477)[0m f1_per_class: [0.511, 0.55, 0.329, 0.473, 0.079, 0.204, 0.289, 0.353, 0.17, 0.333]
[2m[36m(func pid=102433)[0m top1: 0.1525186567164179
[2m[36m(func pid=102433)[0m top5: 0.5970149253731343
[2m[36m(func pid=102433)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=102433)[0m f1_macro: 0.12009839973969458
[2m[36m(func pid=102433)[0m f1_weighted: 0.1676178862545544
[2m[36m(func pid=102433)[0m f1_per_class: [0.189, 0.204, 0.097, 0.148, 0.0, 0.042, 0.236, 0.14, 0.085, 0.06]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.37080223880597013
[2m[36m(func pid=109465)[0m top5: 0.7910447761194029
[2m[36m(func pid=109465)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=109465)[0m f1_macro: 0.27305116600527424
[2m[36m(func pid=109465)[0m f1_weighted: 0.33124417634200237
[2m[36m(func pid=109465)[0m f1_per_class: [0.593, 0.474, 0.4, 0.092, 0.059, 0.19, 0.603, 0.045, 0.08, 0.194]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.4797 | Steps: 4 | Val loss: 2.4903 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7898 | Steps: 4 | Val loss: 1.6835 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4259 | Steps: 4 | Val loss: 2.2652 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 10.5935 | Steps: 4 | Val loss: 13.8350 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:18:49 (running for 00:11:06.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.449 |      0.12  |                   36 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  1.203 |      0.329 |                   35 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.48  |      0.298 |                   14 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  7.311 |      0.273 |                   10 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.32276119402985076
[2m[36m(func pid=108399)[0m top5: 0.8418843283582089
[2m[36m(func pid=108399)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=108399)[0m f1_macro: 0.29826628084901896
[2m[36m(func pid=108399)[0m f1_weighted: 0.2876601260033419
[2m[36m(func pid=108399)[0m f1_per_class: [0.625, 0.315, 0.48, 0.596, 0.174, 0.066, 0.062, 0.252, 0.22, 0.192]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3941231343283582
[2m[36m(func pid=102477)[0m top5: 0.8922574626865671
[2m[36m(func pid=102477)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=102477)[0m f1_macro: 0.3393268152130316
[2m[36m(func pid=102477)[0m f1_weighted: 0.41920013908101506
[2m[36m(func pid=102477)[0m f1_per_class: [0.531, 0.504, 0.364, 0.426, 0.086, 0.164, 0.513, 0.324, 0.19, 0.293]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.14598880597014927
[2m[36m(func pid=102433)[0m top5: 0.5960820895522388
[2m[36m(func pid=102433)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=102433)[0m f1_macro: 0.11992870546845336
[2m[36m(func pid=102433)[0m f1_weighted: 0.16014103797288778
[2m[36m(func pid=102433)[0m f1_per_class: [0.19, 0.169, 0.102, 0.136, 0.0, 0.07, 0.229, 0.148, 0.092, 0.064]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.46222014925373134
[2m[36m(func pid=109465)[0m top5: 0.8661380597014925
[2m[36m(func pid=109465)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=109465)[0m f1_macro: 0.3766840526932677
[2m[36m(func pid=109465)[0m f1_weighted: 0.4623254191166179
[2m[36m(func pid=109465)[0m f1_per_class: [0.697, 0.579, 0.289, 0.565, 0.09, 0.037, 0.504, 0.367, 0.25, 0.39]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4539 | Steps: 4 | Val loss: 2.2531 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9173 | Steps: 4 | Val loss: 1.6829 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5647 | Steps: 4 | Val loss: 1.8287 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 5.9522 | Steps: 4 | Val loss: 38.9423 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 11:18:54 (running for 00:11:12.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.426 |      0.12  |                   37 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.79  |      0.339 |                   36 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.565 |      0.36  |                   15 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 10.593 |      0.377 |                   11 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.15625
[2m[36m(func pid=102433)[0m top5: 0.6058768656716418
[2m[36m(func pid=102433)[0m f1_micro: 0.15625
[2m[36m(func pid=102433)[0m f1_macro: 0.1260159835835844
[2m[36m(func pid=102433)[0m f1_weighted: 0.17418921352634043
[2m[36m(func pid=102433)[0m f1_per_class: [0.194, 0.169, 0.114, 0.168, 0.0, 0.068, 0.245, 0.158, 0.083, 0.06]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.45615671641791045
[2m[36m(func pid=108399)[0m top5: 0.9123134328358209
[2m[36m(func pid=108399)[0m f1_micro: 0.45615671641791045
[2m[36m(func pid=108399)[0m f1_macro: 0.36022650656871064
[2m[36m(func pid=108399)[0m f1_weighted: 0.4529206509766632
[2m[36m(func pid=108399)[0m f1_per_class: [0.667, 0.548, 0.245, 0.554, 0.135, 0.07, 0.507, 0.299, 0.226, 0.353]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.40578358208955223
[2m[36m(func pid=102477)[0m top5: 0.8903917910447762
[2m[36m(func pid=102477)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=102477)[0m f1_macro: 0.3623169054037381
[2m[36m(func pid=102477)[0m f1_weighted: 0.4344084768755854
[2m[36m(func pid=102477)[0m f1_per_class: [0.611, 0.455, 0.471, 0.484, 0.087, 0.11, 0.545, 0.343, 0.192, 0.326]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.1982276119402985
[2m[36m(func pid=109465)[0m top5: 0.5144589552238806
[2m[36m(func pid=109465)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=109465)[0m f1_macro: 0.2235051810152881
[2m[36m(func pid=109465)[0m f1_weighted: 0.1515567619085917
[2m[36m(func pid=109465)[0m f1_per_class: [0.6, 0.451, 0.093, 0.118, 0.174, 0.008, 0.006, 0.257, 0.137, 0.39]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.4275 | Steps: 4 | Val loss: 3.1692 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5166 | Steps: 4 | Val loss: 2.2414 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7916 | Steps: 4 | Val loss: 1.6420 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 6.7176 | Steps: 4 | Val loss: 28.7121 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=108399)[0m top1: 0.34375
[2m[36m(func pid=108399)[0m top5: 0.7546641791044776
[2m[36m(func pid=108399)[0m f1_micro: 0.34375
[2m[36m(func pid=108399)[0m f1_macro: 0.28168560543758336
[2m[36m(func pid=108399)[0m f1_weighted: 0.32067582472477335
[2m[36m(func pid=108399)[0m f1_per_class: [0.604, 0.503, 0.059, 0.013, 0.106, 0.166, 0.592, 0.164, 0.222, 0.387]
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:18:59 (running for 00:11:17.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.454 |      0.126 |                   38 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.917 |      0.362 |                   37 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.427 |      0.282 |                   16 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  5.952 |      0.224 |                   12 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.16791044776119404
[2m[36m(func pid=102433)[0m top5: 0.6226679104477612
[2m[36m(func pid=102433)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=102433)[0m f1_macro: 0.13235459244229408
[2m[36m(func pid=102433)[0m f1_weighted: 0.18484135567276297
[2m[36m(func pid=102433)[0m f1_per_class: [0.188, 0.23, 0.091, 0.176, 0.0, 0.076, 0.231, 0.184, 0.081, 0.066]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.40718283582089554
[2m[36m(func pid=102477)[0m top5: 0.9076492537313433
[2m[36m(func pid=102477)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=102477)[0m f1_macro: 0.36920049152524287
[2m[36m(func pid=102477)[0m f1_weighted: 0.4368744952767724
[2m[36m(func pid=102477)[0m f1_per_class: [0.652, 0.412, 0.489, 0.556, 0.107, 0.122, 0.504, 0.336, 0.191, 0.323]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.2653917910447761
[2m[36m(func pid=109465)[0m top5: 0.6198694029850746
[2m[36m(func pid=109465)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=109465)[0m f1_macro: 0.28920278030650987
[2m[36m(func pid=109465)[0m f1_weighted: 0.2537654541330672
[2m[36m(func pid=109465)[0m f1_per_class: [0.276, 0.463, 0.667, 0.326, 0.131, 0.264, 0.04, 0.339, 0.298, 0.089]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3292 | Steps: 4 | Val loss: 2.2382 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4299 | Steps: 4 | Val loss: 3.8056 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.7529 | Steps: 4 | Val loss: 1.6155 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 6.4734 | Steps: 4 | Val loss: 19.4904 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 11:19:05 (running for 00:11:22.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.517 |      0.132 |                   39 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.792 |      0.369 |                   38 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.43  |      0.257 |                   17 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  6.718 |      0.289 |                   13 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.1669776119402985
[2m[36m(func pid=102433)[0m top5: 0.6268656716417911
[2m[36m(func pid=102433)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=102433)[0m f1_macro: 0.1326969870557179
[2m[36m(func pid=102433)[0m f1_weighted: 0.18449249921960226
[2m[36m(func pid=102433)[0m f1_per_class: [0.221, 0.234, 0.095, 0.201, 0.0, 0.083, 0.205, 0.173, 0.054, 0.061]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.20475746268656717
[2m[36m(func pid=108399)[0m top5: 0.695429104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=108399)[0m f1_macro: 0.25743432788164444
[2m[36m(func pid=108399)[0m f1_weighted: 0.17856741488759797
[2m[36m(func pid=108399)[0m f1_per_class: [0.493, 0.39, 0.386, 0.036, 0.132, 0.213, 0.12, 0.348, 0.108, 0.348]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.404384328358209
[2m[36m(func pid=102477)[0m top5: 0.9165111940298507
[2m[36m(func pid=102477)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=102477)[0m f1_macro: 0.36316732036311755
[2m[36m(func pid=102477)[0m f1_weighted: 0.4232171783651894
[2m[36m(func pid=102477)[0m f1_per_class: [0.6, 0.389, 0.558, 0.604, 0.122, 0.167, 0.414, 0.344, 0.186, 0.248]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.4025186567164179
[2m[36m(func pid=109465)[0m top5: 0.8642723880597015
[2m[36m(func pid=109465)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=109465)[0m f1_macro: 0.36788539710263823
[2m[36m(func pid=109465)[0m f1_weighted: 0.3634312347975207
[2m[36m(func pid=109465)[0m f1_per_class: [0.308, 0.439, 0.833, 0.586, 0.299, 0.354, 0.141, 0.391, 0.081, 0.246]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3863 | Steps: 4 | Val loss: 3.1235 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3991 | Steps: 4 | Val loss: 2.2218 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6630 | Steps: 4 | Val loss: 1.6462 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.0583 | Steps: 4 | Val loss: 17.8544 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:19:10 (running for 00:11:28.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.329 |      0.133 |                   40 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.753 |      0.363 |                   39 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.386 |      0.327 |                   18 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  6.473 |      0.368 |                   14 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.30223880597014924
[2m[36m(func pid=108399)[0m top5: 0.742070895522388
[2m[36m(func pid=108399)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=108399)[0m f1_macro: 0.32693944994310026
[2m[36m(func pid=108399)[0m f1_weighted: 0.252185201776955
[2m[36m(func pid=108399)[0m f1_per_class: [0.325, 0.5, 0.8, 0.239, 0.168, 0.282, 0.074, 0.384, 0.202, 0.295]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.17024253731343283
[2m[36m(func pid=102433)[0m top5: 0.6427238805970149
[2m[36m(func pid=102433)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=102433)[0m f1_macro: 0.13747769537934523
[2m[36m(func pid=102433)[0m f1_weighted: 0.18874027026458653
[2m[36m(func pid=102433)[0m f1_per_class: [0.24, 0.22, 0.096, 0.224, 0.0, 0.095, 0.199, 0.171, 0.061, 0.068]
[2m[36m(func pid=102477)[0m top1: 0.38619402985074625
[2m[36m(func pid=102477)[0m top5: 0.9071828358208955
[2m[36m(func pid=102477)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=102477)[0m f1_macro: 0.34764701914841734
[2m[36m(func pid=102477)[0m f1_weighted: 0.3795760005198166
[2m[36m(func pid=102477)[0m f1_per_class: [0.581, 0.352, 0.533, 0.602, 0.164, 0.208, 0.274, 0.35, 0.189, 0.222]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.44263059701492535
[2m[36m(func pid=109465)[0m top5: 0.9188432835820896
[2m[36m(func pid=109465)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=109465)[0m f1_macro: 0.39974877460669994
[2m[36m(func pid=109465)[0m f1_weighted: 0.40011536463720104
[2m[36m(func pid=109465)[0m f1_per_class: [0.644, 0.526, 0.75, 0.57, 0.265, 0.348, 0.204, 0.372, 0.242, 0.077]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1404 | Steps: 4 | Val loss: 2.1802 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4729 | Steps: 4 | Val loss: 2.2195 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6944 | Steps: 4 | Val loss: 1.6461 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 9.0195 | Steps: 4 | Val loss: 19.7274 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 11:19:15 (running for 00:11:33.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.399 |      0.137 |                   41 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.663 |      0.348 |                   40 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.14  |      0.363 |                   19 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.058 |      0.4   |                   15 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3941231343283582
[2m[36m(func pid=108399)[0m top5: 0.8903917910447762
[2m[36m(func pid=108399)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=108399)[0m f1_macro: 0.36307475522848853
[2m[36m(func pid=108399)[0m f1_weighted: 0.3798363234347019
[2m[36m(func pid=108399)[0m f1_per_class: [0.415, 0.568, 0.71, 0.496, 0.188, 0.272, 0.236, 0.33, 0.193, 0.224]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.16744402985074627
[2m[36m(func pid=102433)[0m top5: 0.6375932835820896
[2m[36m(func pid=102433)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=102433)[0m f1_macro: 0.13612682389497968
[2m[36m(func pid=102433)[0m f1_weighted: 0.18532206043174904
[2m[36m(func pid=102433)[0m f1_per_class: [0.213, 0.211, 0.103, 0.239, 0.0, 0.098, 0.175, 0.192, 0.075, 0.057]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.39598880597014924
[2m[36m(func pid=102477)[0m top5: 0.9071828358208955
[2m[36m(func pid=102477)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=102477)[0m f1_macro: 0.3444983987437373
[2m[36m(func pid=102477)[0m f1_weighted: 0.3826642448453275
[2m[36m(func pid=102477)[0m f1_per_class: [0.612, 0.396, 0.375, 0.598, 0.186, 0.254, 0.241, 0.379, 0.19, 0.212]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.36800373134328357
[2m[36m(func pid=109465)[0m top5: 0.8446828358208955
[2m[36m(func pid=109465)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=109465)[0m f1_macro: 0.30364827083557394
[2m[36m(func pid=109465)[0m f1_weighted: 0.3581712152484907
[2m[36m(func pid=109465)[0m f1_per_class: [0.24, 0.536, 0.647, 0.58, 0.148, 0.205, 0.146, 0.382, 0.153, 0.0]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5681 | Steps: 4 | Val loss: 2.6673 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4037 | Steps: 4 | Val loss: 2.2035 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.9277 | Steps: 4 | Val loss: 1.6122 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 4.9710 | Steps: 4 | Val loss: 35.5329 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:19:21 (running for 00:11:38.72)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.473 |      0.136 |                   42 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.694 |      0.344 |                   41 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.568 |      0.307 |                   20 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  9.019 |      0.304 |                   16 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.41651119402985076
[2m[36m(func pid=102477)[0m top5: 0.9123134328358209
[2m[36m(func pid=102477)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=102477)[0m f1_macro: 0.3588915461121417
[2m[36m(func pid=102477)[0m f1_weighted: 0.4168285462808022
[2m[36m(func pid=102477)[0m f1_per_class: [0.583, 0.504, 0.324, 0.566, 0.189, 0.291, 0.306, 0.403, 0.202, 0.222]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.38572761194029853
[2m[36m(func pid=108399)[0m top5: 0.8297574626865671
[2m[36m(func pid=108399)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=108399)[0m f1_macro: 0.3071629260402873
[2m[36m(func pid=108399)[0m f1_weighted: 0.35632055661788375
[2m[36m(func pid=108399)[0m f1_per_class: [0.637, 0.047, 0.512, 0.544, 0.239, 0.038, 0.524, 0.215, 0.087, 0.228]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.17863805970149255
[2m[36m(func pid=102433)[0m top5: 0.6567164179104478
[2m[36m(func pid=102433)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=102433)[0m f1_macro: 0.14196107704754832
[2m[36m(func pid=102433)[0m f1_weighted: 0.19666857034051402
[2m[36m(func pid=102433)[0m f1_per_class: [0.212, 0.203, 0.118, 0.274, 0.0, 0.117, 0.177, 0.195, 0.062, 0.062]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.1553171641791045
[2m[36m(func pid=109465)[0m top5: 0.6068097014925373
[2m[36m(func pid=109465)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=109465)[0m f1_macro: 0.18336716280287702
[2m[36m(func pid=109465)[0m f1_weighted: 0.14712086925319223
[2m[36m(func pid=109465)[0m f1_per_class: [0.602, 0.005, 0.042, 0.196, 0.104, 0.214, 0.104, 0.288, 0.134, 0.145]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5845 | Steps: 4 | Val loss: 2.7904 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6564 | Steps: 4 | Val loss: 1.5128 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3644 | Steps: 4 | Val loss: 2.2074 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 7.8634 | Steps: 4 | Val loss: 34.3615 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:19:26 (running for 00:11:44.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.404 |      0.142 |                   43 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.928 |      0.359 |                   42 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.584 |      0.284 |                   21 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.971 |      0.183 |                   17 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4556902985074627
[2m[36m(func pid=102477)[0m top5: 0.9323694029850746
[2m[36m(func pid=102477)[0m f1_micro: 0.4556902985074627
[2m[36m(func pid=102477)[0m f1_macro: 0.37591553175966225
[2m[36m(func pid=102477)[0m f1_weighted: 0.46434575176001097
[2m[36m(func pid=102477)[0m f1_per_class: [0.553, 0.54, 0.366, 0.553, 0.213, 0.294, 0.47, 0.351, 0.138, 0.28]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3260261194029851
[2m[36m(func pid=108399)[0m top5: 0.7961753731343284
[2m[36m(func pid=108399)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=108399)[0m f1_macro: 0.2841550148776726
[2m[36m(func pid=108399)[0m f1_weighted: 0.3319960803958391
[2m[36m(func pid=108399)[0m f1_per_class: [0.606, 0.037, 0.325, 0.57, 0.091, 0.11, 0.39, 0.247, 0.145, 0.321]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.16930970149253732
[2m[36m(func pid=102433)[0m top5: 0.6585820895522388
[2m[36m(func pid=102433)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=102433)[0m f1_macro: 0.13725714209831272
[2m[36m(func pid=102433)[0m f1_weighted: 0.18575801207200038
[2m[36m(func pid=102433)[0m f1_per_class: [0.181, 0.16, 0.149, 0.284, 0.014, 0.109, 0.162, 0.181, 0.066, 0.065]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.18470149253731344
[2m[36m(func pid=109465)[0m top5: 0.6534514925373134
[2m[36m(func pid=109465)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=109465)[0m f1_macro: 0.2326691852022434
[2m[36m(func pid=109465)[0m f1_weighted: 0.16278875329073406
[2m[36m(func pid=109465)[0m f1_per_class: [0.571, 0.037, 0.456, 0.157, 0.1, 0.222, 0.15, 0.323, 0.23, 0.08]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8247 | Steps: 4 | Val loss: 1.5516 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3129 | Steps: 4 | Val loss: 2.1840 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5237 | Steps: 4 | Val loss: 2.4862 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2051 | Steps: 4 | Val loss: 18.7071 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=108399)[0m top1: 0.322294776119403
[2m[36m(func pid=108399)[0m top5: 0.8414179104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=108399)[0m f1_macro: 0.29087946723907526
[2m[36m(func pid=108399)[0m f1_weighted: 0.36110458198655654
[2m[36m(func pid=108399)[0m f1_per_class: [0.562, 0.218, 0.222, 0.501, 0.073, 0.196, 0.417, 0.259, 0.158, 0.302]
== Status ==
Current time: 2024-01-07 11:19:31 (running for 00:11:49.37)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.364 |      0.137 |                   44 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.656 |      0.376 |                   43 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.524 |      0.291 |                   22 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  7.863 |      0.233 |                   18 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.45009328358208955
[2m[36m(func pid=102477)[0m top5: 0.9174440298507462
[2m[36m(func pid=102477)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=102477)[0m f1_macro: 0.365240904791713
[2m[36m(func pid=102477)[0m f1_weighted: 0.4534715300148748
[2m[36m(func pid=102477)[0m f1_per_class: [0.5, 0.554, 0.338, 0.458, 0.211, 0.296, 0.52, 0.344, 0.13, 0.302]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.18423507462686567
[2m[36m(func pid=102433)[0m top5: 0.6763059701492538
[2m[36m(func pid=102433)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=102433)[0m f1_macro: 0.15120290321331564
[2m[36m(func pid=102433)[0m f1_weighted: 0.20327690194816914
[2m[36m(func pid=102433)[0m f1_per_class: [0.212, 0.158, 0.174, 0.295, 0.015, 0.113, 0.204, 0.195, 0.071, 0.075]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.36380597014925375
[2m[36m(func pid=109465)[0m top5: 0.8684701492537313
[2m[36m(func pid=109465)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=109465)[0m f1_macro: 0.3610887238466994
[2m[36m(func pid=109465)[0m f1_weighted: 0.4005534999462763
[2m[36m(func pid=109465)[0m f1_per_class: [0.571, 0.52, 0.636, 0.498, 0.107, 0.196, 0.345, 0.346, 0.279, 0.112]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2128 | Steps: 4 | Val loss: 3.0893 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7296 | Steps: 4 | Val loss: 1.6478 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3124 | Steps: 4 | Val loss: 2.1620 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9387 | Steps: 4 | Val loss: 14.1718 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:19:37 (running for 00:11:54.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.313 |      0.151 |                   45 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.825 |      0.365 |                   44 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.524 |      0.291 |                   22 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.205 |      0.361 |                   19 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.42117537313432835
[2m[36m(func pid=102477)[0m top5: 0.9006529850746269
[2m[36m(func pid=102477)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=102477)[0m f1_macro: 0.3655434797726918
[2m[36m(func pid=102477)[0m f1_weighted: 0.4264350527035118
[2m[36m(func pid=102477)[0m f1_per_class: [0.47, 0.531, 0.361, 0.407, 0.17, 0.265, 0.483, 0.392, 0.221, 0.355]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.25886194029850745
[2m[36m(func pid=108399)[0m top5: 0.8013059701492538
[2m[36m(func pid=108399)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=108399)[0m f1_macro: 0.2879714480510861
[2m[36m(func pid=108399)[0m f1_weighted: 0.3039460294376546
[2m[36m(func pid=108399)[0m f1_per_class: [0.439, 0.407, 0.24, 0.4, 0.043, 0.128, 0.209, 0.404, 0.186, 0.423]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.19263059701492538
[2m[36m(func pid=102433)[0m top5: 0.6930970149253731
[2m[36m(func pid=102433)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=102433)[0m f1_macro: 0.158011565796812
[2m[36m(func pid=102433)[0m f1_weighted: 0.2133453768276584
[2m[36m(func pid=102433)[0m f1_per_class: [0.234, 0.2, 0.2, 0.297, 0.016, 0.118, 0.216, 0.159, 0.066, 0.075]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.47761194029850745
[2m[36m(func pid=109465)[0m top5: 0.9197761194029851
[2m[36m(func pid=109465)[0m f1_micro: 0.47761194029850745
[2m[36m(func pid=109465)[0m f1_macro: 0.41415059569490503
[2m[36m(func pid=109465)[0m f1_weighted: 0.4854609552827214
[2m[36m(func pid=109465)[0m f1_per_class: [0.647, 0.575, 0.667, 0.599, 0.121, 0.128, 0.516, 0.371, 0.254, 0.266]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6843 | Steps: 4 | Val loss: 1.7739 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3558 | Steps: 4 | Val loss: 3.0731 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8971 | Steps: 4 | Val loss: 17.1288 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.2818 | Steps: 4 | Val loss: 2.1403 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 11:19:42 (running for 00:11:59.93)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.312 |      0.158 |                   46 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.73  |      0.366 |                   45 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.213 |      0.288 |                   23 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  2.939 |      0.414 |                   20 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3675373134328358
[2m[36m(func pid=102477)[0m top5: 0.8768656716417911
[2m[36m(func pid=102477)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=102477)[0m f1_macro: 0.3393355312665279
[2m[36m(func pid=102477)[0m f1_weighted: 0.3828305100609051
[2m[36m(func pid=102477)[0m f1_per_class: [0.485, 0.495, 0.364, 0.421, 0.187, 0.234, 0.368, 0.358, 0.179, 0.302]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.2896455223880597
[2m[36m(func pid=108399)[0m top5: 0.7957089552238806
[2m[36m(func pid=108399)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=108399)[0m f1_macro: 0.3028337010377983
[2m[36m(func pid=108399)[0m f1_weighted: 0.3054748492257915
[2m[36m(func pid=108399)[0m f1_per_class: [0.516, 0.502, 0.478, 0.437, 0.096, 0.17, 0.13, 0.307, 0.12, 0.273]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.20382462686567165
[2m[36m(func pid=102433)[0m top5: 0.7154850746268657
[2m[36m(func pid=102433)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=102433)[0m f1_macro: 0.16505611468483056
[2m[36m(func pid=102433)[0m f1_weighted: 0.22288267038024895
[2m[36m(func pid=102433)[0m f1_per_class: [0.24, 0.219, 0.22, 0.32, 0.017, 0.116, 0.213, 0.166, 0.065, 0.075]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.4388992537313433
[2m[36m(func pid=109465)[0m top5: 0.8889925373134329
[2m[36m(func pid=109465)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=109465)[0m f1_macro: 0.315673613510003
[2m[36m(func pid=109465)[0m f1_weighted: 0.40831689413736305
[2m[36m(func pid=109465)[0m f1_per_class: [0.575, 0.104, 0.358, 0.628, 0.179, 0.089, 0.55, 0.315, 0.15, 0.207]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8967 | Steps: 4 | Val loss: 1.8217 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7098 | Steps: 4 | Val loss: 2.3593 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3624 | Steps: 4 | Val loss: 2.1223 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.7199 | Steps: 4 | Val loss: 22.6743 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:19:47 (running for 00:12:05.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.282 |      0.165 |                   47 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.897 |      0.318 |                   47 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.356 |      0.303 |                   24 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.897 |      0.316 |                   21 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.34421641791044777
[2m[36m(func pid=102477)[0m top5: 0.8540111940298507
[2m[36m(func pid=102477)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=102477)[0m f1_macro: 0.31833558087463965
[2m[36m(func pid=102477)[0m f1_weighted: 0.3589763287420734
[2m[36m(func pid=102477)[0m f1_per_class: [0.539, 0.451, 0.348, 0.5, 0.145, 0.205, 0.253, 0.358, 0.164, 0.221]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.40158582089552236
[2m[36m(func pid=108399)[0m top5: 0.8731343283582089
[2m[36m(func pid=108399)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=108399)[0m f1_macro: 0.3711640245397641
[2m[36m(func pid=108399)[0m f1_weighted: 0.3911004877070325
[2m[36m(func pid=108399)[0m f1_per_class: [0.557, 0.556, 0.537, 0.509, 0.255, 0.236, 0.28, 0.296, 0.162, 0.323]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21641791044776118
[2m[36m(func pid=102433)[0m top5: 0.7294776119402985
[2m[36m(func pid=102433)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=102433)[0m f1_macro: 0.17672461051639618
[2m[36m(func pid=102433)[0m f1_weighted: 0.23077432158073244
[2m[36m(func pid=102433)[0m f1_per_class: [0.236, 0.242, 0.261, 0.344, 0.036, 0.125, 0.199, 0.167, 0.075, 0.083]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3278917910447761
[2m[36m(func pid=109465)[0m top5: 0.8166977611940298
[2m[36m(func pid=109465)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=109465)[0m f1_macro: 0.2816944930220685
[2m[36m(func pid=109465)[0m f1_weighted: 0.33069538461087933
[2m[36m(func pid=109465)[0m f1_per_class: [0.61, 0.284, 0.078, 0.603, 0.121, 0.232, 0.155, 0.34, 0.127, 0.267]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.6128 | Steps: 4 | Val loss: 1.8376 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.1173 | Steps: 4 | Val loss: 2.6490 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2871 | Steps: 4 | Val loss: 2.1244 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1101 | Steps: 4 | Val loss: 36.8527 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 11:19:53 (running for 00:12:10.64)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.362 |      0.177 |                   48 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.897 |      0.318 |                   47 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.71  |      0.371 |                   25 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.72  |      0.282 |                   22 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.33675373134328357
[2m[36m(func pid=102477)[0m top5: 0.8512126865671642
[2m[36m(func pid=102477)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=102477)[0m f1_macro: 0.3145037348087499
[2m[36m(func pid=102477)[0m f1_weighted: 0.3375343835754534
[2m[36m(func pid=102477)[0m f1_per_class: [0.561, 0.378, 0.421, 0.562, 0.146, 0.217, 0.163, 0.331, 0.168, 0.196]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.32322761194029853
[2m[36m(func pid=108399)[0m top5: 0.8642723880597015
[2m[36m(func pid=108399)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=108399)[0m f1_macro: 0.32931406749454595
[2m[36m(func pid=108399)[0m f1_weighted: 0.3440908948544925
[2m[36m(func pid=108399)[0m f1_per_class: [0.659, 0.497, 0.453, 0.344, 0.185, 0.2, 0.32, 0.307, 0.245, 0.084]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3414179104477612
[2m[36m(func pid=109465)[0m top5: 0.6469216417910447
[2m[36m(func pid=109465)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=109465)[0m f1_macro: 0.2981709987493239
[2m[36m(func pid=109465)[0m f1_weighted: 0.30561264786306663
[2m[36m(func pid=109465)[0m f1_per_class: [0.568, 0.365, 0.093, 0.59, 0.126, 0.259, 0.0, 0.417, 0.22, 0.342]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21548507462686567
[2m[36m(func pid=102433)[0m top5: 0.7336753731343284
[2m[36m(func pid=102433)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=102433)[0m f1_macro: 0.18013439583498725
[2m[36m(func pid=102433)[0m f1_weighted: 0.23197019442876943
[2m[36m(func pid=102433)[0m f1_per_class: [0.235, 0.284, 0.282, 0.326, 0.035, 0.131, 0.194, 0.156, 0.082, 0.076]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5942 | Steps: 4 | Val loss: 1.8247 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2780 | Steps: 4 | Val loss: 2.1195 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2392 | Steps: 4 | Val loss: 2.8423 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.9823 | Steps: 4 | Val loss: 37.5703 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:19:58 (running for 00:12:15.88)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.287 |      0.18  |                   49 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.613 |      0.315 |                   48 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.117 |      0.329 |                   26 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  2.11  |      0.298 |                   23 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3512126865671642
[2m[36m(func pid=102477)[0m top5: 0.8488805970149254
[2m[36m(func pid=102477)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=102477)[0m f1_macro: 0.3214196261920522
[2m[36m(func pid=102477)[0m f1_weighted: 0.3400051761572357
[2m[36m(func pid=102477)[0m f1_per_class: [0.609, 0.443, 0.421, 0.576, 0.1, 0.19, 0.123, 0.345, 0.199, 0.209]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3362873134328358
[2m[36m(func pid=108399)[0m top5: 0.8423507462686567
[2m[36m(func pid=108399)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=108399)[0m f1_macro: 0.32044838023519195
[2m[36m(func pid=108399)[0m f1_weighted: 0.3123523938319255
[2m[36m(func pid=108399)[0m f1_per_class: [0.646, 0.56, 0.4, 0.22, 0.2, 0.222, 0.297, 0.257, 0.19, 0.212]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.30363805970149255
[2m[36m(func pid=109465)[0m top5: 0.6231343283582089
[2m[36m(func pid=109465)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=109465)[0m f1_macro: 0.2516346100894791
[2m[36m(func pid=109465)[0m f1_weighted: 0.2724743144821138
[2m[36m(func pid=109465)[0m f1_per_class: [0.508, 0.364, 0.26, 0.558, 0.098, 0.161, 0.006, 0.222, 0.205, 0.135]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21128731343283583
[2m[36m(func pid=102433)[0m top5: 0.7364738805970149
[2m[36m(func pid=102433)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=102433)[0m f1_macro: 0.18039584400596403
[2m[36m(func pid=102433)[0m f1_weighted: 0.228629188683887
[2m[36m(func pid=102433)[0m f1_per_class: [0.263, 0.271, 0.273, 0.309, 0.021, 0.129, 0.2, 0.181, 0.081, 0.075]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7112 | Steps: 4 | Val loss: 1.7834 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4420 | Steps: 4 | Val loss: 2.1669 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.3592 | Steps: 4 | Val loss: 29.9490 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.2631 | Steps: 4 | Val loss: 2.1260 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:20:03 (running for 00:12:21.22)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.278 |      0.18  |                   50 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.594 |      0.321 |                   49 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.239 |      0.32  |                   27 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.982 |      0.252 |                   24 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.37220149253731344
[2m[36m(func pid=102477)[0m top5: 0.8680037313432836
[2m[36m(func pid=102477)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=102477)[0m f1_macro: 0.33451847507746574
[2m[36m(func pid=102477)[0m f1_weighted: 0.3552004602761627
[2m[36m(func pid=102477)[0m f1_per_class: [0.6, 0.504, 0.429, 0.594, 0.084, 0.186, 0.121, 0.349, 0.201, 0.278]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.2966417910447761
[2m[36m(func pid=109465)[0m top5: 0.6916977611940298
[2m[36m(func pid=109465)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=109465)[0m f1_macro: 0.2723170196267276
[2m[36m(func pid=109465)[0m f1_weighted: 0.29351521600726976
[2m[36m(func pid=109465)[0m f1_per_class: [0.6, 0.491, 0.433, 0.505, 0.059, 0.05, 0.098, 0.178, 0.184, 0.125]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.20848880597014927
[2m[36m(func pid=102433)[0m top5: 0.7346082089552238
[2m[36m(func pid=102433)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=102433)[0m f1_macro: 0.18086263619397122
[2m[36m(func pid=102433)[0m f1_weighted: 0.22302334821226028
[2m[36m(func pid=102433)[0m f1_per_class: [0.262, 0.295, 0.26, 0.316, 0.038, 0.127, 0.162, 0.18, 0.08, 0.089]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.42024253731343286
[2m[36m(func pid=108399)[0m top5: 0.8955223880597015
[2m[36m(func pid=108399)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=108399)[0m f1_macro: 0.39292859033640176
[2m[36m(func pid=108399)[0m f1_weighted: 0.41598864629225485
[2m[36m(func pid=108399)[0m f1_per_class: [0.648, 0.575, 0.537, 0.373, 0.17, 0.23, 0.462, 0.308, 0.286, 0.341]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 4.3279 | Steps: 4 | Val loss: 27.3431 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7822 | Steps: 4 | Val loss: 1.7509 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.1012 | Steps: 4 | Val loss: 2.6256 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1434 | Steps: 4 | Val loss: 2.1347 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:20:09 (running for 00:12:26.55)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.263 |      0.181 |                   51 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.711 |      0.335 |                   50 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.442 |      0.393 |                   28 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.359 |      0.272 |                   25 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.353544776119403
[2m[36m(func pid=109465)[0m top5: 0.8050373134328358
[2m[36m(func pid=109465)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=109465)[0m f1_macro: 0.2999269345101427
[2m[36m(func pid=109465)[0m f1_weighted: 0.3105010511128044
[2m[36m(func pid=109465)[0m f1_per_class: [0.167, 0.487, 0.686, 0.481, 0.069, 0.077, 0.164, 0.268, 0.202, 0.4]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.37406716417910446
[2m[36m(func pid=102477)[0m top5: 0.8861940298507462
[2m[36m(func pid=102477)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=102477)[0m f1_macro: 0.35025279390266334
[2m[36m(func pid=102477)[0m f1_weighted: 0.35002159389818066
[2m[36m(func pid=102477)[0m f1_per_class: [0.61, 0.497, 0.511, 0.594, 0.112, 0.186, 0.106, 0.315, 0.215, 0.357]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.31296641791044777
[2m[36m(func pid=108399)[0m top5: 0.9034514925373134
[2m[36m(func pid=108399)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=108399)[0m f1_macro: 0.34984068279783653
[2m[36m(func pid=108399)[0m f1_weighted: 0.35155373144961405
[2m[36m(func pid=108399)[0m f1_per_class: [0.589, 0.198, 0.688, 0.525, 0.21, 0.225, 0.338, 0.345, 0.111, 0.27]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.208955223880597
[2m[36m(func pid=102433)[0m top5: 0.7299440298507462
[2m[36m(func pid=102433)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=102433)[0m f1_macro: 0.18130859673933292
[2m[36m(func pid=102433)[0m f1_weighted: 0.22675123542153666
[2m[36m(func pid=102433)[0m f1_per_class: [0.241, 0.302, 0.25, 0.296, 0.028, 0.113, 0.19, 0.204, 0.091, 0.097]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 12.9086 | Steps: 4 | Val loss: 25.8218 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4991 | Steps: 4 | Val loss: 2.7042 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8052 | Steps: 4 | Val loss: 1.7136 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1845 | Steps: 4 | Val loss: 2.1281 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:20:14 (running for 00:12:32.12)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.143 |      0.181 |                   52 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.782 |      0.35  |                   51 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.101 |      0.35  |                   29 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.328 |      0.3   |                   26 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.37826492537313433
[2m[36m(func pid=102477)[0m top5: 0.9011194029850746
[2m[36m(func pid=102477)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=102477)[0m f1_macro: 0.3596344499149061
[2m[36m(func pid=102477)[0m f1_weighted: 0.3799505117787765
[2m[36m(func pid=102477)[0m f1_per_class: [0.615, 0.51, 0.48, 0.57, 0.094, 0.144, 0.234, 0.327, 0.209, 0.413]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.39552238805970147
[2m[36m(func pid=109465)[0m top5: 0.8409514925373134
[2m[36m(func pid=109465)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=109465)[0m f1_macro: 0.34995520268012414
[2m[36m(func pid=109465)[0m f1_weighted: 0.3384006217582569
[2m[36m(func pid=109465)[0m f1_per_class: [0.275, 0.542, 0.828, 0.518, 0.077, 0.213, 0.109, 0.372, 0.207, 0.359]
[2m[36m(func pid=108399)[0m top1: 0.34888059701492535
[2m[36m(func pid=108399)[0m top5: 0.8684701492537313
[2m[36m(func pid=108399)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=108399)[0m f1_macro: 0.33961897685414666
[2m[36m(func pid=108399)[0m f1_weighted: 0.3390200197007406
[2m[36m(func pid=108399)[0m f1_per_class: [0.548, 0.226, 0.5, 0.583, 0.166, 0.286, 0.198, 0.368, 0.133, 0.388]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21595149253731344
[2m[36m(func pid=102433)[0m top5: 0.7215485074626866
[2m[36m(func pid=102433)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=102433)[0m f1_macro: 0.1844107760111687
[2m[36m(func pid=102433)[0m f1_weighted: 0.22925053148157298
[2m[36m(func pid=102433)[0m f1_per_class: [0.286, 0.327, 0.21, 0.287, 0.022, 0.105, 0.191, 0.214, 0.093, 0.109]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 10.3111 | Steps: 4 | Val loss: 28.1289 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6824 | Steps: 4 | Val loss: 1.6078 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3801 | Steps: 4 | Val loss: 2.6878 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1774 | Steps: 4 | Val loss: 2.1161 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 11:20:19 (running for 00:12:37.42)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.184 |      0.184 |                   53 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.805 |      0.36  |                   52 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.499 |      0.34  |                   30 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 12.909 |      0.35  |                   27 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.42350746268656714
[2m[36m(func pid=102477)[0m top5: 0.9071828358208955
[2m[36m(func pid=102477)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=102477)[0m f1_macro: 0.3686587602466866
[2m[36m(func pid=102477)[0m f1_weighted: 0.448086415364991
[2m[36m(func pid=102477)[0m f1_per_class: [0.577, 0.493, 0.4, 0.549, 0.106, 0.118, 0.512, 0.316, 0.164, 0.453]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3306902985074627
[2m[36m(func pid=109465)[0m top5: 0.7873134328358209
[2m[36m(func pid=109465)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=109465)[0m f1_macro: 0.342405578683611
[2m[36m(func pid=109465)[0m f1_weighted: 0.3101535034947842
[2m[36m(func pid=109465)[0m f1_per_class: [0.652, 0.526, 0.533, 0.113, 0.143, 0.281, 0.378, 0.252, 0.22, 0.326]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.35447761194029853
[2m[36m(func pid=108399)[0m top5: 0.8605410447761194
[2m[36m(func pid=108399)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=108399)[0m f1_macro: 0.31962349774470833
[2m[36m(func pid=108399)[0m f1_weighted: 0.36292756217830124
[2m[36m(func pid=108399)[0m f1_per_class: [0.585, 0.565, 0.429, 0.521, 0.084, 0.182, 0.196, 0.377, 0.054, 0.204]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22294776119402984
[2m[36m(func pid=102433)[0m top5: 0.7364738805970149
[2m[36m(func pid=102433)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=102433)[0m f1_macro: 0.184765200363869
[2m[36m(func pid=102433)[0m f1_weighted: 0.23944637656373088
[2m[36m(func pid=102433)[0m f1_per_class: [0.29, 0.311, 0.191, 0.286, 0.023, 0.122, 0.229, 0.231, 0.065, 0.099]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7599 | Steps: 4 | Val loss: 1.6136 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2008 | Steps: 4 | Val loss: 2.9540 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 9.2159 | Steps: 4 | Val loss: 39.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2677 | Steps: 4 | Val loss: 2.1089 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:20:25 (running for 00:12:42.80)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.177 |      0.185 |                   54 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.682 |      0.369 |                   53 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.38  |      0.32  |                   31 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 10.311 |      0.342 |                   28 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4193097014925373
[2m[36m(func pid=102477)[0m top5: 0.9015858208955224
[2m[36m(func pid=102477)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=102477)[0m f1_macro: 0.3577028573390712
[2m[36m(func pid=102477)[0m f1_weighted: 0.44209402935140707
[2m[36m(func pid=102477)[0m f1_per_class: [0.574, 0.48, 0.4, 0.491, 0.134, 0.183, 0.535, 0.311, 0.16, 0.308]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3376865671641791
[2m[36m(func pid=108399)[0m top5: 0.8255597014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=108399)[0m f1_macro: 0.31338167343646106
[2m[36m(func pid=108399)[0m f1_weighted: 0.34164832964303166
[2m[36m(func pid=108399)[0m f1_per_class: [0.62, 0.529, 0.444, 0.377, 0.06, 0.093, 0.3, 0.379, 0.168, 0.162]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.25093283582089554
[2m[36m(func pid=109465)[0m top5: 0.6781716417910447
[2m[36m(func pid=109465)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=109465)[0m f1_macro: 0.24004222996480823
[2m[36m(func pid=109465)[0m f1_weighted: 0.2077358940741575
[2m[36m(func pid=109465)[0m f1_per_class: [0.458, 0.079, 0.44, 0.058, 0.312, 0.225, 0.412, 0.179, 0.08, 0.156]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21688432835820895
[2m[36m(func pid=102433)[0m top5: 0.7453358208955224
[2m[36m(func pid=102433)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=102433)[0m f1_macro: 0.17876380256173136
[2m[36m(func pid=102433)[0m f1_weighted: 0.2338674800199928
[2m[36m(func pid=102433)[0m f1_per_class: [0.276, 0.3, 0.19, 0.292, 0.028, 0.116, 0.217, 0.217, 0.07, 0.081]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6235 | Steps: 4 | Val loss: 1.6688 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2335 | Steps: 4 | Val loss: 3.1413 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.7831 | Steps: 4 | Val loss: 37.3656 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3224 | Steps: 4 | Val loss: 2.0954 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:20:30 (running for 00:12:48.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.268 |      0.179 |                   55 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.76  |      0.358 |                   54 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.201 |      0.313 |                   32 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  9.216 |      0.24  |                   29 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.408115671641791
[2m[36m(func pid=102477)[0m top5: 0.8861940298507462
[2m[36m(func pid=102477)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=102477)[0m f1_macro: 0.34919013879458155
[2m[36m(func pid=102477)[0m f1_weighted: 0.4279260056262665
[2m[36m(func pid=102477)[0m f1_per_class: [0.438, 0.479, 0.449, 0.449, 0.13, 0.205, 0.523, 0.312, 0.192, 0.315]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.31949626865671643
[2m[36m(func pid=108399)[0m top5: 0.8017723880597015
[2m[36m(func pid=108399)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=108399)[0m f1_macro: 0.3071023645201582
[2m[36m(func pid=108399)[0m f1_weighted: 0.3150875943045557
[2m[36m(func pid=108399)[0m f1_per_class: [0.638, 0.527, 0.415, 0.283, 0.088, 0.084, 0.303, 0.367, 0.163, 0.202]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.19402985074626866
[2m[36m(func pid=109465)[0m top5: 0.7061567164179104
[2m[36m(func pid=109465)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=109465)[0m f1_macro: 0.24295342242776705
[2m[36m(func pid=109465)[0m f1_weighted: 0.19628071436959807
[2m[36m(func pid=109465)[0m f1_per_class: [0.632, 0.087, 0.464, 0.208, 0.146, 0.168, 0.226, 0.242, 0.142, 0.115]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22061567164179105
[2m[36m(func pid=102433)[0m top5: 0.7527985074626866
[2m[36m(func pid=102433)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=102433)[0m f1_macro: 0.18534890231003595
[2m[36m(func pid=102433)[0m f1_weighted: 0.24184189680686977
[2m[36m(func pid=102433)[0m f1_per_class: [0.257, 0.267, 0.239, 0.314, 0.028, 0.11, 0.241, 0.235, 0.073, 0.09]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8903 | Steps: 4 | Val loss: 1.7102 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2668 | Steps: 4 | Val loss: 3.4967 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 5.3849 | Steps: 4 | Val loss: 36.3579 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1197 | Steps: 4 | Val loss: 2.0783 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=102477)[0m top1: 0.3969216417910448
[2m[36m(func pid=102477)[0m top5: 0.8819962686567164
[2m[36m(func pid=102477)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=102477)[0m f1_macro: 0.3445397738514429
[2m[36m(func pid=102477)[0m f1_weighted: 0.41864800924063705
[2m[36m(func pid=102477)[0m f1_per_class: [0.402, 0.512, 0.444, 0.421, 0.111, 0.205, 0.498, 0.327, 0.215, 0.311]
[2m[36m(func pid=102477)[0m 
== Status ==
Current time: 2024-01-07 11:20:35 (running for 00:12:53.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.322 |      0.185 |                   56 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.89  |      0.345 |                   56 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.233 |      0.307 |                   33 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.783 |      0.243 |                   30 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.261660447761194
[2m[36m(func pid=108399)[0m top5: 0.820429104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=108399)[0m f1_macro: 0.3277711067774943
[2m[36m(func pid=108399)[0m f1_weighted: 0.2759248137875902
[2m[36m(func pid=108399)[0m f1_per_class: [0.645, 0.485, 0.71, 0.247, 0.134, 0.069, 0.234, 0.35, 0.115, 0.288]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.2887126865671642
[2m[36m(func pid=109465)[0m top5: 0.6665111940298507
[2m[36m(func pid=109465)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=109465)[0m f1_macro: 0.30087461140450783
[2m[36m(func pid=109465)[0m f1_weighted: 0.2777462091843496
[2m[36m(func pid=109465)[0m f1_per_class: [0.594, 0.518, 0.632, 0.453, 0.082, 0.024, 0.057, 0.312, 0.185, 0.153]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.23367537313432835
[2m[36m(func pid=102433)[0m top5: 0.773320895522388
[2m[36m(func pid=102433)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=102433)[0m f1_macro: 0.19511433852301918
[2m[36m(func pid=102433)[0m f1_weighted: 0.2532144790596999
[2m[36m(func pid=102433)[0m f1_per_class: [0.305, 0.318, 0.256, 0.318, 0.029, 0.114, 0.246, 0.212, 0.063, 0.091]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6759 | Steps: 4 | Val loss: 1.7965 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3294 | Steps: 4 | Val loss: 2.7220 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 6.1157 | Steps: 4 | Val loss: 40.7602 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2209 | Steps: 4 | Val loss: 2.0687 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:20:41 (running for 00:12:58.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.12  |      0.195 |                   57 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.676 |      0.319 |                   57 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.267 |      0.328 |                   34 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  5.385 |      0.301 |                   31 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.33908582089552236
[2m[36m(func pid=102477)[0m top5: 0.8782649253731343
[2m[36m(func pid=102477)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=102477)[0m f1_macro: 0.3186150571698213
[2m[36m(func pid=102477)[0m f1_weighted: 0.34967151572451494
[2m[36m(func pid=102477)[0m f1_per_class: [0.432, 0.503, 0.369, 0.428, 0.138, 0.25, 0.248, 0.357, 0.148, 0.313]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.32649253731343286
[2m[36m(func pid=109465)[0m top5: 0.6394589552238806
[2m[36m(func pid=109465)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=109465)[0m f1_macro: 0.2919113588803824
[2m[36m(func pid=109465)[0m f1_weighted: 0.2720267375935876
[2m[36m(func pid=109465)[0m f1_per_class: [0.596, 0.471, 0.595, 0.486, 0.07, 0.016, 0.031, 0.386, 0.128, 0.141]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.34654850746268656
[2m[36m(func pid=108399)[0m top5: 0.8479477611940298
[2m[36m(func pid=108399)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=108399)[0m f1_macro: 0.3361166046934055
[2m[36m(func pid=108399)[0m f1_weighted: 0.38421010769617503
[2m[36m(func pid=108399)[0m f1_per_class: [0.42, 0.493, 0.688, 0.44, 0.066, 0.108, 0.407, 0.339, 0.24, 0.159]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.23787313432835822
[2m[36m(func pid=102433)[0m top5: 0.7779850746268657
[2m[36m(func pid=102433)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=102433)[0m f1_macro: 0.19482358796800292
[2m[36m(func pid=102433)[0m f1_weighted: 0.2524922230562695
[2m[36m(func pid=102433)[0m f1_per_class: [0.302, 0.324, 0.244, 0.309, 0.032, 0.112, 0.247, 0.218, 0.071, 0.087]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6168 | Steps: 4 | Val loss: 1.8667 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 5.5540 | Steps: 4 | Val loss: 31.3770 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6906 | Steps: 4 | Val loss: 2.6141 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2088 | Steps: 4 | Val loss: 2.0799 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:20:46 (running for 00:13:03.80)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.221 |      0.195 |                   58 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.617 |      0.326 |                   58 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.329 |      0.336 |                   35 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  6.116 |      0.292 |                   32 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3521455223880597
[2m[36m(func pid=102477)[0m top5: 0.8488805970149254
[2m[36m(func pid=102477)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=102477)[0m f1_macro: 0.32593361238847046
[2m[36m(func pid=102477)[0m f1_weighted: 0.33262932781429005
[2m[36m(func pid=102477)[0m f1_per_class: [0.443, 0.564, 0.414, 0.397, 0.153, 0.279, 0.169, 0.359, 0.152, 0.329]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.38152985074626866
[2m[36m(func pid=109465)[0m top5: 0.7341417910447762
[2m[36m(func pid=109465)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=109465)[0m f1_macro: 0.3288118552026906
[2m[36m(func pid=109465)[0m f1_weighted: 0.3338117952445684
[2m[36m(func pid=109465)[0m f1_per_class: [0.359, 0.536, 0.605, 0.548, 0.151, 0.31, 0.039, 0.357, 0.2, 0.183]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.2355410447761194
[2m[36m(func pid=102433)[0m top5: 0.7747201492537313
[2m[36m(func pid=102433)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=102433)[0m f1_macro: 0.19270272600153332
[2m[36m(func pid=102433)[0m f1_weighted: 0.24985246007113343
[2m[36m(func pid=102433)[0m f1_per_class: [0.249, 0.33, 0.275, 0.285, 0.031, 0.108, 0.261, 0.227, 0.067, 0.095]
[2m[36m(func pid=108399)[0m top1: 0.37173507462686567
[2m[36m(func pid=108399)[0m top5: 0.9020522388059702
[2m[36m(func pid=108399)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=108399)[0m f1_macro: 0.3610217952129383
[2m[36m(func pid=108399)[0m f1_weighted: 0.39288382214077405
[2m[36m(func pid=108399)[0m f1_per_class: [0.521, 0.38, 0.733, 0.609, 0.052, 0.08, 0.34, 0.357, 0.22, 0.318]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7021 | Steps: 4 | Val loss: 1.8353 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 10.5485 | Steps: 4 | Val loss: 23.6459 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2875 | Steps: 4 | Val loss: 2.0751 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3452 | Steps: 4 | Val loss: 2.4759 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:20:51 (running for 00:13:09.12)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.209 |      0.193 |                   59 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.702 |      0.334 |                   59 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.691 |      0.361 |                   36 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  5.554 |      0.329 |                   33 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3712686567164179
[2m[36m(func pid=102477)[0m top5: 0.8418843283582089
[2m[36m(func pid=102477)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=102477)[0m f1_macro: 0.33365338734589706
[2m[36m(func pid=102477)[0m f1_weighted: 0.34425611245657317
[2m[36m(func pid=102477)[0m f1_per_class: [0.515, 0.556, 0.407, 0.463, 0.173, 0.291, 0.143, 0.358, 0.173, 0.259]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.38526119402985076
[2m[36m(func pid=109465)[0m top5: 0.8768656716417911
[2m[36m(func pid=109465)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=109465)[0m f1_macro: 0.32181872822028973
[2m[36m(func pid=109465)[0m f1_weighted: 0.37881739466716885
[2m[36m(func pid=109465)[0m f1_per_class: [0.23, 0.431, 0.553, 0.577, 0.148, 0.37, 0.242, 0.176, 0.185, 0.305]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.24113805970149255
[2m[36m(func pid=102433)[0m top5: 0.7779850746268657
[2m[36m(func pid=102433)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=102433)[0m f1_macro: 0.20002359183532298
[2m[36m(func pid=102433)[0m f1_weighted: 0.25656440685089504
[2m[36m(func pid=102433)[0m f1_per_class: [0.256, 0.333, 0.323, 0.312, 0.034, 0.132, 0.251, 0.205, 0.063, 0.092]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.37406716417910446
[2m[36m(func pid=108399)[0m top5: 0.8936567164179104
[2m[36m(func pid=108399)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=108399)[0m f1_macro: 0.3475915219470944
[2m[36m(func pid=108399)[0m f1_weighted: 0.37733808578885086
[2m[36m(func pid=108399)[0m f1_per_class: [0.5, 0.573, 0.667, 0.441, 0.095, 0.203, 0.3, 0.325, 0.229, 0.143]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.9212 | Steps: 4 | Val loss: 31.0435 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6057 | Steps: 4 | Val loss: 1.8705 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1595 | Steps: 4 | Val loss: 2.0806 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2251 | Steps: 4 | Val loss: 2.7927 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:20:57 (running for 00:13:14.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.287 |      0.2   |                   60 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.606 |      0.323 |                   60 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.345 |      0.348 |                   37 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 10.548 |      0.322 |                   34 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3558768656716418
[2m[36m(func pid=102477)[0m top5: 0.8260261194029851
[2m[36m(func pid=102477)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=102477)[0m f1_macro: 0.32312361993434796
[2m[36m(func pid=102477)[0m f1_weighted: 0.34174939221119277
[2m[36m(func pid=102477)[0m f1_per_class: [0.504, 0.536, 0.429, 0.5, 0.098, 0.214, 0.135, 0.373, 0.225, 0.217]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.29384328358208955
[2m[36m(func pid=109465)[0m top5: 0.8264925373134329
[2m[36m(func pid=109465)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=109465)[0m f1_macro: 0.28697114794091905
[2m[36m(func pid=109465)[0m f1_weighted: 0.3268811644511495
[2m[36m(func pid=109465)[0m f1_per_class: [0.323, 0.232, 0.667, 0.369, 0.256, 0.128, 0.481, 0.133, 0.117, 0.163]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.240205223880597
[2m[36m(func pid=102433)[0m top5: 0.7709888059701493
[2m[36m(func pid=102433)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=102433)[0m f1_macro: 0.20041699017234665
[2m[36m(func pid=102433)[0m f1_weighted: 0.26109554213326563
[2m[36m(func pid=102433)[0m f1_per_class: [0.252, 0.331, 0.321, 0.32, 0.039, 0.128, 0.262, 0.193, 0.071, 0.086]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3269589552238806
[2m[36m(func pid=108399)[0m top5: 0.8600746268656716
[2m[36m(func pid=108399)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=108399)[0m f1_macro: 0.3041871158125683
[2m[36m(func pid=108399)[0m f1_weighted: 0.3474917635907575
[2m[36m(func pid=108399)[0m f1_per_class: [0.328, 0.522, 0.667, 0.338, 0.143, 0.244, 0.338, 0.309, 0.154, 0.0]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6012 | Steps: 4 | Val loss: 1.8068 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1498 | Steps: 4 | Val loss: 33.0119 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1288 | Steps: 4 | Val loss: 2.0890 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2046 | Steps: 4 | Val loss: 2.5990 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:21:02 (running for 00:13:19.84)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.159 |      0.2   |                   61 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.606 |      0.323 |                   60 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.225 |      0.304 |                   38 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  2.921 |      0.287 |                   35 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.36100746268656714
[2m[36m(func pid=102477)[0m top5: 0.8530783582089553
[2m[36m(func pid=102477)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=102477)[0m f1_macro: 0.3327669343224658
[2m[36m(func pid=102477)[0m f1_weighted: 0.3570761739949941
[2m[36m(func pid=102477)[0m f1_per_class: [0.553, 0.525, 0.444, 0.531, 0.091, 0.179, 0.175, 0.359, 0.239, 0.232]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3521455223880597
[2m[36m(func pid=109465)[0m top5: 0.7434701492537313
[2m[36m(func pid=109465)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=109465)[0m f1_macro: 0.2746070543093403
[2m[36m(func pid=109465)[0m f1_weighted: 0.34282490170311347
[2m[36m(func pid=109465)[0m f1_per_class: [0.307, 0.519, 0.49, 0.219, 0.128, 0.038, 0.534, 0.167, 0.237, 0.107]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.2355410447761194
[2m[36m(func pid=102433)[0m top5: 0.7714552238805971
[2m[36m(func pid=102433)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=102433)[0m f1_macro: 0.19870641720485915
[2m[36m(func pid=102433)[0m f1_weighted: 0.25378806568901663
[2m[36m(func pid=102433)[0m f1_per_class: [0.257, 0.312, 0.274, 0.321, 0.041, 0.134, 0.24, 0.206, 0.111, 0.092]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3903917910447761
[2m[36m(func pid=108399)[0m top5: 0.8628731343283582
[2m[36m(func pid=108399)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=108399)[0m f1_macro: 0.35695108460583436
[2m[36m(func pid=108399)[0m f1_weighted: 0.39848458205849896
[2m[36m(func pid=108399)[0m f1_per_class: [0.42, 0.465, 0.478, 0.295, 0.238, 0.275, 0.553, 0.288, 0.164, 0.393]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5526 | Steps: 4 | Val loss: 1.7954 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1272 | Steps: 4 | Val loss: 2.1073 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 6.3649 | Steps: 4 | Val loss: 37.8338 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3217 | Steps: 4 | Val loss: 2.8467 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:21:07 (running for 00:13:25.29)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.129 |      0.199 |                   62 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.601 |      0.333 |                   61 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.205 |      0.357 |                   39 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  2.15  |      0.275 |                   36 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.22014925373134328
[2m[36m(func pid=102433)[0m top5: 0.7495335820895522
[2m[36m(func pid=102433)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=102433)[0m f1_macro: 0.18226719270400882
[2m[36m(func pid=102433)[0m f1_weighted: 0.24197873845782786
[2m[36m(func pid=102433)[0m f1_per_class: [0.234, 0.289, 0.229, 0.312, 0.033, 0.12, 0.235, 0.184, 0.098, 0.087]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.33908582089552236
[2m[36m(func pid=108399)[0m top5: 0.8339552238805971
[2m[36m(func pid=108399)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=108399)[0m f1_macro: 0.3300917335641388
[2m[36m(func pid=108399)[0m f1_weighted: 0.37115047519260735
[2m[36m(func pid=108399)[0m f1_per_class: [0.574, 0.396, 0.44, 0.284, 0.222, 0.288, 0.503, 0.3, 0.195, 0.099]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3694029850746269
[2m[36m(func pid=102477)[0m top5: 0.8572761194029851
[2m[36m(func pid=102477)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=102477)[0m f1_macro: 0.3325419766840654
[2m[36m(func pid=102477)[0m f1_weighted: 0.3665322196478919
[2m[36m(func pid=102477)[0m f1_per_class: [0.497, 0.506, 0.436, 0.554, 0.094, 0.174, 0.201, 0.363, 0.234, 0.268]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.27425373134328357
[2m[36m(func pid=109465)[0m top5: 0.7271455223880597
[2m[36m(func pid=109465)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=109465)[0m f1_macro: 0.26027200923853855
[2m[36m(func pid=109465)[0m f1_weighted: 0.26295661501879347
[2m[36m(func pid=109465)[0m f1_per_class: [0.271, 0.522, 0.688, 0.173, 0.046, 0.037, 0.286, 0.317, 0.19, 0.074]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1620 | Steps: 4 | Val loss: 2.1194 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6525 | Steps: 4 | Val loss: 1.7596 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3316 | Steps: 4 | Val loss: 3.1900 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8597 | Steps: 4 | Val loss: 33.5802 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:21:13 (running for 00:13:30.68)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.127 |      0.182 |                   63 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.553 |      0.333 |                   62 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.322 |      0.33  |                   40 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  6.365 |      0.26  |                   37 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102433)[0m top1: 0.21082089552238806
[2m[36m(func pid=102433)[0m top5: 0.7318097014925373
[2m[36m(func pid=102433)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=102433)[0m f1_macro: 0.18045007090318385
[2m[36m(func pid=102433)[0m f1_weighted: 0.23314992476504315
[2m[36m(func pid=102433)[0m f1_per_class: [0.222, 0.24, 0.239, 0.309, 0.043, 0.103, 0.237, 0.216, 0.101, 0.094]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m top1: 0.26492537313432835
[2m[36m(func pid=108399)[0m top5: 0.8176305970149254
[2m[36m(func pid=108399)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=108399)[0m f1_macro: 0.2925626856575508
[2m[36m(func pid=108399)[0m f1_weighted: 0.290801339383867
[2m[36m(func pid=108399)[0m f1_per_class: [0.562, 0.44, 0.489, 0.356, 0.125, 0.178, 0.184, 0.322, 0.168, 0.103]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.37593283582089554
[2m[36m(func pid=102477)[0m top5: 0.8703358208955224
[2m[36m(func pid=102477)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=102477)[0m f1_macro: 0.3383009980704346
[2m[36m(func pid=102477)[0m f1_weighted: 0.3896189523372477
[2m[36m(func pid=102477)[0m f1_per_class: [0.487, 0.47, 0.49, 0.549, 0.103, 0.171, 0.31, 0.356, 0.196, 0.25]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3064365671641791
[2m[36m(func pid=109465)[0m top5: 0.7625932835820896
[2m[36m(func pid=109465)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=109465)[0m f1_macro: 0.31344876269126876
[2m[36m(func pid=109465)[0m f1_weighted: 0.28403135846218164
[2m[36m(func pid=109465)[0m f1_per_class: [0.628, 0.524, 0.71, 0.374, 0.06, 0.054, 0.119, 0.393, 0.199, 0.074]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0981 | Steps: 4 | Val loss: 2.1247 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2779 | Steps: 4 | Val loss: 2.4395 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 3.0438 | Steps: 4 | Val loss: 30.4563 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4656 | Steps: 4 | Val loss: 1.7492 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:21:18 (running for 00:13:35.97)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.162 |      0.18  |                   64 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.653 |      0.338 |                   63 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.332 |      0.293 |                   41 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.86  |      0.313 |                   38 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.4319029850746269
[2m[36m(func pid=108399)[0m top5: 0.8880597014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=108399)[0m f1_macro: 0.39683921062079064
[2m[36m(func pid=108399)[0m f1_weighted: 0.43068394777798735
[2m[36m(func pid=108399)[0m f1_per_class: [0.621, 0.583, 0.595, 0.368, 0.092, 0.133, 0.553, 0.28, 0.271, 0.473]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.34048507462686567
[2m[36m(func pid=109465)[0m top5: 0.7854477611940298
[2m[36m(func pid=109465)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=109465)[0m f1_macro: 0.35014092567462807
[2m[36m(func pid=109465)[0m f1_weighted: 0.3280037129239217
[2m[36m(func pid=109465)[0m f1_per_class: [0.6, 0.531, 0.733, 0.547, 0.085, 0.094, 0.082, 0.389, 0.173, 0.267]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.37173507462686567
[2m[36m(func pid=102477)[0m top5: 0.8889925373134329
[2m[36m(func pid=102477)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=102477)[0m f1_macro: 0.34258441547664586
[2m[36m(func pid=102477)[0m f1_weighted: 0.4025473848785621
[2m[36m(func pid=102477)[0m f1_per_class: [0.516, 0.342, 0.512, 0.553, 0.127, 0.156, 0.43, 0.353, 0.16, 0.278]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.2103544776119403
[2m[36m(func pid=102433)[0m top5: 0.7280783582089553
[2m[36m(func pid=102433)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=102433)[0m f1_macro: 0.18059289495579
[2m[36m(func pid=102433)[0m f1_weighted: 0.23570049333316237
[2m[36m(func pid=102433)[0m f1_per_class: [0.208, 0.237, 0.244, 0.311, 0.064, 0.1, 0.249, 0.199, 0.101, 0.09]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7319 | Steps: 4 | Val loss: 30.7209 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5743 | Steps: 4 | Val loss: 1.7943 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5342 | Steps: 4 | Val loss: 3.3678 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1311 | Steps: 4 | Val loss: 2.1131 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:21:24 (running for 00:13:41.46)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.098 |      0.181 |                   65 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.466 |      0.343 |                   64 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.278 |      0.397 |                   42 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.044 |      0.35  |                   39 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.37406716417910446
[2m[36m(func pid=102477)[0m top5: 0.8833955223880597
[2m[36m(func pid=102477)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=102477)[0m f1_macro: 0.33575686202209987
[2m[36m(func pid=102477)[0m f1_weighted: 0.4117318272226943
[2m[36m(func pid=102477)[0m f1_per_class: [0.478, 0.322, 0.468, 0.509, 0.123, 0.147, 0.521, 0.349, 0.152, 0.288]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.38899253731343286
[2m[36m(func pid=108399)[0m top5: 0.9263059701492538
[2m[36m(func pid=108399)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=108399)[0m f1_macro: 0.28295746947567596
[2m[36m(func pid=108399)[0m f1_weighted: 0.30970944931377514
[2m[36m(func pid=108399)[0m f1_per_class: [0.667, 0.328, 0.6, 0.243, 0.235, 0.023, 0.513, 0.142, 0.078, 0.0]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3278917910447761
[2m[36m(func pid=109465)[0m top5: 0.7961753731343284
[2m[36m(func pid=109465)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=109465)[0m f1_macro: 0.30539927345728224
[2m[36m(func pid=109465)[0m f1_weighted: 0.30325635057377137
[2m[36m(func pid=109465)[0m f1_per_class: [0.484, 0.278, 0.579, 0.586, 0.107, 0.117, 0.114, 0.384, 0.161, 0.244]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.2178171641791045
[2m[36m(func pid=102433)[0m top5: 0.7364738805970149
[2m[36m(func pid=102433)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=102433)[0m f1_macro: 0.18479691330875933
[2m[36m(func pid=102433)[0m f1_weighted: 0.2401889682259331
[2m[36m(func pid=102433)[0m f1_per_class: [0.244, 0.271, 0.196, 0.333, 0.08, 0.105, 0.219, 0.214, 0.095, 0.09]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4761 | Steps: 4 | Val loss: 1.6985 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6638 | Steps: 4 | Val loss: 2.8238 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0904 | Steps: 4 | Val loss: 33.8779 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2175 | Steps: 4 | Val loss: 2.1241 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:21:29 (running for 00:13:46.79)
Memory usage on this node: 26.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.131 |      0.185 |                   66 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.574 |      0.336 |                   65 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.534 |      0.283 |                   43 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.732 |      0.305 |                   40 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.39972014925373134
[2m[36m(func pid=102477)[0m top5: 0.8889925373134329
[2m[36m(func pid=102477)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=102477)[0m f1_macro: 0.3414228369675277
[2m[36m(func pid=102477)[0m f1_weighted: 0.4315967213105306
[2m[36m(func pid=102477)[0m f1_per_class: [0.519, 0.441, 0.353, 0.523, 0.103, 0.153, 0.501, 0.347, 0.181, 0.294]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3376865671641791
[2m[36m(func pid=108399)[0m top5: 0.90625
[2m[36m(func pid=108399)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=108399)[0m f1_macro: 0.3781280585964111
[2m[36m(func pid=108399)[0m f1_weighted: 0.347940324590181
[2m[36m(func pid=108399)[0m f1_per_class: [0.529, 0.552, 0.69, 0.472, 0.247, 0.254, 0.167, 0.213, 0.268, 0.391]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.30736940298507465
[2m[36m(func pid=109465)[0m top5: 0.769589552238806
[2m[36m(func pid=109465)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=109465)[0m f1_macro: 0.2598198210570108
[2m[36m(func pid=109465)[0m f1_weighted: 0.27813393619308147
[2m[36m(func pid=109465)[0m f1_per_class: [0.452, 0.102, 0.382, 0.582, 0.133, 0.131, 0.14, 0.359, 0.217, 0.101]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.2150186567164179
[2m[36m(func pid=102433)[0m top5: 0.7294776119402985
[2m[36m(func pid=102433)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=102433)[0m f1_macro: 0.18182089708964821
[2m[36m(func pid=102433)[0m f1_weighted: 0.23421874304136994
[2m[36m(func pid=102433)[0m f1_per_class: [0.24, 0.27, 0.176, 0.334, 0.084, 0.109, 0.195, 0.229, 0.099, 0.082]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3057 | Steps: 4 | Val loss: 3.8789 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4356 | Steps: 4 | Val loss: 1.6205 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2606 | Steps: 4 | Val loss: 29.9361 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1735 | Steps: 4 | Val loss: 2.1257 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:21:34 (running for 00:13:52.17)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.217 |      0.182 |                   67 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.476 |      0.341 |                   66 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.664 |      0.378 |                   44 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  1.09  |      0.26  |                   41 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.427705223880597
[2m[36m(func pid=102477)[0m top5: 0.9071828358208955
[2m[36m(func pid=102477)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=102477)[0m f1_macro: 0.3671805689081415
[2m[36m(func pid=102477)[0m f1_weighted: 0.44335274805777414
[2m[36m(func pid=102477)[0m f1_per_class: [0.503, 0.546, 0.388, 0.517, 0.136, 0.236, 0.444, 0.353, 0.242, 0.306]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.23600746268656717
[2m[36m(func pid=108399)[0m top5: 0.8013059701492538
[2m[36m(func pid=108399)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=108399)[0m f1_macro: 0.31584355278612036
[2m[36m(func pid=108399)[0m f1_weighted: 0.2535670753815556
[2m[36m(func pid=108399)[0m f1_per_class: [0.532, 0.366, 0.759, 0.328, 0.198, 0.219, 0.096, 0.315, 0.288, 0.058]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.324160447761194
[2m[36m(func pid=109465)[0m top5: 0.7980410447761194
[2m[36m(func pid=109465)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=109465)[0m f1_macro: 0.2839570154705563
[2m[36m(func pid=109465)[0m f1_weighted: 0.3250911593041336
[2m[36m(func pid=109465)[0m f1_per_class: [0.56, 0.268, 0.232, 0.584, 0.135, 0.182, 0.171, 0.373, 0.234, 0.101]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.21175373134328357
[2m[36m(func pid=102433)[0m top5: 0.726679104477612
[2m[36m(func pid=102433)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=102433)[0m f1_macro: 0.18167311326557772
[2m[36m(func pid=102433)[0m f1_weighted: 0.227571591612024
[2m[36m(func pid=102433)[0m f1_per_class: [0.248, 0.276, 0.171, 0.311, 0.081, 0.092, 0.193, 0.239, 0.106, 0.1]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3065 | Steps: 4 | Val loss: 26.4853 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3957 | Steps: 4 | Val loss: 1.6613 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4309 | Steps: 4 | Val loss: 4.3251 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0384 | Steps: 4 | Val loss: 2.1032 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:21:40 (running for 00:13:57.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.174 |      0.182 |                   68 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.436 |      0.367 |                   67 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.306 |      0.316 |                   45 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  1.261 |      0.284 |                   42 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.36427238805970147
[2m[36m(func pid=109465)[0m top5: 0.8092350746268657
[2m[36m(func pid=109465)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=109465)[0m f1_macro: 0.31658636338022206
[2m[36m(func pid=109465)[0m f1_weighted: 0.3779576047873063
[2m[36m(func pid=109465)[0m f1_per_class: [0.621, 0.529, 0.173, 0.552, 0.144, 0.183, 0.225, 0.357, 0.241, 0.142]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.4253731343283582
[2m[36m(func pid=102477)[0m top5: 0.8917910447761194
[2m[36m(func pid=102477)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=102477)[0m f1_macro: 0.3657101238292264
[2m[36m(func pid=102477)[0m f1_weighted: 0.42553410923146495
[2m[36m(func pid=102477)[0m f1_per_class: [0.507, 0.573, 0.351, 0.505, 0.144, 0.224, 0.378, 0.368, 0.269, 0.338]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.22994402985074627
[2m[36m(func pid=108399)[0m top5: 0.7248134328358209
[2m[36m(func pid=108399)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=108399)[0m f1_macro: 0.24561952910450296
[2m[36m(func pid=108399)[0m f1_weighted: 0.23601628672285407
[2m[36m(func pid=108399)[0m f1_per_class: [0.544, 0.436, 0.268, 0.211, 0.148, 0.141, 0.161, 0.297, 0.184, 0.066]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22201492537313433
[2m[36m(func pid=102433)[0m top5: 0.7430037313432836
[2m[36m(func pid=102433)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=102433)[0m f1_macro: 0.18942448867845144
[2m[36m(func pid=102433)[0m f1_weighted: 0.2351705600047009
[2m[36m(func pid=102433)[0m f1_per_class: [0.251, 0.304, 0.193, 0.314, 0.084, 0.095, 0.198, 0.235, 0.116, 0.106]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0009 | Steps: 4 | Val loss: 26.7639 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4306 | Steps: 4 | Val loss: 1.6785 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2796 | Steps: 4 | Val loss: 3.4637 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1068 | Steps: 4 | Val loss: 2.0843 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:21:45 (running for 00:14:02.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.038 |      0.189 |                   69 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.396 |      0.366 |                   68 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.431 |      0.246 |                   46 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.001 |      0.317 |                   44 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.3675373134328358
[2m[36m(func pid=109465)[0m top5: 0.7947761194029851
[2m[36m(func pid=109465)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=109465)[0m f1_macro: 0.3165204843001899
[2m[36m(func pid=109465)[0m f1_weighted: 0.3687205991325396
[2m[36m(func pid=109465)[0m f1_per_class: [0.614, 0.573, 0.145, 0.5, 0.116, 0.198, 0.209, 0.367, 0.237, 0.207]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.32369402985074625
[2m[36m(func pid=108399)[0m top5: 0.8166977611940298
[2m[36m(func pid=108399)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=108399)[0m f1_macro: 0.27796865843372587
[2m[36m(func pid=108399)[0m f1_weighted: 0.31328244933243254
[2m[36m(func pid=108399)[0m f1_per_class: [0.636, 0.479, 0.124, 0.263, 0.103, 0.167, 0.359, 0.132, 0.152, 0.366]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.41697761194029853
[2m[36m(func pid=102477)[0m top5: 0.8917910447761194
[2m[36m(func pid=102477)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=102477)[0m f1_macro: 0.37205152235712907
[2m[36m(func pid=102477)[0m f1_weighted: 0.4084392559173238
[2m[36m(func pid=102477)[0m f1_per_class: [0.625, 0.556, 0.356, 0.481, 0.15, 0.227, 0.347, 0.365, 0.226, 0.387]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22901119402985073
[2m[36m(func pid=102433)[0m top5: 0.7565298507462687
[2m[36m(func pid=102433)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=102433)[0m f1_macro: 0.191062649611157
[2m[36m(func pid=102433)[0m f1_weighted: 0.24348202458061136
[2m[36m(func pid=102433)[0m f1_per_class: [0.239, 0.322, 0.239, 0.297, 0.04, 0.1, 0.232, 0.225, 0.112, 0.105]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0199 | Steps: 4 | Val loss: 26.7681 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.6553 | Steps: 4 | Val loss: 1.6727 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.8666 | Steps: 4 | Val loss: 2.6288 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0973 | Steps: 4 | Val loss: 2.0830 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=109465)[0m top1: 0.3666044776119403
[2m[36m(func pid=109465)[0m top5: 0.7947761194029851
[2m[36m(func pid=109465)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=109465)[0m f1_macro: 0.31327883713858795
[2m[36m(func pid=109465)[0m f1_weighted: 0.36650566596337425
[2m[36m(func pid=109465)[0m f1_per_class: [0.569, 0.582, 0.148, 0.481, 0.116, 0.203, 0.218, 0.358, 0.216, 0.242]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:21:50 (running for 00:14:08.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.107 |      0.191 |                   70 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.431 |      0.372 |                   69 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.28  |      0.278 |                   47 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.02  |      0.313 |                   45 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4137126865671642
[2m[36m(func pid=102477)[0m top5: 0.8941231343283582
[2m[36m(func pid=102477)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=102477)[0m f1_macro: 0.37489176982788663
[2m[36m(func pid=102477)[0m f1_weighted: 0.41025004386945163
[2m[36m(func pid=102477)[0m f1_per_class: [0.609, 0.571, 0.351, 0.486, 0.151, 0.252, 0.327, 0.382, 0.232, 0.388]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3978544776119403
[2m[36m(func pid=108399)[0m top5: 0.8880597014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=108399)[0m f1_macro: 0.332923479552233
[2m[36m(func pid=108399)[0m f1_weighted: 0.4172077579158203
[2m[36m(func pid=108399)[0m f1_per_class: [0.602, 0.496, 0.095, 0.55, 0.192, 0.3, 0.375, 0.128, 0.169, 0.421]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.23367537313432835
[2m[36m(func pid=102433)[0m top5: 0.7681902985074627
[2m[36m(func pid=102433)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=102433)[0m f1_macro: 0.20297652241380587
[2m[36m(func pid=102433)[0m f1_weighted: 0.24866584717396
[2m[36m(func pid=102433)[0m f1_per_class: [0.247, 0.336, 0.275, 0.295, 0.073, 0.109, 0.239, 0.207, 0.125, 0.124]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7755 | Steps: 4 | Val loss: 23.3378 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3938 | Steps: 4 | Val loss: 1.6925 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5956 | Steps: 4 | Val loss: 4.4070 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1608 | Steps: 4 | Val loss: 2.0712 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:21:55 (running for 00:14:13.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.097 |      0.203 |                   71 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.655 |      0.375 |                   70 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.867 |      0.333 |                   48 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  1.776 |      0.322 |                   46 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.3931902985074627
[2m[36m(func pid=109465)[0m top5: 0.8395522388059702
[2m[36m(func pid=109465)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=109465)[0m f1_macro: 0.32242674209797323
[2m[36m(func pid=109465)[0m f1_weighted: 0.41187180017155434
[2m[36m(func pid=109465)[0m f1_per_class: [0.489, 0.569, 0.138, 0.461, 0.141, 0.188, 0.416, 0.312, 0.187, 0.323]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.4001865671641791
[2m[36m(func pid=102477)[0m top5: 0.8978544776119403
[2m[36m(func pid=102477)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=102477)[0m f1_macro: 0.3562917329839643
[2m[36m(func pid=102477)[0m f1_weighted: 0.40062735508647346
[2m[36m(func pid=102477)[0m f1_per_class: [0.561, 0.563, 0.347, 0.554, 0.159, 0.282, 0.242, 0.356, 0.184, 0.316]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.28218283582089554
[2m[36m(func pid=108399)[0m top5: 0.8157649253731343
[2m[36m(func pid=108399)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=108399)[0m f1_macro: 0.24648075745951464
[2m[36m(func pid=108399)[0m f1_weighted: 0.25156780001882567
[2m[36m(func pid=108399)[0m f1_per_class: [0.602, 0.072, 0.157, 0.542, 0.254, 0.266, 0.093, 0.13, 0.142, 0.207]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.23087686567164178
[2m[36m(func pid=102433)[0m top5: 0.7765858208955224
[2m[36m(func pid=102433)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=102433)[0m f1_macro: 0.20913187546165463
[2m[36m(func pid=102433)[0m f1_weighted: 0.24948202305381167
[2m[36m(func pid=102433)[0m f1_per_class: [0.277, 0.341, 0.289, 0.277, 0.105, 0.12, 0.253, 0.19, 0.101, 0.136]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 7.6554 | Steps: 4 | Val loss: 25.4562 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3515 | Steps: 4 | Val loss: 1.7170 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8102 | Steps: 4 | Val loss: 2.6316 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.0685 | Steps: 4 | Val loss: 2.0661 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:22:01 (running for 00:14:18.84)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.161 |      0.209 |                   72 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.394 |      0.356 |                   71 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.596 |      0.246 |                   49 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  7.655 |      0.287 |                   47 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.40951492537313433
[2m[36m(func pid=109465)[0m top5: 0.8390858208955224
[2m[36m(func pid=109465)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=109465)[0m f1_macro: 0.2866920109890406
[2m[36m(func pid=109465)[0m f1_weighted: 0.3881673339677197
[2m[36m(func pid=109465)[0m f1_per_class: [0.5, 0.234, 0.149, 0.498, 0.242, 0.123, 0.553, 0.157, 0.155, 0.256]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.39132462686567165
[2m[36m(func pid=102477)[0m top5: 0.8973880597014925
[2m[36m(func pid=102477)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=102477)[0m f1_macro: 0.35234163502712285
[2m[36m(func pid=102477)[0m f1_weighted: 0.38789167894781795
[2m[36m(func pid=102477)[0m f1_per_class: [0.587, 0.486, 0.382, 0.591, 0.176, 0.276, 0.209, 0.38, 0.138, 0.298]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3670708955223881
[2m[36m(func pid=108399)[0m top5: 0.8726679104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=108399)[0m f1_macro: 0.32158660947826817
[2m[36m(func pid=108399)[0m f1_weighted: 0.4048072180082873
[2m[36m(func pid=108399)[0m f1_per_class: [0.586, 0.378, 0.296, 0.536, 0.089, 0.18, 0.445, 0.207, 0.217, 0.281]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102433)[0m top1: 0.23274253731343283
[2m[36m(func pid=102433)[0m top5: 0.7877798507462687
[2m[36m(func pid=102433)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=102433)[0m f1_macro: 0.21293397628785798
[2m[36m(func pid=102433)[0m f1_weighted: 0.2541451409835699
[2m[36m(func pid=102433)[0m f1_per_class: [0.309, 0.344, 0.293, 0.275, 0.092, 0.102, 0.272, 0.202, 0.109, 0.132]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 10.0261 | Steps: 4 | Val loss: 27.7933 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5439 | Steps: 4 | Val loss: 1.7181 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0969 | Steps: 4 | Val loss: 2.0614 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0546 | Steps: 4 | Val loss: 4.2703 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 11:22:06 (running for 00:14:24.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.069 |      0.213 |                   73 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.351 |      0.352 |                   72 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.81  |      0.322 |                   50 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 10.026 |      0.293 |                   48 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=109465)[0m top1: 0.33908582089552236
[2m[36m(func pid=109465)[0m top5: 0.8227611940298507
[2m[36m(func pid=109465)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=109465)[0m f1_macro: 0.29276579808941483
[2m[36m(func pid=109465)[0m f1_weighted: 0.33815414095436913
[2m[36m(func pid=109465)[0m f1_per_class: [0.52, 0.251, 0.158, 0.58, 0.146, 0.188, 0.24, 0.309, 0.155, 0.381]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22761194029850745
[2m[36m(func pid=102433)[0m top5: 0.789179104477612
[2m[36m(func pid=102433)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=102433)[0m f1_macro: 0.21316499840634
[2m[36m(func pid=102433)[0m f1_weighted: 0.24580775940759295
[2m[36m(func pid=102433)[0m f1_per_class: [0.281, 0.326, 0.31, 0.285, 0.123, 0.12, 0.238, 0.203, 0.108, 0.137]
[2m[36m(func pid=102433)[0m 
[2m[36m(func pid=102477)[0m top1: 0.384794776119403
[2m[36m(func pid=102477)[0m top5: 0.898320895522388
[2m[36m(func pid=102477)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=102477)[0m f1_macro: 0.35328304741135097
[2m[36m(func pid=102477)[0m f1_weighted: 0.38392396830721365
[2m[36m(func pid=102477)[0m f1_per_class: [0.588, 0.439, 0.429, 0.598, 0.163, 0.273, 0.216, 0.369, 0.168, 0.291]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3260261194029851
[2m[36m(func pid=108399)[0m top5: 0.7504664179104478
[2m[36m(func pid=108399)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=108399)[0m f1_macro: 0.27260723121415614
[2m[36m(func pid=108399)[0m f1_weighted: 0.34289085784510254
[2m[36m(func pid=108399)[0m f1_per_class: [0.622, 0.339, 0.373, 0.259, 0.1, 0.0, 0.603, 0.17, 0.184, 0.076]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4065 | Steps: 4 | Val loss: 22.8141 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4596 | Steps: 4 | Val loss: 1.6445 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=102433)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0549 | Steps: 4 | Val loss: 2.0602 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.0293 | Steps: 4 | Val loss: 4.5899 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=109465)[0m top1: 0.4048507462686567
[2m[36m(func pid=109465)[0m top5: 0.847481343283582
[2m[36m(func pid=109465)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=109465)[0m f1_macro: 0.3302709583199507
[2m[36m(func pid=109465)[0m f1_weighted: 0.4388668140932045
[2m[36m(func pid=109465)[0m f1_per_class: [0.504, 0.511, 0.14, 0.53, 0.127, 0.169, 0.492, 0.258, 0.167, 0.405]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:22:11 (running for 00:14:29.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.32325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00004 | RUNNING    | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.097 |      0.213 |                   74 |
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.544 |      0.353 |                   73 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.055 |      0.273 |                   51 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.407 |      0.33  |                   49 |
| train_98a10_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.40345149253731344
[2m[36m(func pid=102477)[0m top5: 0.9104477611940298
[2m[36m(func pid=102477)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=102477)[0m f1_macro: 0.3611846138460374
[2m[36m(func pid=102477)[0m f1_weighted: 0.419911767580637
[2m[36m(func pid=102477)[0m f1_per_class: [0.559, 0.467, 0.414, 0.585, 0.14, 0.257, 0.341, 0.359, 0.188, 0.303]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=102433)[0m top1: 0.22667910447761194
[2m[36m(func pid=102433)[0m top5: 0.7896455223880597
[2m[36m(func pid=102433)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=102433)[0m f1_macro: 0.2182142360252252
[2m[36m(func pid=102433)[0m f1_weighted: 0.2404183142391463
[2m[36m(func pid=102433)[0m f1_per_class: [0.281, 0.346, 0.386, 0.305, 0.112, 0.119, 0.189, 0.206, 0.105, 0.133]
[2m[36m(func pid=108399)[0m top1: 0.3199626865671642
[2m[36m(func pid=108399)[0m top5: 0.7332089552238806
[2m[36m(func pid=108399)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=108399)[0m f1_macro: 0.2798064296617063
[2m[36m(func pid=108399)[0m f1_weighted: 0.3320664769819175
[2m[36m(func pid=108399)[0m f1_per_class: [0.644, 0.423, 0.31, 0.214, 0.109, 0.0, 0.543, 0.214, 0.251, 0.089]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.9217 | Steps: 4 | Val loss: 28.2111 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5249 | Steps: 4 | Val loss: 1.6509 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2302 | Steps: 4 | Val loss: 4.5520 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=109465)[0m top1: 0.35027985074626866
[2m[36m(func pid=109465)[0m top5: 0.7803171641791045
[2m[36m(func pid=109465)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=109465)[0m f1_macro: 0.3034491312770461
[2m[36m(func pid=109465)[0m f1_weighted: 0.37311623462549015
[2m[36m(func pid=109465)[0m f1_per_class: [0.485, 0.558, 0.144, 0.472, 0.097, 0.13, 0.3, 0.349, 0.17, 0.33]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=102477)[0m top1: 0.40904850746268656
[2m[36m(func pid=102477)[0m top5: 0.9057835820895522
[2m[36m(func pid=102477)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=102477)[0m f1_macro: 0.3647826316256816
[2m[36m(func pid=102477)[0m f1_weighted: 0.4244600647422118
[2m[36m(func pid=102477)[0m f1_per_class: [0.56, 0.522, 0.444, 0.538, 0.121, 0.197, 0.389, 0.349, 0.208, 0.318]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.2719216417910448
[2m[36m(func pid=108399)[0m top5: 0.7252798507462687
[2m[36m(func pid=108399)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=108399)[0m f1_macro: 0.2756809659426909
[2m[36m(func pid=108399)[0m f1_weighted: 0.25646749681052333
[2m[36m(func pid=108399)[0m f1_per_class: [0.652, 0.478, 0.386, 0.31, 0.096, 0.0, 0.158, 0.239, 0.248, 0.19]
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 4.0760 | Steps: 4 | Val loss: 39.6209 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3582 | Steps: 4 | Val loss: 1.5820 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109465)[0m top1: 0.22807835820895522
[2m[36m(func pid=109465)[0m top5: 0.7178171641791045
[2m[36m(func pid=109465)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=109465)[0m f1_macro: 0.2518924333846063
[2m[36m(func pid=109465)[0m f1_weighted: 0.27747232488910734
[2m[36m(func pid=109465)[0m f1_per_class: [0.518, 0.348, 0.153, 0.455, 0.033, 0.08, 0.132, 0.368, 0.188, 0.244]
== Status ==
Current time: 2024-01-07 11:22:17 (running for 00:14:35.11)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.525 |      0.365 |                   75 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.029 |      0.28  |                   52 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  1.922 |      0.303 |                   50 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=121255)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=121255)[0m Configuration completed!
[2m[36m(func pid=121255)[0m New optimizer parameters:
[2m[36m(func pid=121255)[0m SGD (
[2m[36m(func pid=121255)[0m Parameter Group 0
[2m[36m(func pid=121255)[0m     dampening: 0
[2m[36m(func pid=121255)[0m     differentiable: False
[2m[36m(func pid=121255)[0m     foreach: None
[2m[36m(func pid=121255)[0m     lr: 0.0001
[2m[36m(func pid=121255)[0m     maximize: False
[2m[36m(func pid=121255)[0m     momentum: 0.99
[2m[36m(func pid=121255)[0m     nesterov: False
[2m[36m(func pid=121255)[0m     weight_decay: 0.0001
[2m[36m(func pid=121255)[0m )
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:22:23 (running for 00:14:40.55)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.358 |      0.388 |                   76 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  2.23  |      0.276 |                   53 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.076 |      0.252 |                   51 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4319029850746269
[2m[36m(func pid=102477)[0m top5: 0.9197761194029851
[2m[36m(func pid=102477)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=102477)[0m f1_macro: 0.3875287888211022
[2m[36m(func pid=102477)[0m f1_weighted: 0.4479182721426934
[2m[36m(func pid=102477)[0m f1_per_class: [0.591, 0.533, 0.462, 0.529, 0.127, 0.216, 0.454, 0.358, 0.232, 0.373]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2345 | Steps: 4 | Val loss: 3.2074 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 16.4435 | Steps: 4 | Val loss: 35.9362 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4645 | Steps: 4 | Val loss: 1.5949 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1957 | Steps: 4 | Val loss: 2.5296 | Batch size: 32 | lr: 0.0001 | Duration: 4.40s
[2m[36m(func pid=108399)[0m top1: 0.3628731343283582
[2m[36m(func pid=108399)[0m top5: 0.8381529850746269
[2m[36m(func pid=108399)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=108399)[0m f1_macro: 0.3376263609705136
[2m[36m(func pid=108399)[0m f1_weighted: 0.3429146573009495
[2m[36m(func pid=108399)[0m f1_per_class: [0.583, 0.354, 0.458, 0.582, 0.137, 0.229, 0.151, 0.395, 0.196, 0.291]
[2m[36m(func pid=109465)[0m top1: 0.26865671641791045
[2m[36m(func pid=109465)[0m top5: 0.7649253731343284
[2m[36m(func pid=109465)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=109465)[0m f1_macro: 0.27235243958320116
[2m[36m(func pid=109465)[0m f1_weighted: 0.30322533173228566
[2m[36m(func pid=109465)[0m f1_per_class: [0.569, 0.444, 0.141, 0.411, 0.063, 0.192, 0.178, 0.26, 0.167, 0.299]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:22:28 (running for 00:14:45.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.464 |      0.394 |                   77 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.235 |      0.338 |                   54 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 16.443 |      0.272 |                   52 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4351679104477612
[2m[36m(func pid=102477)[0m top5: 0.909981343283582
[2m[36m(func pid=102477)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=102477)[0m f1_macro: 0.3942021766035188
[2m[36m(func pid=102477)[0m f1_weighted: 0.4456187406260342
[2m[36m(func pid=102477)[0m f1_per_class: [0.596, 0.558, 0.471, 0.504, 0.154, 0.25, 0.442, 0.337, 0.266, 0.364]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.0625
[2m[36m(func pid=121255)[0m top5: 0.4771455223880597
[2m[36m(func pid=121255)[0m f1_micro: 0.0625
[2m[36m(func pid=121255)[0m f1_macro: 0.03902221690103432
[2m[36m(func pid=121255)[0m f1_weighted: 0.03704059414048884
[2m[36m(func pid=121255)[0m f1_per_class: [0.094, 0.015, 0.0, 0.082, 0.0, 0.024, 0.0, 0.097, 0.023, 0.056]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 4.0319 | Steps: 4 | Val loss: 36.9082 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4339 | Steps: 4 | Val loss: 4.7672 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3845 | Steps: 4 | Val loss: 1.6178 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1393 | Steps: 4 | Val loss: 2.5485 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=108399)[0m top1: 0.3199626865671642
[2m[36m(func pid=108399)[0m top5: 0.8031716417910447
[2m[36m(func pid=108399)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=108399)[0m f1_macro: 0.30895151708408186
[2m[36m(func pid=108399)[0m f1_weighted: 0.2696511860171118
[2m[36m(func pid=108399)[0m f1_per_class: [0.635, 0.188, 0.537, 0.567, 0.218, 0.283, 0.036, 0.145, 0.192, 0.289]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.4207089552238806
[2m[36m(func pid=109465)[0m top5: 0.7691231343283582
[2m[36m(func pid=109465)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=109465)[0m f1_macro: 0.2911102176813018
[2m[36m(func pid=109465)[0m f1_weighted: 0.36438453034791446
[2m[36m(func pid=109465)[0m f1_per_class: [0.5, 0.507, 0.215, 0.232, 0.13, 0.024, 0.59, 0.182, 0.209, 0.321]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:22:33 (running for 00:14:50.94)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.383 |                   78 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.434 |      0.309 |                   55 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.032 |      0.291 |                   53 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  3.196 |      0.039 |                    1 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4207089552238806
[2m[36m(func pid=102477)[0m top5: 0.9109141791044776
[2m[36m(func pid=102477)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=102477)[0m f1_macro: 0.3827499736291476
[2m[36m(func pid=102477)[0m f1_weighted: 0.4247611199307562
[2m[36m(func pid=102477)[0m f1_per_class: [0.588, 0.563, 0.4, 0.513, 0.196, 0.262, 0.361, 0.329, 0.235, 0.38]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.05830223880597015
[2m[36m(func pid=121255)[0m top5: 0.4603544776119403
[2m[36m(func pid=121255)[0m f1_micro: 0.05830223880597015
[2m[36m(func pid=121255)[0m f1_macro: 0.03461793619778562
[2m[36m(func pid=121255)[0m f1_weighted: 0.039415682094794874
[2m[36m(func pid=121255)[0m f1_per_class: [0.043, 0.04, 0.0, 0.083, 0.0, 0.018, 0.0, 0.091, 0.024, 0.048]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.8171 | Steps: 4 | Val loss: 52.8320 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4892 | Steps: 4 | Val loss: 3.6488 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3215 | Steps: 4 | Val loss: 1.6281 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9478 | Steps: 4 | Val loss: 2.5449 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=108399)[0m top1: 0.3260261194029851
[2m[36m(func pid=108399)[0m top5: 0.7943097014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=108399)[0m f1_macro: 0.295958486383021
[2m[36m(func pid=108399)[0m f1_weighted: 0.31537635808840536
[2m[36m(func pid=108399)[0m f1_per_class: [0.562, 0.265, 0.179, 0.547, 0.236, 0.288, 0.143, 0.338, 0.14, 0.261]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m top1: 0.24207089552238806
[2m[36m(func pid=109465)[0m top5: 0.6688432835820896
[2m[36m(func pid=109465)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=109465)[0m f1_macro: 0.22712653607632075
[2m[36m(func pid=109465)[0m f1_weighted: 0.2560735605993501
[2m[36m(func pid=109465)[0m f1_per_class: [0.071, 0.426, 0.361, 0.064, 0.19, 0.0, 0.47, 0.179, 0.164, 0.347]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:22:38 (running for 00:14:56.14)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.322 |      0.378 |                   79 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.489 |      0.296 |                   56 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.817 |      0.227 |                   54 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  3.139 |      0.035 |                    2 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.41884328358208955
[2m[36m(func pid=102477)[0m top5: 0.9109141791044776
[2m[36m(func pid=102477)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=102477)[0m f1_macro: 0.3777754093107967
[2m[36m(func pid=102477)[0m f1_weighted: 0.41995780000454136
[2m[36m(func pid=102477)[0m f1_per_class: [0.544, 0.562, 0.4, 0.556, 0.19, 0.27, 0.305, 0.335, 0.232, 0.385]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.06809701492537314
[2m[36m(func pid=121255)[0m top5: 0.45009328358208955
[2m[36m(func pid=121255)[0m f1_micro: 0.06809701492537314
[2m[36m(func pid=121255)[0m f1_macro: 0.04447722305379085
[2m[36m(func pid=121255)[0m f1_weighted: 0.06426681602545131
[2m[36m(func pid=121255)[0m f1_per_class: [0.036, 0.079, 0.0, 0.144, 0.0, 0.027, 0.0, 0.086, 0.044, 0.029]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2477 | Steps: 4 | Val loss: 3.1359 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 15.9306 | Steps: 4 | Val loss: 44.3703 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4044 | Steps: 4 | Val loss: 1.6834 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0613 | Steps: 4 | Val loss: 2.5179 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=109465)[0m top1: 0.2196828358208955
[2m[36m(func pid=109465)[0m top5: 0.6394589552238806
[2m[36m(func pid=109465)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=109465)[0m f1_macro: 0.2025686125975307
[2m[36m(func pid=109465)[0m f1_weighted: 0.22878030139845845
[2m[36m(func pid=109465)[0m f1_per_class: [0.241, 0.233, 0.228, 0.225, 0.246, 0.016, 0.321, 0.246, 0.144, 0.128]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3316231343283582
[2m[36m(func pid=108399)[0m top5: 0.8283582089552238
[2m[36m(func pid=108399)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=108399)[0m f1_macro: 0.288634381397407
[2m[36m(func pid=108399)[0m f1_weighted: 0.3743139000468159
[2m[36m(func pid=108399)[0m f1_per_class: [0.579, 0.312, 0.098, 0.507, 0.131, 0.103, 0.426, 0.346, 0.127, 0.259]
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:22:43 (running for 00:15:01.28)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.404 |      0.359 |                   80 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.248 |      0.289 |                   57 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 15.931 |      0.203 |                   55 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.948 |      0.044 |                    3 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.40298507462686567
[2m[36m(func pid=102477)[0m top5: 0.8997201492537313
[2m[36m(func pid=102477)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=102477)[0m f1_macro: 0.3593667990134828
[2m[36m(func pid=102477)[0m f1_weighted: 0.40226131706230533
[2m[36m(func pid=102477)[0m f1_per_class: [0.521, 0.531, 0.419, 0.569, 0.168, 0.26, 0.257, 0.34, 0.253, 0.275]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.07369402985074627
[2m[36m(func pid=121255)[0m top5: 0.44029850746268656
[2m[36m(func pid=121255)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=121255)[0m f1_macro: 0.049319447713218716
[2m[36m(func pid=121255)[0m f1_weighted: 0.0801063632507434
[2m[36m(func pid=121255)[0m f1_per_class: [0.026, 0.101, 0.0, 0.169, 0.0, 0.064, 0.006, 0.077, 0.04, 0.009]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1246 | Steps: 4 | Val loss: 35.1994 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0505 | Steps: 4 | Val loss: 2.7827 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3731 | Steps: 4 | Val loss: 1.6955 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=109465)[0m top1: 0.283115671641791
[2m[36m(func pid=109465)[0m top5: 0.7276119402985075
[2m[36m(func pid=109465)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=109465)[0m f1_macro: 0.2554533253082512
[2m[36m(func pid=109465)[0m f1_weighted: 0.2952801670683477
[2m[36m(func pid=109465)[0m f1_per_class: [0.386, 0.36, 0.302, 0.542, 0.089, 0.103, 0.121, 0.282, 0.16, 0.209]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9766 | Steps: 4 | Val loss: 2.4888 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=108399)[0m top1: 0.39225746268656714
[2m[36m(func pid=108399)[0m top5: 0.8586753731343284
[2m[36m(func pid=108399)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=108399)[0m f1_macro: 0.33387031502982734
[2m[36m(func pid=108399)[0m f1_weighted: 0.41677566743806593
[2m[36m(func pid=108399)[0m f1_per_class: [0.564, 0.417, 0.407, 0.55, 0.104, 0.087, 0.468, 0.32, 0.194, 0.227]
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:22:49 (running for 00:15:06.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.373 |      0.344 |                   81 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.051 |      0.334 |                   58 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  2.125 |      0.255 |                   56 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  3.061 |      0.049 |                    4 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4025186567164179
[2m[36m(func pid=102477)[0m top5: 0.8941231343283582
[2m[36m(func pid=102477)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=102477)[0m f1_macro: 0.3439913827192601
[2m[36m(func pid=102477)[0m f1_weighted: 0.4172948885323874
[2m[36m(func pid=102477)[0m f1_per_class: [0.487, 0.508, 0.333, 0.563, 0.131, 0.243, 0.341, 0.351, 0.216, 0.268]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.08115671641791045
[2m[36m(func pid=121255)[0m top5: 0.44029850746268656
[2m[36m(func pid=121255)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=121255)[0m f1_macro: 0.05335548317043509
[2m[36m(func pid=121255)[0m f1_weighted: 0.0877025822169092
[2m[36m(func pid=121255)[0m f1_per_class: [0.011, 0.111, 0.0, 0.184, 0.0, 0.092, 0.003, 0.058, 0.069, 0.006]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5075 | Steps: 4 | Val loss: 39.3446 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0507 | Steps: 4 | Val loss: 2.6812 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.4457 | Steps: 4 | Val loss: 1.7363 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=109465)[0m top1: 0.32509328358208955
[2m[36m(func pid=109465)[0m top5: 0.7546641791044776
[2m[36m(func pid=109465)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=109465)[0m f1_macro: 0.2717501111126751
[2m[36m(func pid=109465)[0m f1_weighted: 0.29438500368227793
[2m[36m(func pid=109465)[0m f1_per_class: [0.085, 0.333, 0.522, 0.598, 0.063, 0.168, 0.048, 0.35, 0.225, 0.327]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.40625
[2m[36m(func pid=108399)[0m top5: 0.8805970149253731
[2m[36m(func pid=108399)[0m f1_micro: 0.40625
[2m[36m(func pid=108399)[0m f1_macro: 0.36823491362015165
[2m[36m(func pid=108399)[0m f1_weighted: 0.4300917171831975
[2m[36m(func pid=108399)[0m f1_per_class: [0.642, 0.455, 0.579, 0.549, 0.095, 0.096, 0.474, 0.316, 0.249, 0.228]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0907 | Steps: 4 | Val loss: 2.4688 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:22:54 (running for 00:15:11.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.446 |      0.333 |                   82 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.051 |      0.368 |                   59 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.508 |      0.272 |                   57 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.977 |      0.053 |                    5 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3908582089552239
[2m[36m(func pid=102477)[0m top5: 0.8889925373134329
[2m[36m(func pid=102477)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=102477)[0m f1_macro: 0.33272898668215245
[2m[36m(func pid=102477)[0m f1_weighted: 0.41639681333514944
[2m[36m(func pid=102477)[0m f1_per_class: [0.51, 0.485, 0.255, 0.551, 0.115, 0.203, 0.381, 0.347, 0.183, 0.296]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m top1: 0.08255597014925373
[2m[36m(func pid=121255)[0m top5: 0.4361007462686567
[2m[36m(func pid=121255)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=121255)[0m f1_macro: 0.04980697815103219
[2m[36m(func pid=121255)[0m f1_weighted: 0.08440197487751834
[2m[36m(func pid=121255)[0m f1_per_class: [0.02, 0.097, 0.016, 0.183, 0.0, 0.116, 0.003, 0.0, 0.055, 0.008]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.4103 | Steps: 4 | Val loss: 41.0800 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0708 | Steps: 4 | Val loss: 2.5038 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.5687 | Steps: 4 | Val loss: 1.8161 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=109465)[0m top1: 0.3656716417910448
[2m[36m(func pid=109465)[0m top5: 0.7789179104477612
[2m[36m(func pid=109465)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=109465)[0m f1_macro: 0.29724812036571474
[2m[36m(func pid=109465)[0m f1_weighted: 0.30146451699523447
[2m[36m(func pid=109465)[0m f1_per_class: [0.2, 0.374, 0.595, 0.581, 0.15, 0.248, 0.033, 0.338, 0.146, 0.308]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.4221082089552239
[2m[36m(func pid=108399)[0m top5: 0.9011194029850746
[2m[36m(func pid=108399)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=108399)[0m f1_macro: 0.39740244104590616
[2m[36m(func pid=108399)[0m f1_weighted: 0.4420418932936676
[2m[36m(func pid=108399)[0m f1_per_class: [0.66, 0.468, 0.786, 0.535, 0.108, 0.112, 0.507, 0.307, 0.266, 0.225]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7982 | Steps: 4 | Val loss: 2.4287 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:22:59 (running for 00:15:17.08)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.569 |      0.319 |                   83 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.071 |      0.397 |                   60 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.41  |      0.297 |                   58 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  3.091 |      0.05  |                    6 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.3675373134328358
[2m[36m(func pid=102477)[0m top5: 0.8610074626865671
[2m[36m(func pid=102477)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=102477)[0m f1_macro: 0.3194518956159055
[2m[36m(func pid=102477)[0m f1_weighted: 0.3956148692815633
[2m[36m(func pid=102477)[0m f1_per_class: [0.503, 0.476, 0.239, 0.493, 0.114, 0.164, 0.387, 0.345, 0.175, 0.298]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 15.7656 | Steps: 4 | Val loss: 28.8577 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=121255)[0m top1: 0.0914179104477612
[2m[36m(func pid=121255)[0m top5: 0.457089552238806
[2m[36m(func pid=121255)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=121255)[0m f1_macro: 0.054911291706858954
[2m[36m(func pid=121255)[0m f1_weighted: 0.09415350688893369
[2m[36m(func pid=121255)[0m f1_per_class: [0.022, 0.117, 0.019, 0.18, 0.0, 0.133, 0.021, 0.0, 0.057, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1365 | Steps: 4 | Val loss: 2.5302 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4973 | Steps: 4 | Val loss: 1.7745 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=109465)[0m top1: 0.41511194029850745
[2m[36m(func pid=109465)[0m top5: 0.8819962686567164
[2m[36m(func pid=109465)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=109465)[0m f1_macro: 0.3986115775730766
[2m[36m(func pid=109465)[0m f1_weighted: 0.37808524056143206
[2m[36m(func pid=109465)[0m f1_per_class: [0.629, 0.516, 0.65, 0.582, 0.265, 0.179, 0.194, 0.305, 0.21, 0.457]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=108399)[0m top1: 0.4099813432835821
[2m[36m(func pid=108399)[0m top5: 0.9090485074626866
[2m[36m(func pid=108399)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=108399)[0m f1_macro: 0.40727715553126276
[2m[36m(func pid=108399)[0m f1_weighted: 0.4334072923789088
[2m[36m(func pid=108399)[0m f1_per_class: [0.641, 0.495, 0.8, 0.533, 0.117, 0.22, 0.42, 0.317, 0.27, 0.261]
[2m[36m(func pid=108399)[0m 
== Status ==
Current time: 2024-01-07 11:23:04 (running for 00:15:22.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.497 |      0.329 |                   84 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.136 |      0.407 |                   61 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 15.766 |      0.399 |                   59 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.798 |      0.055 |                    7 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.38526119402985076
[2m[36m(func pid=102477)[0m top5: 0.8736007462686567
[2m[36m(func pid=102477)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=102477)[0m f1_macro: 0.3291357429092504
[2m[36m(func pid=102477)[0m f1_weighted: 0.4194835856924392
[2m[36m(func pid=102477)[0m f1_per_class: [0.544, 0.453, 0.325, 0.469, 0.114, 0.15, 0.503, 0.358, 0.208, 0.167]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8074 | Steps: 4 | Val loss: 2.3960 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 4.4380 | Steps: 4 | Val loss: 43.8074 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=121255)[0m top1: 0.08442164179104478
[2m[36m(func pid=121255)[0m top5: 0.48600746268656714
[2m[36m(func pid=121255)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=121255)[0m f1_macro: 0.05507307651741468
[2m[36m(func pid=121255)[0m f1_weighted: 0.08843118238579385
[2m[36m(func pid=121255)[0m f1_per_class: [0.023, 0.13, 0.027, 0.131, 0.0, 0.127, 0.04, 0.0, 0.072, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4262 | Steps: 4 | Val loss: 2.9067 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3173 | Steps: 4 | Val loss: 1.7690 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=109465)[0m top1: 0.25279850746268656
[2m[36m(func pid=109465)[0m top5: 0.8703358208955224
[2m[36m(func pid=109465)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=109465)[0m f1_macro: 0.2817934710525654
[2m[36m(func pid=109465)[0m f1_weighted: 0.27959818033419104
[2m[36m(func pid=109465)[0m f1_per_class: [0.6, 0.23, 0.636, 0.532, 0.3, 0.0, 0.225, 0.016, 0.091, 0.188]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:10 (running for 00:15:27.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.497 |      0.329 |                   84 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.426 |      0.386 |                   62 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  4.438 |      0.282 |                   60 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.807 |      0.055 |                    8 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3656716417910448
[2m[36m(func pid=108399)[0m top5: 0.8777985074626866
[2m[36m(func pid=108399)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=108399)[0m f1_macro: 0.38648569907832175
[2m[36m(func pid=108399)[0m f1_weighted: 0.3860752892158766
[2m[36m(func pid=108399)[0m f1_per_class: [0.647, 0.477, 0.769, 0.495, 0.118, 0.236, 0.298, 0.351, 0.258, 0.216]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7732 | Steps: 4 | Val loss: 2.3720 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=102477)[0m top1: 0.3941231343283582
[2m[36m(func pid=102477)[0m top5: 0.871268656716418
[2m[36m(func pid=102477)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=102477)[0m f1_macro: 0.337583988132934
[2m[36m(func pid=102477)[0m f1_weighted: 0.42380689818562833
[2m[36m(func pid=102477)[0m f1_per_class: [0.569, 0.485, 0.31, 0.445, 0.115, 0.17, 0.508, 0.365, 0.235, 0.174]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 7.5596 | Steps: 4 | Val loss: 29.6750 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=121255)[0m top1: 0.08255597014925373
[2m[36m(func pid=121255)[0m top5: 0.5125932835820896
[2m[36m(func pid=121255)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=121255)[0m f1_macro: 0.055458907720049154
[2m[36m(func pid=121255)[0m f1_weighted: 0.0906229019104179
[2m[36m(func pid=121255)[0m f1_per_class: [0.032, 0.115, 0.021, 0.08, 0.0, 0.122, 0.105, 0.0, 0.08, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.0236 | Steps: 4 | Val loss: 2.6313 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4145 | Steps: 4 | Val loss: 1.7252 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=109465)[0m top1: 0.39598880597014924
[2m[36m(func pid=109465)[0m top5: 0.8199626865671642
[2m[36m(func pid=109465)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=109465)[0m f1_macro: 0.2752741019940679
[2m[36m(func pid=109465)[0m f1_weighted: 0.3873323846440443
[2m[36m(func pid=109465)[0m f1_per_class: [0.259, 0.563, 0.21, 0.522, 0.135, 0.101, 0.385, 0.016, 0.156, 0.405]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:15 (running for 00:15:32.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.317 |      0.338 |                   85 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.024 |      0.402 |                   63 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  7.56  |      0.275 |                   61 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.773 |      0.055 |                    9 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.396455223880597
[2m[36m(func pid=108399)[0m top5: 0.8997201492537313
[2m[36m(func pid=108399)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=108399)[0m f1_macro: 0.40212374707362597
[2m[36m(func pid=108399)[0m f1_weighted: 0.4265423763900749
[2m[36m(func pid=108399)[0m f1_per_class: [0.651, 0.438, 0.769, 0.515, 0.13, 0.233, 0.444, 0.323, 0.228, 0.29]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.404384328358209
[2m[36m(func pid=102477)[0m top5: 0.8857276119402985
[2m[36m(func pid=102477)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=102477)[0m f1_macro: 0.3532331967135985
[2m[36m(func pid=102477)[0m f1_weighted: 0.42973458161312383
[2m[36m(func pid=102477)[0m f1_per_class: [0.504, 0.5, 0.388, 0.473, 0.131, 0.242, 0.463, 0.373, 0.243, 0.214]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8362 | Steps: 4 | Val loss: 2.3465 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 16.4791 | Steps: 4 | Val loss: 36.2832 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.3007 | Steps: 4 | Val loss: 2.5696 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=121255)[0m top1: 0.09421641791044776
[2m[36m(func pid=121255)[0m top5: 0.5405783582089553
[2m[36m(func pid=121255)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=121255)[0m f1_macro: 0.0604062125960958
[2m[36m(func pid=121255)[0m f1_weighted: 0.10457724608891579
[2m[36m(func pid=121255)[0m f1_per_class: [0.037, 0.1, 0.032, 0.051, 0.006, 0.117, 0.189, 0.0, 0.071, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3637 | Steps: 4 | Val loss: 1.6937 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=109465)[0m top1: 0.3605410447761194
[2m[36m(func pid=109465)[0m top5: 0.7877798507462687
[2m[36m(func pid=109465)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=109465)[0m f1_macro: 0.2824270487162899
[2m[36m(func pid=109465)[0m f1_weighted: 0.3538093459266004
[2m[36m(func pid=109465)[0m f1_per_class: [0.411, 0.584, 0.1, 0.459, 0.149, 0.268, 0.21, 0.309, 0.0, 0.333]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:20 (running for 00:15:38.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.415 |      0.353 |                   86 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.301 |      0.416 |                   64 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 16.479 |      0.282 |                   62 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.836 |      0.06  |                   10 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.41091417910447764
[2m[36m(func pid=108399)[0m top5: 0.9067164179104478
[2m[36m(func pid=108399)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=108399)[0m f1_macro: 0.41641361651547343
[2m[36m(func pid=108399)[0m f1_weighted: 0.43784184878589266
[2m[36m(func pid=108399)[0m f1_per_class: [0.66, 0.446, 0.786, 0.507, 0.149, 0.238, 0.479, 0.331, 0.211, 0.358]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3978544776119403
[2m[36m(func pid=102477)[0m top5: 0.8950559701492538
[2m[36m(func pid=102477)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=102477)[0m f1_macro: 0.3554931290129052
[2m[36m(func pid=102477)[0m f1_weighted: 0.4184286403849122
[2m[36m(func pid=102477)[0m f1_per_class: [0.547, 0.477, 0.414, 0.531, 0.137, 0.245, 0.385, 0.365, 0.201, 0.254]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9391 | Steps: 4 | Val loss: 2.3265 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 35.3463 | Steps: 4 | Val loss: 38.7130 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0915 | Steps: 4 | Val loss: 2.7472 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3634 | Steps: 4 | Val loss: 1.7207 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=121255)[0m top1: 0.11707089552238806
[2m[36m(func pid=121255)[0m top5: 0.570429104477612
[2m[36m(func pid=121255)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=121255)[0m f1_macro: 0.06476534793382269
[2m[36m(func pid=121255)[0m f1_weighted: 0.12173060703831314
[2m[36m(func pid=121255)[0m f1_per_class: [0.042, 0.095, 0.03, 0.019, 0.007, 0.077, 0.293, 0.0, 0.085, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.31902985074626866
[2m[36m(func pid=109465)[0m top5: 0.8129664179104478
[2m[36m(func pid=109465)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=109465)[0m f1_macro: 0.2551315146937086
[2m[36m(func pid=109465)[0m f1_weighted: 0.32046556609791743
[2m[36m(func pid=109465)[0m f1_per_class: [0.296, 0.559, 0.222, 0.473, 0.2, 0.209, 0.146, 0.234, 0.0, 0.212]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:25 (running for 00:15:43.38)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.364 |      0.355 |                   87 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.092 |      0.386 |                   65 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 35.346 |      0.255 |                   63 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.939 |      0.065 |                   11 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3917910447761194
[2m[36m(func pid=108399)[0m top5: 0.8969216417910447
[2m[36m(func pid=108399)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=108399)[0m f1_macro: 0.3862977269954726
[2m[36m(func pid=108399)[0m f1_weighted: 0.4059442515639498
[2m[36m(func pid=108399)[0m f1_per_class: [0.352, 0.497, 0.688, 0.48, 0.237, 0.307, 0.36, 0.335, 0.217, 0.39]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3829291044776119
[2m[36m(func pid=102477)[0m top5: 0.8871268656716418
[2m[36m(func pid=102477)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=102477)[0m f1_macro: 0.35596146403606554
[2m[36m(func pid=102477)[0m f1_weighted: 0.3970586087345595
[2m[36m(func pid=102477)[0m f1_per_class: [0.569, 0.445, 0.48, 0.562, 0.149, 0.209, 0.317, 0.365, 0.159, 0.306]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8073 | Steps: 4 | Val loss: 2.2959 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 16.5127 | Steps: 4 | Val loss: 42.2910 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.4311 | Steps: 4 | Val loss: 1.7703 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.0359 | Steps: 4 | Val loss: 3.0780 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121255)[0m top1: 0.16044776119402984
[2m[36m(func pid=121255)[0m top5: 0.5806902985074627
[2m[36m(func pid=121255)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=121255)[0m f1_macro: 0.07682329691767976
[2m[36m(func pid=121255)[0m f1_weighted: 0.14560619539521272
[2m[36m(func pid=121255)[0m f1_per_class: [0.103, 0.107, 0.037, 0.013, 0.0, 0.05, 0.379, 0.0, 0.08, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.2490671641791045
[2m[36m(func pid=109465)[0m top5: 0.8260261194029851
[2m[36m(func pid=109465)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=109465)[0m f1_macro: 0.23010767818586286
[2m[36m(func pid=109465)[0m f1_weighted: 0.27231036467436676
[2m[36m(func pid=109465)[0m f1_per_class: [0.167, 0.227, 0.545, 0.505, 0.162, 0.055, 0.198, 0.248, 0.125, 0.069]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:31 (running for 00:15:48.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.431 |      0.356 |                   89 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.092 |      0.386 |                   65 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 16.513 |      0.23  |                   64 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.807 |      0.077 |                   12 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.37173507462686567
[2m[36m(func pid=102477)[0m top5: 0.8861940298507462
[2m[36m(func pid=102477)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=102477)[0m f1_macro: 0.35594018295715196
[2m[36m(func pid=102477)[0m f1_weighted: 0.37833457466269554
[2m[36m(func pid=102477)[0m f1_per_class: [0.535, 0.413, 0.5, 0.568, 0.159, 0.217, 0.259, 0.381, 0.169, 0.359]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.38152985074626866
[2m[36m(func pid=108399)[0m top5: 0.875
[2m[36m(func pid=108399)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=108399)[0m f1_macro: 0.3597619892085512
[2m[36m(func pid=108399)[0m f1_weighted: 0.3707739151629098
[2m[36m(func pid=108399)[0m f1_per_class: [0.273, 0.543, 0.667, 0.443, 0.108, 0.332, 0.24, 0.382, 0.216, 0.394]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6742 | Steps: 4 | Val loss: 2.2929 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 11.9268 | Steps: 4 | Val loss: 35.0801 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.5730 | Steps: 4 | Val loss: 1.7978 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.0702 | Steps: 4 | Val loss: 3.3130 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=121255)[0m top1: 0.19029850746268656
[2m[36m(func pid=121255)[0m top5: 0.5708955223880597
[2m[36m(func pid=121255)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=121255)[0m f1_macro: 0.07919097244437891
[2m[36m(func pid=121255)[0m f1_weighted: 0.1554903143583711
[2m[36m(func pid=121255)[0m f1_per_class: [0.098, 0.14, 0.047, 0.007, 0.0, 0.006, 0.416, 0.0, 0.078, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3414179104477612
[2m[36m(func pid=109465)[0m top5: 0.8521455223880597
[2m[36m(func pid=109465)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=109465)[0m f1_macro: 0.3287213419712487
[2m[36m(func pid=109465)[0m f1_weighted: 0.335526171616717
[2m[36m(func pid=109465)[0m f1_per_class: [0.559, 0.168, 0.667, 0.589, 0.122, 0.045, 0.31, 0.333, 0.155, 0.34]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:36 (running for 00:15:53.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.573 |      0.349 |                   90 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.036 |      0.36  |                   66 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      | 11.927 |      0.329 |                   65 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.674 |      0.079 |                   13 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.363339552238806
[2m[36m(func pid=102477)[0m top5: 0.882929104477612
[2m[36m(func pid=102477)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=102477)[0m f1_macro: 0.3488661741372372
[2m[36m(func pid=102477)[0m f1_weighted: 0.36189908831536494
[2m[36m(func pid=102477)[0m f1_per_class: [0.565, 0.393, 0.5, 0.573, 0.149, 0.192, 0.221, 0.362, 0.164, 0.368]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.36847014925373134
[2m[36m(func pid=108399)[0m top5: 0.8549440298507462
[2m[36m(func pid=108399)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=108399)[0m f1_macro: 0.34994674176807805
[2m[36m(func pid=108399)[0m f1_weighted: 0.3446622009242879
[2m[36m(func pid=108399)[0m f1_per_class: [0.281, 0.519, 0.629, 0.397, 0.118, 0.308, 0.217, 0.363, 0.256, 0.413]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5722 | Steps: 4 | Val loss: 2.2744 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.7150 | Steps: 4 | Val loss: 34.4923 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.4466 | Steps: 4 | Val loss: 1.7719 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2466 | Steps: 4 | Val loss: 3.4930 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=121255)[0m top1: 0.21828358208955223
[2m[36m(func pid=121255)[0m top5: 0.5778917910447762
[2m[36m(func pid=121255)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=121255)[0m f1_macro: 0.09105066264099981
[2m[36m(func pid=121255)[0m f1_weighted: 0.16922554282650115
[2m[36m(func pid=121255)[0m f1_per_class: [0.149, 0.159, 0.058, 0.007, 0.0, 0.007, 0.446, 0.0, 0.084, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.3726679104477612
[2m[36m(func pid=109465)[0m top5: 0.8568097014925373
[2m[36m(func pid=109465)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=109465)[0m f1_macro: 0.3574852469153832
[2m[36m(func pid=109465)[0m f1_weighted: 0.38307758923085544
[2m[36m(func pid=109465)[0m f1_per_class: [0.667, 0.56, 0.87, 0.49, 0.101, 0.016, 0.342, 0.368, 0.163, 0.0]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:41 (running for 00:15:59.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.447 |      0.35  |                   91 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.07  |      0.35  |                   67 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.715 |      0.357 |                   66 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.572 |      0.091 |                   14 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.38152985074626866
[2m[36m(func pid=102477)[0m top5: 0.8810634328358209
[2m[36m(func pid=102477)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=102477)[0m f1_macro: 0.35007751708017254
[2m[36m(func pid=102477)[0m f1_weighted: 0.3771589067033132
[2m[36m(func pid=102477)[0m f1_per_class: [0.545, 0.522, 0.462, 0.536, 0.123, 0.182, 0.233, 0.377, 0.216, 0.305]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3451492537313433
[2m[36m(func pid=108399)[0m top5: 0.8297574626865671
[2m[36m(func pid=108399)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=108399)[0m f1_macro: 0.31915820869379374
[2m[36m(func pid=108399)[0m f1_weighted: 0.30819530112041416
[2m[36m(func pid=108399)[0m f1_per_class: [0.356, 0.484, 0.524, 0.368, 0.079, 0.254, 0.166, 0.367, 0.206, 0.387]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6725 | Steps: 4 | Val loss: 2.2550 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 8.1706 | Steps: 4 | Val loss: 38.9794 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2032 | Steps: 4 | Val loss: 3.4912 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3533 | Steps: 4 | Val loss: 1.6661 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=121255)[0m top1: 0.23414179104477612
[2m[36m(func pid=121255)[0m top5: 0.5834888059701493
[2m[36m(func pid=121255)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=121255)[0m f1_macro: 0.10395867581132942
[2m[36m(func pid=121255)[0m f1_weighted: 0.18217642932033698
[2m[36m(func pid=121255)[0m f1_per_class: [0.217, 0.175, 0.069, 0.007, 0.0, 0.008, 0.475, 0.0, 0.09, 0.0]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.373134328358209
[2m[36m(func pid=109465)[0m top5: 0.8451492537313433
[2m[36m(func pid=109465)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=109465)[0m f1_macro: 0.3466456140881327
[2m[36m(func pid=109465)[0m f1_weighted: 0.37255478426549976
[2m[36m(func pid=109465)[0m f1_per_class: [0.699, 0.54, 0.667, 0.422, 0.05, 0.029, 0.365, 0.388, 0.235, 0.071]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:47 (running for 00:16:04.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.447 |      0.35  |                   91 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.203 |      0.326 |                   69 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.171 |      0.347 |                   67 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.672 |      0.104 |                   15 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3414179104477612
[2m[36m(func pid=108399)[0m top5: 0.8278917910447762
[2m[36m(func pid=108399)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=108399)[0m f1_macro: 0.32595372510011006
[2m[36m(func pid=108399)[0m f1_weighted: 0.30901580657911065
[2m[36m(func pid=108399)[0m f1_per_class: [0.534, 0.477, 0.49, 0.386, 0.095, 0.144, 0.188, 0.351, 0.234, 0.361]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.41651119402985076
[2m[36m(func pid=102477)[0m top5: 0.9006529850746269
[2m[36m(func pid=102477)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=102477)[0m f1_macro: 0.3682139655698812
[2m[36m(func pid=102477)[0m f1_weighted: 0.4201588860226189
[2m[36m(func pid=102477)[0m f1_per_class: [0.532, 0.558, 0.462, 0.526, 0.129, 0.183, 0.365, 0.348, 0.287, 0.293]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5021 | Steps: 4 | Val loss: 2.2266 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.6878 | Steps: 4 | Val loss: 34.4995 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1566 | Steps: 4 | Val loss: 3.5232 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3295 | Steps: 4 | Val loss: 1.6371 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=121255)[0m top1: 0.24300373134328357
[2m[36m(func pid=121255)[0m top5: 0.6026119402985075
[2m[36m(func pid=121255)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=121255)[0m f1_macro: 0.12069324597095683
[2m[36m(func pid=121255)[0m f1_weighted: 0.19180803224091605
[2m[36m(func pid=121255)[0m f1_per_class: [0.186, 0.202, 0.103, 0.013, 0.03, 0.0, 0.485, 0.0, 0.099, 0.09]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=109465)[0m top1: 0.38199626865671643
[2m[36m(func pid=109465)[0m top5: 0.8376865671641791
[2m[36m(func pid=109465)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=109465)[0m f1_macro: 0.35163183259545827
[2m[36m(func pid=109465)[0m f1_weighted: 0.3944010067460811
[2m[36m(func pid=109465)[0m f1_per_class: [0.637, 0.578, 0.312, 0.471, 0.052, 0.045, 0.368, 0.381, 0.173, 0.5]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:23:52 (running for 00:16:09.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.353 |      0.368 |                   92 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.157 |      0.298 |                   70 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  1.688 |      0.352 |                   68 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.502 |      0.121 |                   16 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.35261194029850745
[2m[36m(func pid=108399)[0m top5: 0.7971082089552238
[2m[36m(func pid=108399)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=108399)[0m f1_macro: 0.29783188579678715
[2m[36m(func pid=108399)[0m f1_weighted: 0.35888633161074496
[2m[36m(func pid=108399)[0m f1_per_class: [0.411, 0.495, 0.342, 0.564, 0.058, 0.044, 0.232, 0.388, 0.174, 0.27]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.4253731343283582
[2m[36m(func pid=102477)[0m top5: 0.9029850746268657
[2m[36m(func pid=102477)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=102477)[0m f1_macro: 0.3746960976529741
[2m[36m(func pid=102477)[0m f1_weighted: 0.43509423318937257
[2m[36m(func pid=102477)[0m f1_per_class: [0.536, 0.556, 0.511, 0.501, 0.112, 0.182, 0.441, 0.338, 0.286, 0.283]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5067 | Steps: 4 | Val loss: 31.2497 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2681 | Steps: 4 | Val loss: 2.2011 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0835 | Steps: 4 | Val loss: 3.7085 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3483 | Steps: 4 | Val loss: 1.6578 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=109465)[0m top1: 0.386660447761194
[2m[36m(func pid=109465)[0m top5: 0.8274253731343284
[2m[36m(func pid=109465)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=109465)[0m f1_macro: 0.3089205519996902
[2m[36m(func pid=109465)[0m f1_weighted: 0.40348982208845635
[2m[36m(func pid=109465)[0m f1_per_class: [0.432, 0.58, 0.158, 0.53, 0.083, 0.109, 0.335, 0.395, 0.234, 0.234]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m top1: 0.2579291044776119
[2m[36m(func pid=121255)[0m top5: 0.6371268656716418
[2m[36m(func pid=121255)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=121255)[0m f1_macro: 0.14005216314779784
[2m[36m(func pid=121255)[0m f1_weighted: 0.21479852363458324
[2m[36m(func pid=121255)[0m f1_per_class: [0.196, 0.26, 0.161, 0.035, 0.025, 0.008, 0.499, 0.015, 0.108, 0.093]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:23:57 (running for 00:16:15.14)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.33  |      0.375 |                   93 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.083 |      0.268 |                   71 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.507 |      0.309 |                   69 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.268 |      0.14  |                   17 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3246268656716418
[2m[36m(func pid=108399)[0m top5: 0.789179104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=108399)[0m f1_macro: 0.2683526142108104
[2m[36m(func pid=108399)[0m f1_weighted: 0.35974456102132085
[2m[36m(func pid=108399)[0m f1_per_class: [0.371, 0.208, 0.255, 0.552, 0.048, 0.028, 0.43, 0.369, 0.127, 0.296]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.42350746268656714
[2m[36m(func pid=102477)[0m top5: 0.8992537313432836
[2m[36m(func pid=102477)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=102477)[0m f1_macro: 0.3701169290959683
[2m[36m(func pid=102477)[0m f1_weighted: 0.4336391119375361
[2m[36m(func pid=102477)[0m f1_per_class: [0.522, 0.559, 0.453, 0.493, 0.119, 0.209, 0.432, 0.355, 0.268, 0.291]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.1944 | Steps: 4 | Val loss: 32.1494 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2079 | Steps: 4 | Val loss: 2.1806 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.4744 | Steps: 4 | Val loss: 1.6194 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2367 | Steps: 4 | Val loss: 3.6916 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=109465)[0m top1: 0.34794776119402987
[2m[36m(func pid=109465)[0m top5: 0.8097014925373134
[2m[36m(func pid=109465)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=109465)[0m f1_macro: 0.2888971561776441
[2m[36m(func pid=109465)[0m f1_weighted: 0.37360572780563356
[2m[36m(func pid=109465)[0m f1_per_class: [0.363, 0.517, 0.1, 0.509, 0.156, 0.23, 0.252, 0.402, 0.214, 0.146]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m top1: 0.23694029850746268
[2m[36m(func pid=121255)[0m top5: 0.664179104477612
[2m[36m(func pid=121255)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=121255)[0m f1_macro: 0.1481860015548383
[2m[36m(func pid=121255)[0m f1_weighted: 0.22023720084643647
[2m[36m(func pid=121255)[0m f1_per_class: [0.172, 0.278, 0.189, 0.068, 0.024, 0.014, 0.462, 0.082, 0.11, 0.082]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:24:03 (running for 00:16:20.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.474 |      0.376 |                   95 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.083 |      0.268 |                   71 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  3.194 |      0.289 |                   70 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.208 |      0.148 |                   18 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=102477)[0m top1: 0.4244402985074627
[2m[36m(func pid=102477)[0m top5: 0.9123134328358209
[2m[36m(func pid=102477)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=102477)[0m f1_macro: 0.3759382094160599
[2m[36m(func pid=102477)[0m f1_weighted: 0.441043470474535
[2m[36m(func pid=102477)[0m f1_per_class: [0.567, 0.541, 0.462, 0.523, 0.134, 0.193, 0.446, 0.35, 0.237, 0.308]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3246268656716418
[2m[36m(func pid=108399)[0m top5: 0.78125
[2m[36m(func pid=108399)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=108399)[0m f1_macro: 0.26136512162775605
[2m[36m(func pid=108399)[0m f1_weighted: 0.3622260501198661
[2m[36m(func pid=108399)[0m f1_per_class: [0.311, 0.227, 0.212, 0.518, 0.062, 0.048, 0.466, 0.306, 0.146, 0.318]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 6.5114 | Steps: 4 | Val loss: 37.5493 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1651 | Steps: 4 | Val loss: 2.1547 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.1678 | Steps: 4 | Val loss: 3.1673 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4527 | Steps: 4 | Val loss: 1.6276 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=109465)[0m top1: 0.300839552238806
[2m[36m(func pid=109465)[0m top5: 0.7789179104477612
[2m[36m(func pid=109465)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=109465)[0m f1_macro: 0.2695981275570494
[2m[36m(func pid=109465)[0m f1_weighted: 0.3140225495053767
[2m[36m(func pid=109465)[0m f1_per_class: [0.348, 0.468, 0.118, 0.354, 0.218, 0.313, 0.206, 0.343, 0.203, 0.125]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m top1: 0.23041044776119404
[2m[36m(func pid=121255)[0m top5: 0.7000932835820896
[2m[36m(func pid=121255)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=121255)[0m f1_macro: 0.17713674734478815
[2m[36m(func pid=121255)[0m f1_weighted: 0.23038326249608326
[2m[36m(func pid=121255)[0m f1_per_class: [0.178, 0.31, 0.302, 0.114, 0.02, 0.035, 0.395, 0.227, 0.115, 0.075]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:24:08 (running for 00:16:25.74)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.474 |      0.376 |                   95 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.168 |      0.3   |                   73 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  6.511 |      0.27  |                   71 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.165 |      0.177 |                   19 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.37453358208955223
[2m[36m(func pid=108399)[0m top5: 0.8278917910447762
[2m[36m(func pid=108399)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=108399)[0m f1_macro: 0.3001097802871117
[2m[36m(func pid=108399)[0m f1_weighted: 0.39706758525720837
[2m[36m(func pid=108399)[0m f1_per_class: [0.388, 0.491, 0.2, 0.419, 0.098, 0.082, 0.501, 0.305, 0.171, 0.349]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.41324626865671643
[2m[36m(func pid=102477)[0m top5: 0.9155783582089553
[2m[36m(func pid=102477)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=102477)[0m f1_macro: 0.3844079040060618
[2m[36m(func pid=102477)[0m f1_weighted: 0.4412551090110697
[2m[36m(func pid=102477)[0m f1_per_class: [0.637, 0.458, 0.524, 0.56, 0.135, 0.192, 0.454, 0.371, 0.19, 0.323]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9897 | Steps: 4 | Val loss: 37.3981 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.4118 | Steps: 4 | Val loss: 2.1472 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1232 | Steps: 4 | Val loss: 3.5623 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.4113 | Steps: 4 | Val loss: 1.7345 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=109465)[0m top1: 0.3218283582089552
[2m[36m(func pid=109465)[0m top5: 0.7798507462686567
[2m[36m(func pid=109465)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=109465)[0m f1_macro: 0.2878919877476018
[2m[36m(func pid=109465)[0m f1_weighted: 0.33312798995854076
[2m[36m(func pid=109465)[0m f1_per_class: [0.344, 0.406, 0.316, 0.238, 0.286, 0.25, 0.448, 0.275, 0.189, 0.127]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m top1: 0.20755597014925373
[2m[36m(func pid=121255)[0m top5: 0.7019589552238806
[2m[36m(func pid=121255)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=121255)[0m f1_macro: 0.1788684234615775
[2m[36m(func pid=121255)[0m f1_weighted: 0.20322610954058642
[2m[36m(func pid=121255)[0m f1_per_class: [0.18, 0.34, 0.41, 0.136, 0.032, 0.021, 0.271, 0.233, 0.084, 0.081]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=102477)[0m top1: 0.37593283582089554== Status ==
Current time: 2024-01-07 11:24:13 (running for 00:16:31.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.34975
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.411 |      0.368 |                   97 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.168 |      0.3   |                   73 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.99  |      0.288 |                   72 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.412 |      0.179 |                   20 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=102477)[0m top5: 0.90625
[2m[36m(func pid=102477)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=102477)[0m f1_macro: 0.36828676297293317
[2m[36m(func pid=102477)[0m f1_weighted: 0.41121026871070754
[2m[36m(func pid=102477)[0m f1_per_class: [0.635, 0.377, 0.537, 0.564, 0.14, 0.182, 0.407, 0.355, 0.157, 0.329]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=108399)[0m top1: 0.35074626865671643
[2m[36m(func pid=108399)[0m top5: 0.8302238805970149
[2m[36m(func pid=108399)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=108399)[0m f1_macro: 0.3186887935213439
[2m[36m(func pid=108399)[0m f1_weighted: 0.356077045016349
[2m[36m(func pid=108399)[0m f1_per_class: [0.604, 0.506, 0.265, 0.278, 0.128, 0.114, 0.46, 0.289, 0.155, 0.388]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5264 | Steps: 4 | Val loss: 40.1130 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.0375 | Steps: 4 | Val loss: 2.1353 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3423 | Steps: 4 | Val loss: 1.7760 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.1034 | Steps: 4 | Val loss: 3.1241 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=109465)[0m top1: 0.353544776119403
[2m[36m(func pid=109465)[0m top5: 0.7817164179104478
[2m[36m(func pid=109465)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=109465)[0m f1_macro: 0.29987999510454644
[2m[36m(func pid=109465)[0m f1_weighted: 0.3267266532387787
[2m[36m(func pid=109465)[0m f1_per_class: [0.365, 0.293, 0.556, 0.219, 0.265, 0.215, 0.526, 0.239, 0.174, 0.149]
[2m[36m(func pid=109465)[0m 
[2m[36m(func pid=121255)[0m top1: 0.19169776119402984
[2m[36m(func pid=121255)[0m top5: 0.7164179104477612
[2m[36m(func pid=121255)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=121255)[0m f1_macro: 0.19266390860382923
[2m[36m(func pid=121255)[0m f1_weighted: 0.1804722104039768
[2m[36m(func pid=121255)[0m f1_per_class: [0.181, 0.341, 0.571, 0.192, 0.063, 0.038, 0.13, 0.233, 0.095, 0.082]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:24:19 (running for 00:16:36.46)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.411 |      0.368 |                   97 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.103 |      0.353 |                   75 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.526 |      0.3   |                   73 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.038 |      0.193 |                   21 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.4295708955223881
[2m[36m(func pid=108399)[0m top5: 0.8791977611940298
[2m[36m(func pid=108399)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=108399)[0m f1_macro: 0.35256928406579113
[2m[36m(func pid=108399)[0m f1_weighted: 0.40425629746852276
[2m[36m(func pid=108399)[0m f1_per_class: [0.681, 0.532, 0.324, 0.31, 0.124, 0.157, 0.56, 0.191, 0.267, 0.378]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=102477)[0m top1: 0.3689365671641791
[2m[36m(func pid=102477)[0m top5: 0.8922574626865671
[2m[36m(func pid=102477)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=102477)[0m f1_macro: 0.35272306433462663
[2m[36m(func pid=102477)[0m f1_weighted: 0.39738805113261516
[2m[36m(func pid=102477)[0m f1_per_class: [0.559, 0.424, 0.468, 0.55, 0.121, 0.2, 0.346, 0.357, 0.162, 0.341]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5449 | Steps: 4 | Val loss: 38.1598 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1287 | Steps: 4 | Val loss: 2.1037 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.5871 | Steps: 4 | Val loss: 2.7768 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3930 | Steps: 4 | Val loss: 1.7995 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=109465)[0m top1: 0.3582089552238806
[2m[36m(func pid=109465)[0m top5: 0.8050373134328358
[2m[36m(func pid=109465)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=109465)[0m f1_macro: 0.3229959518487382
[2m[36m(func pid=109465)[0m f1_weighted: 0.33738089452336056
[2m[36m(func pid=109465)[0m f1_per_class: [0.407, 0.282, 0.75, 0.25, 0.224, 0.23, 0.53, 0.23, 0.153, 0.174]
[2m[36m(func pid=109465)[0m 
== Status ==
Current time: 2024-01-07 11:24:24 (running for 00:16:41.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.342 |      0.353 |                   98 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.103 |      0.353 |                   75 |
| train_98a10_00007 | RUNNING    | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  0.545 |      0.323 |                   74 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.129 |      0.203 |                   22 |
| train_98a10_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.4221082089552239
[2m[36m(func pid=108399)[0m top5: 0.8959888059701493
[2m[36m(func pid=108399)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=108399)[0m f1_macro: 0.3727075933865505
[2m[36m(func pid=108399)[0m f1_weighted: 0.4202266601890443
[2m[36m(func pid=108399)[0m f1_per_class: [0.682, 0.53, 0.393, 0.402, 0.18, 0.205, 0.487, 0.324, 0.267, 0.257]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.2042910447761194
[2m[36m(func pid=121255)[0m top5: 0.7369402985074627
[2m[36m(func pid=121255)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=121255)[0m f1_macro: 0.20325655944500384
[2m[36m(func pid=121255)[0m f1_weighted: 0.18297596871969696
[2m[36m(func pid=121255)[0m f1_per_class: [0.213, 0.382, 0.615, 0.249, 0.065, 0.066, 0.051, 0.224, 0.065, 0.101]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=102477)[0m top1: 0.37220149253731344
[2m[36m(func pid=102477)[0m top5: 0.8805970149253731
[2m[36m(func pid=102477)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=102477)[0m f1_macro: 0.35520611387992373
[2m[36m(func pid=102477)[0m f1_weighted: 0.38219561365959237
[2m[36m(func pid=102477)[0m f1_per_class: [0.56, 0.506, 0.453, 0.486, 0.126, 0.203, 0.3, 0.366, 0.186, 0.366]
[2m[36m(func pid=102477)[0m 
[2m[36m(func pid=109465)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 8.0958 | Steps: 4 | Val loss: 31.3758 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.9258 | Steps: 4 | Val loss: 4.3839 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1324 | Steps: 4 | Val loss: 2.0728 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=102477)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3854 | Steps: 4 | Val loss: 1.7763 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=109465)[0m top1: 0.353544776119403
[2m[36m(func pid=109465)[0m top5: 0.8446828358208955
[2m[36m(func pid=109465)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=109465)[0m f1_macro: 0.34806142306444354
[2m[36m(func pid=109465)[0m f1_weighted: 0.38887576461687623
[2m[36m(func pid=109465)[0m f1_per_class: [0.385, 0.341, 0.8, 0.491, 0.165, 0.216, 0.438, 0.281, 0.165, 0.199]
[2m[36m(func pid=108399)[0m top1: 0.31296641791044777
[2m[36m(func pid=108399)[0m top5: 0.7602611940298507
[2m[36m(func pid=108399)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=108399)[0m f1_macro: 0.31775005966223663
[2m[36m(func pid=108399)[0m f1_weighted: 0.29492491145260075
[2m[36m(func pid=108399)[0m f1_per_class: [0.624, 0.487, 0.44, 0.431, 0.198, 0.294, 0.036, 0.384, 0.192, 0.092]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.23507462686567165
[2m[36m(func pid=121255)[0m top5: 0.7322761194029851
[2m[36m(func pid=121255)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=121255)[0m f1_macro: 0.2204377063501723
[2m[36m(func pid=121255)[0m f1_weighted: 0.2083894983635371
[2m[36m(func pid=121255)[0m f1_per_class: [0.237, 0.425, 0.571, 0.338, 0.123, 0.088, 0.015, 0.234, 0.068, 0.103]
[2m[36m(func pid=102477)[0m top1: 0.384794776119403
[2m[36m(func pid=102477)[0m top5: 0.8805970149253731
[2m[36m(func pid=102477)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=102477)[0m f1_macro: 0.3513287732277979
[2m[36m(func pid=102477)[0m f1_weighted: 0.3941086542201399
[2m[36m(func pid=102477)[0m f1_per_class: [0.515, 0.521, 0.448, 0.471, 0.105, 0.174, 0.361, 0.348, 0.226, 0.345]
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.5689 | Steps: 4 | Val loss: 5.8429 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=108399)[0m top1: 0.33115671641791045
[2m[36m(func pid=108399)[0m top5: 0.7159514925373134
[2m[36m(func pid=108399)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=108399)[0m f1_macro: 0.34786457235388396
[2m[36m(func pid=108399)[0m f1_weighted: 0.30062996567396666
[2m[36m(func pid=108399)[0m f1_per_class: [0.645, 0.526, 0.611, 0.468, 0.14, 0.284, 0.0, 0.356, 0.16, 0.289]
== Status ==
Current time: 2024-01-07 11:24:29 (running for 00:16:46.89)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00005 | RUNNING    | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.393 |      0.355 |                   99 |
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.926 |      0.318 |                   77 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.129 |      0.203 |                   22 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 11:24:36 (running for 00:16:54.25)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.926 |      0.318 |                   77 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.129 |      0.203 |                   22 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127193)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=127193)[0m Configuration completed!
[2m[36m(func pid=127193)[0m New optimizer parameters:
[2m[36m(func pid=127193)[0m SGD (
[2m[36m(func pid=127193)[0m Parameter Group 0
[2m[36m(func pid=127193)[0m     dampening: 0
[2m[36m(func pid=127193)[0m     differentiable: False
[2m[36m(func pid=127193)[0m     foreach: None
[2m[36m(func pid=127193)[0m     lr: 0.001
[2m[36m(func pid=127193)[0m     maximize: False
[2m[36m(func pid=127193)[0m     momentum: 0.99
[2m[36m(func pid=127193)[0m     nesterov: False
[2m[36m(func pid=127193)[0m     weight_decay: 0.0001
[2m[36m(func pid=127193)[0m )
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2069 | Steps: 4 | Val loss: 4.2532 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7595 | Steps: 4 | Val loss: 2.0126 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1333 | Steps: 4 | Val loss: 2.4687 | Batch size: 32 | lr: 0.001 | Duration: 4.89s
[2m[36m(func pid=108399)[0m top1: 0.3125
[2m[36m(func pid=108399)[0m top5: 0.7481343283582089
[2m[36m(func pid=108399)[0m f1_micro: 0.3125
[2m[36m(func pid=108399)[0m f1_macro: 0.30429235968564583
[2m[36m(func pid=108399)[0m f1_weighted: 0.3024165235079886
[2m[36m(func pid=108399)[0m f1_per_class: [0.526, 0.321, 0.268, 0.583, 0.058, 0.133, 0.077, 0.382, 0.162, 0.533]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.25886194029850745
[2m[36m(func pid=121255)[0m top5: 0.7803171641791045
[2m[36m(func pid=121255)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=121255)[0m f1_macro: 0.24450116340818567
[2m[36m(func pid=121255)[0m f1_weighted: 0.23307581305367844
[2m[36m(func pid=121255)[0m f1_per_class: [0.289, 0.449, 0.667, 0.4, 0.133, 0.115, 0.015, 0.232, 0.023, 0.122]
[2m[36m(func pid=127193)[0m top1: 0.0648320895522388
[2m[36m(func pid=127193)[0m top5: 0.4664179104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=127193)[0m f1_macro: 0.046534473551491026
[2m[36m(func pid=127193)[0m f1_weighted: 0.05399340747790541
[2m[36m(func pid=127193)[0m f1_per_class: [0.101, 0.051, 0.0, 0.118, 0.0, 0.024, 0.0, 0.088, 0.06, 0.023]
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3263 | Steps: 4 | Val loss: 4.6910 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:24:42 (running for 00:16:59.66)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.207 |      0.304 |                   79 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  2.132 |      0.22  |                   23 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127722)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=127722)[0m Configuration completed!
[2m[36m(func pid=127722)[0m New optimizer parameters:
[2m[36m(func pid=127722)[0m SGD (
[2m[36m(func pid=127722)[0m Parameter Group 0
[2m[36m(func pid=127722)[0m     dampening: 0
[2m[36m(func pid=127722)[0m     differentiable: False
[2m[36m(func pid=127722)[0m     foreach: None
[2m[36m(func pid=127722)[0m     lr: 0.01
[2m[36m(func pid=127722)[0m     maximize: False
[2m[36m(func pid=127722)[0m     momentum: 0.99
[2m[36m(func pid=127722)[0m     nesterov: False
[2m[36m(func pid=127722)[0m     weight_decay: 0.0001
[2m[36m(func pid=127722)[0m )
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:24:47 (running for 00:17:05.05)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.326 |      0.267 |                   80 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.759 |      0.245 |                   24 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  3.133 |      0.047 |                    1 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.29850746268656714
[2m[36m(func pid=108399)[0m top5: 0.7723880597014925
[2m[36m(func pid=108399)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=108399)[0m f1_macro: 0.2671204841740441
[2m[36m(func pid=108399)[0m f1_weighted: 0.3029279437788072
[2m[36m(func pid=108399)[0m f1_per_class: [0.49, 0.145, 0.222, 0.604, 0.038, 0.015, 0.215, 0.392, 0.141, 0.41]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.9241 | Steps: 4 | Val loss: 1.9431 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8902 | Steps: 4 | Val loss: 2.3919 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2233 | Steps: 4 | Val loss: 2.1796 | Batch size: 32 | lr: 0.01 | Duration: 4.27s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.8732 | Steps: 4 | Val loss: 3.9651 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=127193)[0m top1: 0.09328358208955224
[2m[36m(func pid=127193)[0m top5: 0.470615671641791
[2m[36m(func pid=127193)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=127193)[0m f1_macro: 0.0671204465685842
[2m[36m(func pid=127193)[0m f1_weighted: 0.09944040952045015
[2m[36m(func pid=127193)[0m f1_per_class: [0.066, 0.174, 0.028, 0.191, 0.007, 0.035, 0.015, 0.059, 0.074, 0.022]
[2m[36m(func pid=121255)[0m top1: 0.29011194029850745
[2m[36m(func pid=121255)[0m top5: 0.8330223880597015
[2m[36m(func pid=121255)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=121255)[0m f1_macro: 0.278230114918678
[2m[36m(func pid=121255)[0m f1_weighted: 0.2643755070178947
[2m[36m(func pid=121255)[0m f1_per_class: [0.381, 0.45, 0.769, 0.485, 0.141, 0.132, 0.025, 0.228, 0.026, 0.146]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.21688432835820895
[2m[36m(func pid=127722)[0m top5: 0.7178171641791045
[2m[36m(func pid=127722)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=127722)[0m f1_macro: 0.06528871150716731
[2m[36m(func pid=127722)[0m f1_weighted: 0.1460014647803538
[2m[36m(func pid=127722)[0m f1_per_class: [0.033, 0.0, 0.0, 0.025, 0.042, 0.019, 0.446, 0.0, 0.089, 0.0]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:24:53 (running for 00:17:10.67)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.873 |      0.279 |                   81 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.924 |      0.278 |                   25 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.89  |      0.067 |                    2 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.223 |      0.065 |                    1 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3726679104477612
[2m[36m(func pid=108399)[0m top5: 0.7896455223880597
[2m[36m(func pid=108399)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=108399)[0m f1_macro: 0.2794619621888461
[2m[36m(func pid=108399)[0m f1_weighted: 0.361284392814464
[2m[36m(func pid=108399)[0m f1_per_class: [0.478, 0.151, 0.292, 0.599, 0.064, 0.023, 0.422, 0.323, 0.176, 0.266]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.8993 | Steps: 4 | Val loss: 1.9067 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7289 | Steps: 4 | Val loss: 2.4378 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8720 | Steps: 4 | Val loss: 2.3505 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.0952 | Steps: 4 | Val loss: 3.6387 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121255)[0m top1: 0.30830223880597013
[2m[36m(func pid=121255)[0m top5: 0.8572761194029851
[2m[36m(func pid=121255)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=121255)[0m f1_macro: 0.2873726023355524
[2m[36m(func pid=121255)[0m f1_weighted: 0.2788277259396107
[2m[36m(func pid=121255)[0m f1_per_class: [0.411, 0.417, 0.741, 0.535, 0.176, 0.189, 0.022, 0.24, 0.0, 0.143]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m top1: 0.07416044776119403
[2m[36m(func pid=127193)[0m top5: 0.4337686567164179
[2m[36m(func pid=127193)[0m f1_micro: 0.07416044776119403
[2m[36m(func pid=127193)[0m f1_macro: 0.06029142359281264
[2m[36m(func pid=127193)[0m f1_weighted: 0.07111534390114889
[2m[36m(func pid=127193)[0m f1_per_class: [0.091, 0.101, 0.086, 0.044, 0.014, 0.094, 0.085, 0.0, 0.088, 0.0]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.11007462686567164
[2m[36m(func pid=127722)[0m top5: 0.6665111940298507
[2m[36m(func pid=127722)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=127722)[0m f1_macro: 0.2073937667867593
[2m[36m(func pid=127722)[0m f1_weighted: 0.07697490573999544
[2m[36m(func pid=127722)[0m f1_per_class: [0.548, 0.036, 0.741, 0.042, 0.133, 0.124, 0.031, 0.256, 0.098, 0.065]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:24:58 (running for 00:17:16.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.095 |      0.295 |                   82 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.899 |      0.287 |                   26 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.729 |      0.06  |                    3 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.872 |      0.207 |                    2 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3414179104477612
[2m[36m(func pid=108399)[0m top5: 0.8470149253731343
[2m[36m(func pid=108399)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=108399)[0m f1_macro: 0.2946706826308774
[2m[36m(func pid=108399)[0m f1_weighted: 0.35722503073057893
[2m[36m(func pid=108399)[0m f1_per_class: [0.496, 0.122, 0.489, 0.561, 0.188, 0.173, 0.415, 0.266, 0.158, 0.078]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7930 | Steps: 4 | Val loss: 2.3792 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8452 | Steps: 4 | Val loss: 1.8494 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.0754 | Steps: 4 | Val loss: 4.1627 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3447 | Steps: 4 | Val loss: 3.7060 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=121255)[0m top1: 0.32742537313432835
[2m[36m(func pid=121255)[0m top5: 0.8857276119402985
[2m[36m(func pid=121255)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=121255)[0m f1_macro: 0.2973099368299551
[2m[36m(func pid=121255)[0m f1_weighted: 0.2906171060902725
[2m[36m(func pid=121255)[0m f1_per_class: [0.466, 0.394, 0.714, 0.564, 0.175, 0.205, 0.033, 0.263, 0.0, 0.159]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m top1: 0.11753731343283583
[2m[36m(func pid=127193)[0m top5: 0.5536380597014925
[2m[36m(func pid=127193)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=127193)[0m f1_macro: 0.10665135953908189
[2m[36m(func pid=127193)[0m f1_weighted: 0.10923565845641386
[2m[36m(func pid=127193)[0m f1_per_class: [0.129, 0.145, 0.286, 0.01, 0.022, 0.136, 0.193, 0.0, 0.1, 0.047]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.1259328358208955
[2m[36m(func pid=127722)[0m top5: 0.40951492537313433
[2m[36m(func pid=127722)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=127722)[0m f1_macro: 0.18085371792792398
[2m[36m(func pid=127722)[0m f1_weighted: 0.08557862667024285
[2m[36m(func pid=127722)[0m f1_per_class: [0.614, 0.348, 0.348, 0.0, 0.222, 0.0, 0.0, 0.133, 0.0, 0.144]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:04 (running for 00:17:21.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.345 |      0.304 |                   83 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.845 |      0.297 |                   27 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.793 |      0.107 |                    4 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.075 |      0.181 |                    3 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.31529850746268656
[2m[36m(func pid=108399)[0m top5: 0.875
[2m[36m(func pid=108399)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=108399)[0m f1_macro: 0.30419289037905556
[2m[36m(func pid=108399)[0m f1_weighted: 0.364489234605879
[2m[36m(func pid=108399)[0m f1_per_class: [0.52, 0.215, 0.512, 0.506, 0.1, 0.282, 0.389, 0.308, 0.151, 0.059]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.7486 | Steps: 4 | Val loss: 1.8183 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.5845 | Steps: 4 | Val loss: 2.1194 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4470 | Steps: 4 | Val loss: 2.4035 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.6670 | Steps: 4 | Val loss: 3.3891 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=127193)[0m top1: 0.23274253731343283
[2m[36m(func pid=127193)[0m top5: 0.7467350746268657
[2m[36m(func pid=127193)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=127193)[0m f1_macro: 0.2051553583652516
[2m[36m(func pid=127193)[0m f1_weighted: 0.21620000183941002
[2m[36m(func pid=127193)[0m f1_per_class: [0.273, 0.371, 0.621, 0.065, 0.014, 0.105, 0.362, 0.0, 0.098, 0.143]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3316231343283582
[2m[36m(func pid=121255)[0m top5: 0.894589552238806
[2m[36m(func pid=121255)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=121255)[0m f1_macro: 0.3057829836282258
[2m[36m(func pid=121255)[0m f1_weighted: 0.29592805552138735
[2m[36m(func pid=121255)[0m f1_per_class: [0.491, 0.369, 0.71, 0.577, 0.183, 0.208, 0.045, 0.266, 0.045, 0.164]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.26072761194029853
[2m[36m(func pid=127722)[0m top5: 0.7486007462686567
[2m[36m(func pid=127722)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=127722)[0m f1_macro: 0.21965087189586993
[2m[36m(func pid=127722)[0m f1_weighted: 0.23745437345245757
[2m[36m(func pid=127722)[0m f1_per_class: [0.402, 0.472, 0.088, 0.103, 0.119, 0.215, 0.237, 0.282, 0.129, 0.148]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:09 (running for 00:17:26.81)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.667 |      0.295 |                   84 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.749 |      0.306 |                   28 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.585 |      0.205 |                    5 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.447 |      0.22  |                    4 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3474813432835821
[2m[36m(func pid=108399)[0m top5: 0.840018656716418
[2m[36m(func pid=108399)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=108399)[0m f1_macro: 0.2946462855783948
[2m[36m(func pid=108399)[0m f1_weighted: 0.36779206403679965
[2m[36m(func pid=108399)[0m f1_per_class: [0.468, 0.46, 0.205, 0.451, 0.095, 0.33, 0.292, 0.321, 0.197, 0.128]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6815 | Steps: 4 | Val loss: 1.8967 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.6218 | Steps: 4 | Val loss: 1.7836 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.9509 | Steps: 4 | Val loss: 2.3347 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.9069 | Steps: 4 | Val loss: 4.0501 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121255)[0m top1: 0.3460820895522388
[2m[36m(func pid=121255)[0m top5: 0.9043843283582089
[2m[36m(func pid=121255)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=121255)[0m f1_macro: 0.30793420785490294
[2m[36m(func pid=121255)[0m f1_weighted: 0.31437986019234704
[2m[36m(func pid=121255)[0m f1_per_class: [0.5, 0.36, 0.632, 0.59, 0.157, 0.217, 0.094, 0.269, 0.065, 0.196]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m top1: 0.30783582089552236
[2m[36m(func pid=127193)[0m top5: 0.8446828358208955
[2m[36m(func pid=127193)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=127193)[0m f1_macro: 0.2476792925100222
[2m[36m(func pid=127193)[0m f1_weighted: 0.2740332246534212
[2m[36m(func pid=127193)[0m f1_per_class: [0.519, 0.418, 0.609, 0.347, 0.017, 0.064, 0.272, 0.0, 0.0, 0.233]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.4239738805970149
[2m[36m(func pid=127722)[0m top5: 0.9057835820895522
[2m[36m(func pid=127722)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=127722)[0m f1_macro: 0.330652083297019
[2m[36m(func pid=127722)[0m f1_weighted: 0.3708978840657028
[2m[36m(func pid=127722)[0m f1_per_class: [0.651, 0.092, 0.649, 0.595, 0.241, 0.164, 0.478, 0.0, 0.17, 0.267]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:14 (running for 00:17:32.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.907 |      0.297 |                   85 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.622 |      0.308 |                   29 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.681 |      0.248 |                    6 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.951 |      0.331 |                    5 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.31576492537313433
[2m[36m(func pid=108399)[0m top5: 0.7798507462686567
[2m[36m(func pid=108399)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=108399)[0m f1_macro: 0.297354955874509
[2m[36m(func pid=108399)[0m f1_weighted: 0.28863515286760294
[2m[36m(func pid=108399)[0m f1_per_class: [0.474, 0.52, 0.169, 0.313, 0.1, 0.277, 0.13, 0.305, 0.195, 0.491]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4997 | Steps: 4 | Val loss: 1.7840 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.1530 | Steps: 4 | Val loss: 1.6971 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.6392 | Steps: 4 | Val loss: 2.9917 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.6449 | Steps: 4 | Val loss: 3.9958 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=127193)[0m top1: 0.4001865671641791
[2m[36m(func pid=127193)[0m top5: 0.8833955223880597
[2m[36m(func pid=127193)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=127193)[0m f1_macro: 0.30300013851900964
[2m[36m(func pid=127193)[0m f1_weighted: 0.3784259872484408
[2m[36m(func pid=127193)[0m f1_per_class: [0.5, 0.483, 0.759, 0.521, 0.096, 0.09, 0.41, 0.0, 0.0, 0.171]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.33722014925373134
[2m[36m(func pid=121255)[0m top5: 0.9057835820895522
[2m[36m(func pid=121255)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=121255)[0m f1_macro: 0.28006101958191015
[2m[36m(func pid=121255)[0m f1_weighted: 0.3147258782346321
[2m[36m(func pid=121255)[0m f1_per_class: [0.52, 0.306, 0.375, 0.59, 0.146, 0.208, 0.134, 0.265, 0.059, 0.196]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2966417910447761
[2m[36m(func pid=127722)[0m top5: 0.8563432835820896
[2m[36m(func pid=127722)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=127722)[0m f1_macro: 0.3256295654686168
[2m[36m(func pid=127722)[0m f1_weighted: 0.3029633105325605
[2m[36m(func pid=127722)[0m f1_per_class: [0.691, 0.169, 0.759, 0.513, 0.208, 0.266, 0.241, 0.0, 0.166, 0.243]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:19 (running for 00:17:37.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.645 |      0.33  |                   86 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.5   |      0.28  |                   30 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  2.153 |      0.303 |                    7 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.639 |      0.326 |                    6 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3353544776119403
[2m[36m(func pid=108399)[0m top5: 0.8306902985074627
[2m[36m(func pid=108399)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=108399)[0m f1_macro: 0.3295273597830404
[2m[36m(func pid=108399)[0m f1_weighted: 0.301294766645029
[2m[36m(func pid=108399)[0m f1_per_class: [0.529, 0.488, 0.429, 0.256, 0.118, 0.206, 0.257, 0.328, 0.206, 0.478]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.7492 | Steps: 4 | Val loss: 1.6934 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4877 | Steps: 4 | Val loss: 1.7734 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.8500 | Steps: 4 | Val loss: 3.2486 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.1236 | Steps: 4 | Val loss: 3.0699 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=127193)[0m top1: 0.4006529850746269
[2m[36m(func pid=127193)[0m top5: 0.917910447761194
[2m[36m(func pid=127193)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=127193)[0m f1_macro: 0.2944426189702251
[2m[36m(func pid=127193)[0m f1_weighted: 0.40393344791304703
[2m[36m(func pid=127193)[0m f1_per_class: [0.493, 0.414, 0.353, 0.575, 0.12, 0.098, 0.428, 0.327, 0.0, 0.138]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.33348880597014924
[2m[36m(func pid=121255)[0m top5: 0.8973880597014925
[2m[36m(func pid=121255)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=121255)[0m f1_macro: 0.268018299154853
[2m[36m(func pid=121255)[0m f1_weighted: 0.3219912807784239
[2m[36m(func pid=121255)[0m f1_per_class: [0.5, 0.256, 0.19, 0.586, 0.13, 0.206, 0.187, 0.286, 0.111, 0.227]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3208955223880597
[2m[36m(func pid=127722)[0m top5: 0.7583955223880597
[2m[36m(func pid=127722)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=127722)[0m f1_macro: 0.3338138603704225
[2m[36m(func pid=127722)[0m f1_weighted: 0.3427490323414058
[2m[36m(func pid=127722)[0m f1_per_class: [0.413, 0.489, 0.733, 0.437, 0.089, 0.191, 0.239, 0.341, 0.259, 0.147]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:25 (running for 00:17:42.71)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.124 |      0.346 |                   87 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.488 |      0.268 |                   31 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.749 |      0.294 |                    8 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.85  |      0.334 |                    7 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3824626865671642
[2m[36m(func pid=108399)[0m top5: 0.8675373134328358
[2m[36m(func pid=108399)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=108399)[0m f1_macro: 0.34620551748831574
[2m[36m(func pid=108399)[0m f1_weighted: 0.3983868397171418
[2m[36m(func pid=108399)[0m f1_per_class: [0.538, 0.427, 0.512, 0.562, 0.157, 0.113, 0.378, 0.32, 0.153, 0.301]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.3382 | Steps: 4 | Val loss: 1.9190 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6315 | Steps: 4 | Val loss: 1.7718 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.1802 | Steps: 4 | Val loss: 4.1397 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.4805 | Steps: 4 | Val loss: 3.7225 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=127193)[0m top1: 0.26865671641791045
[2m[36m(func pid=127193)[0m top5: 0.8283582089552238
[2m[36m(func pid=127193)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=127193)[0m f1_macro: 0.21643850282549648
[2m[36m(func pid=127193)[0m f1_weighted: 0.25095326511801347
[2m[36m(func pid=127193)[0m f1_per_class: [0.538, 0.152, 0.203, 0.574, 0.108, 0.078, 0.086, 0.238, 0.058, 0.128]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3316231343283582
[2m[36m(func pid=121255)[0m top5: 0.8927238805970149
[2m[36m(func pid=121255)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=121255)[0m f1_macro: 0.2702886856752551
[2m[36m(func pid=121255)[0m f1_weighted: 0.33496421740025933
[2m[36m(func pid=121255)[0m f1_per_class: [0.558, 0.246, 0.139, 0.57, 0.109, 0.193, 0.252, 0.304, 0.095, 0.237]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3255597014925373
[2m[36m(func pid=127722)[0m top5: 0.7509328358208955
[2m[36m(func pid=127722)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=127722)[0m f1_macro: 0.3250426907606935
[2m[36m(func pid=127722)[0m f1_weighted: 0.31537316713894226
[2m[36m(func pid=127722)[0m f1_per_class: [0.153, 0.46, 0.733, 0.548, 0.103, 0.086, 0.1, 0.403, 0.203, 0.462]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:30 (running for 00:17:47.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.481 |      0.321 |                   88 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.631 |      0.27  |                   32 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.338 |      0.216 |                    9 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.18  |      0.325 |                    8 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.35634328358208955
[2m[36m(func pid=108399)[0m top5: 0.8414179104477612
[2m[36m(func pid=108399)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=108399)[0m f1_macro: 0.32054846551582994
[2m[36m(func pid=108399)[0m f1_weighted: 0.3656100535853847
[2m[36m(func pid=108399)[0m f1_per_class: [0.562, 0.144, 0.733, 0.586, 0.116, 0.053, 0.44, 0.275, 0.189, 0.108]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.3500 | Steps: 4 | Val loss: 2.2387 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.4729 | Steps: 4 | Val loss: 1.7758 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8784 | Steps: 4 | Val loss: 3.1296 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.8234 | Steps: 4 | Val loss: 4.2186 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=127193)[0m top1: 0.1646455223880597
[2m[36m(func pid=127193)[0m top5: 0.7453358208955224
[2m[36m(func pid=127193)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=127193)[0m f1_macro: 0.17184217263619236
[2m[36m(func pid=127193)[0m f1_weighted: 0.14271155689025314
[2m[36m(func pid=127193)[0m f1_per_class: [0.537, 0.047, 0.171, 0.365, 0.094, 0.015, 0.006, 0.174, 0.132, 0.178]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3376865671641791
[2m[36m(func pid=121255)[0m top5: 0.8857276119402985
[2m[36m(func pid=121255)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=121255)[0m f1_macro: 0.2684179733182078
[2m[36m(func pid=121255)[0m f1_weighted: 0.3586005716722163
[2m[36m(func pid=121255)[0m f1_per_class: [0.519, 0.236, 0.11, 0.562, 0.107, 0.167, 0.355, 0.322, 0.083, 0.222]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.40951492537313433
[2m[36m(func pid=127722)[0m top5: 0.8997201492537313
[2m[36m(func pid=127722)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=127722)[0m f1_macro: 0.3865199911349195
[2m[36m(func pid=127722)[0m f1_weighted: 0.3995447875181483
[2m[36m(func pid=127722)[0m f1_per_class: [0.693, 0.499, 0.387, 0.609, 0.237, 0.288, 0.203, 0.38, 0.149, 0.419]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:35 (running for 00:17:53.15)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.823 |      0.274 |                   89 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.473 |      0.268 |                   33 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.35  |      0.172 |                   10 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.878 |      0.387 |                    9 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.302705223880597
[2m[36m(func pid=108399)[0m top5: 0.7840485074626866
[2m[36m(func pid=108399)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=108399)[0m f1_macro: 0.27415413892299273
[2m[36m(func pid=108399)[0m f1_weighted: 0.3268538453353573
[2m[36m(func pid=108399)[0m f1_per_class: [0.271, 0.094, 0.688, 0.524, 0.087, 0.047, 0.412, 0.318, 0.199, 0.103]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.1895 | Steps: 4 | Val loss: 2.1401 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3898 | Steps: 4 | Val loss: 1.7840 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1367 | Steps: 4 | Val loss: 2.9115 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.5286 | Steps: 4 | Val loss: 5.4430 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=127193)[0m top1: 0.22154850746268656
[2m[36m(func pid=127193)[0m top5: 0.7677238805970149
[2m[36m(func pid=127193)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=127193)[0m f1_macro: 0.2066092546653581
[2m[36m(func pid=127193)[0m f1_weighted: 0.20681848403778644
[2m[36m(func pid=127193)[0m f1_per_class: [0.444, 0.042, 0.159, 0.477, 0.101, 0.041, 0.099, 0.237, 0.12, 0.347]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.47341417910447764
[2m[36m(func pid=127722)[0m top5: 0.9379664179104478
[2m[36m(func pid=127722)[0m f1_micro: 0.47341417910447764
[2m[36m(func pid=127722)[0m f1_macro: 0.31788639069493346
[2m[36m(func pid=127722)[0m f1_weighted: 0.48484955016028386
[2m[36m(func pid=127722)[0m f1_per_class: [0.128, 0.432, 0.209, 0.579, 0.176, 0.284, 0.617, 0.344, 0.132, 0.278]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3423507462686567
[2m[36m(func pid=121255)[0m top5: 0.8889925373134329
[2m[36m(func pid=121255)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=121255)[0m f1_macro: 0.27560457787950116
[2m[36m(func pid=121255)[0m f1_weighted: 0.37939955893133376
[2m[36m(func pid=121255)[0m f1_per_class: [0.532, 0.267, 0.093, 0.515, 0.108, 0.162, 0.452, 0.324, 0.1, 0.205]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:25:40 (running for 00:17:58.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.529 |      0.21  |                   90 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.39  |      0.276 |                   34 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.189 |      0.207 |                   11 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.137 |      0.318 |                   10 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.2196828358208955
[2m[36m(func pid=108399)[0m top5: 0.6772388059701493
[2m[36m(func pid=108399)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=108399)[0m f1_macro: 0.21013159818423327
[2m[36m(func pid=108399)[0m f1_weighted: 0.20526422585079954
[2m[36m(func pid=108399)[0m f1_per_class: [0.305, 0.157, 0.356, 0.446, 0.074, 0.042, 0.06, 0.231, 0.164, 0.265]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.1139 | Steps: 4 | Val loss: 1.8291 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2007 | Steps: 4 | Val loss: 3.1395 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3029 | Steps: 4 | Val loss: 1.7958 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.8814 | Steps: 4 | Val loss: 4.4723 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=127193)[0m top1: 0.3451492537313433
[2m[36m(func pid=127193)[0m top5: 0.8530783582089553
[2m[36m(func pid=127193)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=127193)[0m f1_macro: 0.2856592646590814
[2m[36m(func pid=127193)[0m f1_weighted: 0.3565349255053643
[2m[36m(func pid=127193)[0m f1_per_class: [0.376, 0.1, 0.293, 0.572, 0.099, 0.077, 0.444, 0.333, 0.122, 0.441]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.46408582089552236
[2m[36m(func pid=127722)[0m top5: 0.9305037313432836
[2m[36m(func pid=127722)[0m f1_micro: 0.46408582089552236
[2m[36m(func pid=127722)[0m f1_macro: 0.32987044379368746
[2m[36m(func pid=127722)[0m f1_weighted: 0.4804248790263958
[2m[36m(func pid=127722)[0m f1_per_class: [0.044, 0.491, 0.205, 0.572, 0.26, 0.287, 0.582, 0.284, 0.139, 0.435]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:25:46 (running for 00:18:03.49)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.529 |      0.21  |                   90 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.303 |      0.283 |                   35 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.114 |      0.286 |                   12 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.201 |      0.33  |                   11 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m top1: 0.34375
[2m[36m(func pid=121255)[0m top5: 0.8773320895522388
[2m[36m(func pid=121255)[0m f1_micro: 0.34375
[2m[36m(func pid=121255)[0m f1_macro: 0.28298309310487746
[2m[36m(func pid=121255)[0m f1_weighted: 0.3850137884921727
[2m[36m(func pid=121255)[0m f1_per_class: [0.561, 0.308, 0.09, 0.501, 0.104, 0.135, 0.465, 0.319, 0.138, 0.209]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=108399)[0m top1: 0.2826492537313433
[2m[36m(func pid=108399)[0m top5: 0.7588619402985075
[2m[36m(func pid=108399)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=108399)[0m f1_macro: 0.2759760917056956
[2m[36m(func pid=108399)[0m f1_weighted: 0.2783934167053534
[2m[36m(func pid=108399)[0m f1_per_class: [0.459, 0.29, 0.293, 0.545, 0.101, 0.071, 0.096, 0.313, 0.133, 0.459]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.2011 | Steps: 4 | Val loss: 1.6179 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8832 | Steps: 4 | Val loss: 4.2132 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.3680 | Steps: 4 | Val loss: 1.8047 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2107 | Steps: 4 | Val loss: 2.8448 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127193)[0m top1: 0.42630597014925375
[2m[36m(func pid=127193)[0m top5: 0.9104477611940298
[2m[36m(func pid=127193)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=127193)[0m f1_macro: 0.33768579149686684
[2m[36m(func pid=127193)[0m f1_weighted: 0.4479182558247961
[2m[36m(func pid=127193)[0m f1_per_class: [0.374, 0.366, 0.381, 0.584, 0.102, 0.106, 0.567, 0.35, 0.165, 0.382]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:25:51 (running for 00:18:08.79)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.881 |      0.276 |                   91 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.368 |      0.29  |                   36 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.201 |      0.338 |                   13 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.201 |      0.33  |                   11 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m top1: 0.3451492537313433
[2m[36m(func pid=121255)[0m top5: 0.8745335820895522
[2m[36m(func pid=121255)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=121255)[0m f1_macro: 0.2899452150015748
[2m[36m(func pid=121255)[0m f1_weighted: 0.38817159825280084
[2m[36m(func pid=121255)[0m f1_per_class: [0.554, 0.343, 0.106, 0.507, 0.101, 0.161, 0.436, 0.331, 0.145, 0.215]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.39365671641791045
[2m[36m(func pid=127722)[0m top5: 0.8801305970149254
[2m[36m(func pid=127722)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=127722)[0m f1_macro: 0.35987633240401207
[2m[36m(func pid=127722)[0m f1_weighted: 0.39543149749704976
[2m[36m(func pid=127722)[0m f1_per_class: [0.475, 0.495, 0.453, 0.584, 0.146, 0.184, 0.263, 0.429, 0.146, 0.426]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=108399)[0m top1: 0.42490671641791045
[2m[36m(func pid=108399)[0m top5: 0.9118470149253731
[2m[36m(func pid=108399)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=108399)[0m f1_macro: 0.3986471975283694
[2m[36m(func pid=108399)[0m f1_weighted: 0.4217729691181101
[2m[36m(func pid=108399)[0m f1_per_class: [0.591, 0.432, 0.478, 0.596, 0.237, 0.305, 0.323, 0.369, 0.205, 0.452]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7950 | Steps: 4 | Val loss: 1.5162 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.2615 | Steps: 4 | Val loss: 1.8012 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6664 | Steps: 4 | Val loss: 7.8823 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0716 | Steps: 4 | Val loss: 2.7569 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=127193)[0m top1: 0.48134328358208955
[2m[36m(func pid=127193)[0m top5: 0.9165111940298507
[2m[36m(func pid=127193)[0m f1_micro: 0.48134328358208955
[2m[36m(func pid=127193)[0m f1_macro: 0.3817053607089262
[2m[36m(func pid=127193)[0m f1_weighted: 0.4745683720739722
[2m[36m(func pid=127193)[0m f1_per_class: [0.551, 0.582, 0.436, 0.479, 0.127, 0.164, 0.604, 0.234, 0.278, 0.361]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:25:56 (running for 00:18:14.25)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.211 |      0.399 |                   92 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.262 |      0.29  |                   37 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.795 |      0.382 |                   14 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.883 |      0.36  |                   12 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m top1: 0.3451492537313433
[2m[36m(func pid=121255)[0m top5: 0.8745335820895522
[2m[36m(func pid=121255)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=121255)[0m f1_macro: 0.29026046140520645
[2m[36m(func pid=121255)[0m f1_weighted: 0.3858085820082202
[2m[36m(func pid=121255)[0m f1_per_class: [0.538, 0.337, 0.143, 0.454, 0.12, 0.161, 0.484, 0.326, 0.143, 0.197]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3031716417910448
[2m[36m(func pid=127722)[0m top5: 0.7084888059701493
[2m[36m(func pid=127722)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=127722)[0m f1_macro: 0.32397998768235536
[2m[36m(func pid=127722)[0m f1_weighted: 0.2738656839982642
[2m[36m(func pid=127722)[0m f1_per_class: [0.541, 0.365, 0.71, 0.553, 0.134, 0.061, 0.009, 0.391, 0.12, 0.356]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=108399)[0m top1: 0.417910447761194
[2m[36m(func pid=108399)[0m top5: 0.9361007462686567
[2m[36m(func pid=108399)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=108399)[0m f1_macro: 0.3797642046040183
[2m[36m(func pid=108399)[0m f1_weighted: 0.43315435116717405
[2m[36m(func pid=108399)[0m f1_per_class: [0.653, 0.454, 0.595, 0.552, 0.143, 0.3, 0.427, 0.181, 0.237, 0.256]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.0738 | Steps: 4 | Val loss: 1.8460 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.7896 | Steps: 4 | Val loss: 10.7894 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.0774 | Steps: 4 | Val loss: 3.1005 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.3058 | Steps: 4 | Val loss: 1.8511 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=127193)[0m top1: 0.39132462686567165
[2m[36m(func pid=127193)[0m top5: 0.9011194029850746
[2m[36m(func pid=127193)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=127193)[0m f1_macro: 0.3728697803158173
[2m[36m(func pid=127193)[0m f1_weighted: 0.36326096353101117
[2m[36m(func pid=127193)[0m f1_per_class: [0.66, 0.463, 0.611, 0.249, 0.141, 0.29, 0.464, 0.174, 0.295, 0.381]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:02 (running for 00:18:19.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.072 |      0.38  |                   93 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.262 |      0.29  |                   37 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.074 |      0.373 |                   15 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.79  |      0.274 |                   14 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127722)[0m top1: 0.2593283582089552
[2m[36m(func pid=127722)[0m top5: 0.6058768656716418
[2m[36m(func pid=127722)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=127722)[0m f1_macro: 0.2735421119142086
[2m[36m(func pid=127722)[0m f1_weighted: 0.2517198327917564
[2m[36m(func pid=127722)[0m f1_per_class: [0.222, 0.405, 0.6, 0.496, 0.062, 0.036, 0.0, 0.38, 0.13, 0.405]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3885261194029851
[2m[36m(func pid=108399)[0m top5: 0.9193097014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=108399)[0m f1_macro: 0.3699975687221955
[2m[36m(func pid=108399)[0m f1_weighted: 0.4112508296604465
[2m[36m(func pid=108399)[0m f1_per_class: [0.66, 0.475, 0.647, 0.413, 0.169, 0.28, 0.485, 0.128, 0.291, 0.151]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.32975746268656714
[2m[36m(func pid=121255)[0m top5: 0.8544776119402985
[2m[36m(func pid=121255)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=121255)[0m f1_macro: 0.28840906277107276
[2m[36m(func pid=121255)[0m f1_weighted: 0.36727241897517726
[2m[36m(func pid=121255)[0m f1_per_class: [0.532, 0.376, 0.167, 0.373, 0.129, 0.165, 0.471, 0.326, 0.161, 0.185]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7357 | Steps: 4 | Val loss: 1.9367 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.3214 | Steps: 4 | Val loss: 11.8332 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.4737 | Steps: 4 | Val loss: 3.3858 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.1800 | Steps: 4 | Val loss: 1.8648 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127193)[0m top1: 0.36100746268656714
[2m[36m(func pid=127193)[0m top5: 0.8815298507462687
[2m[36m(func pid=127193)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=127193)[0m f1_macro: 0.36033457425332344
[2m[36m(func pid=127193)[0m f1_weighted: 0.332700671205426
[2m[36m(func pid=127193)[0m f1_per_class: [0.646, 0.481, 0.595, 0.286, 0.182, 0.287, 0.301, 0.302, 0.269, 0.255]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:07 (running for 00:18:24.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.077 |      0.37  |                   94 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.306 |      0.288 |                   38 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.736 |      0.36  |                   16 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.321 |      0.241 |                   15 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127722)[0m top1: 0.23227611940298507
[2m[36m(func pid=127722)[0m top5: 0.5904850746268657
[2m[36m(func pid=127722)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=127722)[0m f1_macro: 0.2411035105300079
[2m[36m(func pid=127722)[0m f1_weighted: 0.2303030662709263
[2m[36m(func pid=127722)[0m f1_per_class: [0.13, 0.418, 0.526, 0.416, 0.065, 0.044, 0.003, 0.378, 0.151, 0.28]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=108399)[0m top1: 0.3712686567164179
[2m[36m(func pid=108399)[0m top5: 0.8997201492537313
[2m[36m(func pid=108399)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=108399)[0m f1_macro: 0.35007390377939623
[2m[36m(func pid=108399)[0m f1_weighted: 0.3842296309925392
[2m[36m(func pid=108399)[0m f1_per_class: [0.667, 0.446, 0.606, 0.303, 0.206, 0.26, 0.53, 0.094, 0.288, 0.101]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.32742537313432835
[2m[36m(func pid=121255)[0m top5: 0.8470149253731343
[2m[36m(func pid=121255)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=121255)[0m f1_macro: 0.2894272559253018
[2m[36m(func pid=121255)[0m f1_weighted: 0.3654244521305187
[2m[36m(func pid=121255)[0m f1_per_class: [0.568, 0.388, 0.185, 0.381, 0.122, 0.145, 0.457, 0.327, 0.159, 0.164]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8941 | Steps: 4 | Val loss: 2.1453 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6615 | Steps: 4 | Val loss: 9.0225 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4489 | Steps: 4 | Val loss: 3.5151 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0558 | Steps: 4 | Val loss: 1.8430 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127193)[0m top1: 0.34888059701492535
[2m[36m(func pid=127193)[0m top5: 0.820429104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=127193)[0m f1_macro: 0.3295733160303967
[2m[36m(func pid=127193)[0m f1_weighted: 0.3251029552156642
[2m[36m(func pid=127193)[0m f1_per_class: [0.602, 0.539, 0.407, 0.388, 0.179, 0.276, 0.159, 0.328, 0.254, 0.166]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:12 (running for 00:18:30.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.474 |      0.35  |                   95 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.18  |      0.289 |                   39 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.894 |      0.33  |                   17 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.661 |      0.295 |                   16 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127722)[0m top1: 0.3218283582089552
[2m[36m(func pid=127722)[0m top5: 0.675839552238806
[2m[36m(func pid=127722)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=127722)[0m f1_macro: 0.2949510106664471
[2m[36m(func pid=127722)[0m f1_weighted: 0.2864985532000132
[2m[36m(func pid=127722)[0m f1_per_class: [0.295, 0.496, 0.649, 0.509, 0.1, 0.101, 0.022, 0.386, 0.164, 0.228]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=108399)[0m top1: 0.41044776119402987
[2m[36m(func pid=108399)[0m top5: 0.8903917910447762
[2m[36m(func pid=108399)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=108399)[0m f1_macro: 0.3327554401887251
[2m[36m(func pid=108399)[0m f1_weighted: 0.38114067035357924
[2m[36m(func pid=108399)[0m f1_per_class: [0.692, 0.359, 0.629, 0.378, 0.172, 0.094, 0.575, 0.067, 0.222, 0.141]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=121255)[0m top1: 0.34048507462686567
[2m[36m(func pid=121255)[0m top5: 0.8498134328358209
[2m[36m(func pid=121255)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=121255)[0m f1_macro: 0.30992820041815705
[2m[36m(func pid=121255)[0m f1_weighted: 0.37090193266696936
[2m[36m(func pid=121255)[0m f1_per_class: [0.592, 0.415, 0.308, 0.367, 0.138, 0.136, 0.467, 0.337, 0.174, 0.165]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4648 | Steps: 4 | Val loss: 2.3905 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6601 | Steps: 4 | Val loss: 7.1395 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.1006 | Steps: 4 | Val loss: 2.9904 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.1148 | Steps: 4 | Val loss: 1.8553 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=127193)[0m top1: 0.31669776119402987
[2m[36m(func pid=127193)[0m top5: 0.7845149253731343
[2m[36m(func pid=127193)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=127193)[0m f1_macro: 0.3108716913136198
[2m[36m(func pid=127193)[0m f1_weighted: 0.3215531288086809
[2m[36m(func pid=127193)[0m f1_per_class: [0.598, 0.479, 0.324, 0.521, 0.114, 0.237, 0.063, 0.394, 0.263, 0.115]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:18 (running for 00:18:35.55)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.449 |      0.333 |                   96 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.056 |      0.31  |                   40 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.465 |      0.311 |                   18 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.66  |      0.305 |                   17 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.4141791044776119
[2m[36m(func pid=108399)[0m top5: 0.8838619402985075
[2m[36m(func pid=108399)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=108399)[0m f1_macro: 0.37211909458962017
[2m[36m(func pid=108399)[0m f1_weighted: 0.4181288416005886
[2m[36m(func pid=108399)[0m f1_per_class: [0.667, 0.401, 0.629, 0.587, 0.21, 0.054, 0.444, 0.317, 0.221, 0.193]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3493470149253731
[2m[36m(func pid=127722)[0m top5: 0.7588619402985075
[2m[36m(func pid=127722)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=127722)[0m f1_macro: 0.30513064556992037
[2m[36m(func pid=127722)[0m f1_weighted: 0.34803381028603736
[2m[36m(func pid=127722)[0m f1_per_class: [0.507, 0.487, 0.338, 0.521, 0.159, 0.129, 0.206, 0.384, 0.144, 0.176]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.33348880597014924
[2m[36m(func pid=121255)[0m top5: 0.8418843283582089
[2m[36m(func pid=121255)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=121255)[0m f1_macro: 0.3089285130600358
[2m[36m(func pid=121255)[0m f1_weighted: 0.35226809338250165
[2m[36m(func pid=121255)[0m f1_per_class: [0.558, 0.442, 0.333, 0.324, 0.159, 0.153, 0.423, 0.339, 0.185, 0.173]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4260 | Steps: 4 | Val loss: 2.6497 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.0208 | Steps: 4 | Val loss: 6.0279 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.1204 | Steps: 4 | Val loss: 3.7500 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.1134 | Steps: 4 | Val loss: 1.8723 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=127193)[0m top1: 0.291044776119403
[2m[36m(func pid=127193)[0m top5: 0.7513992537313433
[2m[36m(func pid=127193)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=127193)[0m f1_macro: 0.27099370098266107
[2m[36m(func pid=127193)[0m f1_weighted: 0.2729994547774808
[2m[36m(func pid=127193)[0m f1_per_class: [0.562, 0.182, 0.333, 0.583, 0.105, 0.202, 0.04, 0.369, 0.214, 0.12]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:23 (running for 00:18:40.90)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.101 |      0.372 |                   97 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.115 |      0.309 |                   41 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.426 |      0.271 |                   19 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.021 |      0.324 |                   18 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.375
[2m[36m(func pid=108399)[0m top5: 0.8255597014925373
[2m[36m(func pid=108399)[0m f1_micro: 0.375
[2m[36m(func pid=108399)[0m f1_macro: 0.3410025000570581
[2m[36m(func pid=108399)[0m f1_weighted: 0.3370702147340748
[2m[36m(func pid=108399)[0m f1_per_class: [0.606, 0.404, 0.564, 0.594, 0.209, 0.064, 0.162, 0.337, 0.183, 0.288]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127722)[0m top1: 0.38199626865671643
[2m[36m(func pid=127722)[0m top5: 0.8465485074626866
[2m[36m(func pid=127722)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=127722)[0m f1_macro: 0.32433534483018894
[2m[36m(func pid=127722)[0m f1_weighted: 0.407299211283942
[2m[36m(func pid=127722)[0m f1_per_class: [0.604, 0.476, 0.16, 0.516, 0.203, 0.174, 0.392, 0.384, 0.174, 0.161]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.332089552238806
[2m[36m(func pid=121255)[0m top5: 0.8381529850746269
[2m[36m(func pid=121255)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=121255)[0m f1_macro: 0.31275603985639344
[2m[36m(func pid=121255)[0m f1_weighted: 0.3413552498514663
[2m[36m(func pid=121255)[0m f1_per_class: [0.533, 0.456, 0.393, 0.3, 0.146, 0.165, 0.391, 0.355, 0.202, 0.186]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.7270 | Steps: 4 | Val loss: 3.0414 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.6295 | Steps: 4 | Val loss: 3.7774 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5607 | Steps: 4 | Val loss: 5.9726 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.9289 | Steps: 4 | Val loss: 1.7862 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127193)[0m top1: 0.26865671641791045
[2m[36m(func pid=127193)[0m top5: 0.71875
[2m[36m(func pid=127193)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=127193)[0m f1_macro: 0.2399426298661763
[2m[36m(func pid=127193)[0m f1_weighted: 0.23135667686064967
[2m[36m(func pid=127193)[0m f1_per_class: [0.566, 0.047, 0.32, 0.593, 0.104, 0.134, 0.018, 0.268, 0.148, 0.202]
[2m[36m(func pid=127193)[0m 
== Status ==
Current time: 2024-01-07 11:26:28 (running for 00:18:46.17)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.629 |      0.332 |                   99 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.113 |      0.313 |                   42 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.727 |      0.24  |                   20 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.021 |      0.324 |                   18 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=108399)[0m top1: 0.3628731343283582
[2m[36m(func pid=108399)[0m top5: 0.8059701492537313
[2m[36m(func pid=108399)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=108399)[0m f1_macro: 0.33230354815352037
[2m[36m(func pid=108399)[0m f1_weighted: 0.3431177655910838
[2m[36m(func pid=108399)[0m f1_per_class: [0.584, 0.525, 0.468, 0.545, 0.139, 0.069, 0.161, 0.342, 0.166, 0.324]
[2m[36m(func pid=108399)[0m 
[2m[36m(func pid=127722)[0m top1: 0.4001865671641791
[2m[36m(func pid=127722)[0m top5: 0.8838619402985075
[2m[36m(func pid=127722)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=127722)[0m f1_macro: 0.33169125887162043
[2m[36m(func pid=127722)[0m f1_weighted: 0.43519804125608474
[2m[36m(func pid=127722)[0m f1_per_class: [0.639, 0.495, 0.087, 0.501, 0.275, 0.227, 0.489, 0.269, 0.155, 0.18]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3516791044776119
[2m[36m(func pid=121255)[0m top5: 0.8666044776119403
[2m[36m(func pid=121255)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=121255)[0m f1_macro: 0.33271178530069634
[2m[36m(func pid=121255)[0m f1_weighted: 0.35585757392394946
[2m[36m(func pid=121255)[0m f1_per_class: [0.564, 0.499, 0.471, 0.363, 0.163, 0.153, 0.358, 0.334, 0.207, 0.214]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7950 | Steps: 4 | Val loss: 3.2150 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=108399)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.8659 | Steps: 4 | Val loss: 4.9896 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2289 | Steps: 4 | Val loss: 6.2846 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.9347 | Steps: 4 | Val loss: 1.8201 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:26:33 (running for 00:18:51.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00006 | RUNNING    | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  0.629 |      0.332 |                   99 |
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.929 |      0.333 |                   43 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.795 |      0.258 |                   21 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.561 |      0.332 |                   19 |
| train_98a10_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.27425373134328357
[2m[36m(func pid=127193)[0m top5: 0.7374067164179104
[2m[36m(func pid=127193)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=127193)[0m f1_macro: 0.2577435368577323
[2m[36m(func pid=127193)[0m f1_weighted: 0.22461167583510055
[2m[36m(func pid=127193)[0m f1_per_class: [0.607, 0.021, 0.436, 0.609, 0.14, 0.062, 0.018, 0.248, 0.138, 0.298]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=108399)[0m top1: 0.292910447761194
[2m[36m(func pid=108399)[0m top5: 0.7341417910447762
[2m[36m(func pid=108399)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=108399)[0m f1_macro: 0.2912026218506819
[2m[36m(func pid=108399)[0m f1_weighted: 0.2543608984473307
[2m[36m(func pid=108399)[0m f1_per_class: [0.587, 0.459, 0.5, 0.282, 0.118, 0.079, 0.141, 0.35, 0.216, 0.178]
[2m[36m(func pid=127722)[0m top1: 0.43097014925373134
[2m[36m(func pid=127722)[0m top5: 0.9020522388059702
[2m[36m(func pid=127722)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=127722)[0m f1_macro: 0.35065492528817993
[2m[36m(func pid=127722)[0m f1_weighted: 0.4304035959556765
[2m[36m(func pid=127722)[0m f1_per_class: [0.571, 0.54, 0.206, 0.467, 0.095, 0.326, 0.438, 0.258, 0.169, 0.435]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.34421641791044777
[2m[36m(func pid=121255)[0m top5: 0.8484141791044776
[2m[36m(func pid=121255)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=121255)[0m f1_macro: 0.32306900119234544
[2m[36m(func pid=121255)[0m f1_weighted: 0.34262537438295415
[2m[36m(func pid=121255)[0m f1_per_class: [0.519, 0.497, 0.48, 0.401, 0.145, 0.152, 0.283, 0.326, 0.24, 0.188]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7741 | Steps: 4 | Val loss: 3.2137 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4598 | Steps: 4 | Val loss: 6.9376 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8853 | Steps: 4 | Val loss: 1.7993 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=127193)[0m top1: 0.28638059701492535
[2m[36m(func pid=127193)[0m top5: 0.7639925373134329
[2m[36m(func pid=127193)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=127193)[0m f1_macro: 0.2869681134913897
[2m[36m(func pid=127193)[0m f1_weighted: 0.23759458392132402
[2m[36m(func pid=127193)[0m f1_per_class: [0.614, 0.016, 0.453, 0.611, 0.182, 0.07, 0.046, 0.288, 0.118, 0.473]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3941231343283582
[2m[36m(func pid=127722)[0m top5: 0.909981343283582
[2m[36m(func pid=127722)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=127722)[0m f1_macro: 0.35725122689535665
[2m[36m(func pid=127722)[0m f1_weighted: 0.3938638896349507
[2m[36m(func pid=127722)[0m f1_per_class: [0.548, 0.54, 0.371, 0.465, 0.095, 0.319, 0.305, 0.333, 0.16, 0.435]
[2m[36m(func pid=121255)[0m top1: 0.3493470149253731
[2m[36m(func pid=121255)[0m top5: 0.8572761194029851
[2m[36m(func pid=121255)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=121255)[0m f1_macro: 0.3289219572710285
[2m[36m(func pid=121255)[0m f1_weighted: 0.34442429343621617
[2m[36m(func pid=121255)[0m f1_per_class: [0.519, 0.51, 0.512, 0.419, 0.126, 0.158, 0.258, 0.336, 0.251, 0.2]
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.6214 | Steps: 4 | Val loss: 2.9213 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 11:26:39 (running for 00:18:56.47)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.935 |      0.323 |                   44 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.774 |      0.287 |                   22 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.229 |      0.351 |                   20 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=132824)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=132824)[0m Configuration completed!
[2m[36m(func pid=132824)[0m New optimizer parameters:
[2m[36m(func pid=132824)[0m SGD (
[2m[36m(func pid=132824)[0m Parameter Group 0
[2m[36m(func pid=132824)[0m     dampening: 0
[2m[36m(func pid=132824)[0m     differentiable: False
[2m[36m(func pid=132824)[0m     foreach: None
[2m[36m(func pid=132824)[0m     lr: 0.1
[2m[36m(func pid=132824)[0m     maximize: False
[2m[36m(func pid=132824)[0m     momentum: 0.99
[2m[36m(func pid=132824)[0m     nesterov: False
[2m[36m(func pid=132824)[0m     weight_decay: 0.0001
[2m[36m(func pid=132824)[0m )
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:26:44 (running for 00:19:01.79)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.885 |      0.329 |                   45 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.621 |      0.29  |                   23 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.46  |      0.357 |                   21 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.292910447761194
[2m[36m(func pid=127193)[0m top5: 0.8311567164179104
[2m[36m(func pid=127193)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=127193)[0m f1_macro: 0.2896036722362785
[2m[36m(func pid=127193)[0m f1_weighted: 0.2670083778004021
[2m[36m(func pid=127193)[0m f1_per_class: [0.607, 0.027, 0.407, 0.608, 0.198, 0.083, 0.134, 0.327, 0.106, 0.4]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5379 | Steps: 4 | Val loss: 7.5403 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4012 | Steps: 4 | Val loss: 1.7698 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0249 | Steps: 4 | Val loss: 2.4060 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 7.9226 | Steps: 4 | Val loss: 12.5410 | Batch size: 32 | lr: 0.1 | Duration: 4.59s
[2m[36m(func pid=121255)[0m top1: 0.3474813432835821
[2m[36m(func pid=121255)[0m top5: 0.8544776119402985
[2m[36m(func pid=121255)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=121255)[0m f1_macro: 0.3315089003336912
[2m[36m(func pid=121255)[0m f1_weighted: 0.3452300842844152
[2m[36m(func pid=121255)[0m f1_per_class: [0.493, 0.499, 0.629, 0.419, 0.129, 0.163, 0.273, 0.315, 0.234, 0.163]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.35027985074626866
[2m[36m(func pid=127722)[0m top5: 0.9034514925373134
[2m[36m(func pid=127722)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=127722)[0m f1_macro: 0.387908651501221
[2m[36m(func pid=127722)[0m f1_weighted: 0.363185959137834
[2m[36m(func pid=127722)[0m f1_per_class: [0.703, 0.291, 0.585, 0.548, 0.367, 0.276, 0.262, 0.361, 0.156, 0.329]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:26:49 (running for 00:19:07.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.401 |      0.332 |                   46 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.025 |      0.305 |                   24 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.538 |      0.388 |                   22 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.3498134328358209
[2m[36m(func pid=127193)[0m top5: 0.8815298507462687
[2m[36m(func pid=127193)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=127193)[0m f1_macro: 0.3050654507710789
[2m[36m(func pid=127193)[0m f1_weighted: 0.3525687053913567
[2m[36m(func pid=127193)[0m f1_per_class: [0.554, 0.077, 0.3, 0.613, 0.182, 0.135, 0.373, 0.342, 0.104, 0.372]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.01166044776119403
[2m[36m(func pid=132824)[0m top5: 0.5013992537313433
[2m[36m(func pid=132824)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=132824)[0m f1_macro: 0.0023073373327180437
[2m[36m(func pid=132824)[0m f1_weighted: 0.0002690458643561152
[2m[36m(func pid=132824)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1847 | Steps: 4 | Val loss: 9.4996 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.9319 | Steps: 4 | Val loss: 1.8023 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3125 | Steps: 4 | Val loss: 1.8189 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 27.7978 | Steps: 4 | Val loss: 19.6784 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=127722)[0m top1: 0.30363805970149255
[2m[36m(func pid=127722)[0m top5: 0.8642723880597015
[2m[36m(func pid=127722)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=127722)[0m f1_macro: 0.34865473588392554
[2m[36m(func pid=127722)[0m f1_weighted: 0.3021597789816231
[2m[36m(func pid=127722)[0m f1_per_class: [0.723, 0.08, 0.759, 0.552, 0.308, 0.19, 0.222, 0.301, 0.155, 0.198]
[2m[36m(func pid=121255)[0m top1: 0.34468283582089554
[2m[36m(func pid=121255)[0m top5: 0.8446828358208955
[2m[36m(func pid=121255)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=121255)[0m f1_macro: 0.31920466967196226
[2m[36m(func pid=121255)[0m f1_weighted: 0.33521648209039184
[2m[36m(func pid=121255)[0m f1_per_class: [0.474, 0.505, 0.537, 0.444, 0.126, 0.138, 0.22, 0.325, 0.25, 0.173]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:26:54 (running for 00:19:12.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.932 |      0.319 |                   47 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.313 |      0.369 |                   25 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.185 |      0.349 |                   23 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  7.923 |      0.002 |                    1 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.45802238805970147
[2m[36m(func pid=127193)[0m top5: 0.9137126865671642
[2m[36m(func pid=127193)[0m f1_micro: 0.45802238805970147
[2m[36m(func pid=127193)[0m f1_macro: 0.3692302794063774
[2m[36m(func pid=127193)[0m f1_weighted: 0.46877686244962846
[2m[36m(func pid=127193)[0m f1_per_class: [0.55, 0.289, 0.271, 0.629, 0.214, 0.234, 0.578, 0.347, 0.143, 0.436]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.23180970149253732
[2m[36m(func pid=132824)[0m top5: 0.6529850746268657
[2m[36m(func pid=132824)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=132824)[0m f1_macro: 0.11878842061321722
[2m[36m(func pid=132824)[0m f1_weighted: 0.24006439225594411
[2m[36m(func pid=132824)[0m f1_per_class: [0.0, 0.0, 0.088, 0.509, 0.049, 0.0, 0.275, 0.267, 0.0, 0.0]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.2456 | Steps: 4 | Val loss: 11.5215 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1264 | Steps: 4 | Val loss: 1.7611 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2131 | Steps: 4 | Val loss: 1.6753 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 24.9478 | Steps: 4 | Val loss: 22.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=121255)[0m top1: 0.3530783582089552
[2m[36m(func pid=121255)[0m top5: 0.8652052238805971
[2m[36m(func pid=121255)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=121255)[0m f1_macro: 0.3264290775357612
[2m[36m(func pid=121255)[0m f1_weighted: 0.3489842244432235
[2m[36m(func pid=121255)[0m f1_per_class: [0.514, 0.518, 0.545, 0.476, 0.12, 0.114, 0.237, 0.315, 0.259, 0.167]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2756529850746269
[2m[36m(func pid=127722)[0m top5: 0.8148320895522388
[2m[36m(func pid=127722)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.29850842844646064
[2m[36m(func pid=127722)[0m f1_weighted: 0.26494408881775167
[2m[36m(func pid=127722)[0m f1_per_class: [0.625, 0.042, 0.72, 0.544, 0.153, 0.144, 0.16, 0.28, 0.169, 0.148]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:27:00 (running for 00:19:17.97)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  1.126 |      0.326 |                   48 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.213 |      0.38  |                   26 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.246 |      0.299 |                   24 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 27.798 |      0.119 |                    2 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.4962686567164179
[2m[36m(func pid=127193)[0m top5: 0.9342350746268657
[2m[36m(func pid=127193)[0m f1_micro: 0.4962686567164179
[2m[36m(func pid=127193)[0m f1_macro: 0.3799796071847066
[2m[36m(func pid=127193)[0m f1_weighted: 0.5045287734172542
[2m[36m(func pid=127193)[0m f1_per_class: [0.579, 0.48, 0.234, 0.605, 0.216, 0.273, 0.603, 0.301, 0.192, 0.317]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.18796641791044777
[2m[36m(func pid=132824)[0m top5: 0.757929104477612
[2m[36m(func pid=132824)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=132824)[0m f1_macro: 0.1686898872802883
[2m[36m(func pid=132824)[0m f1_weighted: 0.218350550120746
[2m[36m(func pid=132824)[0m f1_per_class: [0.068, 0.0, 0.375, 0.289, 0.219, 0.248, 0.332, 0.0, 0.156, 0.0]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8673 | Steps: 4 | Val loss: 1.7539 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.1735 | Steps: 4 | Val loss: 11.2358 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 33.9881 | Steps: 4 | Val loss: 18.1445 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3187 | Steps: 4 | Val loss: 1.7029 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127722)[0m top1: 0.25886194029850745
[2m[36m(func pid=127722)[0m top5: 0.8180970149253731
[2m[36m(func pid=127722)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=127722)[0m f1_macro: 0.2847363607759107
[2m[36m(func pid=127722)[0m f1_weighted: 0.2709405711651866
[2m[36m(func pid=127722)[0m f1_per_class: [0.593, 0.231, 0.5, 0.45, 0.098, 0.132, 0.166, 0.293, 0.168, 0.216]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3596082089552239
[2m[36m(func pid=121255)[0m top5: 0.8722014925373134
[2m[36m(func pid=121255)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=121255)[0m f1_macro: 0.32371960812000355
[2m[36m(func pid=121255)[0m f1_weighted: 0.360657394156137
[2m[36m(func pid=121255)[0m f1_per_class: [0.425, 0.507, 0.533, 0.513, 0.142, 0.127, 0.246, 0.331, 0.252, 0.16]
[2m[36m(func pid=121255)[0m 
== Status ==
Current time: 2024-01-07 11:27:05 (running for 00:19:23.12)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.867 |      0.324 |                   49 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.213 |      0.38  |                   26 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.173 |      0.285 |                   25 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 33.988 |      0.219 |                    4 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.365205223880597
[2m[36m(func pid=132824)[0m top5: 0.7523320895522388
[2m[36m(func pid=132824)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=132824)[0m f1_macro: 0.21885350291413905
[2m[36m(func pid=132824)[0m f1_weighted: 0.3390648166803038
[2m[36m(func pid=132824)[0m f1_per_class: [0.529, 0.0, 0.286, 0.545, 0.1, 0.236, 0.493, 0.0, 0.0, 0.0]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.498134328358209
[2m[36m(func pid=127193)[0m top5: 0.9384328358208955
[2m[36m(func pid=127193)[0m f1_micro: 0.498134328358209
[2m[36m(func pid=127193)[0m f1_macro: 0.38343916622631713
[2m[36m(func pid=127193)[0m f1_weighted: 0.5013069413974345
[2m[36m(func pid=127193)[0m f1_per_class: [0.565, 0.576, 0.28, 0.533, 0.206, 0.311, 0.597, 0.29, 0.122, 0.353]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6114 | Steps: 4 | Val loss: 11.3619 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8156 | Steps: 4 | Val loss: 1.7542 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 34.9473 | Steps: 4 | Val loss: 32.7301 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.4058 | Steps: 4 | Val loss: 1.9009 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:27:10 (running for 00:19:28.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.867 |      0.324 |                   49 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.319 |      0.383 |                   27 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.173 |      0.285 |                   25 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 33.988 |      0.219 |                    4 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m top1: 0.3675373134328358
[2m[36m(func pid=121255)[0m top5: 0.8600746268656716
[2m[36m(func pid=121255)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=121255)[0m f1_macro: 0.3247538112842749
[2m[36m(func pid=121255)[0m f1_weighted: 0.36327464721758107
[2m[36m(func pid=121255)[0m f1_per_class: [0.402, 0.51, 0.49, 0.53, 0.126, 0.156, 0.222, 0.347, 0.277, 0.187]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27845149253731344
[2m[36m(func pid=127722)[0m top5: 0.8125
[2m[36m(func pid=127722)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=127722)[0m f1_macro: 0.27254657741940175
[2m[36m(func pid=127722)[0m f1_weighted: 0.2865636675023902
[2m[36m(func pid=127722)[0m f1_per_class: [0.382, 0.452, 0.25, 0.296, 0.083, 0.184, 0.218, 0.35, 0.19, 0.32]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m top1: 0.2294776119402985
[2m[36m(func pid=132824)[0m top5: 0.6399253731343284
[2m[36m(func pid=132824)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=132824)[0m f1_macro: 0.15600840980587338
[2m[36m(func pid=132824)[0m f1_weighted: 0.15086210388540255
[2m[36m(func pid=132824)[0m f1_per_class: [0.0, 0.405, 0.348, 0.162, 0.068, 0.131, 0.03, 0.0, 0.204, 0.212]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.4603544776119403
[2m[36m(func pid=127193)[0m top5: 0.9239738805970149
[2m[36m(func pid=127193)[0m f1_micro: 0.4603544776119403
[2m[36m(func pid=127193)[0m f1_macro: 0.3719180974649665
[2m[36m(func pid=127193)[0m f1_weighted: 0.45538446229700175
[2m[36m(func pid=127193)[0m f1_per_class: [0.593, 0.584, 0.292, 0.406, 0.213, 0.323, 0.557, 0.292, 0.052, 0.407]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7670 | Steps: 4 | Val loss: 1.7685 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 45.5579 | Steps: 4 | Val loss: 52.4544 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 5.0134 | Steps: 4 | Val loss: 13.7801 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3934 | Steps: 4 | Val loss: 2.0298 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:27:16 (running for 00:19:33.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.816 |      0.325 |                   50 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.406 |      0.372 |                   28 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.611 |      0.273 |                   26 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 34.947 |      0.156 |                    5 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m top1: 0.36100746268656714
[2m[36m(func pid=121255)[0m top5: 0.8596082089552238
[2m[36m(func pid=121255)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=121255)[0m f1_macro: 0.3108447773045719
[2m[36m(func pid=121255)[0m f1_weighted: 0.3558093677283786
[2m[36m(func pid=121255)[0m f1_per_class: [0.377, 0.474, 0.436, 0.551, 0.119, 0.171, 0.2, 0.345, 0.242, 0.194]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m top1: 0.19309701492537312
[2m[36m(func pid=132824)[0m top5: 0.6124067164179104
[2m[36m(func pid=132824)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=132824)[0m f1_macro: 0.14857760088542388
[2m[36m(func pid=132824)[0m f1_weighted: 0.2050237224195184
[2m[36m(func pid=132824)[0m f1_per_class: [0.0, 0.298, 0.033, 0.446, 0.105, 0.176, 0.0, 0.016, 0.116, 0.296]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2593283582089552
[2m[36m(func pid=127722)[0m top5: 0.7915111940298507
[2m[36m(func pid=127722)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=127722)[0m f1_macro: 0.2303750362202074
[2m[36m(func pid=127722)[0m f1_weighted: 0.24562376697157176
[2m[36m(func pid=127722)[0m f1_per_class: [0.279, 0.501, 0.267, 0.131, 0.116, 0.192, 0.242, 0.238, 0.138, 0.2]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.44402985074626866
[2m[36m(func pid=127193)[0m top5: 0.9207089552238806
[2m[36m(func pid=127193)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=127193)[0m f1_macro: 0.3787745904600706
[2m[36m(func pid=127193)[0m f1_weighted: 0.43782301511089305
[2m[36m(func pid=127193)[0m f1_per_class: [0.606, 0.595, 0.333, 0.378, 0.263, 0.317, 0.514, 0.289, 0.079, 0.414]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 27.0229 | Steps: 4 | Val loss: 54.3251 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.1329 | Steps: 4 | Val loss: 15.1793 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7347 | Steps: 4 | Val loss: 1.7261 | Batch size: 32 | lr: 0.0001 | Duration: 3.29s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2364 | Steps: 4 | Val loss: 2.2199 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:27:21 (running for 00:19:39.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.767 |      0.311 |                   51 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.393 |      0.379 |                   29 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.013 |      0.23  |                   27 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 45.558 |      0.149 |                    6 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.3670708955223881
[2m[36m(func pid=132824)[0m top5: 0.6609141791044776
[2m[36m(func pid=132824)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=132824)[0m f1_macro: 0.3323815405746162
[2m[36m(func pid=132824)[0m f1_weighted: 0.26937906870309286
[2m[36m(func pid=132824)[0m f1_per_class: [0.68, 0.306, 0.667, 0.542, 0.3, 0.169, 0.0, 0.357, 0.081, 0.221]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27845149253731344
[2m[36m(func pid=127722)[0m top5: 0.7919776119402985
[2m[36m(func pid=127722)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=127722)[0m f1_macro: 0.22619134257839352
[2m[36m(func pid=127722)[0m f1_weighted: 0.23642238144050484
[2m[36m(func pid=127722)[0m f1_per_class: [0.373, 0.497, 0.267, 0.083, 0.134, 0.225, 0.254, 0.14, 0.212, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.4085820895522388
[2m[36m(func pid=127193)[0m top5: 0.9174440298507462
[2m[36m(func pid=127193)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=127193)[0m f1_macro: 0.3743951950761236
[2m[36m(func pid=127193)[0m f1_weighted: 0.397836644102375
[2m[36m(func pid=127193)[0m f1_per_class: [0.635, 0.581, 0.361, 0.365, 0.247, 0.321, 0.388, 0.301, 0.125, 0.419]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3670708955223881
[2m[36m(func pid=121255)[0m top5: 0.8717350746268657
[2m[36m(func pid=121255)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=121255)[0m f1_macro: 0.31955937332843315
[2m[36m(func pid=121255)[0m f1_weighted: 0.3600917197600063
[2m[36m(func pid=121255)[0m f1_per_class: [0.359, 0.444, 0.48, 0.559, 0.144, 0.192, 0.211, 0.367, 0.241, 0.197]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 42.8115 | Steps: 4 | Val loss: 64.6660 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5781 | Steps: 4 | Val loss: 2.3275 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 5.7869 | Steps: 4 | Val loss: 14.4352 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.9918 | Steps: 4 | Val loss: 1.7236 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:27:26 (running for 00:19:44.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.735 |      0.32  |                   52 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.236 |      0.374 |                   30 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.133 |      0.226 |                   28 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 42.811 |      0.183 |                    8 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.21455223880597016
[2m[36m(func pid=132824)[0m top5: 0.6721082089552238
[2m[36m(func pid=132824)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=132824)[0m f1_macro: 0.18307951964929042
[2m[36m(func pid=132824)[0m f1_weighted: 0.20202452998896053
[2m[36m(func pid=132824)[0m f1_per_class: [0.23, 0.425, 0.0, 0.328, 0.253, 0.137, 0.0, 0.188, 0.053, 0.217]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.408115671641791
[2m[36m(func pid=127193)[0m top5: 0.9123134328358209
[2m[36m(func pid=127193)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=127193)[0m f1_macro: 0.3828377703654203
[2m[36m(func pid=127193)[0m f1_weighted: 0.39762388945723953
[2m[36m(func pid=127193)[0m f1_per_class: [0.618, 0.583, 0.4, 0.423, 0.243, 0.317, 0.328, 0.305, 0.17, 0.441]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27611940298507465
[2m[36m(func pid=127722)[0m top5: 0.7961753731343284
[2m[36m(func pid=127722)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=127722)[0m f1_macro: 0.29957282430314147
[2m[36m(func pid=127722)[0m f1_weighted: 0.2567986175565743
[2m[36m(func pid=127722)[0m f1_per_class: [0.426, 0.467, 0.833, 0.219, 0.117, 0.223, 0.199, 0.128, 0.183, 0.2]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3689365671641791
[2m[36m(func pid=121255)[0m top5: 0.8810634328358209
[2m[36m(func pid=121255)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=121255)[0m f1_macro: 0.3120931662454615
[2m[36m(func pid=121255)[0m f1_weighted: 0.3626591129093013
[2m[36m(func pid=121255)[0m f1_per_class: [0.356, 0.438, 0.421, 0.57, 0.15, 0.203, 0.217, 0.347, 0.22, 0.2]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 37.2558 | Steps: 4 | Val loss: 47.0060 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5933 | Steps: 4 | Val loss: 2.4256 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0559 | Steps: 4 | Val loss: 12.4955 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8843 | Steps: 4 | Val loss: 1.6842 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:27:32 (running for 00:19:49.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.992 |      0.312 |                   53 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.578 |      0.383 |                   31 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.787 |      0.3   |                   29 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 37.256 |      0.228 |                    9 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.20755597014925373
[2m[36m(func pid=132824)[0m top5: 0.8390858208955224
[2m[36m(func pid=132824)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=132824)[0m f1_macro: 0.22809874414782932
[2m[36m(func pid=132824)[0m f1_weighted: 0.19206593850030695
[2m[36m(func pid=132824)[0m f1_per_class: [0.468, 0.385, 0.0, 0.135, 0.076, 0.202, 0.074, 0.376, 0.203, 0.361]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.39972014925373134
[2m[36m(func pid=127193)[0m top5: 0.9039179104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=127193)[0m f1_macro: 0.38362350020218794
[2m[36m(func pid=127193)[0m f1_weighted: 0.3829348869130923
[2m[36m(func pid=127193)[0m f1_per_class: [0.643, 0.589, 0.429, 0.487, 0.253, 0.298, 0.217, 0.292, 0.234, 0.395]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.29197761194029853
[2m[36m(func pid=127722)[0m top5: 0.7952425373134329
[2m[36m(func pid=127722)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=127722)[0m f1_macro: 0.31463870715370446
[2m[36m(func pid=127722)[0m f1_weighted: 0.3140331131882013
[2m[36m(func pid=127722)[0m f1_per_class: [0.564, 0.401, 0.52, 0.445, 0.11, 0.169, 0.228, 0.152, 0.137, 0.421]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.38992537313432835
[2m[36m(func pid=121255)[0m top5: 0.8959888059701493
[2m[36m(func pid=121255)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=121255)[0m f1_macro: 0.32449487717230135
[2m[36m(func pid=121255)[0m f1_weighted: 0.38669105206904075
[2m[36m(func pid=121255)[0m f1_per_class: [0.392, 0.477, 0.375, 0.589, 0.147, 0.215, 0.248, 0.358, 0.209, 0.235]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 46.0444 | Steps: 4 | Val loss: 62.1461 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.0803 | Steps: 4 | Val loss: 2.2832 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2644 | Steps: 4 | Val loss: 13.9257 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9325 | Steps: 4 | Val loss: 1.6670 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:27:37 (running for 00:19:54.82)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.884 |      0.324 |                   54 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.593 |      0.384 |                   32 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.056 |      0.315 |                   30 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 46.044 |      0.161 |                   10 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.27845149253731344
[2m[36m(func pid=132824)[0m top5: 0.7653917910447762
[2m[36m(func pid=132824)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=132824)[0m f1_macro: 0.16052476331413107
[2m[36m(func pid=132824)[0m f1_weighted: 0.24149092173574419
[2m[36m(func pid=132824)[0m f1_per_class: [0.475, 0.172, 0.0, 0.023, 0.085, 0.136, 0.589, 0.0, 0.127, 0.0]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.42350746268656714
[2m[36m(func pid=127193)[0m top5: 0.9095149253731343
[2m[36m(func pid=127193)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=127193)[0m f1_macro: 0.41457661385045314
[2m[36m(func pid=127193)[0m f1_weighted: 0.4171379131124275
[2m[36m(func pid=127193)[0m f1_per_class: [0.636, 0.587, 0.571, 0.564, 0.253, 0.277, 0.261, 0.311, 0.242, 0.444]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3041044776119403
[2m[36m(func pid=127722)[0m top5: 0.7117537313432836
[2m[36m(func pid=127722)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=127722)[0m f1_macro: 0.2553735252271139
[2m[36m(func pid=127722)[0m f1_weighted: 0.31193223619516974
[2m[36m(func pid=127722)[0m f1_per_class: [0.659, 0.417, 0.155, 0.528, 0.116, 0.08, 0.189, 0.113, 0.105, 0.193]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3885261194029851
[2m[36m(func pid=121255)[0m top5: 0.9034514925373134
[2m[36m(func pid=121255)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=121255)[0m f1_macro: 0.334767062160661
[2m[36m(func pid=121255)[0m f1_weighted: 0.3921461043319932
[2m[36m(func pid=121255)[0m f1_per_class: [0.426, 0.498, 0.358, 0.569, 0.153, 0.259, 0.254, 0.339, 0.235, 0.257]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 40.7309 | Steps: 4 | Val loss: 47.6479 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2270 | Steps: 4 | Val loss: 2.3721 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.1819 | Steps: 4 | Val loss: 16.5516 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7548 | Steps: 4 | Val loss: 1.6560 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:27:42 (running for 00:20:00.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.932 |      0.335 |                   55 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.08  |      0.415 |                   33 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.264 |      0.255 |                   31 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 40.731 |      0.307 |                   11 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.37593283582089554
[2m[36m(func pid=132824)[0m top5: 0.8782649253731343
[2m[36m(func pid=132824)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=132824)[0m f1_macro: 0.3070762429578625
[2m[36m(func pid=132824)[0m f1_weighted: 0.3501074446887394
[2m[36m(func pid=132824)[0m f1_per_class: [0.2, 0.52, 0.526, 0.195, 0.154, 0.266, 0.516, 0.0, 0.259, 0.435]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.417910447761194
[2m[36m(func pid=127193)[0m top5: 0.9132462686567164
[2m[36m(func pid=127193)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=127193)[0m f1_macro: 0.40556946689997025
[2m[36m(func pid=127193)[0m f1_weighted: 0.41871437788313326
[2m[36m(func pid=127193)[0m f1_per_class: [0.623, 0.566, 0.55, 0.584, 0.235, 0.267, 0.268, 0.32, 0.187, 0.455]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.25886194029850745
[2m[36m(func pid=127722)[0m top5: 0.6786380597014925
[2m[36m(func pid=127722)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=127722)[0m f1_macro: 0.21092905281926155
[2m[36m(func pid=127722)[0m f1_weighted: 0.26951813096912136
[2m[36m(func pid=127722)[0m f1_per_class: [0.492, 0.354, 0.066, 0.516, 0.164, 0.015, 0.132, 0.119, 0.093, 0.158]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.39972014925373134
[2m[36m(func pid=121255)[0m top5: 0.9067164179104478
[2m[36m(func pid=121255)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=121255)[0m f1_macro: 0.35325134160243127
[2m[36m(func pid=121255)[0m f1_weighted: 0.39956801465767144
[2m[36m(func pid=121255)[0m f1_per_class: [0.462, 0.522, 0.462, 0.594, 0.132, 0.265, 0.232, 0.368, 0.198, 0.298]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 26.5728 | Steps: 4 | Val loss: 59.1146 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.0923 | Steps: 4 | Val loss: 2.5821 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0301 | Steps: 4 | Val loss: 18.0481 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6198 | Steps: 4 | Val loss: 1.6581 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 11:27:47 (running for 00:20:05.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.755 |      0.353 |                   56 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.227 |      0.406 |                   34 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.182 |      0.211 |                   32 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 26.573 |      0.263 |                   12 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.3591417910447761
[2m[36m(func pid=132824)[0m top5: 0.7714552238805971
[2m[36m(func pid=132824)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=132824)[0m f1_macro: 0.2631749490002258
[2m[36m(func pid=132824)[0m f1_weighted: 0.32162927575068534
[2m[36m(func pid=132824)[0m f1_per_class: [0.125, 0.554, 0.453, 0.497, 0.171, 0.297, 0.125, 0.0, 0.241, 0.169]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.376865671641791
[2m[36m(func pid=127193)[0m top5: 0.9034514925373134
[2m[36m(func pid=127193)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=127193)[0m f1_macro: 0.3948679982055408
[2m[36m(func pid=127193)[0m f1_weighted: 0.3878818217928478
[2m[36m(func pid=127193)[0m f1_per_class: [0.652, 0.415, 0.595, 0.591, 0.286, 0.279, 0.24, 0.335, 0.15, 0.405]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.25466417910447764
[2m[36m(func pid=127722)[0m top5: 0.6557835820895522
[2m[36m(func pid=127722)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=127722)[0m f1_macro: 0.1905339499639624
[2m[36m(func pid=127722)[0m f1_weighted: 0.26194754053073677
[2m[36m(func pid=127722)[0m f1_per_class: [0.235, 0.401, 0.05, 0.496, 0.161, 0.008, 0.104, 0.192, 0.115, 0.145]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.4048507462686567
[2m[36m(func pid=121255)[0m top5: 0.9076492537313433
[2m[36m(func pid=121255)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=121255)[0m f1_macro: 0.35477493196041354
[2m[36m(func pid=121255)[0m f1_weighted: 0.397465949710676
[2m[36m(func pid=121255)[0m f1_per_class: [0.5, 0.551, 0.393, 0.58, 0.149, 0.281, 0.215, 0.362, 0.179, 0.337]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 28.3646 | Steps: 4 | Val loss: 82.6281 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.1802 | Steps: 4 | Val loss: 2.7951 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 4.1659 | Steps: 4 | Val loss: 18.1509 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7129 | Steps: 4 | Val loss: 1.6163 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=132824)[0m top1: 0.2579291044776119
[2m[36m(func pid=132824)[0m top5: 0.679570895522388
[2m[36m(func pid=132824)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=132824)[0m f1_macro: 0.16800898476638804
[2m[36m(func pid=132824)[0m f1_weighted: 0.2396875224306325
[2m[36m(func pid=132824)[0m f1_per_class: [0.044, 0.412, 0.114, 0.451, 0.152, 0.069, 0.064, 0.104, 0.187, 0.082]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:27:53 (running for 00:20:10.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.62  |      0.355 |                   57 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.092 |      0.395 |                   35 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.03  |      0.191 |                   33 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 28.365 |      0.168 |                   13 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127193)[0m top1: 0.35074626865671643
[2m[36m(func pid=127193)[0m top5: 0.90625
[2m[36m(func pid=127193)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=127193)[0m f1_macro: 0.39469120919325723
[2m[36m(func pid=127193)[0m f1_weighted: 0.3647057413023293
[2m[36m(func pid=127193)[0m f1_per_class: [0.682, 0.294, 0.688, 0.578, 0.264, 0.263, 0.245, 0.338, 0.159, 0.438]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3003731343283582
[2m[36m(func pid=127722)[0m top5: 0.644589552238806
[2m[36m(func pid=127722)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=127722)[0m f1_macro: 0.2035842595421003
[2m[36m(func pid=127722)[0m f1_weighted: 0.27060535957433696
[2m[36m(func pid=127722)[0m f1_per_class: [0.167, 0.464, 0.083, 0.514, 0.184, 0.008, 0.068, 0.257, 0.128, 0.163]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.40904850746268656
[2m[36m(func pid=121255)[0m top5: 0.917910447761194
[2m[36m(func pid=121255)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=121255)[0m f1_macro: 0.3530152128080971
[2m[36m(func pid=121255)[0m f1_weighted: 0.40103182145367483
[2m[36m(func pid=121255)[0m f1_per_class: [0.53, 0.562, 0.343, 0.582, 0.169, 0.289, 0.217, 0.362, 0.171, 0.306]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 51.4878 | Steps: 4 | Val loss: 99.6013 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.1834 | Steps: 4 | Val loss: 2.8846 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6289 | Steps: 4 | Val loss: 17.9725 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6606 | Steps: 4 | Val loss: 1.6065 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:27:58 (running for 00:20:15.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.713 |      0.353 |                   58 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.18  |      0.395 |                   36 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.166 |      0.204 |                   34 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 51.488 |      0.188 |                   14 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.20848880597014927
[2m[36m(func pid=132824)[0m top5: 0.5918843283582089
[2m[36m(func pid=132824)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=132824)[0m f1_macro: 0.1882269596515148
[2m[36m(func pid=132824)[0m f1_weighted: 0.21452537294823737
[2m[36m(func pid=132824)[0m f1_per_class: [0.327, 0.451, 0.039, 0.333, 0.109, 0.016, 0.042, 0.25, 0.171, 0.144]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.34328358208955223
[2m[36m(func pid=127193)[0m top5: 0.9085820895522388
[2m[36m(func pid=127193)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=127193)[0m f1_macro: 0.40044060906113926
[2m[36m(func pid=127193)[0m f1_weighted: 0.37784440446886564
[2m[36m(func pid=127193)[0m f1_per_class: [0.675, 0.257, 0.733, 0.545, 0.264, 0.278, 0.332, 0.366, 0.138, 0.417]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3218283582089552
[2m[36m(func pid=127722)[0m top5: 0.6791044776119403
[2m[36m(func pid=127722)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=127722)[0m f1_macro: 0.24253205767569558
[2m[36m(func pid=127722)[0m f1_weighted: 0.268878261978487
[2m[36m(func pid=127722)[0m f1_per_class: [0.2, 0.464, 0.257, 0.492, 0.203, 0.037, 0.051, 0.315, 0.103, 0.302]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.41697761194029853
[2m[36m(func pid=121255)[0m top5: 0.9174440298507462
[2m[36m(func pid=121255)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=121255)[0m f1_macro: 0.3598617893237954
[2m[36m(func pid=121255)[0m f1_weighted: 0.4102066754245275
[2m[36m(func pid=121255)[0m f1_per_class: [0.531, 0.566, 0.338, 0.584, 0.179, 0.299, 0.238, 0.368, 0.167, 0.329]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 11.4793 | Steps: 4 | Val loss: 86.0074 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3068 | Steps: 4 | Val loss: 2.8171 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6459 | Steps: 4 | Val loss: 17.1621 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8182 | Steps: 4 | Val loss: 1.6026 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:28:03 (running for 00:20:21.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.661 |      0.36  |                   59 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.183 |      0.4   |                   37 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.629 |      0.243 |                   35 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 11.479 |      0.27  |                   15 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.30550373134328357
[2m[36m(func pid=132824)[0m top5: 0.6800373134328358
[2m[36m(func pid=132824)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=132824)[0m f1_macro: 0.26964989514170135
[2m[36m(func pid=132824)[0m f1_weighted: 0.2714978838640548
[2m[36m(func pid=132824)[0m f1_per_class: [0.593, 0.521, 0.162, 0.449, 0.079, 0.016, 0.031, 0.408, 0.15, 0.288]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.35867537313432835
[2m[36m(func pid=127193)[0m top5: 0.9113805970149254
[2m[36m(func pid=127193)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=127193)[0m f1_macro: 0.3982099584444668
[2m[36m(func pid=127193)[0m f1_weighted: 0.3945851064611165
[2m[36m(func pid=127193)[0m f1_per_class: [0.682, 0.262, 0.688, 0.552, 0.258, 0.275, 0.387, 0.336, 0.143, 0.4]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3460820895522388
[2m[36m(func pid=127722)[0m top5: 0.7206156716417911
[2m[36m(func pid=127722)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=127722)[0m f1_macro: 0.31480091061281834
[2m[36m(func pid=127722)[0m f1_weighted: 0.2953525125218062
[2m[36m(func pid=127722)[0m f1_per_class: [0.421, 0.499, 0.473, 0.519, 0.242, 0.124, 0.025, 0.369, 0.143, 0.333]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.41511194029850745
[2m[36m(func pid=121255)[0m top5: 0.9216417910447762
[2m[36m(func pid=121255)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=121255)[0m f1_macro: 0.36124324624773535
[2m[36m(func pid=121255)[0m f1_weighted: 0.404717007111505
[2m[36m(func pid=121255)[0m f1_per_class: [0.544, 0.576, 0.338, 0.579, 0.198, 0.3, 0.221, 0.354, 0.139, 0.364]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 30.6641 | Steps: 4 | Val loss: 99.9524 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5196 | Steps: 4 | Val loss: 2.7436 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8994 | Steps: 4 | Val loss: 17.6816 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7158 | Steps: 4 | Val loss: 1.6052 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 11:28:08 (running for 00:20:26.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.818 |      0.361 |                   60 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.307 |      0.398 |                   38 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.646 |      0.315 |                   36 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 30.664 |      0.282 |                   16 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.30177238805970147
[2m[36m(func pid=132824)[0m top5: 0.7481343283582089
[2m[36m(func pid=132824)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=132824)[0m f1_macro: 0.2817034345368471
[2m[36m(func pid=132824)[0m f1_weighted: 0.2736578306283871
[2m[36m(func pid=132824)[0m f1_per_class: [0.45, 0.473, 0.325, 0.523, 0.069, 0.008, 0.009, 0.353, 0.163, 0.444]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3712686567164179
[2m[36m(func pid=127193)[0m top5: 0.9085820895522388
[2m[36m(func pid=127193)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=127193)[0m f1_macro: 0.3921682273981553
[2m[36m(func pid=127193)[0m f1_weighted: 0.40719644236711877
[2m[36m(func pid=127193)[0m f1_per_class: [0.674, 0.264, 0.667, 0.542, 0.245, 0.319, 0.426, 0.327, 0.152, 0.306]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3358208955223881
[2m[36m(func pid=127722)[0m top5: 0.7168843283582089
[2m[36m(func pid=127722)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=127722)[0m f1_macro: 0.3546940876357446
[2m[36m(func pid=127722)[0m f1_weighted: 0.30706541542322247
[2m[36m(func pid=127722)[0m f1_per_class: [0.592, 0.501, 0.632, 0.529, 0.183, 0.201, 0.012, 0.33, 0.167, 0.4]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.41744402985074625
[2m[36m(func pid=121255)[0m top5: 0.9225746268656716
[2m[36m(func pid=121255)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=121255)[0m f1_macro: 0.37390375289612726
[2m[36m(func pid=121255)[0m f1_weighted: 0.412535452092851
[2m[36m(func pid=121255)[0m f1_per_class: [0.55, 0.565, 0.312, 0.575, 0.22, 0.303, 0.246, 0.363, 0.189, 0.417]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 17.4517 | Steps: 4 | Val loss: 117.4566 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.0923 | Steps: 4 | Val loss: 2.5610 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5968 | Steps: 4 | Val loss: 18.6837 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:28:14 (running for 00:20:31.55)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.716 |      0.374 |                   61 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.52  |      0.392 |                   39 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.899 |      0.355 |                   37 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 17.452 |      0.28  |                   17 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7391 | Steps: 4 | Val loss: 1.6117 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=132824)[0m top1: 0.25093283582089554
[2m[36m(func pid=132824)[0m top5: 0.7779850746268657
[2m[36m(func pid=132824)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=132824)[0m f1_macro: 0.2803909086803473
[2m[36m(func pid=132824)[0m f1_weighted: 0.24034931970467657
[2m[36m(func pid=132824)[0m f1_per_class: [0.275, 0.29, 0.706, 0.5, 0.094, 0.121, 0.009, 0.224, 0.185, 0.4]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.40578358208955223
[2m[36m(func pid=127193)[0m top5: 0.9118470149253731
[2m[36m(func pid=127193)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=127193)[0m f1_macro: 0.3866443109317369
[2m[36m(func pid=127193)[0m f1_weighted: 0.4383924900597001
[2m[36m(func pid=127193)[0m f1_per_class: [0.638, 0.343, 0.564, 0.571, 0.212, 0.315, 0.464, 0.327, 0.178, 0.254]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.31716417910447764
[2m[36m(func pid=127722)[0m top5: 0.7262126865671642
[2m[36m(func pid=127722)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=127722)[0m f1_macro: 0.3634187270487129
[2m[36m(func pid=127722)[0m f1_weighted: 0.29935248470429315
[2m[36m(func pid=127722)[0m f1_per_class: [0.659, 0.508, 0.667, 0.511, 0.14, 0.197, 0.003, 0.26, 0.2, 0.489]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.4006529850746269
[2m[36m(func pid=121255)[0m top5: 0.9221082089552238
[2m[36m(func pid=121255)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=121255)[0m f1_macro: 0.36555848992761875
[2m[36m(func pid=121255)[0m f1_weighted: 0.3985180443402568
[2m[36m(func pid=121255)[0m f1_per_class: [0.615, 0.55, 0.293, 0.565, 0.19, 0.287, 0.225, 0.343, 0.183, 0.405]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 19.9942 | Steps: 4 | Val loss: 102.2371 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4158 | Steps: 4 | Val loss: 2.4588 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.4231 | Steps: 4 | Val loss: 20.3271 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:28:19 (running for 00:20:36.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.739 |      0.366 |                   62 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.092 |      0.387 |                   40 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.597 |      0.363 |                   38 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 19.994 |      0.295 |                   18 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.29850746268656714
[2m[36m(func pid=132824)[0m top5: 0.7933768656716418
[2m[36m(func pid=132824)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=132824)[0m f1_macro: 0.29528772383259533
[2m[36m(func pid=132824)[0m f1_weighted: 0.2665124543545681
[2m[36m(func pid=132824)[0m f1_per_class: [0.338, 0.17, 0.759, 0.562, 0.155, 0.285, 0.04, 0.256, 0.189, 0.2]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5909 | Steps: 4 | Val loss: 1.6338 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127193)[0m top1: 0.42490671641791045
[2m[36m(func pid=127193)[0m top5: 0.9095149253731343
[2m[36m(func pid=127193)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=127193)[0m f1_macro: 0.37890856462039785
[2m[36m(func pid=127193)[0m f1_weighted: 0.4522767462016275
[2m[36m(func pid=127193)[0m f1_per_class: [0.587, 0.374, 0.522, 0.587, 0.218, 0.298, 0.49, 0.305, 0.216, 0.19]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2756529850746269
[2m[36m(func pid=127722)[0m top5: 0.7406716417910447
[2m[36m(func pid=127722)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.3212093632438379
[2m[36m(func pid=127722)[0m f1_weighted: 0.2704436792372809
[2m[36m(func pid=127722)[0m f1_per_class: [0.558, 0.443, 0.733, 0.475, 0.109, 0.182, 0.006, 0.216, 0.197, 0.293]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.40578358208955223
[2m[36m(func pid=121255)[0m top5: 0.914179104477612
[2m[36m(func pid=121255)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=121255)[0m f1_macro: 0.37072807513863876
[2m[36m(func pid=121255)[0m f1_weighted: 0.4060873525911321
[2m[36m(func pid=121255)[0m f1_per_class: [0.612, 0.574, 0.289, 0.541, 0.18, 0.292, 0.252, 0.356, 0.222, 0.39]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 45.2172 | Steps: 4 | Val loss: 70.6167 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2100 | Steps: 4 | Val loss: 2.4208 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.2678 | Steps: 4 | Val loss: 19.8795 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:28:24 (running for 00:20:42.21)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.591 |      0.371 |                   63 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.416 |      0.379 |                   41 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.423 |      0.321 |                   39 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 45.217 |      0.358 |                   19 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.37779850746268656
[2m[36m(func pid=132824)[0m top5: 0.8372201492537313
[2m[36m(func pid=132824)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=132824)[0m f1_macro: 0.3576753712717094
[2m[36m(func pid=132824)[0m f1_weighted: 0.36702932951772405
[2m[36m(func pid=132824)[0m f1_per_class: [0.431, 0.185, 0.815, 0.588, 0.226, 0.329, 0.307, 0.317, 0.178, 0.2]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6409 | Steps: 4 | Val loss: 1.5837 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=127193)[0m top1: 0.4337686567164179
[2m[36m(func pid=127193)[0m top5: 0.9090485074626866
[2m[36m(func pid=127193)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=127193)[0m f1_macro: 0.38133913417384674
[2m[36m(func pid=127193)[0m f1_weighted: 0.46032386418008475
[2m[36m(func pid=127193)[0m f1_per_class: [0.613, 0.492, 0.49, 0.573, 0.168, 0.285, 0.465, 0.305, 0.256, 0.168]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.22807835820895522
[2m[36m(func pid=127722)[0m top5: 0.7943097014925373
[2m[36m(func pid=127722)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=127722)[0m f1_macro: 0.28341999167892484
[2m[36m(func pid=127722)[0m f1_weighted: 0.22742614938293457
[2m[36m(func pid=127722)[0m f1_per_class: [0.398, 0.311, 0.846, 0.398, 0.079, 0.179, 0.027, 0.219, 0.155, 0.222]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.41091417910447764
[2m[36m(func pid=121255)[0m top5: 0.9221082089552238
[2m[36m(func pid=121255)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=121255)[0m f1_macro: 0.39062922131655253
[2m[36m(func pid=121255)[0m f1_weighted: 0.4171966580876522
[2m[36m(func pid=121255)[0m f1_per_class: [0.636, 0.568, 0.4, 0.514, 0.196, 0.286, 0.314, 0.347, 0.244, 0.4]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 27.2906 | Steps: 4 | Val loss: 69.9728 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2842 | Steps: 4 | Val loss: 2.4383 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:28:30 (running for 00:20:47.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.641 |      0.391 |                   64 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.21  |      0.381 |                   42 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.268 |      0.283 |                   40 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 27.291 |      0.357 |                   20 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1908 | Steps: 4 | Val loss: 17.3578 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=132824)[0m top1: 0.435634328358209
[2m[36m(func pid=132824)[0m top5: 0.8278917910447762
[2m[36m(func pid=132824)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=132824)[0m f1_macro: 0.3572336069140922
[2m[36m(func pid=132824)[0m f1_weighted: 0.3956570724375564
[2m[36m(func pid=132824)[0m f1_per_class: [0.6, 0.311, 0.846, 0.47, 0.211, 0.14, 0.522, 0.257, 0.145, 0.071]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.8630 | Steps: 4 | Val loss: 1.6354 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=127193)[0m top1: 0.44216417910447764
[2m[36m(func pid=127193)[0m top5: 0.914179104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=127193)[0m f1_macro: 0.3894581097684508
[2m[36m(func pid=127193)[0m f1_weighted: 0.4612409059455005
[2m[36m(func pid=127193)[0m f1_per_class: [0.617, 0.568, 0.471, 0.558, 0.184, 0.296, 0.429, 0.339, 0.218, 0.215]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.23087686567164178
[2m[36m(func pid=127722)[0m top5: 0.8171641791044776
[2m[36m(func pid=127722)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=127722)[0m f1_macro: 0.2570293579740179
[2m[36m(func pid=127722)[0m f1_weighted: 0.2554214599721035
[2m[36m(func pid=127722)[0m f1_per_class: [0.226, 0.283, 0.667, 0.338, 0.077, 0.175, 0.205, 0.257, 0.146, 0.195]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.3969216417910448
[2m[36m(func pid=121255)[0m top5: 0.9081156716417911
[2m[36m(func pid=121255)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=121255)[0m f1_macro: 0.38127418116021095
[2m[36m(func pid=121255)[0m f1_weighted: 0.4056082779074757
[2m[36m(func pid=121255)[0m f1_per_class: [0.667, 0.566, 0.364, 0.458, 0.195, 0.287, 0.33, 0.346, 0.225, 0.375]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 18.7944 | Steps: 4 | Val loss: 55.6132 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.0494 | Steps: 4 | Val loss: 2.5129 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=132824)[0m top1: 0.47901119402985076
[2m[36m(func pid=132824)[0m top5: 0.8759328358208955
[2m[36m(func pid=132824)[0m f1_micro: 0.47901119402985076
[2m[36m(func pid=132824)[0m f1_macro: 0.42080217382808227
[2m[36m(func pid=132824)[0m f1_weighted: 0.47529756713583576
[2m[36m(func pid=132824)[0m f1_per_class: [0.635, 0.562, 0.846, 0.506, 0.146, 0.212, 0.573, 0.222, 0.202, 0.303]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:28:35 (running for 00:20:52.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.863 |      0.381 |                   65 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.284 |      0.389 |                   43 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.191 |      0.257 |                   41 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 18.794 |      0.421 |                   21 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.5489 | Steps: 4 | Val loss: 16.6441 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5503 | Steps: 4 | Val loss: 1.6699 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=127193)[0m top1: 0.43050373134328357
[2m[36m(func pid=127193)[0m top5: 0.9118470149253731
[2m[36m(func pid=127193)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=127193)[0m f1_macro: 0.3784995037623827
[2m[36m(func pid=127193)[0m f1_weighted: 0.4427052115878938
[2m[36m(func pid=127193)[0m f1_per_class: [0.64, 0.569, 0.478, 0.559, 0.179, 0.283, 0.378, 0.323, 0.156, 0.22]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2658582089552239
[2m[36m(func pid=127722)[0m top5: 0.8260261194029851
[2m[36m(func pid=127722)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=127722)[0m f1_macro: 0.2618875819387491
[2m[36m(func pid=127722)[0m f1_weighted: 0.31007602546325086
[2m[36m(func pid=127722)[0m f1_per_class: [0.169, 0.266, 0.6, 0.332, 0.066, 0.168, 0.409, 0.289, 0.12, 0.2]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 15.0672 | Steps: 4 | Val loss: 88.6785 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=121255)[0m top1: 0.39132462686567165
[2m[36m(func pid=121255)[0m top5: 0.9053171641791045
[2m[36m(func pid=121255)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=121255)[0m f1_macro: 0.3690691713327371
[2m[36m(func pid=121255)[0m f1_weighted: 0.39498897916174835
[2m[36m(func pid=121255)[0m f1_per_class: [0.622, 0.561, 0.324, 0.489, 0.179, 0.286, 0.274, 0.341, 0.232, 0.384]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3285 | Steps: 4 | Val loss: 2.6604 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:28:40 (running for 00:20:58.10)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.55  |      0.369 |                   66 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.049 |      0.378 |                   44 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.549 |      0.262 |                   42 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 15.067 |      0.379 |                   22 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.345615671641791
[2m[36m(func pid=132824)[0m top5: 0.7845149253731343
[2m[36m(func pid=132824)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=132824)[0m f1_macro: 0.37929086738597373
[2m[36m(func pid=132824)[0m f1_weighted: 0.37423166553983833
[2m[36m(func pid=132824)[0m f1_per_class: [0.629, 0.511, 0.815, 0.425, 0.095, 0.219, 0.333, 0.284, 0.15, 0.333]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 5.0029 | Steps: 4 | Val loss: 15.6008 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6104 | Steps: 4 | Val loss: 1.6848 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=127193)[0m top1: 0.41511194029850745
[2m[36m(func pid=127193)[0m top5: 0.9029850746268657
[2m[36m(func pid=127193)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=127193)[0m f1_macro: 0.37157824200678624
[2m[36m(func pid=127193)[0m f1_weighted: 0.42266449641518217
[2m[36m(func pid=127193)[0m f1_per_class: [0.61, 0.564, 0.533, 0.524, 0.174, 0.289, 0.348, 0.321, 0.14, 0.213]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.33255597014925375
[2m[36m(func pid=127722)[0m top5: 0.832089552238806
[2m[36m(func pid=127722)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=127722)[0m f1_macro: 0.27427870262729426
[2m[36m(func pid=127722)[0m f1_weighted: 0.33625737068690476
[2m[36m(func pid=127722)[0m f1_per_class: [0.242, 0.244, 0.526, 0.296, 0.095, 0.205, 0.527, 0.271, 0.132, 0.204]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 30.2615 | Steps: 4 | Val loss: 155.5623 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=121255)[0m top1: 0.386660447761194
[2m[36m(func pid=121255)[0m top5: 0.9020522388059702
[2m[36m(func pid=121255)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=121255)[0m f1_macro: 0.3655650554125777
[2m[36m(func pid=121255)[0m f1_weighted: 0.3995526152042076
[2m[36m(func pid=121255)[0m f1_per_class: [0.622, 0.541, 0.32, 0.493, 0.161, 0.287, 0.301, 0.333, 0.214, 0.386]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0847 | Steps: 4 | Val loss: 2.8701 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:28:46 (running for 00:21:03.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.61  |      0.366 |                   67 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.328 |      0.372 |                   45 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.003 |      0.274 |                   43 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 30.261 |      0.327 |                   23 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.22388059701492538
[2m[36m(func pid=132824)[0m top5: 0.6814365671641791
[2m[36m(func pid=132824)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=132824)[0m f1_macro: 0.32690206283058265
[2m[36m(func pid=132824)[0m f1_weighted: 0.24983228801331475
[2m[36m(func pid=132824)[0m f1_per_class: [0.675, 0.365, 0.759, 0.366, 0.082, 0.21, 0.057, 0.298, 0.11, 0.348]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4954 | Steps: 4 | Val loss: 14.5219 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5727 | Steps: 4 | Val loss: 1.6795 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=127193)[0m top1: 0.40625
[2m[36m(func pid=127193)[0m top5: 0.8950559701492538
[2m[36m(func pid=127193)[0m f1_micro: 0.40625
[2m[36m(func pid=127193)[0m f1_macro: 0.36499971135846726
[2m[36m(func pid=127193)[0m f1_weighted: 0.4049277390530902
[2m[36m(func pid=127193)[0m f1_per_class: [0.635, 0.553, 0.5, 0.464, 0.161, 0.28, 0.354, 0.321, 0.125, 0.259]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.38152985074626866
[2m[36m(func pid=127722)[0m top5: 0.8549440298507462
[2m[36m(func pid=127722)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=127722)[0m f1_macro: 0.2927910409811164
[2m[36m(func pid=127722)[0m f1_weighted: 0.3721625763690092
[2m[36m(func pid=127722)[0m f1_per_class: [0.302, 0.243, 0.444, 0.386, 0.114, 0.199, 0.565, 0.209, 0.202, 0.264]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 13.3515 | Steps: 4 | Val loss: 166.0075 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=121255)[0m top1: 0.37593283582089554
[2m[36m(func pid=121255)[0m top5: 0.9048507462686567
[2m[36m(func pid=121255)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=121255)[0m f1_macro: 0.35893013289470715
[2m[36m(func pid=121255)[0m f1_weighted: 0.39711098529481226
[2m[36m(func pid=121255)[0m f1_per_class: [0.636, 0.508, 0.324, 0.497, 0.17, 0.257, 0.322, 0.321, 0.202, 0.352]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2302 | Steps: 4 | Val loss: 2.9071 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 11:28:51 (running for 00:21:08.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.573 |      0.359 |                   68 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.085 |      0.365 |                   46 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.495 |      0.293 |                   44 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 13.352 |      0.297 |                   24 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.20848880597014927
[2m[36m(func pid=132824)[0m top5: 0.6394589552238806
[2m[36m(func pid=132824)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=132824)[0m f1_macro: 0.29730449129626974
[2m[36m(func pid=132824)[0m f1_weighted: 0.23519278097471952
[2m[36m(func pid=132824)[0m f1_per_class: [0.62, 0.277, 0.733, 0.399, 0.065, 0.187, 0.04, 0.337, 0.109, 0.206]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.3737 | Steps: 4 | Val loss: 14.0691 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.9053 | Steps: 4 | Val loss: 1.7118 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=127193)[0m top1: 0.4025186567164179
[2m[36m(func pid=127193)[0m top5: 0.894589552238806
[2m[36m(func pid=127193)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=127193)[0m f1_macro: 0.36856516682383117
[2m[36m(func pid=127193)[0m f1_weighted: 0.39895515040854523
[2m[36m(func pid=127193)[0m f1_per_class: [0.62, 0.554, 0.481, 0.477, 0.144, 0.261, 0.322, 0.318, 0.165, 0.343]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.365205223880597
[2m[36m(func pid=127722)[0m top5: 0.8731343283582089
[2m[36m(func pid=127722)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=127722)[0m f1_macro: 0.3124011484146435
[2m[36m(func pid=127722)[0m f1_weighted: 0.3929882675463139
[2m[36m(func pid=127722)[0m f1_per_class: [0.359, 0.286, 0.571, 0.464, 0.084, 0.115, 0.572, 0.178, 0.157, 0.339]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 83.7517 | Steps: 4 | Val loss: 196.3808 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=121255)[0m top1: 0.373134328358209
[2m[36m(func pid=121255)[0m top5: 0.8941231343283582
[2m[36m(func pid=121255)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=121255)[0m f1_macro: 0.3473587295930827
[2m[36m(func pid=121255)[0m f1_weighted: 0.3920850869917065
[2m[36m(func pid=121255)[0m f1_per_class: [0.596, 0.517, 0.333, 0.505, 0.141, 0.23, 0.308, 0.32, 0.207, 0.317]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4544 | Steps: 4 | Val loss: 3.0913 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:28:56 (running for 00:21:14.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.905 |      0.347 |                   69 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.23  |      0.369 |                   47 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.374 |      0.312 |                   45 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 83.752 |      0.256 |                   25 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.16930970149253732
[2m[36m(func pid=132824)[0m top5: 0.6077425373134329
[2m[36m(func pid=132824)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=132824)[0m f1_macro: 0.25643600993733917
[2m[36m(func pid=132824)[0m f1_weighted: 0.20020668968445707
[2m[36m(func pid=132824)[0m f1_per_class: [0.508, 0.191, 0.759, 0.382, 0.06, 0.111, 0.028, 0.337, 0.124, 0.065]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.4553 | Steps: 4 | Val loss: 16.8212 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7178 | Steps: 4 | Val loss: 1.7408 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=127193)[0m top1: 0.38992537313432835
[2m[36m(func pid=127193)[0m top5: 0.8791977611940298
[2m[36m(func pid=127193)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=127193)[0m f1_macro: 0.36046434394452637
[2m[36m(func pid=127193)[0m f1_weighted: 0.385772354479536
[2m[36m(func pid=127193)[0m f1_per_class: [0.642, 0.564, 0.419, 0.473, 0.126, 0.25, 0.278, 0.302, 0.22, 0.33]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.314365671641791
[2m[36m(func pid=127722)[0m top5: 0.8586753731343284
[2m[36m(func pid=127722)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=127722)[0m f1_macro: 0.3010829736883786
[2m[36m(func pid=127722)[0m f1_weighted: 0.36445905987204213
[2m[36m(func pid=127722)[0m f1_per_class: [0.411, 0.272, 0.571, 0.461, 0.074, 0.049, 0.505, 0.223, 0.111, 0.333]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 25.1509 | Steps: 4 | Val loss: 165.2691 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=121255)[0m top1: 0.37966417910447764
[2m[36m(func pid=121255)[0m top5: 0.886660447761194
[2m[36m(func pid=121255)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=121255)[0m f1_macro: 0.34882086354606157
[2m[36m(func pid=121255)[0m f1_weighted: 0.3954553349623143
[2m[36m(func pid=121255)[0m f1_per_class: [0.596, 0.519, 0.329, 0.509, 0.136, 0.232, 0.309, 0.354, 0.198, 0.308]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2092 | Steps: 4 | Val loss: 2.9685 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:29:01 (running for 00:21:19.26)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.718 |      0.349 |                   70 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.454 |      0.36  |                   48 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.455 |      0.301 |                   46 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 25.151 |      0.278 |                   26 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.2126865671641791
[2m[36m(func pid=132824)[0m top5: 0.6520522388059702
[2m[36m(func pid=132824)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=132824)[0m f1_macro: 0.2781460301088906
[2m[36m(func pid=132824)[0m f1_weighted: 0.24457209904912416
[2m[36m(func pid=132824)[0m f1_per_class: [0.619, 0.229, 0.611, 0.439, 0.078, 0.117, 0.08, 0.398, 0.153, 0.059]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.5654 | Steps: 4 | Val loss: 19.2386 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5745 | Steps: 4 | Val loss: 1.7403 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=127193)[0m top1: 0.4025186567164179
[2m[36m(func pid=127193)[0m top5: 0.8852611940298507
[2m[36m(func pid=127193)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=127193)[0m f1_macro: 0.3622142264595844
[2m[36m(func pid=127193)[0m f1_weighted: 0.4048719826848417
[2m[36m(func pid=127193)[0m f1_per_class: [0.636, 0.578, 0.351, 0.499, 0.126, 0.227, 0.316, 0.316, 0.231, 0.342]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27472014925373134
[2m[36m(func pid=127722)[0m top5: 0.8306902985074627
[2m[36m(func pid=127722)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=127722)[0m f1_macro: 0.3030445460415806
[2m[36m(func pid=127722)[0m f1_weighted: 0.3165225003859915
[2m[36m(func pid=127722)[0m f1_per_class: [0.496, 0.317, 0.636, 0.485, 0.094, 0.038, 0.278, 0.313, 0.108, 0.265]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.7134 | Steps: 4 | Val loss: 136.8757 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=121255)[0m top1: 0.38013059701492535
[2m[36m(func pid=121255)[0m top5: 0.8843283582089553
[2m[36m(func pid=121255)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=121255)[0m f1_macro: 0.3473624983330983
[2m[36m(func pid=121255)[0m f1_weighted: 0.39565328315588827
[2m[36m(func pid=121255)[0m f1_per_class: [0.619, 0.525, 0.353, 0.489, 0.116, 0.19, 0.337, 0.357, 0.209, 0.278]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1311 | Steps: 4 | Val loss: 2.7512 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:29:07 (running for 00:21:24.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.575 |      0.347 |                   71 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.209 |      0.362 |                   49 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.565 |      0.303 |                   47 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  4.713 |      0.295 |                   27 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.2868470149253731
[2m[36m(func pid=132824)[0m top5: 0.7178171641791045
[2m[36m(func pid=132824)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=132824)[0m f1_macro: 0.2953431686176232
[2m[36m(func pid=132824)[0m f1_weighted: 0.30681101196899085
[2m[36m(func pid=132824)[0m f1_per_class: [0.533, 0.399, 0.533, 0.495, 0.073, 0.14, 0.134, 0.397, 0.16, 0.089]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3480 | Steps: 4 | Val loss: 19.9724 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4904 | Steps: 4 | Val loss: 1.6827 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127193)[0m top1: 0.43050373134328357
[2m[36m(func pid=127193)[0m top5: 0.9006529850746269
[2m[36m(func pid=127193)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=127193)[0m f1_macro: 0.37705466283415673
[2m[36m(func pid=127193)[0m f1_weighted: 0.4456253905793197
[2m[36m(func pid=127193)[0m f1_per_class: [0.66, 0.576, 0.295, 0.523, 0.145, 0.238, 0.425, 0.312, 0.247, 0.35]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0829 | Steps: 4 | Val loss: 127.0013 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=127722)[0m top1: 0.292910447761194
[2m[36m(func pid=127722)[0m top5: 0.7924440298507462
[2m[36m(func pid=127722)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=127722)[0m f1_macro: 0.3280008988585418
[2m[36m(func pid=127722)[0m f1_weighted: 0.3099192185782387
[2m[36m(func pid=127722)[0m f1_per_class: [0.571, 0.388, 0.714, 0.531, 0.133, 0.038, 0.159, 0.326, 0.114, 0.306]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.39225746268656714
[2m[36m(func pid=121255)[0m top5: 0.9011194029850746
[2m[36m(func pid=121255)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=121255)[0m f1_macro: 0.35772117078250704
[2m[36m(func pid=121255)[0m f1_weighted: 0.4133002366711968
[2m[36m(func pid=121255)[0m f1_per_class: [0.611, 0.515, 0.4, 0.496, 0.127, 0.182, 0.4, 0.335, 0.228, 0.283]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.0463 | Steps: 4 | Val loss: 2.6263 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:29:12 (running for 00:21:29.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.49  |      0.358 |                   72 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.131 |      0.377 |                   50 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.348 |      0.328 |                   48 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  1.083 |      0.298 |                   28 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.333955223880597
[2m[36m(func pid=132824)[0m top5: 0.7448694029850746
[2m[36m(func pid=132824)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=132824)[0m f1_macro: 0.2977475685804607
[2m[36m(func pid=132824)[0m f1_weighted: 0.3354996758538839
[2m[36m(func pid=132824)[0m f1_per_class: [0.355, 0.493, 0.491, 0.465, 0.072, 0.14, 0.212, 0.392, 0.204, 0.157]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8699 | Steps: 4 | Val loss: 21.5666 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5124 | Steps: 4 | Val loss: 1.7314 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127193)[0m top1: 0.435634328358209
[2m[36m(func pid=127193)[0m top5: 0.9048507462686567
[2m[36m(func pid=127193)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=127193)[0m f1_macro: 0.3678195151371747
[2m[36m(func pid=127193)[0m f1_weighted: 0.4576426442542384
[2m[36m(func pid=127193)[0m f1_per_class: [0.629, 0.553, 0.252, 0.538, 0.154, 0.216, 0.482, 0.303, 0.211, 0.34]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 18.1744 | Steps: 4 | Val loss: 128.1908 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=127722)[0m top1: 0.30363805970149255
[2m[36m(func pid=127722)[0m top5: 0.7271455223880597
[2m[36m(func pid=127722)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=127722)[0m f1_macro: 0.32154288045293034
[2m[36m(func pid=127722)[0m f1_weighted: 0.28832346190648395
[2m[36m(func pid=127722)[0m f1_per_class: [0.6, 0.459, 0.645, 0.529, 0.133, 0.038, 0.045, 0.331, 0.12, 0.316]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.384794776119403
[2m[36m(func pid=121255)[0m top5: 0.8950559701492538
[2m[36m(func pid=121255)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=121255)[0m f1_macro: 0.34614230447371414
[2m[36m(func pid=121255)[0m f1_weighted: 0.4085710920895571
[2m[36m(func pid=121255)[0m f1_per_class: [0.592, 0.505, 0.393, 0.502, 0.116, 0.171, 0.396, 0.335, 0.201, 0.252]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.0648 | Steps: 4 | Val loss: 2.7028 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:29:17 (running for 00:21:35.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.512 |      0.346 |                   73 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.046 |      0.368 |                   51 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.87  |      0.322 |                   49 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 18.174 |      0.301 |                   29 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.34421641791044777
[2m[36m(func pid=132824)[0m top5: 0.7593283582089553
[2m[36m(func pid=132824)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=132824)[0m f1_macro: 0.30051730127315474
[2m[36m(func pid=132824)[0m f1_weighted: 0.32915617754983395
[2m[36m(func pid=132824)[0m f1_per_class: [0.228, 0.5, 0.542, 0.453, 0.106, 0.08, 0.226, 0.362, 0.215, 0.293]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2246 | Steps: 4 | Val loss: 21.0449 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.6105 | Steps: 4 | Val loss: 1.6747 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=127193)[0m top1: 0.4388992537313433
[2m[36m(func pid=127193)[0m top5: 0.9057835820895522
[2m[36m(func pid=127193)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=127193)[0m f1_macro: 0.35643650963568196
[2m[36m(func pid=127193)[0m f1_weighted: 0.46541657296208705
[2m[36m(func pid=127193)[0m f1_per_class: [0.591, 0.513, 0.218, 0.571, 0.144, 0.196, 0.515, 0.308, 0.17, 0.338]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 24.0254 | Steps: 4 | Val loss: 124.7618 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=127722)[0m top1: 0.3278917910447761
[2m[36m(func pid=127722)[0m top5: 0.7355410447761194
[2m[36m(func pid=127722)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=127722)[0m f1_macro: 0.31883878832648505
[2m[36m(func pid=127722)[0m f1_weighted: 0.2971424340376054
[2m[36m(func pid=127722)[0m f1_per_class: [0.612, 0.484, 0.55, 0.535, 0.154, 0.038, 0.057, 0.32, 0.119, 0.319]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.40531716417910446
[2m[36m(func pid=121255)[0m top5: 0.9039179104477612
[2m[36m(func pid=121255)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=121255)[0m f1_macro: 0.3584963972129827
[2m[36m(func pid=121255)[0m f1_weighted: 0.4307241881826139
[2m[36m(func pid=121255)[0m f1_per_class: [0.611, 0.501, 0.393, 0.52, 0.123, 0.161, 0.454, 0.34, 0.219, 0.264]
[2m[36m(func pid=121255)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.0573 | Steps: 4 | Val loss: 2.7183 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:29:23 (running for 00:21:40.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00008 | RUNNING    | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.611 |      0.358 |                   74 |
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.065 |      0.356 |                   52 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.225 |      0.319 |                   50 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 24.025 |      0.328 |                   30 |
| train_98a10_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=132824)[0m top1: 0.39225746268656714
[2m[36m(func pid=132824)[0m top5: 0.7915111940298507
[2m[36m(func pid=132824)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=132824)[0m f1_macro: 0.3276521322188509
[2m[36m(func pid=132824)[0m f1_weighted: 0.36198335610342613
[2m[36m(func pid=132824)[0m f1_per_class: [0.282, 0.546, 0.471, 0.497, 0.128, 0.058, 0.27, 0.355, 0.19, 0.478]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.7954 | Steps: 4 | Val loss: 21.1799 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=121255)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5716 | Steps: 4 | Val loss: 1.6734 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127193)[0m top1: 0.43656716417910446
[2m[36m(func pid=127193)[0m top5: 0.9034514925373134
[2m[36m(func pid=127193)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=127193)[0m f1_macro: 0.3584590787642471
[2m[36m(func pid=127193)[0m f1_weighted: 0.4639328766563124
[2m[36m(func pid=127193)[0m f1_per_class: [0.569, 0.437, 0.248, 0.567, 0.168, 0.195, 0.563, 0.282, 0.149, 0.406]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 42.5216 | Steps: 4 | Val loss: 118.9165 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=127722)[0m top1: 0.3591417910447761
[2m[36m(func pid=127722)[0m top5: 0.7341417910447762
[2m[36m(func pid=127722)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=127722)[0m f1_macro: 0.3318208848831641
[2m[36m(func pid=127722)[0m f1_weighted: 0.3090714283184616
[2m[36m(func pid=127722)[0m f1_per_class: [0.636, 0.514, 0.491, 0.568, 0.202, 0.045, 0.045, 0.299, 0.132, 0.387]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=121255)[0m top1: 0.39972014925373134
[2m[36m(func pid=121255)[0m top5: 0.8973880597014925
[2m[36m(func pid=121255)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=121255)[0m f1_macro: 0.34569984967096534
[2m[36m(func pid=121255)[0m f1_weighted: 0.42919094859504364
[2m[36m(func pid=121255)[0m f1_per_class: [0.571, 0.469, 0.358, 0.524, 0.12, 0.162, 0.471, 0.332, 0.193, 0.257]
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1660 | Steps: 4 | Val loss: 2.9813 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=132824)[0m top1: 0.40531716417910446
[2m[36m(func pid=132824)[0m top5: 0.7999067164179104
[2m[36m(func pid=132824)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=132824)[0m f1_macro: 0.31074741032485187
[2m[36m(func pid=132824)[0m f1_weighted: 0.3855768108008107
[2m[36m(func pid=132824)[0m f1_per_class: [0.286, 0.561, 0.473, 0.5, 0.139, 0.096, 0.338, 0.342, 0.18, 0.194]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 4.9473 | Steps: 4 | Val loss: 21.9408 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=127193)[0m top1: 0.41138059701492535
[2m[36m(func pid=127193)[0m top5: 0.8894589552238806
[2m[36m(func pid=127193)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=127193)[0m f1_macro: 0.3429273577485568
[2m[36m(func pid=127193)[0m f1_weighted: 0.4431321597571209
[2m[36m(func pid=127193)[0m f1_per_class: [0.574, 0.348, 0.197, 0.569, 0.166, 0.165, 0.558, 0.271, 0.126, 0.456]
[2m[36m(func pid=127722)[0m top1: 0.3694029850746269
[2m[36m(func pid=127722)[0m top5: 0.730410447761194
[2m[36m(func pid=127722)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.3244156180680058
[2m[36m(func pid=127722)[0m f1_weighted: 0.30391835799083106
[2m[36m(func pid=127722)[0m f1_per_class: [0.61, 0.516, 0.413, 0.537, 0.169, 0.065, 0.046, 0.314, 0.147, 0.429]
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 11.7396 | Steps: 4 | Val loss: 108.8462 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=132824)[0m top1: 0.41091417910447764
[2m[36m(func pid=132824)[0m top5: 0.7919776119402985
[2m[36m(func pid=132824)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=132824)[0m f1_macro: 0.32288608919411954
[2m[36m(func pid=132824)[0m f1_weighted: 0.4053923702017937
[2m[36m(func pid=132824)[0m f1_per_class: [0.41, 0.552, 0.361, 0.458, 0.139, 0.138, 0.431, 0.316, 0.166, 0.258]
== Status ==
Current time: 2024-01-07 11:29:28 (running for 00:21:46.10)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.057 |      0.358 |                   53 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.795 |      0.332 |                   51 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 42.522 |      0.311 |                   31 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=140546)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=140546)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=140546)[0m Configuration completed!
[2m[36m(func pid=140546)[0m New optimizer parameters:
[2m[36m(func pid=140546)[0m SGD (
[2m[36m(func pid=140546)[0m Parameter Group 0
[2m[36m(func pid=140546)[0m     dampening: 0
[2m[36m(func pid=140546)[0m     differentiable: False== Status ==
Current time: 2024-01-07 11:29:35 (running for 00:21:53.04)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.166 |      0.343 |                   54 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.795 |      0.332 |                   51 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 42.522 |      0.311 |                   31 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)



[2m[36m(func pid=140546)[0m     foreach: None
[2m[36m(func pid=140546)[0m     lr: 0.0001
[2m[36m(func pid=140546)[0m     maximize: False
[2m[36m(func pid=140546)[0m     momentum: 0.9
[2m[36m(func pid=140546)[0m     nesterov: False
[2m[36m(func pid=140546)[0m     weight_decay: 0.0001
[2m[36m(func pid=140546)[0m )
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 48.2273 | Steps: 4 | Val loss: 111.7625 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3443 | Steps: 4 | Val loss: 3.1335 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.8651 | Steps: 4 | Val loss: 21.6714 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9793 | Steps: 4 | Val loss: 2.5404 | Batch size: 32 | lr: 0.0001 | Duration: 4.70s
== Status ==
Current time: 2024-01-07 11:29:40 (running for 00:21:58.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.166 |      0.343 |                   54 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.947 |      0.324 |                   52 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 11.74  |      0.323 |                   32 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3763992537313433
[2m[36m(func pid=132824)[0m top5: 0.777518656716418
[2m[36m(func pid=132824)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=132824)[0m f1_macro: 0.3388004807016222
[2m[36m(func pid=132824)[0m f1_weighted: 0.39146523060291966
[2m[36m(func pid=132824)[0m f1_per_class: [0.576, 0.505, 0.387, 0.375, 0.137, 0.197, 0.461, 0.278, 0.147, 0.324]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3763992537313433
[2m[36m(func pid=127722)[0m top5: 0.7467350746268657
[2m[36m(func pid=127722)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=127722)[0m f1_macro: 0.3246989059833514
[2m[36m(func pid=127722)[0m f1_weighted: 0.3092145501347213
[2m[36m(func pid=127722)[0m f1_per_class: [0.571, 0.512, 0.292, 0.532, 0.259, 0.094, 0.057, 0.315, 0.188, 0.426]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.4123134328358209
[2m[36m(func pid=127193)[0m top5: 0.875
[2m[36m(func pid=127193)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=127193)[0m f1_macro: 0.3199353161784719
[2m[36m(func pid=127193)[0m f1_weighted: 0.42515984595085254
[2m[36m(func pid=127193)[0m f1_per_class: [0.534, 0.198, 0.168, 0.581, 0.186, 0.13, 0.589, 0.289, 0.118, 0.407]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.0625
[2m[36m(func pid=140546)[0m top5: 0.4818097014925373
[2m[36m(func pid=140546)[0m f1_micro: 0.0625
[2m[36m(func pid=140546)[0m f1_macro: 0.034724861455794225
[2m[36m(func pid=140546)[0m f1_weighted: 0.03472795841629402
[2m[36m(func pid=140546)[0m f1_per_class: [0.08, 0.015, 0.0, 0.077, 0.0, 0.019, 0.0, 0.101, 0.024, 0.033]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 14.6296 | Steps: 4 | Val loss: 132.2015 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.0527 | Steps: 4 | Val loss: 19.9234 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4452 | Steps: 4 | Val loss: 3.3123 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1953 | Steps: 4 | Val loss: 2.5768 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:29:46 (running for 00:22:03.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.344 |      0.32  |                   55 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.865 |      0.325 |                   53 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 14.63  |      0.316 |                   34 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.979 |      0.035 |                    1 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3162313432835821
[2m[36m(func pid=132824)[0m top5: 0.7257462686567164
[2m[36m(func pid=132824)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=132824)[0m f1_macro: 0.3163175179826225
[2m[36m(func pid=132824)[0m f1_weighted: 0.3458462016035349
[2m[36m(func pid=132824)[0m f1_per_class: [0.633, 0.363, 0.32, 0.274, 0.169, 0.329, 0.441, 0.261, 0.111, 0.263]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3666044776119403
[2m[36m(func pid=127722)[0m top5: 0.7700559701492538
[2m[36m(func pid=127722)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=127722)[0m f1_macro: 0.308744820766761
[2m[36m(func pid=127722)[0m f1_weighted: 0.33225394946317244
[2m[36m(func pid=127722)[0m f1_per_class: [0.571, 0.513, 0.181, 0.556, 0.28, 0.209, 0.079, 0.318, 0.179, 0.202]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.39972014925373134
[2m[36m(func pid=127193)[0m top5: 0.867070895522388
[2m[36m(func pid=127193)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=127193)[0m f1_macro: 0.3086785096081373
[2m[36m(func pid=127193)[0m f1_weighted: 0.4151745053528242
[2m[36m(func pid=127193)[0m f1_per_class: [0.544, 0.168, 0.148, 0.567, 0.174, 0.135, 0.588, 0.272, 0.115, 0.375]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.05643656716417911
[2m[36m(func pid=140546)[0m top5: 0.4608208955223881
[2m[36m(func pid=140546)[0m f1_micro: 0.05643656716417911
[2m[36m(func pid=140546)[0m f1_macro: 0.031244888445847857
[2m[36m(func pid=140546)[0m f1_weighted: 0.03423327935591713
[2m[36m(func pid=140546)[0m f1_per_class: [0.052, 0.023, 0.0, 0.074, 0.0, 0.02, 0.0, 0.092, 0.024, 0.028]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 11.6367 | Steps: 4 | Val loss: 177.3002 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5091 | Steps: 4 | Val loss: 3.4537 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.4868 | Steps: 4 | Val loss: 21.0643 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9910 | Steps: 4 | Val loss: 2.5762 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:29:51 (running for 00:22:09.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.445 |      0.309 |                   56 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.053 |      0.309 |                   54 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 11.637 |      0.274 |                   35 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  3.195 |      0.031 |                    2 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.27098880597014924
[2m[36m(func pid=132824)[0m top5: 0.6422574626865671
[2m[36m(func pid=132824)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=132824)[0m f1_macro: 0.27412811815650184
[2m[36m(func pid=132824)[0m f1_weighted: 0.28272227477838296
[2m[36m(func pid=132824)[0m f1_per_class: [0.469, 0.196, 0.289, 0.175, 0.192, 0.357, 0.416, 0.287, 0.091, 0.27]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.32136194029850745
[2m[36m(func pid=127722)[0m top5: 0.7961753731343284
[2m[36m(func pid=127722)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=127722)[0m f1_macro: 0.2893623577632714
[2m[36m(func pid=127722)[0m f1_weighted: 0.3191292279635176
[2m[36m(func pid=127722)[0m f1_per_class: [0.521, 0.418, 0.17, 0.541, 0.333, 0.278, 0.089, 0.306, 0.147, 0.09]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3726679104477612
[2m[36m(func pid=127193)[0m top5: 0.8624067164179104
[2m[36m(func pid=127193)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=127193)[0m f1_macro: 0.299522433775426
[2m[36m(func pid=127193)[0m f1_weighted: 0.39884493004194793
[2m[36m(func pid=127193)[0m f1_per_class: [0.534, 0.123, 0.141, 0.56, 0.159, 0.209, 0.542, 0.267, 0.105, 0.356]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.060167910447761194
[2m[36m(func pid=140546)[0m top5: 0.4519589552238806
[2m[36m(func pid=140546)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=140546)[0m f1_macro: 0.03517008315637519
[2m[36m(func pid=140546)[0m f1_weighted: 0.04244652469690368
[2m[36m(func pid=140546)[0m f1_per_class: [0.047, 0.059, 0.0, 0.086, 0.0, 0.006, 0.0, 0.094, 0.023, 0.037]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 51.3189 | Steps: 4 | Val loss: 202.3722 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.0850 | Steps: 4 | Val loss: 3.5018 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 3.8549 | Steps: 4 | Val loss: 20.0931 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0177 | Steps: 4 | Val loss: 2.5701 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:29:56 (running for 00:22:14.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.509 |      0.3   |                   57 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.487 |      0.289 |                   55 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 51.319 |      0.231 |                   36 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.991 |      0.035 |                    3 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.2462686567164179
[2m[36m(func pid=132824)[0m top5: 0.6226679104477612
[2m[36m(func pid=132824)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=132824)[0m f1_macro: 0.23060344090394325
[2m[36m(func pid=132824)[0m f1_weighted: 0.24427144401106105
[2m[36m(func pid=132824)[0m f1_per_class: [0.196, 0.134, 0.26, 0.184, 0.149, 0.313, 0.345, 0.308, 0.083, 0.333]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3045708955223881
[2m[36m(func pid=127722)[0m top5: 0.8423507462686567
[2m[36m(func pid=127722)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=127722)[0m f1_macro: 0.2826243882051764
[2m[36m(func pid=127722)[0m f1_weighted: 0.3245392469987708
[2m[36m(func pid=127722)[0m f1_per_class: [0.485, 0.344, 0.187, 0.53, 0.333, 0.287, 0.165, 0.295, 0.126, 0.074]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.36427238805970147
[2m[36m(func pid=127193)[0m top5: 0.8642723880597015
[2m[36m(func pid=127193)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=127193)[0m f1_macro: 0.29611349038599616
[2m[36m(func pid=127193)[0m f1_weighted: 0.38179272100788786
[2m[36m(func pid=127193)[0m f1_per_class: [0.5, 0.1, 0.157, 0.588, 0.188, 0.264, 0.449, 0.287, 0.111, 0.318]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.05970149253731343
[2m[36m(func pid=140546)[0m top5: 0.43796641791044777
[2m[36m(func pid=140546)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=140546)[0m f1_macro: 0.03894855134493483
[2m[36m(func pid=140546)[0m f1_weighted: 0.04630324105053712
[2m[36m(func pid=140546)[0m f1_per_class: [0.051, 0.074, 0.0, 0.086, 0.0, 0.012, 0.0, 0.092, 0.042, 0.032]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 68.3237 | Steps: 4 | Val loss: 196.3653 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6725 | Steps: 4 | Val loss: 18.5784 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1553 | Steps: 4 | Val loss: 3.6211 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0643 | Steps: 4 | Val loss: 2.5629 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=132824)[0m top1: 0.25093283582089554
[2m[36m(func pid=132824)[0m top5: 0.6651119402985075
[2m[36m(func pid=132824)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=132824)[0m f1_macro: 0.23227503079313966
[2m[36m(func pid=132824)[0m f1_weighted: 0.2559405724073665
[2m[36m(func pid=132824)[0m f1_per_class: [0.2, 0.13, 0.295, 0.339, 0.148, 0.315, 0.249, 0.27, 0.091, 0.286]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:30:02 (running for 00:22:19.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.085 |      0.296 |                   58 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.855 |      0.283 |                   56 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 68.324 |      0.232 |                   37 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  3.018 |      0.039 |                    4 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127722)[0m top1: 0.3069029850746269
[2m[36m(func pid=127722)[0m top5: 0.8777985074626866
[2m[36m(func pid=127722)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.29847394843866476
[2m[36m(func pid=127722)[0m f1_weighted: 0.3386382863292457
[2m[36m(func pid=127722)[0m f1_per_class: [0.56, 0.335, 0.286, 0.494, 0.286, 0.318, 0.237, 0.287, 0.114, 0.069]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.37033582089552236
[2m[36m(func pid=127193)[0m top5: 0.8549440298507462
[2m[36m(func pid=127193)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=127193)[0m f1_macro: 0.2984967806412234
[2m[36m(func pid=127193)[0m f1_weighted: 0.3741534709103013
[2m[36m(func pid=127193)[0m f1_per_class: [0.474, 0.087, 0.179, 0.607, 0.195, 0.301, 0.395, 0.311, 0.118, 0.318]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.06436567164179105
[2m[36m(func pid=140546)[0m top5: 0.41091417910447764
[2m[36m(func pid=140546)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=140546)[0m f1_macro: 0.044582102161864375
[2m[36m(func pid=140546)[0m f1_weighted: 0.056228923735318445
[2m[36m(func pid=140546)[0m f1_per_class: [0.043, 0.082, 0.0, 0.108, 0.0, 0.018, 0.003, 0.096, 0.065, 0.032]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 195.9656 | Steps: 4 | Val loss: 173.8976 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.1863 | Steps: 4 | Val loss: 17.2651 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5507 | Steps: 4 | Val loss: 3.6124 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9631 | Steps: 4 | Val loss: 2.5486 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:30:07 (running for 00:22:25.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |   0.155 |      0.298 |                   59 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |   1.673 |      0.298 |                   57 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 195.966 |      0.253 |                   38 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |   3.064 |      0.045 |                    5 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |   0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |   0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |   3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      |  13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |   2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |   0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |   1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |   8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.29244402985074625
[2m[36m(func pid=132824)[0m top5: 0.7509328358208955
[2m[36m(func pid=132824)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=132824)[0m f1_macro: 0.25256637733923737
[2m[36m(func pid=132824)[0m f1_weighted: 0.3032259154309045
[2m[36m(func pid=132824)[0m f1_per_class: [0.087, 0.332, 0.366, 0.47, 0.148, 0.323, 0.171, 0.263, 0.112, 0.253]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3292910447761194
[2m[36m(func pid=127722)[0m top5: 0.8922574626865671
[2m[36m(func pid=127722)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=127722)[0m f1_macro: 0.3080549548718114
[2m[36m(func pid=127722)[0m f1_weighted: 0.3662095367539019
[2m[36m(func pid=127722)[0m f1_per_class: [0.543, 0.295, 0.316, 0.487, 0.267, 0.329, 0.355, 0.267, 0.146, 0.076]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m top1: 0.37453358208955223
[2m[36m(func pid=127193)[0m top5: 0.863339552238806
[2m[36m(func pid=127193)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=127193)[0m f1_macro: 0.31710273618862517
[2m[36m(func pid=127193)[0m f1_weighted: 0.36405566578148824
[2m[36m(func pid=127193)[0m f1_per_class: [0.464, 0.096, 0.286, 0.62, 0.22, 0.322, 0.326, 0.326, 0.136, 0.375]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=140546)[0m top1: 0.06763059701492537
[2m[36m(func pid=140546)[0m top5: 0.39925373134328357
[2m[36m(func pid=140546)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=140546)[0m f1_macro: 0.04847161992330858
[2m[36m(func pid=140546)[0m f1_weighted: 0.0621496830070376
[2m[36m(func pid=140546)[0m f1_per_class: [0.05, 0.116, 0.0, 0.107, 0.0, 0.023, 0.003, 0.093, 0.063, 0.03]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 8.4400 | Steps: 4 | Val loss: 176.3666 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.0601 | Steps: 4 | Val loss: 3.8938 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0285 | Steps: 4 | Val loss: 16.1098 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9099 | Steps: 4 | Val loss: 2.5272 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 11:30:13 (running for 00:22:30.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.551 |      0.317 |                   60 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.186 |      0.308 |                   58 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  8.44  |      0.278 |                   39 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.963 |      0.048 |                    6 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3185634328358209
[2m[36m(func pid=132824)[0m top5: 0.7826492537313433
[2m[36m(func pid=132824)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=132824)[0m f1_macro: 0.27790586939329753
[2m[36m(func pid=132824)[0m f1_weighted: 0.31292092763296264
[2m[36m(func pid=132824)[0m f1_per_class: [0.2, 0.455, 0.473, 0.524, 0.095, 0.174, 0.121, 0.281, 0.152, 0.305]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3656716417910448
[2m[36m(func pid=127193)[0m top5: 0.8484141791044776
[2m[36m(func pid=127193)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=127193)[0m f1_macro: 0.30968940571720355
[2m[36m(func pid=127193)[0m f1_weighted: 0.3377825608449071
[2m[36m(func pid=127193)[0m f1_per_class: [0.458, 0.105, 0.295, 0.616, 0.2, 0.314, 0.234, 0.358, 0.14, 0.377]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.355410447761194
[2m[36m(func pid=127722)[0m top5: 0.8852611940298507
[2m[36m(func pid=127722)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=127722)[0m f1_macro: 0.3351496391836465
[2m[36m(func pid=127722)[0m f1_weighted: 0.3852332165302348
[2m[36m(func pid=127722)[0m f1_per_class: [0.628, 0.327, 0.421, 0.43, 0.258, 0.307, 0.454, 0.251, 0.154, 0.121]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.06996268656716417
[2m[36m(func pid=140546)[0m top5: 0.4006529850746269
[2m[36m(func pid=140546)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=140546)[0m f1_macro: 0.05071529456599098
[2m[36m(func pid=140546)[0m f1_weighted: 0.06785044274928019
[2m[36m(func pid=140546)[0m f1_per_class: [0.048, 0.133, 0.0, 0.115, 0.0, 0.027, 0.006, 0.078, 0.068, 0.034]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 20.4443 | Steps: 4 | Val loss: 192.3301 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1089 | Steps: 4 | Val loss: 4.0721 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5911 | Steps: 4 | Val loss: 15.9029 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0563 | Steps: 4 | Val loss: 2.5160 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:30:18 (running for 00:22:35.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.06  |      0.31  |                   61 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.029 |      0.335 |                   59 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 20.444 |      0.289 |                   40 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.91  |      0.051 |                    7 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3582089552238806
[2m[36m(func pid=132824)[0m top5: 0.8138992537313433
[2m[36m(func pid=132824)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=132824)[0m f1_macro: 0.28863203310847274
[2m[36m(func pid=132824)[0m f1_weighted: 0.31497588336111365
[2m[36m(func pid=132824)[0m f1_per_class: [0.296, 0.471, 0.55, 0.585, 0.103, 0.037, 0.099, 0.315, 0.166, 0.265]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.365205223880597
[2m[36m(func pid=127193)[0m top5: 0.8446828358208955
[2m[36m(func pid=127193)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=127193)[0m f1_macro: 0.30489382239806767
[2m[36m(func pid=127193)[0m f1_weighted: 0.3192064195508473
[2m[36m(func pid=127193)[0m f1_per_class: [0.466, 0.116, 0.329, 0.617, 0.188, 0.315, 0.164, 0.358, 0.135, 0.36]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.376865671641791
[2m[36m(func pid=127722)[0m top5: 0.898320895522388
[2m[36m(func pid=127722)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=127722)[0m f1_macro: 0.3466995599621803
[2m[36m(func pid=127722)[0m f1_weighted: 0.39419049220002744
[2m[36m(func pid=127722)[0m f1_per_class: [0.636, 0.314, 0.436, 0.408, 0.294, 0.311, 0.52, 0.185, 0.138, 0.224]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.0708955223880597
[2m[36m(func pid=140546)[0m top5: 0.39225746268656714
[2m[36m(func pid=140546)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=140546)[0m f1_macro: 0.05168810118160686
[2m[36m(func pid=140546)[0m f1_weighted: 0.06704728427910985
[2m[36m(func pid=140546)[0m f1_per_class: [0.053, 0.156, 0.0, 0.095, 0.0, 0.034, 0.006, 0.072, 0.067, 0.033]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 5.0298 | Steps: 4 | Val loss: 224.3749 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5226 | Steps: 4 | Val loss: 4.1502 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5213 | Steps: 4 | Val loss: 17.2327 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9102 | Steps: 4 | Val loss: 2.5065 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:30:23 (running for 00:22:41.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.109 |      0.305 |                   62 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.591 |      0.347 |                   60 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  5.03  |      0.321 |                   41 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  3.056 |      0.052 |                    8 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.376865671641791
[2m[36m(func pid=132824)[0m top5: 0.8218283582089553
[2m[36m(func pid=132824)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=132824)[0m f1_macro: 0.32133736457861717
[2m[36m(func pid=132824)[0m f1_weighted: 0.3061869887186187
[2m[36m(func pid=132824)[0m f1_per_class: [0.484, 0.491, 0.733, 0.585, 0.122, 0.0, 0.054, 0.33, 0.155, 0.26]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.36473880597014924
[2m[36m(func pid=127193)[0m top5: 0.8512126865671642
[2m[36m(func pid=127193)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=127193)[0m f1_macro: 0.31204290972551707
[2m[36m(func pid=127193)[0m f1_weighted: 0.32136634531653474
[2m[36m(func pid=127193)[0m f1_per_class: [0.415, 0.167, 0.406, 0.626, 0.165, 0.298, 0.138, 0.351, 0.177, 0.377]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3773320895522388
[2m[36m(func pid=127722)[0m top5: 0.9029850746268657
[2m[36m(func pid=127722)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=127722)[0m f1_macro: 0.363781549604838
[2m[36m(func pid=127722)[0m f1_weighted: 0.3800619223708672
[2m[36m(func pid=127722)[0m f1_per_class: [0.659, 0.217, 0.595, 0.386, 0.367, 0.279, 0.558, 0.165, 0.126, 0.286]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.06763059701492537
[2m[36m(func pid=140546)[0m top5: 0.3903917910447761
[2m[36m(func pid=140546)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=140546)[0m f1_macro: 0.051341670635471126
[2m[36m(func pid=140546)[0m f1_weighted: 0.06445597302822732
[2m[36m(func pid=140546)[0m f1_per_class: [0.043, 0.153, 0.02, 0.083, 0.0, 0.049, 0.006, 0.077, 0.051, 0.031]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 9.5903 | Steps: 4 | Val loss: 262.8132 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5909 | Steps: 4 | Val loss: 4.1921 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5568 | Steps: 4 | Val loss: 19.0287 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8710 | Steps: 4 | Val loss: 2.4740 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 11:30:29 (running for 00:22:46.49)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.523 |      0.312 |                   63 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.521 |      0.364 |                   61 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  9.59  |      0.332 |                   42 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.91  |      0.051 |                    9 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.35027985074626866
[2m[36m(func pid=127193)[0m top5: 0.8414179104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=127193)[0m f1_macro: 0.30267164064289903
[2m[36m(func pid=127193)[0m f1_weighted: 0.31665435397392194
[2m[36m(func pid=127193)[0m f1_per_class: [0.396, 0.204, 0.366, 0.61, 0.132, 0.276, 0.125, 0.367, 0.174, 0.377]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.3805970149253731
[2m[36m(func pid=132824)[0m top5: 0.8190298507462687
[2m[36m(func pid=132824)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=132824)[0m f1_macro: 0.3316406743640578
[2m[36m(func pid=132824)[0m f1_weighted: 0.2929215799133189
[2m[36m(func pid=132824)[0m f1_per_class: [0.638, 0.46, 0.75, 0.566, 0.128, 0.0, 0.034, 0.343, 0.132, 0.266]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.34328358208955223
[2m[36m(func pid=127722)[0m top5: 0.8950559701492538
[2m[36m(func pid=127722)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=127722)[0m f1_macro: 0.3310427812594168
[2m[36m(func pid=127722)[0m f1_weighted: 0.35551896078974915
[2m[36m(func pid=127722)[0m f1_per_class: [0.667, 0.164, 0.667, 0.357, 0.261, 0.224, 0.563, 0.161, 0.119, 0.129]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.0708955223880597
[2m[36m(func pid=140546)[0m top5: 0.40578358208955223
[2m[36m(func pid=140546)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=140546)[0m f1_macro: 0.05379999797169419
[2m[36m(func pid=140546)[0m f1_weighted: 0.07176991090507143
[2m[36m(func pid=140546)[0m f1_per_class: [0.043, 0.156, 0.019, 0.092, 0.0, 0.056, 0.018, 0.072, 0.057, 0.024]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.0374 | Steps: 4 | Val loss: 4.2350 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 40.8546 | Steps: 4 | Val loss: 276.2054 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 4.6350 | Steps: 4 | Val loss: 20.6809 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8388 | Steps: 4 | Val loss: 2.4601 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:30:34 (running for 00:22:51.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.591 |      0.303 |                   64 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.557 |      0.331 |                   62 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 40.855 |      0.328 |                   43 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.871 |      0.054 |                   10 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.35027985074626866
[2m[36m(func pid=127193)[0m top5: 0.8330223880597015
[2m[36m(func pid=127193)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=127193)[0m f1_macro: 0.29301434889945743
[2m[36m(func pid=127193)[0m f1_weighted: 0.3124807452929146
[2m[36m(func pid=127193)[0m f1_per_class: [0.408, 0.214, 0.361, 0.616, 0.119, 0.261, 0.109, 0.356, 0.184, 0.302]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.37546641791044777
[2m[36m(func pid=132824)[0m top5: 0.8129664179104478
[2m[36m(func pid=132824)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=132824)[0m f1_macro: 0.32798147615714157
[2m[36m(func pid=132824)[0m f1_weighted: 0.2886825623808774
[2m[36m(func pid=132824)[0m f1_per_class: [0.642, 0.447, 0.75, 0.562, 0.137, 0.0, 0.031, 0.345, 0.136, 0.23]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3125
[2m[36m(func pid=127722)[0m top5: 0.8796641791044776
[2m[36m(func pid=127722)[0m f1_micro: 0.3125
[2m[36m(func pid=127722)[0m f1_macro: 0.31871400442815123
[2m[36m(func pid=127722)[0m f1_weighted: 0.3369455262457044
[2m[36m(func pid=127722)[0m f1_per_class: [0.674, 0.133, 0.759, 0.334, 0.159, 0.171, 0.552, 0.224, 0.105, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.0708955223880597
[2m[36m(func pid=140546)[0m top5: 0.4146455223880597
[2m[36m(func pid=140546)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=140546)[0m f1_macro: 0.05408634126973366
[2m[36m(func pid=140546)[0m f1_weighted: 0.07445414136129187
[2m[36m(func pid=140546)[0m f1_per_class: [0.062, 0.147, 0.0, 0.1, 0.0, 0.061, 0.024, 0.062, 0.064, 0.022]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 18.0282 | Steps: 4 | Val loss: 256.1376 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1530 | Steps: 4 | Val loss: 4.2776 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 5.4241 | Steps: 4 | Val loss: 20.4489 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8199 | Steps: 4 | Val loss: 2.4523 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 11:30:39 (running for 00:22:57.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.037 |      0.293 |                   65 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.635 |      0.319 |                   63 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 18.028 |      0.322 |                   44 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.839 |      0.054 |                   11 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.35867537313432835
[2m[36m(func pid=132824)[0m top5: 0.7639925373134329
[2m[36m(func pid=132824)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=132824)[0m f1_macro: 0.32222125580790306
[2m[36m(func pid=132824)[0m f1_weighted: 0.2940425514680957
[2m[36m(func pid=132824)[0m f1_per_class: [0.561, 0.483, 0.786, 0.542, 0.112, 0.0, 0.051, 0.346, 0.162, 0.179]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3516791044776119
[2m[36m(func pid=127193)[0m top5: 0.820429104477612
[2m[36m(func pid=127193)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=127193)[0m f1_macro: 0.30124724085722804
[2m[36m(func pid=127193)[0m f1_weighted: 0.3121974022986773
[2m[36m(func pid=127193)[0m f1_per_class: [0.464, 0.263, 0.413, 0.602, 0.113, 0.239, 0.095, 0.364, 0.195, 0.266]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.31296641791044777
[2m[36m(func pid=127722)[0m top5: 0.8880597014925373
[2m[36m(func pid=127722)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=127722)[0m f1_macro: 0.32120006319048383
[2m[36m(func pid=127722)[0m f1_weighted: 0.3623509015810492
[2m[36m(func pid=127722)[0m f1_per_class: [0.604, 0.187, 0.786, 0.476, 0.077, 0.098, 0.491, 0.296, 0.12, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.07555970149253731
[2m[36m(func pid=140546)[0m top5: 0.42117537313432835
[2m[36m(func pid=140546)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=140546)[0m f1_macro: 0.06254149397614307
[2m[36m(func pid=140546)[0m f1_weighted: 0.08099883261228971
[2m[36m(func pid=140546)[0m f1_per_class: [0.059, 0.141, 0.062, 0.099, 0.0, 0.071, 0.044, 0.075, 0.051, 0.022]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 160.4448 | Steps: 4 | Val loss: 270.6527 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2166 | Steps: 4 | Val loss: 4.5532 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.5378 | Steps: 4 | Val loss: 23.5141 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8827 | Steps: 4 | Val loss: 2.4290 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:30:44 (running for 00:23:02.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |   0.153 |      0.301 |                   66 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |   5.424 |      0.321 |                   64 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 160.445 |      0.277 |                   45 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |   2.82  |      0.063 |                   12 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |   0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |   0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |   3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      |  13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |   2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |   0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |   1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |   8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.2957089552238806
[2m[36m(func pid=132824)[0m top5: 0.6963619402985075
[2m[36m(func pid=132824)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=132824)[0m f1_macro: 0.27671409368924743
[2m[36m(func pid=132824)[0m f1_weighted: 0.2525737793000483
[2m[36m(func pid=132824)[0m f1_per_class: [0.387, 0.444, 0.688, 0.427, 0.101, 0.0, 0.054, 0.355, 0.185, 0.128]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3512126865671642
[2m[36m(func pid=127193)[0m top5: 0.8017723880597015
[2m[36m(func pid=127193)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=127193)[0m f1_macro: 0.30110414484013
[2m[36m(func pid=127193)[0m f1_weighted: 0.3131089072212759
[2m[36m(func pid=127193)[0m f1_per_class: [0.506, 0.322, 0.394, 0.585, 0.105, 0.203, 0.087, 0.381, 0.221, 0.207]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27845149253731344
[2m[36m(func pid=127722)[0m top5: 0.8614738805970149
[2m[36m(func pid=127722)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=127722)[0m f1_macro: 0.300463344567787
[2m[36m(func pid=127722)[0m f1_weighted: 0.3172997077876264
[2m[36m(func pid=127722)[0m f1_per_class: [0.587, 0.203, 0.769, 0.53, 0.047, 0.055, 0.298, 0.284, 0.158, 0.074]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.08255597014925373
[2m[36m(func pid=140546)[0m top5: 0.44029850746268656
[2m[36m(func pid=140546)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=140546)[0m f1_macro: 0.06533141000484925
[2m[36m(func pid=140546)[0m f1_weighted: 0.09119337207534513
[2m[36m(func pid=140546)[0m f1_per_class: [0.077, 0.147, 0.047, 0.125, 0.0, 0.074, 0.052, 0.062, 0.047, 0.024]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 10.7975 | Steps: 4 | Val loss: 255.7028 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.9598 | Steps: 4 | Val loss: 4.4620 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 5.3280 | Steps: 4 | Val loss: 31.2050 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8565 | Steps: 4 | Val loss: 2.3994 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:30:50 (running for 00:23:07.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.217 |      0.301 |                   67 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.538 |      0.3   |                   65 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 10.797 |      0.255 |                   46 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.883 |      0.065 |                   13 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.26399253731343286
[2m[36m(func pid=132824)[0m top5: 0.6707089552238806
[2m[36m(func pid=132824)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=132824)[0m f1_macro: 0.2547566768591797
[2m[36m(func pid=132824)[0m f1_weighted: 0.23034501134466337
[2m[36m(func pid=132824)[0m f1_per_class: [0.245, 0.448, 0.688, 0.26, 0.087, 0.016, 0.143, 0.325, 0.175, 0.162]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.3451492537313433
[2m[36m(func pid=127193)[0m top5: 0.8022388059701493
[2m[36m(func pid=127193)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=127193)[0m f1_macro: 0.31023257590988573
[2m[36m(func pid=127193)[0m f1_weighted: 0.3195818235021873
[2m[36m(func pid=127193)[0m f1_per_class: [0.529, 0.379, 0.481, 0.58, 0.1, 0.193, 0.087, 0.359, 0.218, 0.175]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2439365671641791
[2m[36m(func pid=127722)[0m top5: 0.8316231343283582
[2m[36m(func pid=127722)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=127722)[0m f1_macro: 0.2776443634834173
[2m[36m(func pid=127722)[0m f1_weighted: 0.2578937106164168
[2m[36m(func pid=127722)[0m f1_per_class: [0.585, 0.211, 0.692, 0.54, 0.038, 0.015, 0.099, 0.263, 0.195, 0.138]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.08908582089552239
[2m[36m(func pid=140546)[0m top5: 0.4612873134328358
[2m[36m(func pid=140546)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=140546)[0m f1_macro: 0.0703620333746649
[2m[36m(func pid=140546)[0m f1_weighted: 0.10216550203701477
[2m[36m(func pid=140546)[0m f1_per_class: [0.084, 0.127, 0.043, 0.141, 0.0, 0.081, 0.08, 0.066, 0.056, 0.026]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 21.3329 | Steps: 4 | Val loss: 245.8283 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3812 | Steps: 4 | Val loss: 4.3163 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 6.9594 | Steps: 4 | Val loss: 32.2338 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8395 | Steps: 4 | Val loss: 2.3889 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 11:30:55 (running for 00:23:12.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.96  |      0.31  |                   68 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.328 |      0.278 |                   66 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 21.333 |      0.253 |                   47 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.856 |      0.07  |                   14 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.24440298507462688
[2m[36m(func pid=132824)[0m top5: 0.6637126865671642
[2m[36m(func pid=132824)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=132824)[0m f1_macro: 0.25315056996641644
[2m[36m(func pid=132824)[0m f1_weighted: 0.24130135060034902
[2m[36m(func pid=132824)[0m f1_per_class: [0.156, 0.432, 0.71, 0.18, 0.101, 0.044, 0.262, 0.304, 0.182, 0.161]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.33722014925373134
[2m[36m(func pid=127193)[0m top5: 0.816231343283582
[2m[36m(func pid=127193)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=127193)[0m f1_macro: 0.30851068209116106
[2m[36m(func pid=127193)[0m f1_weighted: 0.3237102421087185
[2m[36m(func pid=127193)[0m f1_per_class: [0.574, 0.374, 0.433, 0.573, 0.1, 0.221, 0.101, 0.348, 0.217, 0.144]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.27238805970149255
[2m[36m(func pid=127722)[0m top5: 0.8134328358208955
[2m[36m(func pid=127722)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=127722)[0m f1_macro: 0.2930590813035323
[2m[36m(func pid=127722)[0m f1_weighted: 0.26759714769352855
[2m[36m(func pid=127722)[0m f1_per_class: [0.574, 0.292, 0.645, 0.578, 0.047, 0.008, 0.054, 0.231, 0.188, 0.314]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.08861940298507463
[2m[36m(func pid=140546)[0m top5: 0.47574626865671643
[2m[36m(func pid=140546)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=140546)[0m f1_macro: 0.0741375621930972
[2m[36m(func pid=140546)[0m f1_weighted: 0.09777804941153115
[2m[36m(func pid=140546)[0m f1_per_class: [0.092, 0.111, 0.064, 0.111, 0.0, 0.097, 0.092, 0.072, 0.072, 0.03]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 42.4640 | Steps: 4 | Val loss: 241.4524 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.0751 | Steps: 4 | Val loss: 4.3810 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 5.3543 | Steps: 4 | Val loss: 31.7668 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7882 | Steps: 4 | Val loss: 2.3757 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:31:00 (running for 00:23:18.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.381 |      0.309 |                   69 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  6.959 |      0.293 |                   67 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 42.464 |      0.262 |                   48 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.84  |      0.074 |                   15 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.25326492537313433
[2m[36m(func pid=132824)[0m top5: 0.6441231343283582
[2m[36m(func pid=132824)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=132824)[0m f1_macro: 0.2616228547315311
[2m[36m(func pid=132824)[0m f1_weighted: 0.278033359820802
[2m[36m(func pid=132824)[0m f1_per_class: [0.118, 0.368, 0.688, 0.189, 0.109, 0.075, 0.405, 0.299, 0.176, 0.19]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127193)[0m top1: 0.33675373134328357
[2m[36m(func pid=127193)[0m top5: 0.808768656716418
[2m[36m(func pid=127193)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=127193)[0m f1_macro: 0.3087262587573184
[2m[36m(func pid=127193)[0m f1_weighted: 0.333501065545494
[2m[36m(func pid=127193)[0m f1_per_class: [0.611, 0.416, 0.338, 0.569, 0.107, 0.204, 0.114, 0.372, 0.226, 0.13]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.33302238805970147
[2m[36m(func pid=127722)[0m top5: 0.7229477611940298
[2m[36m(func pid=127722)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=127722)[0m f1_macro: 0.2956441957939436
[2m[36m(func pid=127722)[0m f1_weighted: 0.29270068386538517
[2m[36m(func pid=127722)[0m f1_per_class: [0.43, 0.435, 0.625, 0.599, 0.092, 0.008, 0.04, 0.262, 0.2, 0.267]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.0960820895522388
[2m[36m(func pid=140546)[0m top5: 0.48740671641791045
[2m[36m(func pid=140546)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=140546)[0m f1_macro: 0.07799956352880448
[2m[36m(func pid=140546)[0m f1_weighted: 0.10988395963045372
[2m[36m(func pid=140546)[0m f1_per_class: [0.078, 0.119, 0.056, 0.134, 0.0, 0.093, 0.104, 0.09, 0.083, 0.022]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 26.9955 | Steps: 4 | Val loss: 215.7652 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6873 | Steps: 4 | Val loss: 4.4788 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3979 | Steps: 4 | Val loss: 30.2375 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7552 | Steps: 4 | Val loss: 2.3590 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=132824)[0m top1: 0.28824626865671643
[2m[36m(func pid=132824)[0m top5: 0.6702425373134329
[2m[36m(func pid=132824)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=132824)[0m f1_macro: 0.26097195442563964
[2m[36m(func pid=132824)[0m f1_weighted: 0.31719192261223733
[2m[36m(func pid=132824)[0m f1_per_class: [0.129, 0.3, 0.537, 0.251, 0.137, 0.103, 0.515, 0.28, 0.142, 0.217]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:31:06 (running for 00:23:23.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.075 |      0.309 |                   70 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.354 |      0.296 |                   68 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 26.996 |      0.261 |                   49 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.788 |      0.078 |                   16 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.3162313432835821
[2m[36m(func pid=127193)[0m top5: 0.7882462686567164
[2m[36m(func pid=127193)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=127193)[0m f1_macro: 0.29468697359325174
[2m[36m(func pid=127193)[0m f1_weighted: 0.32161602299201736
[2m[36m(func pid=127193)[0m f1_per_class: [0.62, 0.45, 0.245, 0.515, 0.104, 0.192, 0.111, 0.377, 0.222, 0.109]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3204291044776119
[2m[36m(func pid=127722)[0m top5: 0.7084888059701493
[2m[36m(func pid=127722)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=127722)[0m f1_macro: 0.27292058214953097
[2m[36m(func pid=127722)[0m f1_weighted: 0.28260458218728585
[2m[36m(func pid=127722)[0m f1_per_class: [0.364, 0.408, 0.625, 0.558, 0.161, 0.038, 0.06, 0.296, 0.107, 0.113]
[2m[36m(func pid=140546)[0m top1: 0.09888059701492537
[2m[36m(func pid=140546)[0m top5: 0.5046641791044776
[2m[36m(func pid=140546)[0m f1_micro: 0.09888059701492537
[2m[36m(func pid=140546)[0m f1_macro: 0.07722654932383011
[2m[36m(func pid=140546)[0m f1_weighted: 0.1168799561888213
[2m[36m(func pid=140546)[0m f1_per_class: [0.073, 0.126, 0.057, 0.156, 0.0, 0.089, 0.111, 0.076, 0.066, 0.02]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5857 | Steps: 4 | Val loss: 4.2833 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 14.4802 | Steps: 4 | Val loss: 197.6076 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6996 | Steps: 4 | Val loss: 2.3355 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 3.2956 | Steps: 4 | Val loss: 32.4342 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:31:11 (running for 00:23:28.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.687 |      0.295 |                   71 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.398 |      0.273 |                   69 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 14.48  |      0.27  |                   50 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.755 |      0.077 |                   17 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.3292910447761194
[2m[36m(func pid=127193)[0m top5: 0.8041044776119403
[2m[36m(func pid=127193)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=127193)[0m f1_macro: 0.30465116794683234
[2m[36m(func pid=127193)[0m f1_weighted: 0.33426050547170505
[2m[36m(func pid=127193)[0m f1_per_class: [0.659, 0.501, 0.268, 0.488, 0.11, 0.174, 0.157, 0.358, 0.214, 0.117]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.33861940298507465
[2m[36m(func pid=132824)[0m top5: 0.6744402985074627
[2m[36m(func pid=132824)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=132824)[0m f1_macro: 0.26974770617812743
[2m[36m(func pid=132824)[0m f1_weighted: 0.3448231194368948
[2m[36m(func pid=132824)[0m f1_per_class: [0.192, 0.221, 0.431, 0.287, 0.142, 0.184, 0.59, 0.266, 0.11, 0.274]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m top1: 0.1142723880597015
[2m[36m(func pid=140546)[0m top5: 0.5265858208955224
[2m[36m(func pid=140546)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=140546)[0m f1_macro: 0.08664240073586593
[2m[36m(func pid=140546)[0m f1_weighted: 0.1343887240081281
[2m[36m(func pid=140546)[0m f1_per_class: [0.074, 0.151, 0.066, 0.192, 0.0, 0.075, 0.123, 0.09, 0.063, 0.033]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.28824626865671643
[2m[36m(func pid=127722)[0m top5: 0.7131529850746269
[2m[36m(func pid=127722)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=127722)[0m f1_macro: 0.2657670935622473
[2m[36m(func pid=127722)[0m f1_weighted: 0.2690884603974224
[2m[36m(func pid=127722)[0m f1_per_class: [0.324, 0.34, 0.621, 0.533, 0.212, 0.079, 0.057, 0.369, 0.055, 0.068]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3105 | Steps: 4 | Val loss: 4.4356 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 35.7480 | Steps: 4 | Val loss: 181.2688 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6935 | Steps: 4 | Val loss: 2.3282 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 7.6944 | Steps: 4 | Val loss: 34.9005 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:31:16 (running for 00:23:34.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  1.311 |      0.296 |                   73 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.296 |      0.266 |                   70 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 14.48  |      0.27  |                   50 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.7   |      0.087 |                   18 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.3306902985074627
[2m[36m(func pid=127193)[0m top5: 0.8041044776119403
[2m[36m(func pid=127193)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=127193)[0m f1_macro: 0.29571609061084614
[2m[36m(func pid=127193)[0m f1_weighted: 0.32256729323954764
[2m[36m(func pid=127193)[0m f1_per_class: [0.674, 0.502, 0.205, 0.449, 0.105, 0.122, 0.172, 0.368, 0.21, 0.151]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.3666044776119403
[2m[36m(func pid=132824)[0m top5: 0.6902985074626866
[2m[36m(func pid=132824)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=132824)[0m f1_macro: 0.29690043554498347
[2m[36m(func pid=132824)[0m f1_weighted: 0.37029283630584875
[2m[36m(func pid=132824)[0m f1_per_class: [0.274, 0.19, 0.415, 0.381, 0.198, 0.273, 0.57, 0.236, 0.093, 0.338]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m top1: 0.11847014925373134
[2m[36m(func pid=140546)[0m top5: 0.5298507462686567
[2m[36m(func pid=140546)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=140546)[0m f1_macro: 0.0877524713071289
[2m[36m(func pid=140546)[0m f1_weighted: 0.1343495976416062
[2m[36m(func pid=140546)[0m f1_per_class: [0.085, 0.122, 0.046, 0.219, 0.0, 0.078, 0.112, 0.078, 0.078, 0.058]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.26725746268656714
[2m[36m(func pid=127722)[0m top5: 0.7080223880597015
[2m[36m(func pid=127722)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=127722)[0m f1_macro: 0.26790701436199804
[2m[36m(func pid=127722)[0m f1_weighted: 0.26209099814773734
[2m[36m(func pid=127722)[0m f1_per_class: [0.315, 0.324, 0.621, 0.523, 0.316, 0.105, 0.051, 0.314, 0.055, 0.055]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.0431 | Steps: 4 | Val loss: 4.1769 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 18.1883 | Steps: 4 | Val loss: 172.7321 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6695 | Steps: 4 | Val loss: 2.2934 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 5.1438 | Steps: 4 | Val loss: 30.4135 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:31:22 (running for 00:23:39.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00009 | RUNNING    | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.043 |      0.3   |                   74 |
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  7.694 |      0.268 |                   71 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 35.748 |      0.297 |                   51 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.693 |      0.088 |                   19 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=127193)[0m top1: 0.34328358208955223
[2m[36m(func pid=127193)[0m top5: 0.8325559701492538
[2m[36m(func pid=127193)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=127193)[0m f1_macro: 0.2998799794390552
[2m[36m(func pid=127193)[0m f1_weighted: 0.334340666752967
[2m[36m(func pid=127193)[0m f1_per_class: [0.651, 0.5, 0.182, 0.423, 0.109, 0.14, 0.234, 0.346, 0.207, 0.208]
[2m[36m(func pid=127193)[0m 
[2m[36m(func pid=132824)[0m top1: 0.3908582089552239
[2m[36m(func pid=132824)[0m top5: 0.7196828358208955
[2m[36m(func pid=132824)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=132824)[0m f1_macro: 0.3074091801648872
[2m[36m(func pid=132824)[0m f1_weighted: 0.38679578734664927
[2m[36m(func pid=132824)[0m f1_per_class: [0.408, 0.101, 0.282, 0.499, 0.228, 0.342, 0.534, 0.22, 0.092, 0.367]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m top1: 0.13013059701492538
[2m[36m(func pid=140546)[0m top5: 0.5727611940298507
[2m[36m(func pid=140546)[0m f1_micro: 0.13013059701492538
[2m[36m(func pid=140546)[0m f1_macro: 0.09234229460602567
[2m[36m(func pid=140546)[0m f1_weighted: 0.14522420965463115
[2m[36m(func pid=140546)[0m f1_per_class: [0.091, 0.127, 0.053, 0.238, 0.0, 0.076, 0.125, 0.114, 0.058, 0.043]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.291044776119403
[2m[36m(func pid=127722)[0m top5: 0.7639925373134329
[2m[36m(func pid=127722)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=127722)[0m f1_macro: 0.27392699004302035
[2m[36m(func pid=127722)[0m f1_weighted: 0.2858132677920833
[2m[36m(func pid=127722)[0m f1_per_class: [0.343, 0.353, 0.581, 0.53, 0.214, 0.154, 0.084, 0.313, 0.107, 0.06]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=127193)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0504 | Steps: 4 | Val loss: 4.2254 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 68.3278 | Steps: 4 | Val loss: 180.8247 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 7.6260 | Steps: 4 | Val loss: 24.1807 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6633 | Steps: 4 | Val loss: 2.2956 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=127193)[0m top1: 0.3451492537313433
[2m[36m(func pid=127193)[0m top5: 0.84375
[2m[36m(func pid=127193)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=127193)[0m f1_macro: 0.3004497398273024
[2m[36m(func pid=127193)[0m f1_weighted: 0.3359450443277418
[2m[36m(func pid=127193)[0m f1_per_class: [0.658, 0.497, 0.164, 0.384, 0.098, 0.125, 0.281, 0.357, 0.2, 0.241]
== Status ==
Current time: 2024-01-07 11:31:27 (running for 00:23:44.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.35175
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (11 PENDING, 3 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.144 |      0.274 |                   72 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 18.188 |      0.307 |                   52 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.669 |      0.092 |                   20 |
| train_98a10_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 1 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3931902985074627
[2m[36m(func pid=132824)[0m top5: 0.7164179104477612
[2m[36m(func pid=132824)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=132824)[0m f1_macro: 0.31873559647747607
[2m[36m(func pid=132824)[0m f1_weighted: 0.392069323823221
[2m[36m(func pid=132824)[0m f1_per_class: [0.504, 0.083, 0.239, 0.558, 0.274, 0.369, 0.473, 0.323, 0.102, 0.263]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m top1: 0.34654850746268656
[2m[36m(func pid=127722)[0m top5: 0.8176305970149254
[2m[36m(func pid=127722)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=127722)[0m f1_macro: 0.3063333851724524
[2m[36m(func pid=127722)[0m f1_weighted: 0.3351104913488509
[2m[36m(func pid=127722)[0m f1_per_class: [0.317, 0.414, 0.714, 0.54, 0.148, 0.241, 0.175, 0.264, 0.154, 0.096]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.1259328358208955
[2m[36m(func pid=140546)[0m top5: 0.5769589552238806
[2m[36m(func pid=140546)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=140546)[0m f1_macro: 0.08989092019771386
[2m[36m(func pid=140546)[0m f1_weighted: 0.1394394060872275
[2m[36m(func pid=140546)[0m f1_per_class: [0.097, 0.112, 0.04, 0.238, 0.0, 0.085, 0.111, 0.107, 0.06, 0.049]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 9.5212 | Steps: 4 | Val loss: 189.6244 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.6146 | Steps: 4 | Val loss: 20.0580 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6993 | Steps: 4 | Val loss: 2.3086 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=132824)[0m top1: 0.36473880597014924
[2m[36m(func pid=132824)[0m top5: 0.7388059701492538
[2m[36m(func pid=132824)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=132824)[0m f1_macro: 0.3059885855199228
[2m[36m(func pid=132824)[0m f1_weighted: 0.3630930713642933
[2m[36m(func pid=132824)[0m f1_per_class: [0.55, 0.133, 0.226, 0.577, 0.232, 0.345, 0.341, 0.307, 0.09, 0.258]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m top1: 0.11847014925373134
[2m[36m(func pid=140546)[0m top5: 0.5652985074626866
[2m[36m(func pid=140546)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=140546)[0m f1_macro: 0.08764407859836584
[2m[36m(func pid=140546)[0m f1_weighted: 0.1324742831934171
[2m[36m(func pid=140546)[0m f1_per_class: [0.104, 0.117, 0.051, 0.232, 0.0, 0.07, 0.094, 0.117, 0.057, 0.034]
[2m[36m(func pid=127722)[0m top1: 0.416044776119403
[2m[36m(func pid=127722)[0m top5: 0.8777985074626866
[2m[36m(func pid=127722)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=127722)[0m f1_macro: 0.3482560523897669
[2m[36m(func pid=127722)[0m f1_weighted: 0.3898525929611396
[2m[36m(func pid=127722)[0m f1_per_class: [0.409, 0.474, 0.606, 0.559, 0.154, 0.326, 0.262, 0.219, 0.252, 0.221]
== Status ==
Current time: 2024-01-07 11:31:33 (running for 00:23:50.67)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.35175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  7.626 |      0.306 |                   73 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  9.521 |      0.306 |                   54 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.663 |      0.09  |                   21 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=145792)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=145792)[0m Configuration completed!
[2m[36m(func pid=145792)[0m New optimizer parameters:
[2m[36m(func pid=145792)[0m SGD (
[2m[36m(func pid=145792)[0m Parameter Group 0
[2m[36m(func pid=145792)[0m     dampening: 0
[2m[36m(func pid=145792)[0m     differentiable: False
[2m[36m(func pid=145792)[0m     foreach: None
[2m[36m(func pid=145792)[0m     lr: 0.001
[2m[36m(func pid=145792)[0m     maximize: False
[2m[36m(func pid=145792)[0m     momentum: 0.9
[2m[36m(func pid=145792)[0m     nesterov: False
[2m[36m(func pid=145792)[0m     weight_decay: 0.0001
[2m[36m(func pid=145792)[0m )
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 23.5998 | Steps: 4 | Val loss: 203.5889 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:31:38 (running for 00:23:55.87)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.35175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.615 |      0.348 |                   74 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 23.6   |      0.318 |                   55 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.699 |      0.088 |                   22 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3763992537313433
[2m[36m(func pid=132824)[0m top5: 0.7905783582089553
[2m[36m(func pid=132824)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=132824)[0m f1_macro: 0.31820076063027436
[2m[36m(func pid=132824)[0m f1_weighted: 0.3544668248943185
[2m[36m(func pid=132824)[0m f1_per_class: [0.624, 0.246, 0.186, 0.612, 0.242, 0.358, 0.194, 0.345, 0.126, 0.25]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.2336 | Steps: 4 | Val loss: 17.8482 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5909 | Steps: 4 | Val loss: 2.3106 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9925 | Steps: 4 | Val loss: 2.4542 | Batch size: 32 | lr: 0.001 | Duration: 4.56s
[2m[36m(func pid=127722)[0m top1: 0.4412313432835821
[2m[36m(func pid=127722)[0m top5: 0.9235074626865671
[2m[36m(func pid=127722)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=127722)[0m f1_macro: 0.3648276719844776
[2m[36m(func pid=127722)[0m f1_weighted: 0.4207134243758371
[2m[36m(func pid=127722)[0m f1_per_class: [0.471, 0.494, 0.564, 0.579, 0.2, 0.325, 0.362, 0.061, 0.185, 0.406]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 6.8152 | Steps: 4 | Val loss: 211.9623 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=140546)[0m top1: 0.11986940298507463
[2m[36m(func pid=140546)[0m top5: 0.5583022388059702
[2m[36m(func pid=140546)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=140546)[0m f1_macro: 0.08809677125671082
[2m[36m(func pid=140546)[0m f1_weighted: 0.13623275624343953
[2m[36m(func pid=140546)[0m f1_per_class: [0.097, 0.139, 0.029, 0.227, 0.0, 0.054, 0.106, 0.107, 0.064, 0.057]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.06902985074626866
[2m[36m(func pid=145792)[0m top5: 0.49113805970149255
[2m[36m(func pid=145792)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=145792)[0m f1_macro: 0.045038187064805016
[2m[36m(func pid=145792)[0m f1_weighted: 0.05910279288073877
[2m[36m(func pid=145792)[0m f1_per_class: [0.081, 0.082, 0.0, 0.123, 0.0, 0.02, 0.0, 0.09, 0.042, 0.012]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:31:43 (running for 00:24:01.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.234 |      0.365 |                   75 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  6.815 |      0.309 |                   56 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.591 |      0.088 |                   23 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.992 |      0.045 |                    1 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3871268656716418
[2m[36m(func pid=132824)[0m top5: 0.8194962686567164
[2m[36m(func pid=132824)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=132824)[0m f1_macro: 0.30944971875859817
[2m[36m(func pid=132824)[0m f1_weighted: 0.3571060684854061
[2m[36m(func pid=132824)[0m f1_per_class: [0.593, 0.417, 0.173, 0.609, 0.18, 0.33, 0.117, 0.364, 0.173, 0.138]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 7.3242 | Steps: 4 | Val loss: 17.2331 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6971 | Steps: 4 | Val loss: 2.3070 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0738 | Steps: 4 | Val loss: 2.3458 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 38.3458 | Steps: 4 | Val loss: 227.3238 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=127722)[0m top1: 0.4375
[2m[36m(func pid=127722)[0m top5: 0.9244402985074627
[2m[36m(func pid=127722)[0m f1_micro: 0.4375
[2m[36m(func pid=127722)[0m f1_macro: 0.37377545505698073
[2m[36m(func pid=127722)[0m f1_weighted: 0.44106559924314565
[2m[36m(func pid=127722)[0m f1_per_class: [0.493, 0.5, 0.462, 0.573, 0.293, 0.306, 0.445, 0.016, 0.19, 0.462]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.11986940298507463
[2m[36m(func pid=140546)[0m top5: 0.5713619402985075
[2m[36m(func pid=140546)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=140546)[0m f1_macro: 0.0866163182118254
[2m[36m(func pid=140546)[0m f1_weighted: 0.13354936352533867
[2m[36m(func pid=140546)[0m f1_per_class: [0.109, 0.136, 0.029, 0.228, 0.0, 0.057, 0.095, 0.118, 0.074, 0.021]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:31:48 (running for 00:24:06.29)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  7.324 |      0.374 |                   76 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 38.346 |      0.32  |                   57 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.697 |      0.087 |                   24 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.992 |      0.045 |                    1 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.4076492537313433
[2m[36m(func pid=132824)[0m top5: 0.8278917910447762
[2m[36m(func pid=132824)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=132824)[0m f1_macro: 0.31971215757169125
[2m[36m(func pid=132824)[0m f1_weighted: 0.3633351184073257
[2m[36m(func pid=132824)[0m f1_per_class: [0.603, 0.576, 0.255, 0.599, 0.159, 0.312, 0.062, 0.342, 0.215, 0.074]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=145792)[0m top1: 0.13619402985074627
[2m[36m(func pid=145792)[0m top5: 0.5181902985074627
[2m[36m(func pid=145792)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=145792)[0m f1_macro: 0.07983523877587294
[2m[36m(func pid=145792)[0m f1_weighted: 0.11444279270971014
[2m[36m(func pid=145792)[0m f1_per_class: [0.129, 0.261, 0.0, 0.193, 0.0, 0.028, 0.006, 0.105, 0.052, 0.024]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 5.9600 | Steps: 4 | Val loss: 18.5299 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6214 | Steps: 4 | Val loss: 2.3238 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 23.8741 | Steps: 4 | Val loss: 264.2449 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=127722)[0m top1: 0.40111940298507465
[2m[36m(func pid=127722)[0m top5: 0.9319029850746269
[2m[36m(func pid=127722)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=127722)[0m f1_macro: 0.3256585833057286
[2m[36m(func pid=127722)[0m f1_weighted: 0.4176449825106157
[2m[36m(func pid=127722)[0m f1_per_class: [0.569, 0.484, 0.414, 0.51, 0.286, 0.282, 0.458, 0.016, 0.162, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m top1: 0.11240671641791045
[2m[36m(func pid=140546)[0m top5: 0.5503731343283582
[2m[36m(func pid=140546)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=140546)[0m f1_macro: 0.08337401597217911
[2m[36m(func pid=140546)[0m f1_weighted: 0.12501281901394892
[2m[36m(func pid=140546)[0m f1_per_class: [0.119, 0.134, 0.029, 0.234, 0.0, 0.047, 0.069, 0.096, 0.067, 0.039]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9127 | Steps: 4 | Val loss: 2.3437 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 11:31:54 (running for 00:24:11.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.96  |      0.326 |                   77 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 23.874 |      0.325 |                   58 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.621 |      0.083 |                   25 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  3.074 |      0.08  |                    2 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3917910447761194
[2m[36m(func pid=132824)[0m top5: 0.8129664179104478
[2m[36m(func pid=132824)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=132824)[0m f1_macro: 0.32513318351637577
[2m[36m(func pid=132824)[0m f1_weighted: 0.34247189512210363
[2m[36m(func pid=132824)[0m f1_per_class: [0.588, 0.584, 0.241, 0.582, 0.148, 0.251, 0.025, 0.318, 0.212, 0.303]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=145792)[0m top1: 0.10774253731343283
[2m[36m(func pid=145792)[0m top5: 0.5065298507462687
[2m[36m(func pid=145792)[0m f1_micro: 0.10774253731343283
[2m[36m(func pid=145792)[0m f1_macro: 0.08086885344757463
[2m[36m(func pid=145792)[0m f1_weighted: 0.12051704439060984
[2m[36m(func pid=145792)[0m f1_per_class: [0.105, 0.194, 0.06, 0.071, 0.008, 0.032, 0.183, 0.08, 0.049, 0.027]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6255 | Steps: 4 | Val loss: 2.3191 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 2.7540 | Steps: 4 | Val loss: 21.4261 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 20.5088 | Steps: 4 | Val loss: 265.8234 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=140546)[0m top1: 0.11567164179104478
[2m[36m(func pid=140546)[0m top5: 0.5569029850746269
[2m[36m(func pid=140546)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=140546)[0m f1_macro: 0.09075407242944825
[2m[36m(func pid=140546)[0m f1_weighted: 0.12718621067954544
[2m[36m(func pid=140546)[0m f1_per_class: [0.109, 0.137, 0.062, 0.224, 0.0, 0.071, 0.068, 0.125, 0.071, 0.041]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.35261194029850745
[2m[36m(func pid=127722)[0m top5: 0.909981343283582
[2m[36m(func pid=127722)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=127722)[0m f1_macro: 0.2852558895339049
[2m[36m(func pid=127722)[0m f1_weighted: 0.3823095320242569
[2m[36m(func pid=127722)[0m f1_per_class: [0.595, 0.419, 0.26, 0.438, 0.231, 0.257, 0.454, 0.062, 0.138, 0.0]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6174 | Steps: 4 | Val loss: 2.4372 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:31:59 (running for 00:24:17.05)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  2.754 |      0.285 |                   78 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 20.509 |      0.308 |                   59 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.625 |      0.091 |                   26 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.913 |      0.081 |                    3 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.37826492537313433
[2m[36m(func pid=132824)[0m top5: 0.8143656716417911
[2m[36m(func pid=132824)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=132824)[0m f1_macro: 0.30792077617766284
[2m[36m(func pid=132824)[0m f1_weighted: 0.33272440132514197
[2m[36m(func pid=132824)[0m f1_per_class: [0.508, 0.582, 0.218, 0.575, 0.125, 0.189, 0.034, 0.303, 0.193, 0.353]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5798 | Steps: 4 | Val loss: 2.3173 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=145792)[0m top1: 0.08442164179104478
[2m[36m(func pid=145792)[0m top5: 0.4855410447761194
[2m[36m(func pid=145792)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=145792)[0m f1_macro: 0.05762359202016487
[2m[36m(func pid=145792)[0m f1_weighted: 0.08590815624331143
[2m[36m(func pid=145792)[0m f1_per_class: [0.063, 0.025, 0.136, 0.007, 0.018, 0.074, 0.229, 0.0, 0.024, 0.0]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 3.9375 | Steps: 4 | Val loss: 23.9817 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 34.8826 | Steps: 4 | Val loss: 281.7845 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.11707089552238806
[2m[36m(func pid=140546)[0m top5: 0.5517723880597015
[2m[36m(func pid=140546)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=140546)[0m f1_macro: 0.09460255357896702
[2m[36m(func pid=140546)[0m f1_weighted: 0.13068510448542783
[2m[36m(func pid=140546)[0m f1_per_class: [0.137, 0.129, 0.059, 0.212, 0.0, 0.062, 0.094, 0.143, 0.075, 0.036]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.30923507462686567
[2m[36m(func pid=127722)[0m top5: 0.8815298507462687
[2m[36m(func pid=127722)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=127722)[0m f1_macro: 0.24971261265212838
[2m[36m(func pid=127722)[0m f1_weighted: 0.3486268703235031
[2m[36m(func pid=127722)[0m f1_per_class: [0.568, 0.319, 0.191, 0.429, 0.149, 0.229, 0.424, 0.06, 0.129, 0.0]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7558 | Steps: 4 | Val loss: 2.4397 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 11:32:05 (running for 00:24:22.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.938 |      0.25  |                   79 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 34.883 |      0.295 |                   60 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.58  |      0.095 |                   27 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.617 |      0.058 |                    4 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.35494402985074625
[2m[36m(func pid=132824)[0m top5: 0.7770522388059702
[2m[36m(func pid=132824)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=132824)[0m f1_macro: 0.2946012566782804
[2m[36m(func pid=132824)[0m f1_weighted: 0.3087174545353272
[2m[36m(func pid=132824)[0m f1_per_class: [0.441, 0.569, 0.245, 0.521, 0.096, 0.105, 0.045, 0.304, 0.181, 0.439]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6468 | Steps: 4 | Val loss: 2.3080 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=145792)[0m top1: 0.12686567164179105
[2m[36m(func pid=145792)[0m top5: 0.48740671641791045
[2m[36m(func pid=145792)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=145792)[0m f1_macro: 0.06938102370092138
[2m[36m(func pid=145792)[0m f1_weighted: 0.12034400868769442
[2m[36m(func pid=145792)[0m f1_per_class: [0.075, 0.016, 0.108, 0.007, 0.032, 0.065, 0.351, 0.0, 0.041, 0.0]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 3.1985 | Steps: 4 | Val loss: 26.1141 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 22.5330 | Steps: 4 | Val loss: 291.3309 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=140546)[0m top1: 0.11100746268656717
[2m[36m(func pid=140546)[0m top5: 0.5690298507462687
[2m[36m(func pid=140546)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=140546)[0m f1_macro: 0.09549057555117729
[2m[36m(func pid=140546)[0m f1_weighted: 0.12295610682719929
[2m[36m(func pid=140546)[0m f1_per_class: [0.151, 0.124, 0.071, 0.183, 0.0, 0.056, 0.1, 0.126, 0.08, 0.063]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2719216417910448
[2m[36m(func pid=127722)[0m top5: 0.8591417910447762
[2m[36m(func pid=127722)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=127722)[0m f1_macro: 0.22595750355160887
[2m[36m(func pid=127722)[0m f1_weighted: 0.32469998015220064
[2m[36m(func pid=127722)[0m f1_per_class: [0.435, 0.263, 0.105, 0.434, 0.098, 0.214, 0.356, 0.237, 0.119, 0.0]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5773 | Steps: 4 | Val loss: 2.2994 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=132824)[0m top1: 0.32136194029850745
[2m[36m(func pid=132824)[0m top5: 0.7364738805970149
[2m[36m(func pid=132824)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=132824)[0m f1_macro: 0.28262542312175476
[2m[36m(func pid=132824)[0m f1_weighted: 0.2653413155388296
[2m[36m(func pid=132824)[0m f1_per_class: [0.414, 0.518, 0.361, 0.397, 0.086, 0.06, 0.06, 0.308, 0.206, 0.417]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:32:10 (running for 00:24:27.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.199 |      0.226 |                   80 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 22.533 |      0.283 |                   61 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.647 |      0.095 |                   28 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.756 |      0.069 |                    5 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5636 | Steps: 4 | Val loss: 2.3020 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=145792)[0m top1: 0.18516791044776118
[2m[36m(func pid=145792)[0m top5: 0.5615671641791045
[2m[36m(func pid=145792)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=145792)[0m f1_macro: 0.10120354402351368
[2m[36m(func pid=145792)[0m f1_weighted: 0.16670981917872396
[2m[36m(func pid=145792)[0m f1_per_class: [0.135, 0.068, 0.083, 0.032, 0.039, 0.081, 0.436, 0.0, 0.077, 0.06]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 3.8505 | Steps: 4 | Val loss: 31.2296 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 11.2355 | Steps: 4 | Val loss: 277.9068 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=140546)[0m top1: 0.11847014925373134
[2m[36m(func pid=140546)[0m top5: 0.5699626865671642
[2m[36m(func pid=140546)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=140546)[0m f1_macro: 0.10244570733311467
[2m[36m(func pid=140546)[0m f1_weighted: 0.13000720468646684
[2m[36m(func pid=140546)[0m f1_per_class: [0.161, 0.151, 0.077, 0.172, 0.0, 0.072, 0.109, 0.134, 0.089, 0.059]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2248134328358209
[2m[36m(func pid=127722)[0m top5: 0.8097014925373134
[2m[36m(func pid=127722)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=127722)[0m f1_macro: 0.1917592504483702
[2m[36m(func pid=127722)[0m f1_weighted: 0.278641583948958
[2m[36m(func pid=127722)[0m f1_per_class: [0.262, 0.239, 0.073, 0.425, 0.056, 0.159, 0.237, 0.338, 0.127, 0.0]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.4310 | Steps: 4 | Val loss: 2.2002 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:32:15 (running for 00:24:32.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.85  |      0.192 |                   81 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 11.235 |      0.279 |                   62 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.564 |      0.102 |                   29 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.577 |      0.101 |                    6 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3101679104477612
[2m[36m(func pid=132824)[0m top5: 0.7136194029850746
[2m[36m(func pid=132824)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=132824)[0m f1_macro: 0.2788062846574381
[2m[36m(func pid=132824)[0m f1_weighted: 0.2660127326917026
[2m[36m(func pid=132824)[0m f1_per_class: [0.386, 0.493, 0.421, 0.287, 0.081, 0.056, 0.185, 0.304, 0.2, 0.375]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5754 | Steps: 4 | Val loss: 2.2861 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=145792)[0m top1: 0.166044776119403
[2m[36m(func pid=145792)[0m top5: 0.6828358208955224
[2m[36m(func pid=145792)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=145792)[0m f1_macro: 0.13365147025376284
[2m[36m(func pid=145792)[0m f1_weighted: 0.1862626317710658
[2m[36m(func pid=145792)[0m f1_per_class: [0.109, 0.175, 0.286, 0.118, 0.031, 0.13, 0.335, 0.013, 0.085, 0.054]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 5.9820 | Steps: 4 | Val loss: 35.4163 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=140546)[0m top1: 0.13152985074626866
[2m[36m(func pid=140546)[0m top5: 0.5816231343283582
[2m[36m(func pid=140546)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=140546)[0m f1_macro: 0.10746934544682929
[2m[36m(func pid=140546)[0m f1_weighted: 0.14878101318028314
[2m[36m(func pid=140546)[0m f1_per_class: [0.159, 0.162, 0.075, 0.164, 0.0, 0.067, 0.175, 0.133, 0.101, 0.037]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 4.6302 | Steps: 4 | Val loss: 237.1305 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=127722)[0m top1: 0.20335820895522388
[2m[36m(func pid=127722)[0m top5: 0.7966417910447762
[2m[36m(func pid=127722)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=127722)[0m f1_macro: 0.18386196974521526
[2m[36m(func pid=127722)[0m f1_weighted: 0.25295143973918277
[2m[36m(func pid=127722)[0m f1_per_class: [0.218, 0.207, 0.079, 0.399, 0.042, 0.119, 0.205, 0.356, 0.136, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2780 | Steps: 4 | Val loss: 2.1304 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=132824)[0m top1: 0.3474813432835821
[2m[36m(func pid=132824)[0m top5: 0.7173507462686567
[2m[36m(func pid=132824)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=132824)[0m f1_macro: 0.31469830725139003
[2m[36m(func pid=132824)[0m f1_weighted: 0.3317855414149828
[2m[36m(func pid=132824)[0m f1_per_class: [0.492, 0.518, 0.468, 0.319, 0.083, 0.036, 0.357, 0.316, 0.214, 0.343]
[2m[36m(func pid=132824)[0m 
== Status ==
Current time: 2024-01-07 11:32:20 (running for 00:24:38.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.982 |      0.184 |                   82 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  4.63  |      0.315 |                   63 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.575 |      0.107 |                   30 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.431 |      0.134 |                    7 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4342 | Steps: 4 | Val loss: 2.2723 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 13.5524 | Steps: 4 | Val loss: 39.2876 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=145792)[0m top1: 0.19776119402985073
[2m[36m(func pid=145792)[0m top5: 0.7290111940298507
[2m[36m(func pid=145792)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=145792)[0m f1_macro: 0.18164460579817243
[2m[36m(func pid=145792)[0m f1_weighted: 0.2111160433735984
[2m[36m(func pid=145792)[0m f1_per_class: [0.302, 0.306, 0.275, 0.199, 0.043, 0.146, 0.216, 0.161, 0.111, 0.056]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0000 | Steps: 4 | Val loss: 232.8437 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=140546)[0m top1: 0.13712686567164178
[2m[36m(func pid=140546)[0m top5: 0.5895522388059702
[2m[36m(func pid=140546)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=140546)[0m f1_macro: 0.11168186195018506
[2m[36m(func pid=140546)[0m f1_weighted: 0.15330418003806298
[2m[36m(func pid=140546)[0m f1_per_class: [0.176, 0.182, 0.062, 0.148, 0.0, 0.086, 0.186, 0.141, 0.092, 0.045]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m top1: 0.19402985074626866
[2m[36m(func pid=127722)[0m top5: 0.7947761194029851
[2m[36m(func pid=127722)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=127722)[0m f1_macro: 0.18013261213022855
[2m[36m(func pid=127722)[0m f1_weighted: 0.23915377873168253
[2m[36m(func pid=127722)[0m f1_per_class: [0.192, 0.321, 0.127, 0.374, 0.031, 0.064, 0.141, 0.348, 0.128, 0.074]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.0615 | Steps: 4 | Val loss: 2.1224 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 11:32:25 (running for 00:24:43.42)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 13.552 |      0.18  |                   83 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  0     |      0.326 |                   64 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.434 |      0.112 |                   31 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.278 |      0.182 |                    8 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.34048507462686567
[2m[36m(func pid=132824)[0m top5: 0.7066231343283582
[2m[36m(func pid=132824)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=132824)[0m f1_macro: 0.32633628766393386
[2m[36m(func pid=132824)[0m f1_weighted: 0.3633743317971025
[2m[36m(func pid=132824)[0m f1_per_class: [0.475, 0.457, 0.667, 0.313, 0.078, 0.042, 0.517, 0.274, 0.146, 0.295]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5398 | Steps: 4 | Val loss: 2.2642 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 11.7654 | Steps: 4 | Val loss: 41.0057 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=145792)[0m top1: 0.208955223880597
[2m[36m(func pid=145792)[0m top5: 0.7290111940298507
[2m[36m(func pid=145792)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=145792)[0m f1_macro: 0.16144030190759912
[2m[36m(func pid=145792)[0m f1_weighted: 0.2176230591076441
[2m[36m(func pid=145792)[0m f1_per_class: [0.238, 0.328, 0.091, 0.224, 0.036, 0.133, 0.208, 0.23, 0.042, 0.084]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m top1: 0.1501865671641791
[2m[36m(func pid=140546)[0m top5: 0.6040111940298507
[2m[36m(func pid=140546)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=140546)[0m f1_macro: 0.12291315795428615
[2m[36m(func pid=140546)[0m f1_weighted: 0.1636184022887636
[2m[36m(func pid=140546)[0m f1_per_class: [0.188, 0.22, 0.123, 0.13, 0.0, 0.065, 0.22, 0.145, 0.09, 0.048]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 20.9950 | Steps: 4 | Val loss: 266.3188 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=127722)[0m top1: 0.21455223880597016
[2m[36m(func pid=127722)[0m top5: 0.804570895522388
[2m[36m(func pid=127722)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=127722)[0m f1_macro: 0.19620642543938055
[2m[36m(func pid=127722)[0m f1_weighted: 0.24582062399645588
[2m[36m(func pid=127722)[0m f1_per_class: [0.226, 0.404, 0.232, 0.397, 0.032, 0.036, 0.104, 0.321, 0.14, 0.069]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:31 (running for 00:24:48.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 11.765 |      0.196 |                   84 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 20.995 |      0.309 |                   65 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.54  |      0.123 |                   32 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.062 |      0.161 |                    9 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3180970149253731
[2m[36m(func pid=132824)[0m top5: 0.6576492537313433
[2m[36m(func pid=132824)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=132824)[0m f1_macro: 0.30867656598542986
[2m[36m(func pid=132824)[0m f1_weighted: 0.3350106797193835
[2m[36m(func pid=132824)[0m f1_per_class: [0.4, 0.303, 0.815, 0.261, 0.096, 0.044, 0.571, 0.25, 0.125, 0.222]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.0882 | Steps: 4 | Val loss: 2.1544 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4965 | Steps: 4 | Val loss: 2.2756 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 5.9044 | Steps: 4 | Val loss: 34.9425 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140546)[0m top1: 0.15391791044776118
[2m[36m(func pid=140546)[0m top5: 0.5890858208955224
[2m[36m(func pid=140546)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=140546)[0m f1_macro: 0.12724612500545654
[2m[36m(func pid=140546)[0m f1_weighted: 0.16385392292902043
[2m[36m(func pid=140546)[0m f1_per_class: [0.209, 0.236, 0.113, 0.107, 0.0, 0.067, 0.227, 0.159, 0.091, 0.063]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.19682835820895522
[2m[36m(func pid=145792)[0m top5: 0.7061567164179104
[2m[36m(func pid=145792)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=145792)[0m f1_macro: 0.15582047050123762
[2m[36m(func pid=145792)[0m f1_weighted: 0.2139161638711904
[2m[36m(func pid=145792)[0m f1_per_class: [0.137, 0.269, 0.104, 0.26, 0.11, 0.117, 0.206, 0.259, 0.0, 0.096]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 59.1685 | Steps: 4 | Val loss: 306.3870 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=127722)[0m top1: 0.28125
[2m[36m(func pid=127722)[0m top5: 0.8134328358208955
[2m[36m(func pid=127722)[0m f1_micro: 0.28125
[2m[36m(func pid=127722)[0m f1_macro: 0.2856657874295724
[2m[36m(func pid=127722)[0m f1_weighted: 0.29075502878850124
[2m[36m(func pid=127722)[0m f1_per_class: [0.367, 0.506, 0.347, 0.455, 0.045, 0.029, 0.115, 0.3, 0.182, 0.512]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:36 (running for 00:24:53.91)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.904 |      0.286 |                   85 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 59.169 |      0.286 |                   66 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.497 |      0.127 |                   33 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  2.088 |      0.156 |                   10 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3180970149253731
[2m[36m(func pid=132824)[0m top5: 0.6184701492537313
[2m[36m(func pid=132824)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=132824)[0m f1_macro: 0.28555999172760166
[2m[36m(func pid=132824)[0m f1_weighted: 0.3117330876510676
[2m[36m(func pid=132824)[0m f1_per_class: [0.37, 0.163, 0.8, 0.245, 0.144, 0.053, 0.601, 0.188, 0.112, 0.18]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5794 | Steps: 4 | Val loss: 2.2715 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.0194 | Steps: 4 | Val loss: 2.1017 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.9030 | Steps: 4 | Val loss: 32.3209 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140546)[0m top1: 0.1553171641791045
[2m[36m(func pid=140546)[0m top5: 0.5890858208955224
[2m[36m(func pid=140546)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=140546)[0m f1_macro: 0.12562722323129957
[2m[36m(func pid=140546)[0m f1_weighted: 0.1680891769171291
[2m[36m(func pid=140546)[0m f1_per_class: [0.195, 0.228, 0.102, 0.124, 0.0, 0.063, 0.233, 0.161, 0.082, 0.068]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 120.5784 | Steps: 4 | Val loss: 333.9815 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=145792)[0m top1: 0.21735074626865672
[2m[36m(func pid=145792)[0m top5: 0.742070895522388
[2m[36m(func pid=145792)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=145792)[0m f1_macro: 0.1832497872625569
[2m[36m(func pid=145792)[0m f1_weighted: 0.21344522596061538
[2m[36m(func pid=145792)[0m f1_per_class: [0.197, 0.361, 0.276, 0.306, 0.072, 0.122, 0.1, 0.25, 0.0, 0.15]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.31156716417910446
[2m[36m(func pid=127722)[0m top5: 0.8227611940298507
[2m[36m(func pid=127722)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=127722)[0m f1_macro: 0.30506582741044275
[2m[36m(func pid=127722)[0m f1_weighted: 0.30375259886349326
[2m[36m(func pid=127722)[0m f1_per_class: [0.56, 0.534, 0.462, 0.481, 0.076, 0.022, 0.117, 0.243, 0.226, 0.33]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:41 (running for 00:24:59.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |   1.903 |      0.305 |                   86 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 120.578 |      0.267 |                   67 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |   2.579 |      0.126 |                   34 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |   2.019 |      0.183 |                   11 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |   0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |   0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |   3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      |  13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |   2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |   0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |   1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |   8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.2947761194029851
[2m[36m(func pid=132824)[0m top5: 0.6105410447761194
[2m[36m(func pid=132824)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=132824)[0m f1_macro: 0.26724549100820716
[2m[36m(func pid=132824)[0m f1_weighted: 0.2882429496790699
[2m[36m(func pid=132824)[0m f1_per_class: [0.4, 0.076, 0.75, 0.251, 0.163, 0.044, 0.581, 0.124, 0.108, 0.175]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5706 | Steps: 4 | Val loss: 2.2764 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9097 | Steps: 4 | Val loss: 2.0385 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 11.9432 | Steps: 4 | Val loss: 31.4436 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140546)[0m top1: 0.15111940298507462
[2m[36m(func pid=140546)[0m top5: 0.5848880597014925
[2m[36m(func pid=140546)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=140546)[0m f1_macro: 0.12041086275406476
[2m[36m(func pid=140546)[0m f1_weighted: 0.16891203419060577
[2m[36m(func pid=140546)[0m f1_per_class: [0.186, 0.21, 0.078, 0.148, 0.0, 0.059, 0.228, 0.158, 0.082, 0.055]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 57.7269 | Steps: 4 | Val loss: 296.2534 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=145792)[0m top1: 0.24207089552238806
[2m[36m(func pid=145792)[0m top5: 0.7737873134328358
[2m[36m(func pid=145792)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=145792)[0m f1_macro: 0.2412939725331622
[2m[36m(func pid=145792)[0m f1_weighted: 0.24096905390958975
[2m[36m(func pid=145792)[0m f1_per_class: [0.277, 0.35, 0.468, 0.407, 0.079, 0.156, 0.061, 0.244, 0.166, 0.205]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.30130597014925375
[2m[36m(func pid=127722)[0m top5: 0.851679104477612
[2m[36m(func pid=127722)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=127722)[0m f1_macro: 0.3021730334483666
[2m[36m(func pid=127722)[0m f1_weighted: 0.29841252709517957
[2m[36m(func pid=127722)[0m f1_per_class: [0.556, 0.509, 0.55, 0.468, 0.16, 0.037, 0.129, 0.218, 0.214, 0.183]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:47 (running for 00:25:04.46)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 11.943 |      0.302 |                   87 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 57.727 |      0.297 |                   68 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.571 |      0.12  |                   35 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.91  |      0.241 |                   12 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.30363805970149255
[2m[36m(func pid=132824)[0m top5: 0.6375932835820896
[2m[36m(func pid=132824)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=132824)[0m f1_macro: 0.2971811243088471
[2m[36m(func pid=132824)[0m f1_weighted: 0.31808251303604945
[2m[36m(func pid=132824)[0m f1_per_class: [0.5, 0.124, 0.8, 0.301, 0.155, 0.068, 0.574, 0.211, 0.124, 0.115]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4878 | Steps: 4 | Val loss: 2.2623 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.8499 | Steps: 4 | Val loss: 1.9418 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.0457 | Steps: 4 | Val loss: 34.4936 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=140546)[0m top1: 0.15205223880597016
[2m[36m(func pid=140546)[0m top5: 0.5979477611940298
[2m[36m(func pid=140546)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=140546)[0m f1_macro: 0.11937945698770727
[2m[36m(func pid=140546)[0m f1_weighted: 0.1726088127369835
[2m[36m(func pid=140546)[0m f1_per_class: [0.17, 0.201, 0.069, 0.178, 0.0, 0.045, 0.222, 0.17, 0.073, 0.064]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 38.8562 | Steps: 4 | Val loss: 260.0079 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=145792)[0m top1: 0.3069029850746269
[2m[36m(func pid=145792)[0m top5: 0.8138992537313433
[2m[36m(func pid=145792)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=145792)[0m f1_macro: 0.2831451152680721
[2m[36m(func pid=145792)[0m f1_weighted: 0.2640400809590759
[2m[36m(func pid=145792)[0m f1_per_class: [0.472, 0.223, 0.537, 0.561, 0.159, 0.154, 0.045, 0.294, 0.126, 0.262]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.24860074626865672
[2m[36m(func pid=127722)[0m top5: 0.8600746268656716
[2m[36m(func pid=127722)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=127722)[0m f1_macro: 0.2886027843801152
[2m[36m(func pid=127722)[0m f1_weighted: 0.2667040520930133
[2m[36m(func pid=127722)[0m f1_per_class: [0.591, 0.367, 0.581, 0.399, 0.233, 0.043, 0.167, 0.217, 0.204, 0.086]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:52 (running for 00:25:09.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.046 |      0.289 |                   88 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 38.856 |      0.337 |                   69 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.488 |      0.119 |                   36 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.85  |      0.283 |                   13 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3111007462686567
[2m[36m(func pid=132824)[0m top5: 0.6875
[2m[36m(func pid=132824)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=132824)[0m f1_macro: 0.3374984895964642
[2m[36m(func pid=132824)[0m f1_weighted: 0.3561796926887788
[2m[36m(func pid=132824)[0m f1_per_class: [0.658, 0.156, 0.846, 0.459, 0.144, 0.129, 0.49, 0.261, 0.145, 0.088]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5215 | Steps: 4 | Val loss: 2.2430 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.8089 | Steps: 4 | Val loss: 1.9236 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 3.9984 | Steps: 4 | Val loss: 39.9014 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=140546)[0m top1: 0.15811567164179105
[2m[36m(func pid=140546)[0m top5: 0.6152052238805971
[2m[36m(func pid=140546)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=140546)[0m f1_macro: 0.12223018605993285
[2m[36m(func pid=140546)[0m f1_weighted: 0.1812567032682313
[2m[36m(func pid=140546)[0m f1_per_class: [0.166, 0.199, 0.065, 0.21, 0.0, 0.058, 0.218, 0.171, 0.069, 0.066]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9023 | Steps: 4 | Val loss: 235.9978 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=145792)[0m top1: 0.32509328358208955
[2m[36m(func pid=145792)[0m top5: 0.8180970149253731
[2m[36m(func pid=145792)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=145792)[0m f1_macro: 0.2589768743907145
[2m[36m(func pid=145792)[0m f1_weighted: 0.2697472473090989
[2m[36m(func pid=145792)[0m f1_per_class: [0.516, 0.116, 0.338, 0.559, 0.167, 0.138, 0.131, 0.326, 0.133, 0.166]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.2019589552238806
[2m[36m(func pid=127722)[0m top5: 0.8470149253731343
[2m[36m(func pid=127722)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=127722)[0m f1_macro: 0.2719639323833391
[2m[36m(func pid=127722)[0m f1_weighted: 0.231064359876483
[2m[36m(func pid=127722)[0m f1_per_class: [0.5, 0.256, 0.571, 0.297, 0.333, 0.088, 0.193, 0.223, 0.202, 0.055]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:32:57 (running for 00:25:15.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.998 |      0.272 |                   89 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 |  1.902 |      0.346 |                   70 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.521 |      0.122 |                   37 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.809 |      0.259 |                   14 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3358208955223881
[2m[36m(func pid=132824)[0m top5: 0.7406716417910447
[2m[36m(func pid=132824)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=132824)[0m f1_macro: 0.34578130469482843
[2m[36m(func pid=132824)[0m f1_weighted: 0.369913437764857
[2m[36m(func pid=132824)[0m f1_per_class: [0.592, 0.274, 0.696, 0.547, 0.118, 0.254, 0.323, 0.346, 0.198, 0.111]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3925 | Steps: 4 | Val loss: 2.2084 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.5774 | Steps: 4 | Val loss: 1.8680 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 3.9696 | Steps: 4 | Val loss: 41.0550 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.17817164179104478
[2m[36m(func pid=140546)[0m top5: 0.648320895522388
[2m[36m(func pid=140546)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=140546)[0m f1_macro: 0.13208652832710627
[2m[36m(func pid=140546)[0m f1_weighted: 0.20080010518533975
[2m[36m(func pid=140546)[0m f1_per_class: [0.18, 0.217, 0.068, 0.226, 0.0, 0.068, 0.254, 0.169, 0.059, 0.08]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 19.0091 | Steps: 4 | Val loss: 268.1533 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=145792)[0m top1: 0.3204291044776119
[2m[36m(func pid=145792)[0m top5: 0.8605410447761194
[2m[36m(func pid=145792)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=145792)[0m f1_macro: 0.26855381041651666
[2m[36m(func pid=145792)[0m f1_weighted: 0.3248042544681075
[2m[36m(func pid=145792)[0m f1_per_class: [0.459, 0.191, 0.267, 0.539, 0.202, 0.165, 0.292, 0.29, 0.143, 0.137]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.19869402985074627
[2m[36m(func pid=127722)[0m top5: 0.8512126865671642
[2m[36m(func pid=127722)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=127722)[0m f1_macro: 0.24518181391301813
[2m[36m(func pid=127722)[0m f1_weighted: 0.23483751706292372
[2m[36m(func pid=127722)[0m f1_per_class: [0.444, 0.178, 0.471, 0.271, 0.235, 0.112, 0.273, 0.25, 0.169, 0.048]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:33:03 (running for 00:25:20.62)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  3.97  |      0.245 |                   90 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 19.009 |      0.346 |                   71 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.393 |      0.132 |                   38 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.577 |      0.269 |                   15 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.3414179104477612
[2m[36m(func pid=132824)[0m top5: 0.7751865671641791
[2m[36m(func pid=132824)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=132824)[0m f1_macro: 0.3459996005287983
[2m[36m(func pid=132824)[0m f1_weighted: 0.33665550018943946
[2m[36m(func pid=132824)[0m f1_per_class: [0.547, 0.377, 0.696, 0.569, 0.124, 0.281, 0.112, 0.37, 0.24, 0.143]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4722 | Steps: 4 | Val loss: 2.1935 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 11.1712 | Steps: 4 | Val loss: 37.8636 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.4810 | Steps: 4 | Val loss: 1.8189 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=140546)[0m top1: 0.18889925373134328
[2m[36m(func pid=140546)[0m top5: 0.6660447761194029
[2m[36m(func pid=140546)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=140546)[0m f1_macro: 0.13127404326028638
[2m[36m(func pid=140546)[0m f1_weighted: 0.21237973578574001
[2m[36m(func pid=140546)[0m f1_per_class: [0.163, 0.218, 0.071, 0.237, 0.0, 0.057, 0.295, 0.124, 0.078, 0.07]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 205.0698 | Steps: 4 | Val loss: 320.1640 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=127722)[0m top1: 0.22388059701492538
[2m[36m(func pid=127722)[0m top5: 0.8446828358208955
[2m[36m(func pid=127722)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=127722)[0m f1_macro: 0.253330180091034
[2m[36m(func pid=127722)[0m f1_weighted: 0.2644012260209155
[2m[36m(func pid=127722)[0m f1_per_class: [0.358, 0.231, 0.471, 0.271, 0.276, 0.138, 0.339, 0.253, 0.146, 0.051]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m top1: 0.33302238805970147
[2m[36m(func pid=145792)[0m top5: 0.8824626865671642
[2m[36m(func pid=145792)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=145792)[0m f1_macro: 0.27813062924968796
[2m[36m(func pid=145792)[0m f1_weighted: 0.3454820840417446
[2m[36m(func pid=145792)[0m f1_per_class: [0.358, 0.407, 0.312, 0.484, 0.186, 0.14, 0.306, 0.269, 0.157, 0.164]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:33:08 (running for 00:25:25.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  11.171 |      0.253 |                   91 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 205.07  |      0.339 |                   72 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |   2.472 |      0.131 |                   39 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |   1.481 |      0.278 |                   16 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |   0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |   0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |   3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      |  13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |   2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |   0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |   1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |   8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.34328358208955223
[2m[36m(func pid=132824)[0m top5: 0.7803171641791045
[2m[36m(func pid=132824)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=132824)[0m f1_macro: 0.33942912571690637
[2m[36m(func pid=132824)[0m f1_weighted: 0.32263489890316105
[2m[36m(func pid=132824)[0m f1_per_class: [0.49, 0.477, 0.696, 0.545, 0.107, 0.284, 0.043, 0.329, 0.2, 0.225]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4039 | Steps: 4 | Val loss: 2.1789 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.3946 | Steps: 4 | Val loss: 1.9180 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 10.0172 | Steps: 4 | Val loss: 29.7135 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=140546)[0m top1: 0.20055970149253732
[2m[36m(func pid=140546)[0m top5: 0.6805037313432836
[2m[36m(func pid=140546)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=140546)[0m f1_macro: 0.14565641029128043
[2m[36m(func pid=140546)[0m f1_weighted: 0.22216657126132755
[2m[36m(func pid=140546)[0m f1_per_class: [0.175, 0.259, 0.084, 0.254, 0.0, 0.067, 0.271, 0.182, 0.083, 0.081]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 16.4973 | Steps: 4 | Val loss: 361.2964 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=127722)[0m top1: 0.29151119402985076
[2m[36m(func pid=127722)[0m top5: 0.8619402985074627
[2m[36m(func pid=127722)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=127722)[0m f1_macro: 0.23628932010326453
[2m[36m(func pid=127722)[0m f1_weighted: 0.32737777756984443
[2m[36m(func pid=127722)[0m f1_per_class: [0.246, 0.376, 0.143, 0.366, 0.095, 0.204, 0.359, 0.266, 0.22, 0.088]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m top1: 0.2621268656716418
[2m[36m(func pid=145792)[0m top5: 0.847481343283582
[2m[36m(func pid=145792)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=145792)[0m f1_macro: 0.23683028691088576
[2m[36m(func pid=145792)[0m f1_weighted: 0.25822823037721937
[2m[36m(func pid=145792)[0m f1_per_class: [0.221, 0.445, 0.421, 0.347, 0.162, 0.095, 0.156, 0.222, 0.127, 0.171]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:33:13 (running for 00:25:31.39)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 10.017 |      0.236 |                   92 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 16.497 |      0.338 |                   73 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.404 |      0.146 |                   40 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.395 |      0.237 |                   17 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.33908582089552236
[2m[36m(func pid=132824)[0m top5: 0.7737873134328358
[2m[36m(func pid=132824)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=132824)[0m f1_macro: 0.33782862008942527
[2m[36m(func pid=132824)[0m f1_weighted: 0.31433634888611384
[2m[36m(func pid=132824)[0m f1_per_class: [0.407, 0.516, 0.75, 0.523, 0.095, 0.278, 0.022, 0.31, 0.2, 0.278]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3186 | Steps: 4 | Val loss: 2.1720 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 6.3187 | Steps: 4 | Val loss: 27.2251 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140546)[0m top1: 0.20569029850746268
[2m[36m(func pid=140546)[0m top5: 0.6865671641791045
[2m[36m(func pid=140546)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=140546)[0m f1_macro: 0.14890550144926645
[2m[36m(func pid=140546)[0m f1_weighted: 0.227168870483605
[2m[36m(func pid=140546)[0m f1_per_class: [0.184, 0.271, 0.1, 0.242, 0.0, 0.079, 0.291, 0.165, 0.071, 0.086]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.3285 | Steps: 4 | Val loss: 1.9238 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 73.7642 | Steps: 4 | Val loss: 399.3590 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=145792)[0m top1: 0.2560634328358209
[2m[36m(func pid=145792)[0m top5: 0.8390858208955224
[2m[36m(func pid=145792)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=145792)[0m f1_macro: 0.2473921942500387
[2m[36m(func pid=145792)[0m f1_weighted: 0.26049512297403554
[2m[36m(func pid=145792)[0m f1_per_class: [0.215, 0.421, 0.512, 0.323, 0.177, 0.082, 0.204, 0.219, 0.133, 0.19]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3180970149253731
[2m[36m(func pid=127722)[0m top5: 0.8656716417910447
[2m[36m(func pid=127722)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=127722)[0m f1_macro: 0.2530079508907509
[2m[36m(func pid=127722)[0m f1_weighted: 0.3471336052665094
[2m[36m(func pid=127722)[0m f1_per_class: [0.189, 0.428, 0.267, 0.362, 0.0, 0.255, 0.382, 0.251, 0.237, 0.16]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4857 | Steps: 4 | Val loss: 2.1849 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:33:19 (running for 00:25:36.67)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  6.319 |      0.253 |                   93 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 73.764 |      0.331 |                   74 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.319 |      0.149 |                   41 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.328 |      0.247 |                   18 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=132824)[0m top1: 0.32882462686567165
[2m[36m(func pid=132824)[0m top5: 0.7569962686567164
[2m[36m(func pid=132824)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=132824)[0m f1_macro: 0.33075635672905335
[2m[36m(func pid=132824)[0m f1_weighted: 0.30160969171109786
[2m[36m(func pid=132824)[0m f1_per_class: [0.328, 0.534, 0.71, 0.476, 0.096, 0.274, 0.016, 0.322, 0.2, 0.353]
[2m[36m(func pid=132824)[0m 
[2m[36m(func pid=140546)[0m top1: 0.19962686567164178
[2m[36m(func pid=140546)[0m top5: 0.6763059701492538
[2m[36m(func pid=140546)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=140546)[0m f1_macro: 0.15078913560078896
[2m[36m(func pid=140546)[0m f1_weighted: 0.22257732582912695
[2m[36m(func pid=140546)[0m f1_per_class: [0.204, 0.282, 0.099, 0.228, 0.011, 0.101, 0.272, 0.174, 0.065, 0.073]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.4198 | Steps: 4 | Val loss: 1.9229 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 23.5972 | Steps: 4 | Val loss: 24.5481 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=132824)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 32.0505 | Steps: 4 | Val loss: 396.1158 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:33:24 (running for 00:25:41.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  6.319 |      0.253 |                   93 |
| train_98a10_00011 | RUNNING    | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 73.764 |      0.331 |                   74 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.486 |      0.151 |                   42 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.42  |      0.254 |                   19 |
| train_98a10_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.271455223880597
[2m[36m(func pid=145792)[0m top5: 0.8367537313432836
[2m[36m(func pid=145792)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=145792)[0m f1_macro: 0.25384525112261586
[2m[36m(func pid=145792)[0m f1_weighted: 0.26407686907277034
[2m[36m(func pid=145792)[0m f1_per_class: [0.325, 0.449, 0.462, 0.301, 0.145, 0.069, 0.211, 0.24, 0.177, 0.16]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3414179104477612
[2m[36m(func pid=127722)[0m top5: 0.8736007462686567
[2m[36m(func pid=127722)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=127722)[0m f1_macro: 0.2798588004828365
[2m[36m(func pid=127722)[0m f1_weighted: 0.3623709647854437
[2m[36m(func pid=127722)[0m f1_per_class: [0.229, 0.465, 0.267, 0.352, 0.0, 0.266, 0.409, 0.255, 0.197, 0.359]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4998 | Steps: 4 | Val loss: 2.1772 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=132824)[0m top1: 0.3246268656716418
[2m[36m(func pid=132824)[0m top5: 0.7486007462686567
[2m[36m(func pid=132824)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=132824)[0m f1_macro: 0.32393318315275044
[2m[36m(func pid=132824)[0m f1_weighted: 0.2932942909369304
[2m[36m(func pid=132824)[0m f1_per_class: [0.295, 0.546, 0.615, 0.438, 0.092, 0.268, 0.015, 0.345, 0.2, 0.424]
[2m[36m(func pid=140546)[0m top1: 0.19402985074626866
[2m[36m(func pid=140546)[0m top5: 0.6861007462686567
[2m[36m(func pid=140546)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=140546)[0m f1_macro: 0.15369386623913592
[2m[36m(func pid=140546)[0m f1_weighted: 0.2165186401837544
[2m[36m(func pid=140546)[0m f1_per_class: [0.22, 0.251, 0.112, 0.24, 0.01, 0.108, 0.247, 0.206, 0.068, 0.074]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 4.4213 | Steps: 4 | Val loss: 25.3247 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.3632 | Steps: 4 | Val loss: 1.8602 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3686 | Steps: 4 | Val loss: 2.1738 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=127722)[0m top1: 0.33302238805970147
[2m[36m(func pid=127722)[0m top5: 0.8917910447761194
[2m[36m(func pid=127722)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=127722)[0m f1_macro: 0.29828758059161575
[2m[36m(func pid=127722)[0m f1_weighted: 0.3623388124922982
[2m[36m(func pid=127722)[0m f1_per_class: [0.339, 0.457, 0.375, 0.341, 0.0, 0.266, 0.421, 0.244, 0.139, 0.4]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=145792)[0m top1: 0.31203358208955223
[2m[36m(func pid=145792)[0m top5: 0.8544776119402985
[2m[36m(func pid=145792)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=145792)[0m f1_macro: 0.2820814008338359
[2m[36m(func pid=145792)[0m f1_weighted: 0.331635655904991
[2m[36m(func pid=145792)[0m f1_per_class: [0.5, 0.435, 0.312, 0.415, 0.123, 0.149, 0.289, 0.3, 0.171, 0.125]
[2m[36m(func pid=140546)[0m top1: 0.20242537313432835
[2m[36m(func pid=140546)[0m top5: 0.6875
[2m[36m(func pid=140546)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=140546)[0m f1_macro: 0.16039675030599992
[2m[36m(func pid=140546)[0m f1_weighted: 0.2241833375725882
[2m[36m(func pid=140546)[0m f1_per_class: [0.207, 0.294, 0.113, 0.243, 0.01, 0.109, 0.241, 0.218, 0.087, 0.081]
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 11.1716 | Steps: 4 | Val loss: 26.3320 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:33:29 (running for 00:25:47.33)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.421 |      0.298 |                   95 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.5   |      0.154 |                   43 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.42  |      0.254 |                   19 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=150999)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=150999)[0m Configuration completed!
[2m[36m(func pid=150999)[0m New optimizer parameters:
[2m[36m(func pid=150999)[0m SGD (
[2m[36m(func pid=150999)[0m Parameter Group 0
[2m[36m(func pid=150999)[0m     dampening: 0
[2m[36m(func pid=150999)[0m     differentiable: False
[2m[36m(func pid=150999)[0m     foreach: None
[2m[36m(func pid=150999)[0m     lr: 0.01
[2m[36m(func pid=150999)[0m     maximize: False
[2m[36m(func pid=150999)[0m     momentum: 0.9
[2m[36m(func pid=150999)[0m     nesterov: False
[2m[36m(func pid=150999)[0m     weight_decay: 0.0001
[2m[36m(func pid=150999)[0m )
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=127722)[0m top1: 0.31529850746268656
[2m[36m(func pid=127722)[0m top5: 0.9057835820895522
[2m[36m(func pid=127722)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=127722)[0m f1_macro: 0.29053154181381535
[2m[36m(func pid=127722)[0m f1_weighted: 0.3524609548427386
[2m[36m(func pid=127722)[0m f1_per_class: [0.436, 0.37, 0.6, 0.356, 0.0, 0.265, 0.438, 0.183, 0.125, 0.133]
[2m[36m(func pid=127722)[0m 
== Status ==
Current time: 2024-01-07 11:33:35 (running for 00:25:52.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 11.172 |      0.291 |                   96 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.369 |      0.16  |                   44 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.363 |      0.282 |                   20 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3220 | Steps: 4 | Val loss: 2.1708 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2868 | Steps: 4 | Val loss: 1.8513 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0663 | Steps: 4 | Val loss: 2.1806 | Batch size: 32 | lr: 0.01 | Duration: 4.59s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 4.6989 | Steps: 4 | Val loss: 27.3354 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=140546)[0m top1: 0.19263059701492538
[2m[36m(func pid=140546)[0m top5: 0.6944962686567164
[2m[36m(func pid=140546)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=140546)[0m f1_macro: 0.15756117541663575
[2m[36m(func pid=140546)[0m f1_weighted: 0.21283043247117148
[2m[36m(func pid=140546)[0m f1_per_class: [0.212, 0.276, 0.119, 0.24, 0.009, 0.118, 0.214, 0.214, 0.076, 0.098]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.29990671641791045
[2m[36m(func pid=145792)[0m top5: 0.8563432835820896
[2m[36m(func pid=145792)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=145792)[0m f1_macro: 0.2855424755940909
[2m[36m(func pid=145792)[0m f1_weighted: 0.31437902780495824
[2m[36m(func pid=145792)[0m f1_per_class: [0.575, 0.44, 0.296, 0.42, 0.148, 0.158, 0.216, 0.285, 0.189, 0.128]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:33:40 (running for 00:25:57.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 | 11.172 |      0.291 |                   96 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.322 |      0.158 |                   45 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.287 |      0.286 |                   21 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  3.066 |      0.08  |                    1 |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.24253731343283583
[2m[36m(func pid=150999)[0m top5: 0.7089552238805971
[2m[36m(func pid=150999)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=150999)[0m f1_macro: 0.08015040730523967
[2m[36m(func pid=150999)[0m f1_weighted: 0.1596601927377005
[2m[36m(func pid=150999)[0m f1_per_class: [0.119, 0.105, 0.105, 0.003, 0.0, 0.012, 0.457, 0.0, 0.0, 0.0]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=127722)[0m top1: 0.30223880597014924
[2m[36m(func pid=127722)[0m top5: 0.9286380597014925
[2m[36m(func pid=127722)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=127722)[0m f1_macro: 0.30113035002269334
[2m[36m(func pid=127722)[0m f1_weighted: 0.3427235869124357
[2m[36m(func pid=127722)[0m f1_per_class: [0.537, 0.232, 0.8, 0.406, 0.0, 0.281, 0.433, 0.134, 0.114, 0.074]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3216 | Steps: 4 | Val loss: 2.1651 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.1689 | Steps: 4 | Val loss: 1.8629 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9410 | Steps: 4 | Val loss: 2.1425 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 5.4096 | Steps: 4 | Val loss: 25.8685 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=140546)[0m top1: 0.20009328358208955
[2m[36m(func pid=140546)[0m top5: 0.6996268656716418
[2m[36m(func pid=140546)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=140546)[0m f1_macro: 0.159300593818014
[2m[36m(func pid=140546)[0m f1_weighted: 0.22426469007192967
[2m[36m(func pid=140546)[0m f1_per_class: [0.23, 0.28, 0.118, 0.237, 0.012, 0.113, 0.257, 0.198, 0.073, 0.074]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.28591417910447764
[2m[36m(func pid=145792)[0m top5: 0.8591417910447762
[2m[36m(func pid=145792)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=145792)[0m f1_macro: 0.29110278823639624
[2m[36m(func pid=145792)[0m f1_weighted: 0.29189011255570524
[2m[36m(func pid=145792)[0m f1_per_class: [0.575, 0.401, 0.393, 0.454, 0.18, 0.16, 0.129, 0.293, 0.151, 0.175]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.2905783582089552
[2m[36m(func pid=150999)[0m top5: 0.6637126865671642
[2m[36m(func pid=150999)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=150999)[0m f1_macro: 0.17720921035859058
[2m[36m(func pid=150999)[0m f1_weighted: 0.1769934536881929
[2m[36m(func pid=150999)[0m f1_per_class: [0.167, 0.166, 0.529, 0.465, 0.129, 0.07, 0.0, 0.0, 0.018, 0.229]
[2m[36m(func pid=150999)[0m 
== Status ==
Current time: 2024-01-07 11:33:46 (running for 00:26:03.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  4.699 |      0.301 |                   97 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.322 |      0.159 |                   46 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.169 |      0.291 |                   22 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  2.941 |      0.177 |                    2 |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=127722)[0m top1: 0.3381529850746269
[2m[36m(func pid=127722)[0m top5: 0.9342350746268657
[2m[36m(func pid=127722)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.324634432978544
[2m[36m(func pid=127722)[0m f1_weighted: 0.3641636911230787
[2m[36m(func pid=127722)[0m f1_per_class: [0.529, 0.171, 0.733, 0.473, 0.333, 0.283, 0.474, 0.127, 0.122, 0.0]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3677 | Steps: 4 | Val loss: 2.1750 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.3290 | Steps: 4 | Val loss: 1.7890 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1197 | Steps: 4 | Val loss: 3.3905 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8423 | Steps: 4 | Val loss: 24.0929 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=140546)[0m top1: 0.19076492537313433
[2m[36m(func pid=140546)[0m top5: 0.6930970149253731
[2m[36m(func pid=140546)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=140546)[0m f1_macro: 0.15349269472932753
[2m[36m(func pid=140546)[0m f1_weighted: 0.21371604603525915
[2m[36m(func pid=140546)[0m f1_per_class: [0.229, 0.264, 0.104, 0.245, 0.013, 0.118, 0.227, 0.181, 0.062, 0.092]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3316231343283582
[2m[36m(func pid=145792)[0m top5: 0.8628731343283582
[2m[36m(func pid=145792)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=145792)[0m f1_macro: 0.31691719421248843
[2m[36m(func pid=145792)[0m f1_weighted: 0.3229450302976349
[2m[36m(func pid=145792)[0m f1_per_class: [0.607, 0.445, 0.4, 0.508, 0.19, 0.155, 0.152, 0.293, 0.172, 0.247]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:33:51 (running for 00:26:08.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  5.41  |      0.325 |                   98 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.368 |      0.153 |                   47 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.329 |      0.317 |                   23 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  2.12  |      0.114 |                    3 |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.07695895522388059
[2m[36m(func pid=150999)[0m top5: 0.5373134328358209
[2m[36m(func pid=150999)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=150999)[0m f1_macro: 0.11399071961904098
[2m[36m(func pid=150999)[0m f1_weighted: 0.04421748201795375
[2m[36m(func pid=150999)[0m f1_per_class: [0.087, 0.117, 0.478, 0.007, 0.024, 0.0, 0.0, 0.267, 0.0, 0.16]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=127722)[0m top1: 0.37220149253731344
[2m[36m(func pid=127722)[0m top5: 0.9291044776119403
[2m[36m(func pid=127722)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=127722)[0m f1_macro: 0.3051849893921448
[2m[36m(func pid=127722)[0m f1_weighted: 0.388719287021371
[2m[36m(func pid=127722)[0m f1_per_class: [0.492, 0.265, 0.478, 0.539, 0.267, 0.282, 0.458, 0.069, 0.125, 0.077]
[2m[36m(func pid=127722)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4184 | Steps: 4 | Val loss: 2.1690 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2799 | Steps: 4 | Val loss: 1.6433 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0620 | Steps: 4 | Val loss: 2.0742 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=127722)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.9766 | Steps: 4 | Val loss: 25.5816 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=140546)[0m top1: 0.19309701492537312
[2m[36m(func pid=140546)[0m top5: 0.6916977611940298
[2m[36m(func pid=140546)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=140546)[0m f1_macro: 0.156312533279433
[2m[36m(func pid=140546)[0m f1_weighted: 0.21392504750908803
[2m[36m(func pid=140546)[0m f1_per_class: [0.236, 0.265, 0.109, 0.26, 0.013, 0.122, 0.209, 0.186, 0.074, 0.091]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:33:56 (running for 00:26:13.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00010 | RUNNING    | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  0.842 |      0.305 |                   99 |
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.418 |      0.156 |                   48 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.28  |      0.349 |                   24 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  2.12  |      0.114 |                    3 |
| train_98a10_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.40951492537313433
[2m[36m(func pid=145792)[0m top5: 0.9193097014925373
[2m[36m(func pid=145792)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=145792)[0m f1_macro: 0.3488945749066172
[2m[36m(func pid=145792)[0m f1_weighted: 0.4220939295794226
[2m[36m(func pid=145792)[0m f1_per_class: [0.544, 0.512, 0.308, 0.539, 0.16, 0.208, 0.396, 0.31, 0.175, 0.338]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.29244402985074625
[2m[36m(func pid=150999)[0m top5: 0.8059701492537313
[2m[36m(func pid=150999)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=150999)[0m f1_macro: 0.2887900183184039
[2m[36m(func pid=150999)[0m f1_weighted: 0.24539894465797457
[2m[36m(func pid=150999)[0m f1_per_class: [0.516, 0.428, 0.733, 0.312, 0.08, 0.3, 0.045, 0.34, 0.0, 0.133]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=127722)[0m top1: 0.3694029850746269
[2m[36m(func pid=127722)[0m top5: 0.9057835820895522
[2m[36m(func pid=127722)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=127722)[0m f1_macro: 0.2773181754553523
[2m[36m(func pid=127722)[0m f1_weighted: 0.3864930309712481
[2m[36m(func pid=127722)[0m f1_per_class: [0.564, 0.255, 0.268, 0.558, 0.128, 0.25, 0.449, 0.086, 0.139, 0.077]
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3191 | Steps: 4 | Val loss: 2.1712 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.1129 | Steps: 4 | Val loss: 1.6215 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.0382 | Steps: 4 | Val loss: 1.6175 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=140546)[0m top1: 0.19263059701492538
[2m[36m(func pid=140546)[0m top5: 0.6982276119402985
[2m[36m(func pid=140546)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=140546)[0m f1_macro: 0.15639063333821665
[2m[36m(func pid=140546)[0m f1_weighted: 0.2153186298493245
[2m[36m(func pid=140546)[0m f1_per_class: [0.267, 0.254, 0.118, 0.275, 0.0, 0.121, 0.206, 0.184, 0.067, 0.071]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=150999)[0m top1: 0.4388992537313433
[2m[36m(func pid=150999)[0m top5: 0.9165111940298507
[2m[36m(func pid=150999)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=150999)[0m f1_macro: 0.3098286144617933
[2m[36m(func pid=150999)[0m f1_weighted: 0.3937701616477743
[2m[36m(func pid=150999)[0m f1_per_class: [0.537, 0.071, 0.55, 0.6, 0.2, 0.226, 0.553, 0.016, 0.129, 0.216]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.4141791044776119
[2m[36m(func pid=145792)[0m top5: 0.9286380597014925
[2m[36m(func pid=145792)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=145792)[0m f1_macro: 0.3224779595063124
[2m[36m(func pid=145792)[0m f1_weighted: 0.4362474701297179
[2m[36m(func pid=145792)[0m f1_per_class: [0.516, 0.459, 0.32, 0.539, 0.088, 0.203, 0.501, 0.258, 0.112, 0.23]
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2344 | Steps: 4 | Val loss: 2.1789 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3472 | Steps: 4 | Val loss: 2.5998 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=140546)[0m top1: 0.1865671641791045
[2m[36m(func pid=140546)[0m top5: 0.6898320895522388
[2m[36m(func pid=140546)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=140546)[0m f1_macro: 0.15472131520732868
[2m[36m(func pid=140546)[0m f1_weighted: 0.20792249639383534
[2m[36m(func pid=140546)[0m f1_per_class: [0.265, 0.241, 0.118, 0.291, 0.0, 0.122, 0.17, 0.195, 0.069, 0.076]
== Status ==
Current time: 2024-01-07 11:34:01 (running for 00:26:19.25)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.319 |      0.156 |                   49 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.28  |      0.349 |                   24 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  2.113 |      0.31  |                    5 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=152562)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=152562)[0m Configuration completed!
[2m[36m(func pid=152562)[0m New optimizer parameters:
[2m[36m(func pid=152562)[0m SGD (
[2m[36m(func pid=152562)[0m Parameter Group 0
[2m[36m(func pid=152562)[0m     dampening: 0
[2m[36m(func pid=152562)[0m     differentiable: False
[2m[36m(func pid=152562)[0m     foreach: None
[2m[36m(func pid=152562)[0m     lr: 0.1
[2m[36m(func pid=152562)[0m     maximize: False
[2m[36m(func pid=152562)[0m     momentum: 0.9
[2m[36m(func pid=152562)[0m     nesterov: False
[2m[36m(func pid=152562)[0m     weight_decay: 0.0001
[2m[36m(func pid=152562)[0m )
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:34:07 (running for 00:26:24.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.234 |      0.155 |                   50 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.038 |      0.322 |                   25 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.347 |      0.281 |                    6 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.30363805970149255
[2m[36m(func pid=150999)[0m top5: 0.7196828358208955
[2m[36m(func pid=150999)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=150999)[0m f1_macro: 0.2811193044382447
[2m[36m(func pid=150999)[0m f1_weighted: 0.27483019459384417
[2m[36m(func pid=150999)[0m f1_per_class: [0.537, 0.301, 0.407, 0.592, 0.038, 0.022, 0.028, 0.433, 0.133, 0.319]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.2271 | Steps: 4 | Val loss: 2.1738 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0487 | Steps: 4 | Val loss: 1.6884 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.2113 | Steps: 4 | Val loss: 7.6668 | Batch size: 32 | lr: 0.1 | Duration: 4.60s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1073 | Steps: 4 | Val loss: 2.5691 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=140546)[0m top1: 0.18796641791044777
[2m[36m(func pid=140546)[0m top5: 0.6944962686567164
[2m[36m(func pid=140546)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=140546)[0m f1_macro: 0.15659850722315388
[2m[36m(func pid=140546)[0m f1_weighted: 0.20671978216532028
[2m[36m(func pid=140546)[0m f1_per_class: [0.273, 0.242, 0.101, 0.291, 0.017, 0.126, 0.16, 0.214, 0.059, 0.083]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3666044776119403
[2m[36m(func pid=145792)[0m top5: 0.9095149253731343
[2m[36m(func pid=145792)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=145792)[0m f1_macro: 0.3037368036859071
[2m[36m(func pid=145792)[0m f1_weighted: 0.3995484114643902
[2m[36m(func pid=145792)[0m f1_per_class: [0.528, 0.456, 0.242, 0.483, 0.079, 0.223, 0.427, 0.244, 0.101, 0.252]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:34:12 (running for 00:26:30.03)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.227 |      0.157 |                   51 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.049 |      0.304 |                   26 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.347 |      0.281 |                    6 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.211 |      0.153 |                    1 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.39132462686567165
[2m[36m(func pid=150999)[0m top5: 0.7444029850746269
[2m[36m(func pid=150999)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=150999)[0m f1_macro: 0.34208061537447254
[2m[36m(func pid=150999)[0m f1_weighted: 0.305935955635962
[2m[36m(func pid=150999)[0m f1_per_class: [0.603, 0.547, 0.49, 0.506, 0.218, 0.165, 0.006, 0.454, 0.072, 0.359]
[2m[36m(func pid=152562)[0m top1: 0.31343283582089554
[2m[36m(func pid=152562)[0m top5: 0.597481343283582
[2m[36m(func pid=152562)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=152562)[0m f1_macro: 0.15281803951158537
[2m[36m(func pid=152562)[0m f1_weighted: 0.20520140731871594
[2m[36m(func pid=152562)[0m f1_per_class: [0.485, 0.405, 0.0, 0.442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.197]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.2706 | Steps: 4 | Val loss: 2.1724 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0471 | Steps: 4 | Val loss: 1.7785 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.2816 | Steps: 4 | Val loss: 2.1058 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 30.8261 | Steps: 4 | Val loss: 13.4708 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=140546)[0m top1: 0.1875
[2m[36m(func pid=140546)[0m top5: 0.6870335820895522
[2m[36m(func pid=140546)[0m f1_micro: 0.1875
[2m[36m(func pid=140546)[0m f1_macro: 0.15922755833858127
[2m[36m(func pid=140546)[0m f1_weighted: 0.20500938744074876
[2m[36m(func pid=140546)[0m f1_per_class: [0.286, 0.229, 0.093, 0.315, 0.014, 0.115, 0.14, 0.211, 0.089, 0.101]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3246268656716418
[2m[36m(func pid=145792)[0m top5: 0.8903917910447762
[2m[36m(func pid=145792)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=145792)[0m f1_macro: 0.291267746435968
[2m[36m(func pid=145792)[0m f1_weighted: 0.34986370054794974
[2m[36m(func pid=145792)[0m f1_per_class: [0.504, 0.408, 0.289, 0.461, 0.11, 0.254, 0.282, 0.335, 0.108, 0.16]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:34:17 (running for 00:26:35.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.271 |      0.159 |                   52 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.047 |      0.291 |                   27 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.282 |      0.32  |                    8 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.211 |      0.153 |                    1 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3306902985074627
[2m[36m(func pid=150999)[0m top5: 0.867070895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=150999)[0m f1_macro: 0.31997306329355435
[2m[36m(func pid=150999)[0m f1_weighted: 0.3091807619488457
[2m[36m(func pid=150999)[0m f1_per_class: [0.562, 0.137, 0.49, 0.569, 0.242, 0.323, 0.154, 0.337, 0.169, 0.215]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.16324626865671643
[2m[36m(func pid=152562)[0m top5: 0.6319962686567164
[2m[36m(func pid=152562)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=152562)[0m f1_macro: 0.11285752224375593
[2m[36m(func pid=152562)[0m f1_weighted: 0.13348374748411543
[2m[36m(func pid=152562)[0m f1_per_class: [0.31, 0.407, 0.0, 0.0, 0.0, 0.0, 0.154, 0.175, 0.0, 0.082]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3428 | Steps: 4 | Val loss: 2.1882 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1372 | Steps: 4 | Val loss: 1.7453 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4433 | Steps: 4 | Val loss: 2.9193 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.18003731343283583
[2m[36m(func pid=140546)[0m top5: 0.679570895522388
[2m[36m(func pid=140546)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=140546)[0m f1_macro: 0.15442867422315293
[2m[36m(func pid=140546)[0m f1_weighted: 0.19703297762910835
[2m[36m(func pid=140546)[0m f1_per_class: [0.247, 0.261, 0.098, 0.258, 0.012, 0.119, 0.147, 0.224, 0.085, 0.093]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 48.0254 | Steps: 4 | Val loss: 10.0964 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=145792)[0m top1: 0.34421641791044777
[2m[36m(func pid=145792)[0m top5: 0.8941231343283582
[2m[36m(func pid=145792)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=145792)[0m f1_macro: 0.32413567565531387
[2m[36m(func pid=145792)[0m f1_weighted: 0.3579226038475279
[2m[36m(func pid=145792)[0m f1_per_class: [0.5, 0.449, 0.429, 0.491, 0.15, 0.255, 0.239, 0.352, 0.221, 0.155]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:34:23 (running for 00:26:40.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.343 |      0.154 |                   53 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.137 |      0.324 |                   28 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.443 |      0.217 |                    9 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 30.826 |      0.113 |                    2 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.33348880597014924
[2m[36m(func pid=150999)[0m top5: 0.6819029850746269
[2m[36m(func pid=150999)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=150999)[0m f1_macro: 0.21651048891678068
[2m[36m(func pid=150999)[0m f1_weighted: 0.24212610140879776
[2m[36m(func pid=150999)[0m f1_per_class: [0.447, 0.152, 0.162, 0.03, 0.268, 0.0, 0.588, 0.232, 0.161, 0.125]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.39365671641791045
[2m[36m(func pid=152562)[0m top5: 0.9090485074626866
[2m[36m(func pid=152562)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=152562)[0m f1_macro: 0.20750316420834825
[2m[36m(func pid=152562)[0m f1_weighted: 0.29879314351361264
[2m[36m(func pid=152562)[0m f1_per_class: [0.667, 0.005, 0.25, 0.45, 0.0, 0.0, 0.52, 0.0, 0.0, 0.183]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.0795 | Steps: 4 | Val loss: 2.1783 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0486 | Steps: 4 | Val loss: 1.7322 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2174 | Steps: 4 | Val loss: 2.5536 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=140546)[0m top1: 0.18796641791044777
[2m[36m(func pid=140546)[0m top5: 0.6861007462686567
[2m[36m(func pid=140546)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=140546)[0m f1_macro: 0.16052990342153525
[2m[36m(func pid=140546)[0m f1_weighted: 0.20526654708970488
[2m[36m(func pid=140546)[0m f1_per_class: [0.247, 0.298, 0.104, 0.253, 0.022, 0.121, 0.157, 0.221, 0.091, 0.091]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 25.1569 | Steps: 4 | Val loss: 33.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=145792)[0m top1: 0.37546641791044777
[2m[36m(func pid=145792)[0m top5: 0.8810634328358209
[2m[36m(func pid=145792)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=145792)[0m f1_macro: 0.3347559002382338
[2m[36m(func pid=145792)[0m f1_weighted: 0.3739518337554599
[2m[36m(func pid=145792)[0m f1_per_class: [0.448, 0.458, 0.436, 0.544, 0.189, 0.221, 0.25, 0.362, 0.199, 0.239]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:34:28 (running for 00:26:45.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.079 |      0.161 |                   54 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.049 |      0.335 |                   29 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.217 |      0.273 |                   10 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 48.025 |      0.208 |                    3 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.300839552238806
[2m[36m(func pid=150999)[0m top5: 0.8013059701492538
[2m[36m(func pid=150999)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=150999)[0m f1_macro: 0.2727888363127254
[2m[36m(func pid=150999)[0m f1_weighted: 0.307403690680865
[2m[36m(func pid=150999)[0m f1_per_class: [0.62, 0.525, 0.068, 0.484, 0.063, 0.068, 0.113, 0.351, 0.077, 0.358]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1692 | Steps: 4 | Val loss: 2.1680 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=152562)[0m top1: 0.25886194029850745
[2m[36m(func pid=152562)[0m top5: 0.49533582089552236
[2m[36m(func pid=152562)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=152562)[0m f1_macro: 0.10462644430858806
[2m[36m(func pid=152562)[0m f1_weighted: 0.1675199693785985
[2m[36m(func pid=152562)[0m f1_per_class: [0.085, 0.0, 0.289, 0.56, 0.046, 0.066, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9682 | Steps: 4 | Val loss: 1.7760 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4399 | Steps: 4 | Val loss: 2.8761 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.19169776119402984
[2m[36m(func pid=140546)[0m top5: 0.6958955223880597
[2m[36m(func pid=140546)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=140546)[0m f1_macro: 0.15931830320197854
[2m[36m(func pid=140546)[0m f1_weighted: 0.2144576210370695
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=140546)[0m f1_per_class: [0.257, 0.283, 0.107, 0.291, 0.032, 0.123, 0.17, 0.177, 0.077, 0.076]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 22.9129 | Steps: 4 | Val loss: 16.9702 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:34:33 (running for 00:26:51.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.169 |      0.159 |                   55 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.968 |      0.337 |                   30 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.217 |      0.273 |                   10 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 25.157 |      0.105 |                    4 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.38152985074626866
[2m[36m(func pid=145792)[0m top5: 0.8465485074626866
[2m[36m(func pid=145792)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=145792)[0m f1_macro: 0.3370772211389893
[2m[36m(func pid=145792)[0m f1_weighted: 0.3576937623349389
[2m[36m(func pid=145792)[0m f1_per_class: [0.4, 0.531, 0.453, 0.492, 0.161, 0.183, 0.208, 0.385, 0.239, 0.317]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.3306902985074627
[2m[36m(func pid=150999)[0m top5: 0.7593283582089553
[2m[36m(func pid=150999)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=150999)[0m f1_macro: 0.319278403418077
[2m[36m(func pid=150999)[0m f1_weighted: 0.30084046346440324
[2m[36m(func pid=150999)[0m f1_per_class: [0.592, 0.507, 0.759, 0.538, 0.125, 0.101, 0.019, 0.427, 0.127, 0.0]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1244 | Steps: 4 | Val loss: 2.1676 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152562)[0m top1: 0.3111007462686567
[2m[36m(func pid=152562)[0m top5: 0.7691231343283582
[2m[36m(func pid=152562)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=152562)[0m f1_macro: 0.26643675867593136
[2m[36m(func pid=152562)[0m f1_weighted: 0.23450939277615157
[2m[36m(func pid=152562)[0m f1_per_class: [0.556, 0.456, 0.143, 0.294, 0.111, 0.317, 0.033, 0.0, 0.29, 0.465]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m top1: 0.19263059701492538
[2m[36m(func pid=140546)[0m top5: 0.7028917910447762
[2m[36m(func pid=140546)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=140546)[0m f1_macro: 0.15951459865147882
[2m[36m(func pid=140546)[0m f1_weighted: 0.2146814415336619
[2m[36m(func pid=140546)[0m f1_per_class: [0.245, 0.298, 0.098, 0.254, 0.024, 0.13, 0.191, 0.197, 0.082, 0.076]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0546 | Steps: 4 | Val loss: 1.8301 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.7532 | Steps: 4 | Val loss: 2.1703 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 28.1849 | Steps: 4 | Val loss: 9.3057 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:34:39 (running for 00:26:56.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.124 |      0.16  |                   56 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.968 |      0.337 |                   30 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.753 |      0.416 |                   12 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 22.913 |      0.266 |                    5 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.41884328358208955
[2m[36m(func pid=150999)[0m top5: 0.8680037313432836
[2m[36m(func pid=150999)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=150999)[0m f1_macro: 0.4164236965516728
[2m[36m(func pid=150999)[0m f1_weighted: 0.37866087048532815
[2m[36m(func pid=150999)[0m f1_per_class: [0.707, 0.593, 0.636, 0.547, 0.169, 0.287, 0.114, 0.402, 0.286, 0.423]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3628731343283582
[2m[36m(func pid=145792)[0m top5: 0.8264925373134329
[2m[36m(func pid=145792)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=145792)[0m f1_macro: 0.32008989732888654
[2m[36m(func pid=145792)[0m f1_weighted: 0.3426802554148103
[2m[36m(func pid=145792)[0m f1_per_class: [0.386, 0.529, 0.436, 0.484, 0.124, 0.179, 0.174, 0.367, 0.251, 0.27]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3052 | Steps: 4 | Val loss: 2.1638 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152562)[0m top1: 0.48600746268656714
[2m[36m(func pid=152562)[0m top5: 0.9221082089552238
[2m[36m(func pid=152562)[0m f1_micro: 0.48600746268656714
[2m[36m(func pid=152562)[0m f1_macro: 0.3561460549074891
[2m[36m(func pid=152562)[0m f1_weighted: 0.4600617821532859
[2m[36m(func pid=152562)[0m f1_per_class: [0.266, 0.523, 0.727, 0.475, 0.269, 0.229, 0.647, 0.0, 0.08, 0.345]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7873 | Steps: 4 | Val loss: 2.4470 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.1921641791044776
[2m[36m(func pid=140546)[0m top5: 0.7005597014925373
[2m[36m(func pid=140546)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=140546)[0m f1_macro: 0.16413572248526595
[2m[36m(func pid=140546)[0m f1_weighted: 0.21504966562060157
[2m[36m(func pid=140546)[0m f1_per_class: [0.227, 0.28, 0.116, 0.239, 0.035, 0.123, 0.216, 0.202, 0.101, 0.102]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.0388 | Steps: 4 | Val loss: 1.8451 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 21.5557 | Steps: 4 | Val loss: 15.9616 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:34:44 (running for 00:27:02.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.305 |      0.164 |                   57 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.055 |      0.32  |                   31 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.787 |      0.305 |                   13 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 28.185 |      0.356 |                    6 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3512126865671642
[2m[36m(func pid=150999)[0m top5: 0.8773320895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=150999)[0m f1_macro: 0.30524163056105696
[2m[36m(func pid=150999)[0m f1_weighted: 0.353022815224985
[2m[36m(func pid=150999)[0m f1_per_class: [0.55, 0.115, 0.625, 0.596, 0.286, 0.132, 0.404, 0.221, 0.0, 0.123]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.333955223880597
[2m[36m(func pid=145792)[0m top5: 0.8372201492537313
[2m[36m(func pid=145792)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=145792)[0m f1_macro: 0.30597923114004955
[2m[36m(func pid=145792)[0m f1_weighted: 0.34719669827349714
[2m[36m(func pid=145792)[0m f1_per_class: [0.471, 0.458, 0.414, 0.468, 0.083, 0.145, 0.261, 0.363, 0.239, 0.159]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2243 | Steps: 4 | Val loss: 2.1440 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=152562)[0m top1: 0.3516791044776119
[2m[36m(func pid=152562)[0m top5: 0.8428171641791045
[2m[36m(func pid=152562)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=152562)[0m f1_macro: 0.21357373941415925
[2m[36m(func pid=152562)[0m f1_weighted: 0.2845830115667989
[2m[36m(func pid=152562)[0m f1_per_class: [0.0, 0.221, 0.165, 0.631, 0.293, 0.287, 0.031, 0.346, 0.163, 0.0]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m top1: 0.19776119402985073
[2m[36m(func pid=140546)[0m top5: 0.7168843283582089
[2m[36m(func pid=140546)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=140546)[0m f1_macro: 0.16681542709091407
[2m[36m(func pid=140546)[0m f1_weighted: 0.21791365681405878
[2m[36m(func pid=140546)[0m f1_per_class: [0.242, 0.291, 0.13, 0.246, 0.028, 0.119, 0.213, 0.204, 0.099, 0.096]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8261 | Steps: 4 | Val loss: 2.5099 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8037 | Steps: 4 | Val loss: 1.8019 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 17.4298 | Steps: 4 | Val loss: 22.9756 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:34:50 (running for 00:27:07.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.224 |      0.167 |                   58 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  1.039 |      0.306 |                   32 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.826 |      0.255 |                   14 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 21.556 |      0.214 |                    7 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.34841417910447764
[2m[36m(func pid=150999)[0m top5: 0.8372201492537313
[2m[36m(func pid=150999)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=150999)[0m f1_macro: 0.2545843897140559
[2m[36m(func pid=150999)[0m f1_weighted: 0.3816502619404522
[2m[36m(func pid=150999)[0m f1_per_class: [0.475, 0.34, 0.126, 0.411, 0.095, 0.121, 0.553, 0.295, 0.0, 0.13]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.1903 | Steps: 4 | Val loss: 2.1261 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=145792)[0m top1: 0.34375
[2m[36m(func pid=145792)[0m top5: 0.8722014925373134
[2m[36m(func pid=145792)[0m f1_micro: 0.34375
[2m[36m(func pid=145792)[0m f1_macro: 0.30899215575328487
[2m[36m(func pid=145792)[0m f1_weighted: 0.3742529103764242
[2m[36m(func pid=145792)[0m f1_per_class: [0.504, 0.385, 0.367, 0.477, 0.092, 0.12, 0.399, 0.339, 0.22, 0.187]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.2439365671641791
[2m[36m(func pid=152562)[0m top5: 0.7168843283582089
[2m[36m(func pid=152562)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=152562)[0m f1_macro: 0.1838267213162303
[2m[36m(func pid=152562)[0m f1_weighted: 0.20526795015464855
[2m[36m(func pid=152562)[0m f1_per_class: [0.0, 0.388, 0.236, 0.07, 0.187, 0.281, 0.251, 0.0, 0.198, 0.226]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m top1: 0.21548507462686567
[2m[36m(func pid=140546)[0m top5: 0.730410447761194
[2m[36m(func pid=140546)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=140546)[0m f1_macro: 0.17718606927162803
[2m[36m(func pid=140546)[0m f1_weighted: 0.23727710492995427
[2m[36m(func pid=140546)[0m f1_per_class: [0.231, 0.314, 0.157, 0.269, 0.039, 0.121, 0.239, 0.227, 0.089, 0.086]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8500 | Steps: 4 | Val loss: 2.7235 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8994 | Steps: 4 | Val loss: 1.7713 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 15.4146 | Steps: 4 | Val loss: 22.3536 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:34:55 (running for 00:27:12.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.19  |      0.177 |                   59 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.804 |      0.309 |                   33 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.85  |      0.289 |                   15 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 17.43  |      0.184 |                    8 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3101679104477612
[2m[36m(func pid=150999)[0m top5: 0.8115671641791045
[2m[36m(func pid=150999)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=150999)[0m f1_macro: 0.28895095609055454
[2m[36m(func pid=150999)[0m f1_weighted: 0.29972286602881404
[2m[36m(func pid=150999)[0m f1_per_class: [0.53, 0.545, 0.141, 0.18, 0.075, 0.165, 0.302, 0.36, 0.311, 0.28]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2397 | Steps: 4 | Val loss: 2.1192 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=145792)[0m top1: 0.363339552238806
[2m[36m(func pid=145792)[0m top5: 0.8698694029850746
[2m[36m(func pid=145792)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=145792)[0m f1_macro: 0.31439856965252294
[2m[36m(func pid=145792)[0m f1_weighted: 0.3965829141416351
[2m[36m(func pid=145792)[0m f1_per_class: [0.533, 0.353, 0.355, 0.457, 0.098, 0.142, 0.507, 0.323, 0.184, 0.19]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.26072761194029853
[2m[36m(func pid=152562)[0m top5: 0.8278917910447762
[2m[36m(func pid=152562)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=152562)[0m f1_macro: 0.2873227315207573
[2m[36m(func pid=152562)[0m f1_weighted: 0.223094960991292
[2m[36m(func pid=152562)[0m f1_per_class: [0.646, 0.341, 0.786, 0.181, 0.109, 0.077, 0.248, 0.0, 0.306, 0.179]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m top1: 0.21875
[2m[36m(func pid=140546)[0m top5: 0.7322761194029851
[2m[36m(func pid=140546)[0m f1_micro: 0.21875
[2m[36m(func pid=140546)[0m f1_macro: 0.18126942726552833
[2m[36m(func pid=140546)[0m f1_weighted: 0.2355230807555036
[2m[36m(func pid=140546)[0m f1_per_class: [0.254, 0.343, 0.161, 0.289, 0.04, 0.123, 0.197, 0.208, 0.103, 0.095]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7986 | Steps: 4 | Val loss: 2.9392 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7660 | Steps: 4 | Val loss: 1.7553 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 14.1084 | Steps: 4 | Val loss: 28.4814 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:35:00 (running for 00:27:18.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.24  |      0.181 |                   60 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.899 |      0.314 |                   34 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.799 |      0.285 |                   16 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 15.415 |      0.287 |                    9 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2635261194029851
[2m[36m(func pid=150999)[0m top5: 0.7840485074626866
[2m[36m(func pid=150999)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=150999)[0m f1_macro: 0.2848809053271876
[2m[36m(func pid=150999)[0m f1_weighted: 0.2627279355729041
[2m[36m(func pid=150999)[0m f1_per_class: [0.529, 0.469, 0.333, 0.299, 0.107, 0.151, 0.127, 0.379, 0.128, 0.327]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1837 | Steps: 4 | Val loss: 2.1033 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=145792)[0m top1: 0.36800373134328357
[2m[36m(func pid=145792)[0m top5: 0.8745335820895522
[2m[36m(func pid=145792)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=145792)[0m f1_macro: 0.31927190994409227
[2m[36m(func pid=145792)[0m f1_weighted: 0.3915733286352635
[2m[36m(func pid=145792)[0m f1_per_class: [0.547, 0.486, 0.273, 0.399, 0.1, 0.171, 0.457, 0.307, 0.203, 0.25]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.25699626865671643
[2m[36m(func pid=152562)[0m top5: 0.78125
[2m[36m(func pid=152562)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=152562)[0m f1_macro: 0.23242518123964412
[2m[36m(func pid=152562)[0m f1_weighted: 0.20454132303931494
[2m[36m(func pid=152562)[0m f1_per_class: [0.252, 0.031, 0.632, 0.539, 0.41, 0.098, 0.03, 0.221, 0.11, 0.0]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4074 | Steps: 4 | Val loss: 3.5156 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=140546)[0m top1: 0.22154850746268656
[2m[36m(func pid=140546)[0m top5: 0.746268656716418
[2m[36m(func pid=140546)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=140546)[0m f1_macro: 0.18675472119124997
[2m[36m(func pid=140546)[0m f1_weighted: 0.24325714136741336
[2m[36m(func pid=140546)[0m f1_per_class: [0.281, 0.332, 0.164, 0.298, 0.032, 0.124, 0.215, 0.233, 0.095, 0.093]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9128 | Steps: 4 | Val loss: 1.7018 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 8.3059 | Steps: 4 | Val loss: 14.1500 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:35:05 (running for 00:27:23.25)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.184 |      0.187 |                   61 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.766 |      0.319 |                   35 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.407 |      0.24  |                   17 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 14.108 |      0.232 |                   10 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.22154850746268656
[2m[36m(func pid=150999)[0m top5: 0.8166977611940298
[2m[36m(func pid=150999)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=150999)[0m f1_macro: 0.24001305883304283
[2m[36m(func pid=150999)[0m f1_weighted: 0.24807985273782138
[2m[36m(func pid=150999)[0m f1_per_class: [0.328, 0.215, 0.022, 0.506, 0.182, 0.127, 0.048, 0.393, 0.165, 0.415]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.1414 | Steps: 4 | Val loss: 2.1053 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=145792)[0m top1: 0.38572761194029853
[2m[36m(func pid=145792)[0m top5: 0.8889925373134329
[2m[36m(func pid=145792)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=145792)[0m f1_macro: 0.3397345395414674
[2m[36m(func pid=145792)[0m f1_weighted: 0.394984104077678
[2m[36m(func pid=145792)[0m f1_per_class: [0.558, 0.566, 0.286, 0.405, 0.106, 0.161, 0.412, 0.307, 0.222, 0.375]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3605410447761194
[2m[36m(func pid=152562)[0m top5: 0.9169776119402985
[2m[36m(func pid=152562)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=152562)[0m f1_macro: 0.33735841996686594
[2m[36m(func pid=152562)[0m f1_weighted: 0.3627981654125904
[2m[36m(func pid=152562)[0m f1_per_class: [0.535, 0.445, 0.234, 0.46, 0.222, 0.313, 0.254, 0.406, 0.134, 0.368]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.3890 | Steps: 4 | Val loss: 2.2903 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=140546)[0m top1: 0.22014925373134328
[2m[36m(func pid=140546)[0m top5: 0.7486007462686567
[2m[36m(func pid=140546)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=140546)[0m f1_macro: 0.18651944401449197
[2m[36m(func pid=140546)[0m f1_weighted: 0.2391576558451106
[2m[36m(func pid=140546)[0m f1_per_class: [0.276, 0.344, 0.19, 0.295, 0.032, 0.123, 0.201, 0.215, 0.094, 0.094]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7779 | Steps: 4 | Val loss: 1.5765 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 11:35:10 (running for 00:27:28.42)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.141 |      0.187 |                   62 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.913 |      0.34  |                   36 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.389 |      0.336 |                   18 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  8.306 |      0.337 |                   11 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.38992537313432835
[2m[36m(func pid=150999)[0m top5: 0.867070895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=150999)[0m f1_macro: 0.3363327803047219
[2m[36m(func pid=150999)[0m f1_weighted: 0.37087792804397474
[2m[36m(func pid=150999)[0m f1_per_class: [0.562, 0.403, 0.464, 0.57, 0.171, 0.19, 0.253, 0.359, 0.213, 0.179]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 7.5891 | Steps: 4 | Val loss: 20.7485 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1509 | Steps: 4 | Val loss: 2.1050 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=145792)[0m top1: 0.4193097014925373
[2m[36m(func pid=145792)[0m top5: 0.9263059701492538
[2m[36m(func pid=145792)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=145792)[0m f1_macro: 0.3596090871238945
[2m[36m(func pid=145792)[0m f1_weighted: 0.4313724090064977
[2m[36m(func pid=145792)[0m f1_per_class: [0.586, 0.563, 0.28, 0.521, 0.153, 0.23, 0.401, 0.323, 0.156, 0.384]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3003731343283582
[2m[36m(func pid=152562)[0m top5: 0.8246268656716418
[2m[36m(func pid=152562)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=152562)[0m f1_macro: 0.2697789210202831
[2m[36m(func pid=152562)[0m f1_weighted: 0.3144864189121425
[2m[36m(func pid=152562)[0m f1_per_class: [0.606, 0.309, 0.152, 0.179, 0.224, 0.305, 0.467, 0.365, 0.0, 0.089]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.1777 | Steps: 4 | Val loss: 3.0901 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=140546)[0m top1: 0.22154850746268656
[2m[36m(func pid=140546)[0m top5: 0.7541977611940298
[2m[36m(func pid=140546)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=140546)[0m f1_macro: 0.19330766672634808
[2m[36m(func pid=140546)[0m f1_weighted: 0.24066517029084045
[2m[36m(func pid=140546)[0m f1_per_class: [0.318, 0.346, 0.212, 0.301, 0.03, 0.137, 0.19, 0.221, 0.086, 0.091]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=150999)[0m top1: 0.33255597014925375
[2m[36m(func pid=150999)[0m top5: 0.8311567164179104
[2m[36m(func pid=150999)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=150999)[0m f1_macro: 0.27096488613485253
[2m[36m(func pid=150999)[0m f1_weighted: 0.3086059800047096
[2m[36m(func pid=150999)[0m f1_per_class: [0.652, 0.444, 0.0, 0.103, 0.074, 0.087, 0.513, 0.289, 0.167, 0.381]
[2m[36m(func pid=150999)[0m 
== Status ==
Current time: 2024-01-07 11:35:15 (running for 00:27:33.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.151 |      0.193 |                   63 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.778 |      0.36  |                   37 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.178 |      0.271 |                   19 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  7.589 |      0.27  |                   12 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7609 | Steps: 4 | Val loss: 1.5428 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 16.2227 | Steps: 4 | Val loss: 26.5309 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1221 | Steps: 4 | Val loss: 2.1321 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8038 | Steps: 4 | Val loss: 2.8066 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=145792)[0m top1: 0.43236940298507465
[2m[36m(func pid=145792)[0m top5: 0.9356343283582089
[2m[36m(func pid=145792)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=145792)[0m f1_macro: 0.3694837073600796
[2m[36m(func pid=145792)[0m f1_weighted: 0.42247238315341357
[2m[36m(func pid=145792)[0m f1_per_class: [0.592, 0.544, 0.321, 0.592, 0.171, 0.255, 0.298, 0.341, 0.168, 0.412]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.34048507462686567
[2m[36m(func pid=152562)[0m top5: 0.8801305970149254
[2m[36m(func pid=152562)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=152562)[0m f1_macro: 0.288189924673856
[2m[36m(func pid=152562)[0m f1_weighted: 0.23043809622806144
[2m[36m(func pid=152562)[0m f1_per_class: [0.5, 0.021, 0.733, 0.117, 0.256, 0.099, 0.48, 0.332, 0.0, 0.344]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=140546)[0m top1: 0.20615671641791045
[2m[36m(func pid=140546)[0m top5: 0.7327425373134329
[2m[36m(func pid=140546)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=140546)[0m f1_macro: 0.1887875540155886
[2m[36m(func pid=140546)[0m f1_weighted: 0.22271709096695624
[2m[36m(func pid=140546)[0m f1_per_class: [0.318, 0.349, 0.256, 0.268, 0.035, 0.131, 0.165, 0.199, 0.09, 0.078]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:35:21 (running for 00:27:38.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.122 |      0.189 |                   64 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.761 |      0.369 |                   38 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.804 |      0.307 |                   20 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 16.223 |      0.288 |                   13 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3628731343283582
[2m[36m(func pid=150999)[0m top5: 0.804570895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=150999)[0m f1_macro: 0.30743308234962535
[2m[36m(func pid=150999)[0m f1_weighted: 0.32978064298500637
[2m[36m(func pid=150999)[0m f1_per_class: [0.485, 0.494, 0.267, 0.079, 0.212, 0.211, 0.534, 0.265, 0.214, 0.312]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8721 | Steps: 4 | Val loss: 1.5454 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 8.6549 | Steps: 4 | Val loss: 35.0458 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0838 | Steps: 4 | Val loss: 2.1340 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.9372 | Steps: 4 | Val loss: 2.7876 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=152562)[0m top1: 0.18470149253731344
[2m[36m(func pid=152562)[0m top5: 0.8484141791044776
[2m[36m(func pid=152562)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=152562)[0m f1_macro: 0.2822127812521561
[2m[36m(func pid=152562)[0m f1_weighted: 0.21157988868913225
[2m[36m(func pid=152562)[0m f1_per_class: [0.629, 0.069, 0.556, 0.331, 0.26, 0.0, 0.204, 0.376, 0.09, 0.308]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.4221082089552239
[2m[36m(func pid=145792)[0m top5: 0.9407649253731343
[2m[36m(func pid=145792)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=145792)[0m f1_macro: 0.3640496535585894
[2m[36m(func pid=145792)[0m f1_weighted: 0.415237783733945
[2m[36m(func pid=145792)[0m f1_per_class: [0.566, 0.514, 0.393, 0.607, 0.186, 0.268, 0.279, 0.34, 0.128, 0.358]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m top1: 0.20009328358208955
[2m[36m(func pid=140546)[0m top5: 0.7276119402985075
[2m[36m(func pid=140546)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=140546)[0m f1_macro: 0.1891694154394851
[2m[36m(func pid=140546)[0m f1_weighted: 0.21227455036467752
[2m[36m(func pid=140546)[0m f1_per_class: [0.307, 0.351, 0.275, 0.248, 0.042, 0.127, 0.146, 0.213, 0.09, 0.092]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:35:26 (running for 00:27:43.89)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.084 |      0.189 |                   65 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.872 |      0.364 |                   39 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.937 |      0.307 |                   21 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  8.655 |      0.282 |                   14 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2849813432835821
[2m[36m(func pid=150999)[0m top5: 0.8563432835820896
[2m[36m(func pid=150999)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=150999)[0m f1_macro: 0.3066695890116099
[2m[36m(func pid=150999)[0m f1_weighted: 0.2967823019069789
[2m[36m(func pid=150999)[0m f1_per_class: [0.583, 0.227, 0.6, 0.499, 0.143, 0.23, 0.164, 0.334, 0.183, 0.103]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.7739 | Steps: 4 | Val loss: 1.6728 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 9.5718 | Steps: 4 | Val loss: 39.8328 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.1015 | Steps: 4 | Val loss: 2.1389 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4612 | Steps: 4 | Val loss: 3.7545 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=145792)[0m top1: 0.37919776119402987
[2m[36m(func pid=145792)[0m top5: 0.9109141791044776
[2m[36m(func pid=145792)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=145792)[0m f1_macro: 0.33492501863151214
[2m[36m(func pid=145792)[0m f1_weighted: 0.3796989293394149
[2m[36m(func pid=145792)[0m f1_per_class: [0.574, 0.477, 0.28, 0.566, 0.169, 0.275, 0.216, 0.357, 0.159, 0.276]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m top1: 0.20055970149253732
[2m[36m(func pid=140546)[0m top5: 0.7229477611940298
[2m[36m(func pid=140546)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=140546)[0m f1_macro: 0.18985669979779965
[2m[36m(func pid=140546)[0m f1_weighted: 0.20753481084026146
[2m[36m(func pid=140546)[0m f1_per_class: [0.309, 0.362, 0.276, 0.23, 0.049, 0.128, 0.138, 0.22, 0.099, 0.088]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m top1: 0.26725746268656714
[2m[36m(func pid=152562)[0m top5: 0.6035447761194029
[2m[36m(func pid=152562)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=152562)[0m f1_macro: 0.2738145127832666
[2m[36m(func pid=152562)[0m f1_weighted: 0.21519539211301913
[2m[36m(func pid=152562)[0m f1_per_class: [0.223, 0.39, 0.786, 0.369, 0.088, 0.0, 0.0, 0.426, 0.246, 0.211]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:35:31 (running for 00:27:49.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.101 |      0.19  |                   66 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.774 |      0.335 |                   40 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.461 |      0.32  |                   22 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  9.572 |      0.274 |                   15 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3474813432835821
[2m[36m(func pid=150999)[0m top5: 0.820429104477612
[2m[36m(func pid=150999)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=150999)[0m f1_macro: 0.3195187097810585
[2m[36m(func pid=150999)[0m f1_weighted: 0.2641342246412013
[2m[36m(func pid=150999)[0m f1_per_class: [0.571, 0.0, 0.692, 0.563, 0.093, 0.161, 0.126, 0.402, 0.209, 0.378]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1272 | Steps: 4 | Val loss: 2.1142 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 14.6077 | Steps: 4 | Val loss: 50.2619 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8898 | Steps: 4 | Val loss: 1.7109 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8009 | Steps: 4 | Val loss: 3.2510 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=140546)[0m top1: 0.21641791044776118
[2m[36m(func pid=140546)[0m top5: 0.742070895522388
[2m[36m(func pid=140546)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=140546)[0m f1_macro: 0.19984970723736478
[2m[36m(func pid=140546)[0m f1_weighted: 0.22975072867219698
[2m[36m(func pid=140546)[0m f1_per_class: [0.302, 0.341, 0.286, 0.279, 0.038, 0.131, 0.172, 0.243, 0.107, 0.099]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m top1: 0.125
[2m[36m(func pid=152562)[0m top5: 0.46175373134328357
[2m[36m(func pid=152562)[0m f1_micro: 0.125
[2m[36m(func pid=152562)[0m f1_macro: 0.13584820286553895
[2m[36m(func pid=152562)[0m f1_weighted: 0.10233684294588925
[2m[36m(func pid=152562)[0m f1_per_class: [0.296, 0.338, 0.059, 0.02, 0.05, 0.008, 0.019, 0.342, 0.141, 0.086]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3726679104477612
[2m[36m(func pid=145792)[0m top5: 0.8987873134328358
[2m[36m(func pid=145792)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=145792)[0m f1_macro: 0.32135892330690147
[2m[36m(func pid=145792)[0m f1_weighted: 0.3682811095371548
[2m[36m(func pid=145792)[0m f1_per_class: [0.504, 0.408, 0.282, 0.586, 0.177, 0.266, 0.205, 0.353, 0.193, 0.238]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:35:37 (running for 00:27:54.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.127 |      0.2   |                   67 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.89  |      0.321 |                   41 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.801 |      0.281 |                   23 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 14.608 |      0.136 |                   16 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.27052238805970147
[2m[36m(func pid=150999)[0m top5: 0.8605410447761194
[2m[36m(func pid=150999)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=150999)[0m f1_macro: 0.2811928350965075
[2m[36m(func pid=150999)[0m f1_weighted: 0.3227806941531757
[2m[36m(func pid=150999)[0m f1_per_class: [0.475, 0.187, 0.375, 0.507, 0.041, 0.049, 0.348, 0.335, 0.125, 0.372]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0395 | Steps: 4 | Val loss: 2.1161 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 15.5304 | Steps: 4 | Val loss: 31.6720 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6427 | Steps: 4 | Val loss: 1.8199 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.5307 | Steps: 4 | Val loss: 3.5427 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=140546)[0m top1: 0.21875
[2m[36m(func pid=140546)[0m top5: 0.7388059701492538
[2m[36m(func pid=140546)[0m f1_micro: 0.21875
[2m[36m(func pid=140546)[0m f1_macro: 0.19621273894919683
[2m[36m(func pid=140546)[0m f1_weighted: 0.2290801961491517
[2m[36m(func pid=140546)[0m f1_per_class: [0.305, 0.354, 0.259, 0.268, 0.022, 0.131, 0.174, 0.24, 0.106, 0.103]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m top1: 0.27845149253731344
[2m[36m(func pid=152562)[0m top5: 0.7625932835820896
[2m[36m(func pid=152562)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=152562)[0m f1_macro: 0.23201529773426052
[2m[36m(func pid=152562)[0m f1_weighted: 0.2591473074530632
[2m[36m(func pid=152562)[0m f1_per_class: [0.467, 0.337, 0.149, 0.023, 0.126, 0.27, 0.484, 0.0, 0.135, 0.329]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.33255597014925375
[2m[36m(func pid=145792)[0m top5: 0.8572761194029851
[2m[36m(func pid=145792)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=145792)[0m f1_macro: 0.31194870615518183
[2m[36m(func pid=145792)[0m f1_weighted: 0.33254382548661926
[2m[36m(func pid=145792)[0m f1_per_class: [0.508, 0.365, 0.393, 0.537, 0.18, 0.229, 0.172, 0.348, 0.174, 0.213]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:35:42 (running for 00:27:59.82)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.04  |      0.196 |                   68 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.643 |      0.312 |                   42 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.531 |      0.256 |                   24 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 15.53  |      0.232 |                   17 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.24673507462686567
[2m[36m(func pid=150999)[0m top5: 0.7672574626865671
[2m[36m(func pid=150999)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=150999)[0m f1_macro: 0.2562594519318967
[2m[36m(func pid=150999)[0m f1_weighted: 0.23977347310943642
[2m[36m(func pid=150999)[0m f1_per_class: [0.386, 0.468, 0.255, 0.157, 0.068, 0.124, 0.216, 0.276, 0.18, 0.433]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.9665 | Steps: 4 | Val loss: 2.1163 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 17.5929 | Steps: 4 | Val loss: 25.6739 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7533 | Steps: 4 | Val loss: 1.8577 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7908 | Steps: 4 | Val loss: 2.2187 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=140546)[0m top1: 0.21875
[2m[36m(func pid=140546)[0m top5: 0.738339552238806
[2m[36m(func pid=140546)[0m f1_micro: 0.21875
[2m[36m(func pid=140546)[0m f1_macro: 0.19286091045193923
[2m[36m(func pid=140546)[0m f1_weighted: 0.2280381665563697
[2m[36m(func pid=140546)[0m f1_per_class: [0.295, 0.354, 0.222, 0.263, 0.031, 0.122, 0.177, 0.258, 0.1, 0.107]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=152562)[0m top1: 0.37779850746268656
[2m[36m(func pid=152562)[0m top5: 0.9524253731343284
[2m[36m(func pid=152562)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=152562)[0m f1_macro: 0.26877274315319044
[2m[36m(func pid=152562)[0m f1_weighted: 0.33164205001634645
[2m[36m(func pid=152562)[0m f1_per_class: [0.467, 0.09, 0.741, 0.333, 0.0, 0.351, 0.55, 0.0, 0.157, 0.0]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:35:47 (running for 00:28:04.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.967 |      0.193 |                   69 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.753 |      0.326 |                   43 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.531 |      0.256 |                   24 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 17.593 |      0.269 |                   18 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.3204291044776119
[2m[36m(func pid=145792)[0m top5: 0.8521455223880597
[2m[36m(func pid=145792)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=145792)[0m f1_macro: 0.32579567208729177
[2m[36m(func pid=145792)[0m f1_weighted: 0.32035253436483546
[2m[36m(func pid=145792)[0m f1_per_class: [0.526, 0.342, 0.489, 0.523, 0.213, 0.214, 0.156, 0.364, 0.153, 0.278]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.40718283582089554
[2m[36m(func pid=150999)[0m top5: 0.9039179104477612
[2m[36m(func pid=150999)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=150999)[0m f1_macro: 0.3752015411256969
[2m[36m(func pid=150999)[0m f1_weighted: 0.41597649334211173
[2m[36m(func pid=150999)[0m f1_per_class: [0.515, 0.54, 0.444, 0.415, 0.111, 0.34, 0.416, 0.308, 0.221, 0.441]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.0995 | Steps: 4 | Val loss: 2.1391 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 18.6169 | Steps: 4 | Val loss: 30.9399 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6099 | Steps: 4 | Val loss: 1.9032 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.6642 | Steps: 4 | Val loss: 2.1515 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.21082089552238806
[2m[36m(func pid=140546)[0m top5: 0.7229477611940298
[2m[36m(func pid=140546)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=140546)[0m f1_macro: 0.1841874513835015
[2m[36m(func pid=140546)[0m f1_weighted: 0.21815707983671193
[2m[36m(func pid=140546)[0m f1_per_class: [0.249, 0.36, 0.202, 0.241, 0.037, 0.118, 0.165, 0.261, 0.107, 0.103]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:35:52 (running for 00:28:09.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.1   |      0.184 |                   70 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.753 |      0.326 |                   43 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.791 |      0.375 |                   25 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 18.617 |      0.316 |                   19 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m top1: 0.35447761194029853
[2m[36m(func pid=152562)[0m top5: 0.8736007462686567
[2m[36m(func pid=152562)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=152562)[0m f1_macro: 0.3157818036528318
[2m[36m(func pid=152562)[0m f1_weighted: 0.27092008976244486
[2m[36m(func pid=152562)[0m f1_per_class: [0.588, 0.191, 0.696, 0.551, 0.0, 0.235, 0.037, 0.354, 0.183, 0.323]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.2947761194029851
[2m[36m(func pid=145792)[0m top5: 0.8493470149253731
[2m[36m(func pid=145792)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=145792)[0m f1_macro: 0.3299601675101861
[2m[36m(func pid=145792)[0m f1_weighted: 0.30496134214093107
[2m[36m(func pid=145792)[0m f1_per_class: [0.561, 0.381, 0.595, 0.478, 0.18, 0.144, 0.151, 0.34, 0.136, 0.333]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.435634328358209
[2m[36m(func pid=150999)[0m top5: 0.9538246268656716
[2m[36m(func pid=150999)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=150999)[0m f1_macro: 0.42639455476588406
[2m[36m(func pid=150999)[0m f1_weighted: 0.4426213461079919
[2m[36m(func pid=150999)[0m f1_per_class: [0.684, 0.418, 0.71, 0.573, 0.261, 0.229, 0.456, 0.306, 0.154, 0.473]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0854 | Steps: 4 | Val loss: 2.1298 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 6.3703 | Steps: 4 | Val loss: 38.4720 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.4528 | Steps: 4 | Val loss: 2.1565 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8546 | Steps: 4 | Val loss: 1.8227 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=140546)[0m top1: 0.20475746268656717
[2m[36m(func pid=140546)[0m top5: 0.7285447761194029
[2m[36m(func pid=140546)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=140546)[0m f1_macro: 0.18136259972726154
[2m[36m(func pid=140546)[0m f1_weighted: 0.21106739747478873
[2m[36m(func pid=140546)[0m f1_per_class: [0.255, 0.342, 0.202, 0.239, 0.039, 0.121, 0.155, 0.247, 0.105, 0.11]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:35:57 (running for 00:28:15.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.085 |      0.181 |                   71 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.61  |      0.33  |                   44 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.664 |      0.426 |                   26 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.37  |      0.325 |                   20 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m top1: 0.3041044776119403
[2m[36m(func pid=152562)[0m top5: 0.6828358208955224
[2m[36m(func pid=152562)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=152562)[0m f1_macro: 0.32516497031271396
[2m[36m(func pid=152562)[0m f1_weighted: 0.2588687317082692
[2m[36m(func pid=152562)[0m f1_per_class: [0.6, 0.454, 0.8, 0.453, 0.163, 0.052, 0.009, 0.275, 0.284, 0.161]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m top1: 0.44776119402985076
[2m[36m(func pid=150999)[0m top5: 0.9375
[2m[36m(func pid=150999)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=150999)[0m f1_macro: 0.41451191393006714
[2m[36m(func pid=150999)[0m f1_weighted: 0.4519173272133561
[2m[36m(func pid=150999)[0m f1_per_class: [0.693, 0.451, 0.686, 0.589, 0.215, 0.303, 0.425, 0.33, 0.205, 0.248]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.34095149253731344
[2m[36m(func pid=145792)[0m top5: 0.8512126865671642
[2m[36m(func pid=145792)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=145792)[0m f1_macro: 0.33126866084165896
[2m[36m(func pid=145792)[0m f1_weighted: 0.3338332084816553
[2m[36m(func pid=145792)[0m f1_per_class: [0.545, 0.497, 0.478, 0.472, 0.116, 0.157, 0.179, 0.345, 0.206, 0.317]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0937 | Steps: 4 | Val loss: 2.1178 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 12.1533 | Steps: 4 | Val loss: 49.0605 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.0929 | Steps: 4 | Val loss: 3.9378 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9159 | Steps: 4 | Val loss: 1.8359 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=140546)[0m top1: 0.21595149253731344
[2m[36m(func pid=140546)[0m top5: 0.7350746268656716
[2m[36m(func pid=140546)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=140546)[0m f1_macro: 0.18613618956744235
[2m[36m(func pid=140546)[0m f1_weighted: 0.222014087529573
[2m[36m(func pid=140546)[0m f1_per_class: [0.249, 0.357, 0.19, 0.254, 0.044, 0.121, 0.169, 0.246, 0.109, 0.125]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:36:03 (running for 00:28:20.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.094 |      0.186 |                   72 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.855 |      0.331 |                   45 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.453 |      0.415 |                   27 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 12.153 |      0.247 |                   21 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m top1: 0.20149253731343283
[2m[36m(func pid=152562)[0m top5: 0.6660447761194029
[2m[36m(func pid=152562)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=152562)[0m f1_macro: 0.246993821286808
[2m[36m(func pid=152562)[0m f1_weighted: 0.18917809369469238
[2m[36m(func pid=152562)[0m f1_per_class: [0.598, 0.356, 0.545, 0.265, 0.044, 0.015, 0.025, 0.419, 0.107, 0.096]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m top1: 0.19169776119402984
[2m[36m(func pid=150999)[0m top5: 0.8334888059701493
[2m[36m(func pid=150999)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=150999)[0m f1_macro: 0.2682648537533533
[2m[36m(func pid=150999)[0m f1_weighted: 0.24890509187521861
[2m[36m(func pid=150999)[0m f1_per_class: [0.5, 0.223, 0.533, 0.324, 0.113, 0.13, 0.204, 0.383, 0.23, 0.043]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3498134328358209
[2m[36m(func pid=145792)[0m top5: 0.8484141791044776
[2m[36m(func pid=145792)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=145792)[0m f1_macro: 0.3203895782956391
[2m[36m(func pid=145792)[0m f1_weighted: 0.34770850205627374
[2m[36m(func pid=145792)[0m f1_per_class: [0.429, 0.538, 0.453, 0.454, 0.075, 0.137, 0.229, 0.361, 0.256, 0.273]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.0035 | Steps: 4 | Val loss: 2.1269 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 10.5567 | Steps: 4 | Val loss: 30.5613 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3987 | Steps: 4 | Val loss: 3.9314 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5684 | Steps: 4 | Val loss: 1.7916 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=140546)[0m top1: 0.20942164179104478
[2m[36m(func pid=140546)[0m top5: 0.7248134328358209
[2m[36m(func pid=140546)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=140546)[0m f1_macro: 0.18633837824341584
[2m[36m(func pid=140546)[0m f1_weighted: 0.2185426614869202
[2m[36m(func pid=140546)[0m f1_per_class: [0.245, 0.348, 0.222, 0.255, 0.059, 0.122, 0.163, 0.241, 0.094, 0.116]
[2m[36m(func pid=140546)[0m 
== Status ==
Current time: 2024-01-07 11:36:08 (running for 00:28:26.28)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.003 |      0.186 |                   73 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.916 |      0.32  |                   46 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.093 |      0.268 |                   28 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 10.557 |      0.337 |                   22 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m top1: 0.31949626865671643
[2m[36m(func pid=152562)[0m top5: 0.777518656716418
[2m[36m(func pid=152562)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=152562)[0m f1_macro: 0.33715168606815954
[2m[36m(func pid=152562)[0m f1_weighted: 0.2952338646373302
[2m[36m(func pid=152562)[0m f1_per_class: [0.565, 0.431, 0.688, 0.352, 0.18, 0.26, 0.166, 0.274, 0.263, 0.194]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m top1: 0.22154850746268656
[2m[36m(func pid=150999)[0m top5: 0.7663246268656716
[2m[36m(func pid=150999)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=150999)[0m f1_macro: 0.26710083702383636
[2m[36m(func pid=150999)[0m f1_weighted: 0.2586349548487904
[2m[36m(func pid=150999)[0m f1_per_class: [0.571, 0.393, 0.4, 0.283, 0.056, 0.1, 0.186, 0.384, 0.227, 0.071]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3414179104477612
[2m[36m(func pid=145792)[0m top5: 0.8773320895522388
[2m[36m(func pid=145792)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=145792)[0m f1_macro: 0.3224345460642187
[2m[36m(func pid=145792)[0m f1_weighted: 0.35337352075164186
[2m[36m(func pid=145792)[0m f1_per_class: [0.384, 0.516, 0.462, 0.467, 0.09, 0.204, 0.23, 0.338, 0.248, 0.286]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1013 | Steps: 4 | Val loss: 2.1132 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.8388 | Steps: 4 | Val loss: 26.9593 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.1177 | Steps: 4 | Val loss: 2.7936 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=140546)[0m top1: 0.22154850746268656
[2m[36m(func pid=140546)[0m top5: 0.7360074626865671
[2m[36m(func pid=140546)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=140546)[0m f1_macro: 0.19200146165542947
[2m[36m(func pid=140546)[0m f1_weighted: 0.2271890527687089
[2m[36m(func pid=140546)[0m f1_per_class: [0.269, 0.373, 0.218, 0.279, 0.054, 0.11, 0.156, 0.243, 0.099, 0.119]
[2m[36m(func pid=140546)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.9540 | Steps: 4 | Val loss: 1.7835 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:36:14 (running for 00:28:31.74)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.3555
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00012 | RUNNING    | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  2.101 |      0.192 |                   74 |
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.568 |      0.322 |                   47 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.399 |      0.267 |                   29 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.839 |      0.279 |                   23 |
| train_98a10_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3628731343283582
[2m[36m(func pid=150999)[0m top5: 0.8544776119402985
[2m[36m(func pid=150999)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=150999)[0m f1_macro: 0.35448133106078145
[2m[36m(func pid=150999)[0m f1_weighted: 0.3702024867814004
[2m[36m(func pid=150999)[0m f1_per_class: [0.55, 0.547, 0.632, 0.483, 0.069, 0.105, 0.274, 0.378, 0.2, 0.308]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3694029850746269
[2m[36m(func pid=152562)[0m top5: 0.8432835820895522
[2m[36m(func pid=152562)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=152562)[0m f1_macro: 0.27883312191753296
[2m[36m(func pid=152562)[0m f1_weighted: 0.36798540603499164
[2m[36m(func pid=152562)[0m f1_per_class: [0.49, 0.032, 0.088, 0.568, 0.312, 0.375, 0.466, 0.031, 0.126, 0.301]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.34281716417910446
[2m[36m(func pid=145792)[0m top5: 0.8847947761194029
[2m[36m(func pid=145792)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=145792)[0m f1_macro: 0.31277510216992954
[2m[36m(func pid=145792)[0m f1_weighted: 0.3455414803349464
[2m[36m(func pid=145792)[0m f1_per_class: [0.418, 0.5, 0.369, 0.493, 0.109, 0.246, 0.179, 0.347, 0.154, 0.312]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=140546)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9357 | Steps: 4 | Val loss: 2.1173 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1293 | Steps: 4 | Val loss: 2.4826 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.2621 | Steps: 4 | Val loss: 34.0586 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=140546)[0m top1: 0.20475746268656717
[2m[36m(func pid=140546)[0m top5: 0.7336753731343284
[2m[36m(func pid=140546)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=140546)[0m f1_macro: 0.18473334863176577
[2m[36m(func pid=140546)[0m f1_weighted: 0.21355044240774315
[2m[36m(func pid=140546)[0m f1_per_class: [0.265, 0.328, 0.247, 0.275, 0.053, 0.104, 0.146, 0.232, 0.095, 0.103]
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7008 | Steps: 4 | Val loss: 1.7346 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=150999)[0m top1: 0.40578358208955223
[2m[36m(func pid=150999)[0m top5: 0.9043843283582089
[2m[36m(func pid=150999)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=150999)[0m f1_macro: 0.39673205994338434
[2m[36m(func pid=150999)[0m f1_weighted: 0.40025341730557995
[2m[36m(func pid=150999)[0m f1_per_class: [0.524, 0.557, 0.8, 0.55, 0.105, 0.138, 0.29, 0.35, 0.221, 0.432]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.37453358208955223
[2m[36m(func pid=152562)[0m top5: 0.8390858208955224
[2m[36m(func pid=152562)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=152562)[0m f1_macro: 0.2588280268338533
[2m[36m(func pid=152562)[0m f1_weighted: 0.32398388870601
[2m[36m(func pid=152562)[0m f1_per_class: [0.5, 0.011, 0.098, 0.546, 0.229, 0.276, 0.377, 0.141, 0.028, 0.383]
[2m[36m(func pid=145792)[0m top1: 0.35634328358208955
[2m[36m(func pid=145792)[0m top5: 0.8922574626865671
[2m[36m(func pid=145792)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=145792)[0m f1_macro: 0.336849888835954
[2m[36m(func pid=145792)[0m f1_weighted: 0.3621285917162744
[2m[36m(func pid=145792)[0m f1_per_class: [0.489, 0.418, 0.373, 0.533, 0.167, 0.271, 0.22, 0.363, 0.193, 0.342]
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7086 | Steps: 4 | Val loss: 2.4036 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=150999)[0m top1: 0.41277985074626866
[2m[36m(func pid=150999)[0m top5: 0.9235074626865671
[2m[36m(func pid=150999)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=150999)[0m f1_macro: 0.40207333016863717
[2m[36m(func pid=150999)[0m f1_weighted: 0.4119661929146395
[2m[36m(func pid=150999)[0m f1_per_class: [0.521, 0.387, 0.8, 0.588, 0.12, 0.145, 0.389, 0.347, 0.188, 0.537]
== Status ==
Current time: 2024-01-07 11:36:19 (running for 00:28:37.19)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.954 |      0.313 |                   48 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.129 |      0.397 |                   31 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.839 |      0.279 |                   23 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 11:36:27 (running for 00:28:44.51)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.701 |      0.337 |                   49 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.129 |      0.397 |                   31 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.839 |      0.279 |                   23 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=158650)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=158650)[0m Configuration completed!
[2m[36m(func pid=158650)[0m New optimizer parameters:
[2m[36m(func pid=158650)[0m SGD (
[2m[36m(func pid=158650)[0m Parameter Group 0
[2m[36m(func pid=158650)[0m     dampening: 0
[2m[36m(func pid=158650)[0m     differentiable: False
[2m[36m(func pid=158650)[0m     foreach: None
[2m[36m(func pid=158650)[0m     lr: 0.0001
[2m[36m(func pid=158650)[0m     maximize: False
[2m[36m(func pid=158650)[0m     momentum: 0.99
[2m[36m(func pid=158650)[0m     nesterov: False
[2m[36m(func pid=158650)[0m     weight_decay: 1e-05
[2m[36m(func pid=158650)[0m )
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6755 | Steps: 4 | Val loss: 1.6650 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2707 | Steps: 4 | Val loss: 3.0381 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 7.8312 | Steps: 4 | Val loss: 20.9791 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0631 | Steps: 4 | Val loss: 2.5535 | Batch size: 32 | lr: 0.0001 | Duration: 4.68s
== Status ==
Current time: 2024-01-07 11:36:32 (running for 00:28:49.52)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.701 |      0.337 |                   49 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.709 |      0.402 |                   32 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.262 |      0.259 |                   24 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.34095149253731344
[2m[36m(func pid=150999)[0m top5: 0.8549440298507462
[2m[36m(func pid=150999)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=150999)[0m f1_macro: 0.33463093954619383
[2m[36m(func pid=150999)[0m f1_weighted: 0.3320580465682168
[2m[36m(func pid=150999)[0m f1_per_class: [0.561, 0.205, 0.786, 0.371, 0.104, 0.074, 0.484, 0.225, 0.187, 0.348]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.37593283582089554
[2m[36m(func pid=145792)[0m top5: 0.9160447761194029
[2m[36m(func pid=145792)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=145792)[0m f1_macro: 0.36356548342413947
[2m[36m(func pid=145792)[0m f1_weighted: 0.38113672348597405
[2m[36m(func pid=145792)[0m f1_per_class: [0.571, 0.365, 0.458, 0.577, 0.214, 0.282, 0.264, 0.354, 0.167, 0.384]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.4239738805970149
[2m[36m(func pid=152562)[0m top5: 0.9076492537313433
[2m[36m(func pid=152562)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=152562)[0m f1_macro: 0.33734726727399067
[2m[36m(func pid=152562)[0m f1_weighted: 0.4158624725709531
[2m[36m(func pid=152562)[0m f1_per_class: [0.5, 0.309, 0.361, 0.598, 0.118, 0.289, 0.41, 0.344, 0.111, 0.333]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.06203358208955224
[2m[36m(func pid=158650)[0m top5: 0.47761194029850745
[2m[36m(func pid=158650)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=158650)[0m f1_macro: 0.03533441048716422
[2m[36m(func pid=158650)[0m f1_weighted: 0.03471037938029689
[2m[36m(func pid=158650)[0m f1_per_class: [0.082, 0.015, 0.0, 0.076, 0.0, 0.019, 0.0, 0.099, 0.024, 0.038]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5996 | Steps: 4 | Val loss: 3.3334 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.9715 | Steps: 4 | Val loss: 27.7710 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.8676 | Steps: 4 | Val loss: 1.6679 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0614 | Steps: 4 | Val loss: 2.5813 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:36:37 (running for 00:28:55.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.676 |      0.364 |                   50 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.6   |      0.281 |                   34 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  7.831 |      0.337 |                   25 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  3.063 |      0.035 |                    1 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.26119402985074625
[2m[36m(func pid=150999)[0m top5: 0.816231343283582
[2m[36m(func pid=150999)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=150999)[0m f1_macro: 0.28053219923438366
[2m[36m(func pid=150999)[0m f1_weighted: 0.28612899158707217
[2m[36m(func pid=150999)[0m f1_per_class: [0.471, 0.249, 0.533, 0.341, 0.141, 0.156, 0.31, 0.255, 0.245, 0.105]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.30830223880597013
[2m[36m(func pid=152562)[0m top5: 0.8614738805970149
[2m[36m(func pid=152562)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=152562)[0m f1_macro: 0.29075224704287495
[2m[36m(func pid=152562)[0m f1_weighted: 0.3056012705684128
[2m[36m(func pid=152562)[0m f1_per_class: [0.586, 0.47, 0.143, 0.317, 0.112, 0.204, 0.24, 0.298, 0.204, 0.333]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3871268656716418
[2m[36m(func pid=145792)[0m top5: 0.9090485074626866
[2m[36m(func pid=145792)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=145792)[0m f1_macro: 0.3502651243522879
[2m[36m(func pid=145792)[0m f1_weighted: 0.38333078050185887
[2m[36m(func pid=145792)[0m f1_per_class: [0.577, 0.381, 0.393, 0.605, 0.177, 0.245, 0.253, 0.351, 0.174, 0.346]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.05690298507462686
[2m[36m(func pid=158650)[0m top5: 0.457089552238806
[2m[36m(func pid=158650)[0m f1_micro: 0.05690298507462686
[2m[36m(func pid=158650)[0m f1_macro: 0.03049040114341044
[2m[36m(func pid=158650)[0m f1_weighted: 0.035967012781434306
[2m[36m(func pid=158650)[0m f1_per_class: [0.051, 0.035, 0.0, 0.076, 0.0, 0.02, 0.0, 0.091, 0.0, 0.034]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2044 | Steps: 4 | Val loss: 5.0811 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5499 | Steps: 4 | Val loss: 1.6328 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3011 | Steps: 4 | Val loss: 28.5230 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0045 | Steps: 4 | Val loss: 2.5807 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:36:43 (running for 00:29:00.79)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.868 |      0.35  |                   51 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.204 |      0.286 |                   35 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.972 |      0.291 |                   26 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  3.061 |      0.03  |                    2 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.28125
[2m[36m(func pid=150999)[0m top5: 0.6520522388059702
[2m[36m(func pid=150999)[0m f1_micro: 0.28125
[2m[36m(func pid=150999)[0m f1_macro: 0.28571656149796304
[2m[36m(func pid=150999)[0m f1_weighted: 0.24460861958038038
[2m[36m(func pid=150999)[0m f1_per_class: [0.212, 0.113, 0.815, 0.54, 0.194, 0.26, 0.006, 0.419, 0.209, 0.09]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.32136194029850745
[2m[36m(func pid=152562)[0m top5: 0.8708022388059702
[2m[36m(func pid=152562)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=152562)[0m f1_macro: 0.27374100904561416
[2m[36m(func pid=152562)[0m f1_weighted: 0.3102011506775427
[2m[36m(func pid=152562)[0m f1_per_class: [0.325, 0.326, 0.267, 0.561, 0.149, 0.15, 0.156, 0.26, 0.155, 0.389]
[2m[36m(func pid=145792)[0m top1: 0.4006529850746269
[2m[36m(func pid=145792)[0m top5: 0.9118470149253731
[2m[36m(func pid=145792)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=145792)[0m f1_macro: 0.3569347261990271
[2m[36m(func pid=145792)[0m f1_weighted: 0.4051307024067812
[2m[36m(func pid=145792)[0m f1_per_class: [0.607, 0.549, 0.364, 0.547, 0.114, 0.197, 0.3, 0.35, 0.189, 0.353]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.061567164179104475
[2m[36m(func pid=158650)[0m top5: 0.4314365671641791
[2m[36m(func pid=158650)[0m f1_micro: 0.061567164179104475
[2m[36m(func pid=158650)[0m f1_macro: 0.03963439392314337
[2m[36m(func pid=158650)[0m f1_weighted: 0.05060481287301653
[2m[36m(func pid=158650)[0m f1_per_class: [0.055, 0.114, 0.0, 0.082, 0.0, 0.006, 0.0, 0.085, 0.024, 0.029]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6046 | Steps: 4 | Val loss: 5.6743 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6648 | Steps: 4 | Val loss: 1.6582 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2124 | Steps: 4 | Val loss: 28.5756 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9809 | Steps: 4 | Val loss: 2.5577 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:36:48 (running for 00:29:06.06)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.55  |      0.357 |                   52 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.605 |      0.321 |                   36 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.301 |      0.274 |                   27 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  3.005 |      0.04  |                    3 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.34654850746268656
[2m[36m(func pid=150999)[0m top5: 0.6506529850746269
[2m[36m(func pid=150999)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=150999)[0m f1_macro: 0.32072919593846416
[2m[36m(func pid=150999)[0m f1_weighted: 0.27967567261571347
[2m[36m(func pid=150999)[0m f1_per_class: [0.484, 0.341, 0.706, 0.54, 0.169, 0.224, 0.0, 0.353, 0.205, 0.187]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3381529850746269
[2m[36m(func pid=152562)[0m top5: 0.8894589552238806
[2m[36m(func pid=152562)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=152562)[0m f1_macro: 0.33015583258794534
[2m[36m(func pid=152562)[0m f1_weighted: 0.3016598468210386
[2m[36m(func pid=152562)[0m f1_per_class: [0.329, 0.159, 0.72, 0.58, 0.314, 0.151, 0.189, 0.259, 0.166, 0.435]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.39972014925373134
[2m[36m(func pid=145792)[0m top5: 0.902518656716418
[2m[36m(func pid=145792)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=145792)[0m f1_macro: 0.35006465290298966
[2m[36m(func pid=145792)[0m f1_weighted: 0.3948369572083552
[2m[36m(func pid=145792)[0m f1_per_class: [0.55, 0.582, 0.4, 0.492, 0.094, 0.152, 0.317, 0.352, 0.206, 0.356]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.07136194029850747
[2m[36m(func pid=158650)[0m top5: 0.40718283582089554
[2m[36m(func pid=158650)[0m f1_micro: 0.07136194029850747
[2m[36m(func pid=158650)[0m f1_macro: 0.05077447506016852
[2m[36m(func pid=158650)[0m f1_weighted: 0.0645361080317443
[2m[36m(func pid=158650)[0m f1_per_class: [0.063, 0.164, 0.0, 0.096, 0.0, 0.012, 0.0, 0.075, 0.065, 0.033]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.6562 | Steps: 4 | Val loss: 4.5624 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6287 | Steps: 4 | Val loss: 1.7001 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4557 | Steps: 4 | Val loss: 23.9965 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:36:54 (running for 00:29:11.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.665 |      0.35  |                   53 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.656 |      0.337 |                   37 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  1.212 |      0.33  |                   28 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.981 |      0.051 |                    4 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.32649253731343286
[2m[36m(func pid=150999)[0m top5: 0.6702425373134329
[2m[36m(func pid=150999)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=150999)[0m f1_macro: 0.3367811284746809
[2m[36m(func pid=150999)[0m f1_weighted: 0.3008544840982389
[2m[36m(func pid=150999)[0m f1_per_class: [0.611, 0.491, 0.571, 0.509, 0.122, 0.213, 0.009, 0.389, 0.129, 0.324]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9427 | Steps: 4 | Val loss: 2.5341 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=145792)[0m top1: 0.3829291044776119
[2m[36m(func pid=145792)[0m top5: 0.8992537313432836
[2m[36m(func pid=145792)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=145792)[0m f1_macro: 0.33396689923842304
[2m[36m(func pid=145792)[0m f1_weighted: 0.36929022092559854
[2m[36m(func pid=145792)[0m f1_per_class: [0.511, 0.561, 0.419, 0.458, 0.102, 0.216, 0.259, 0.364, 0.138, 0.312]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.365205223880597
[2m[36m(func pid=152562)[0m top5: 0.9081156716417911
[2m[36m(func pid=152562)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=152562)[0m f1_macro: 0.35122111207953044
[2m[36m(func pid=152562)[0m f1_weighted: 0.3469234509266005
[2m[36m(func pid=152562)[0m f1_per_class: [0.45, 0.26, 0.634, 0.578, 0.324, 0.196, 0.263, 0.264, 0.151, 0.394]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.07695895522388059
[2m[36m(func pid=158650)[0m top5: 0.3908582089552239
[2m[36m(func pid=158650)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=158650)[0m f1_macro: 0.05040364304895645
[2m[36m(func pid=158650)[0m f1_weighted: 0.062307843420576416
[2m[36m(func pid=158650)[0m f1_per_class: [0.059, 0.192, 0.0, 0.068, 0.0, 0.022, 0.0, 0.069, 0.072, 0.023]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1645 | Steps: 4 | Val loss: 2.8350 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6029 | Steps: 4 | Val loss: 1.6444 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2430 | Steps: 4 | Val loss: 21.6407 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 11:36:59 (running for 00:29:17.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.629 |      0.334 |                   54 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.164 |      0.351 |                   38 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  1.456 |      0.351 |                   29 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.943 |      0.05  |                    5 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.38013059701492535
[2m[36m(func pid=150999)[0m top5: 0.8624067164179104
[2m[36m(func pid=150999)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=150999)[0m f1_macro: 0.35124352740198456
[2m[36m(func pid=150999)[0m f1_weighted: 0.3798506684773783
[2m[36m(func pid=150999)[0m f1_per_class: [0.621, 0.508, 0.421, 0.404, 0.106, 0.186, 0.373, 0.342, 0.231, 0.32]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9274 | Steps: 4 | Val loss: 2.5193 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
[2m[36m(func pid=145792)[0m top1: 0.4048507462686567
[2m[36m(func pid=145792)[0m top5: 0.90625
[2m[36m(func pid=145792)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=145792)[0m f1_macro: 0.34712959801636484
[2m[36m(func pid=145792)[0m f1_weighted: 0.3974042995330955
[2m[36m(func pid=145792)[0m f1_per_class: [0.565, 0.561, 0.388, 0.498, 0.14, 0.268, 0.293, 0.378, 0.133, 0.248]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.40718283582089554
[2m[36m(func pid=152562)[0m top5: 0.8992537313432836
[2m[36m(func pid=152562)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=152562)[0m f1_macro: 0.33645578465895626
[2m[36m(func pid=152562)[0m f1_weighted: 0.42314753415552847
[2m[36m(func pid=152562)[0m f1_per_class: [0.6, 0.363, 0.333, 0.585, 0.096, 0.204, 0.451, 0.285, 0.138, 0.31]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8141 | Steps: 4 | Val loss: 2.2564 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=158650)[0m top1: 0.08348880597014925
[2m[36m(func pid=158650)[0m top5: 0.38899253731343286
[2m[36m(func pid=158650)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=158650)[0m f1_macro: 0.04605096552411049
[2m[36m(func pid=158650)[0m f1_weighted: 0.05648312902715234
[2m[36m(func pid=158650)[0m f1_per_class: [0.072, 0.221, 0.0, 0.027, 0.0, 0.035, 0.006, 0.028, 0.053, 0.019]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2274 | Steps: 4 | Val loss: 30.4550 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5117 | Steps: 4 | Val loss: 1.6778 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 11:37:05 (running for 00:29:22.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.603 |      0.347 |                   55 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.814 |      0.394 |                   39 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.243 |      0.336 |                   30 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.927 |      0.046 |                    6 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.4841417910447761
[2m[36m(func pid=150999)[0m top5: 0.9146455223880597
[2m[36m(func pid=150999)[0m f1_micro: 0.4841417910447761
[2m[36m(func pid=150999)[0m f1_macro: 0.394439788022636
[2m[36m(func pid=150999)[0m f1_weighted: 0.4748189030430813
[2m[36m(func pid=150999)[0m f1_per_class: [0.625, 0.575, 0.388, 0.478, 0.151, 0.195, 0.583, 0.3, 0.25, 0.4]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9358 | Steps: 4 | Val loss: 2.4977 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=145792)[0m top1: 0.39365671641791045
[2m[36m(func pid=145792)[0m top5: 0.8987873134328358
[2m[36m(func pid=145792)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=145792)[0m f1_macro: 0.33746364522330186
[2m[36m(func pid=145792)[0m f1_weighted: 0.393177273996395
[2m[36m(func pid=145792)[0m f1_per_class: [0.454, 0.543, 0.356, 0.543, 0.162, 0.278, 0.247, 0.351, 0.222, 0.219]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.32322761194029853
[2m[36m(func pid=152562)[0m top5: 0.8894589552238806
[2m[36m(func pid=152562)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=152562)[0m f1_macro: 0.2855903426318101
[2m[36m(func pid=152562)[0m f1_weighted: 0.3861554645152088
[2m[36m(func pid=152562)[0m f1_per_class: [0.485, 0.325, 0.299, 0.484, 0.038, 0.125, 0.501, 0.237, 0.08, 0.282]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2730 | Steps: 4 | Val loss: 2.3614 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=158650)[0m top1: 0.08162313432835822
[2m[36m(func pid=158650)[0m top5: 0.3927238805970149
[2m[36m(func pid=158650)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=158650)[0m f1_macro: 0.04984676597886596
[2m[36m(func pid=158650)[0m f1_weighted: 0.054124120131808624
[2m[36m(func pid=158650)[0m f1_per_class: [0.06, 0.216, 0.056, 0.01, 0.0, 0.038, 0.018, 0.0, 0.085, 0.016]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6115 | Steps: 4 | Val loss: 1.7101 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 4.4351 | Steps: 4 | Val loss: 27.2560 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:37:10 (running for 00:29:28.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.512 |      0.337 |                   56 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.273 |      0.404 |                   40 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.227 |      0.286 |                   31 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.936 |      0.05  |                    7 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.45475746268656714
[2m[36m(func pid=150999)[0m top5: 0.9486940298507462
[2m[36m(func pid=150999)[0m f1_micro: 0.45475746268656714
[2m[36m(func pid=150999)[0m f1_macro: 0.4038878638990882
[2m[36m(func pid=150999)[0m f1_weighted: 0.43630927033070954
[2m[36m(func pid=150999)[0m f1_per_class: [0.633, 0.506, 0.462, 0.575, 0.209, 0.263, 0.361, 0.352, 0.259, 0.419]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8623 | Steps: 4 | Val loss: 2.4914 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=145792)[0m top1: 0.373134328358209
[2m[36m(func pid=145792)[0m top5: 0.8969216417910447
[2m[36m(func pid=145792)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=145792)[0m f1_macro: 0.31844449650911405
[2m[36m(func pid=145792)[0m f1_weighted: 0.3785510893174741
[2m[36m(func pid=145792)[0m f1_per_class: [0.4, 0.433, 0.324, 0.557, 0.206, 0.287, 0.253, 0.359, 0.173, 0.193]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3306902985074627
[2m[36m(func pid=152562)[0m top5: 0.8572761194029851
[2m[36m(func pid=152562)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=152562)[0m f1_macro: 0.29297867891094026
[2m[36m(func pid=152562)[0m f1_weighted: 0.36408813699938936
[2m[36m(func pid=152562)[0m f1_per_class: [0.611, 0.447, 0.151, 0.402, 0.081, 0.227, 0.387, 0.161, 0.222, 0.239]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1744 | Steps: 4 | Val loss: 2.6708 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=158650)[0m top1: 0.08768656716417911
[2m[36m(func pid=158650)[0m top5: 0.4039179104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=158650)[0m f1_macro: 0.0548490173582073
[2m[36m(func pid=158650)[0m f1_weighted: 0.06273444191053268
[2m[36m(func pid=158650)[0m f1_per_class: [0.07, 0.234, 0.043, 0.007, 0.0, 0.079, 0.024, 0.0, 0.082, 0.011]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 5.4503 | Steps: 4 | Val loss: 33.7896 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5673 | Steps: 4 | Val loss: 1.7726 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 11:37:15 (running for 00:29:33.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.612 |      0.318 |                   57 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.174 |      0.384 |                   41 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  4.435 |      0.293 |                   32 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.862 |      0.055 |                    8 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.4216417910447761
[2m[36m(func pid=150999)[0m top5: 0.9375
[2m[36m(func pid=150999)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=150999)[0m f1_macro: 0.3837463033865484
[2m[36m(func pid=150999)[0m f1_weighted: 0.3965250139563205
[2m[36m(func pid=150999)[0m f1_per_class: [0.648, 0.519, 0.579, 0.512, 0.157, 0.307, 0.268, 0.363, 0.227, 0.258]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7848 | Steps: 4 | Val loss: 2.4717 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=152562)[0m top1: 0.283115671641791
[2m[36m(func pid=152562)[0m top5: 0.8470149253731343
[2m[36m(func pid=152562)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=152562)[0m f1_macro: 0.26624840698800467
[2m[36m(func pid=152562)[0m f1_weighted: 0.29117861025065706
[2m[36m(func pid=152562)[0m f1_per_class: [0.441, 0.41, 0.292, 0.262, 0.24, 0.365, 0.267, 0.114, 0.189, 0.083]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.35261194029850745
[2m[36m(func pid=145792)[0m top5: 0.882929104477612
[2m[36m(func pid=145792)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=145792)[0m f1_macro: 0.31599302842756477
[2m[36m(func pid=145792)[0m f1_weighted: 0.366427231767437
[2m[36m(func pid=145792)[0m f1_per_class: [0.394, 0.348, 0.358, 0.532, 0.25, 0.301, 0.278, 0.34, 0.23, 0.13]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4897 | Steps: 4 | Val loss: 2.9317 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=158650)[0m top1: 0.09235074626865672
[2m[36m(func pid=158650)[0m top5: 0.4375
[2m[36m(func pid=158650)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=158650)[0m f1_macro: 0.060942393408884535
[2m[36m(func pid=158650)[0m f1_weighted: 0.07144547278947222
[2m[36m(func pid=158650)[0m f1_per_class: [0.074, 0.236, 0.047, 0.007, 0.0, 0.101, 0.042, 0.0, 0.086, 0.017]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.8005 | Steps: 4 | Val loss: 1.7663 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.3352 | Steps: 4 | Val loss: 30.8055 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:37:21 (running for 00:29:38.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.567 |      0.316 |                   58 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.49  |      0.374 |                   42 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  5.45  |      0.266 |                   33 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.785 |      0.061 |                    9 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3824626865671642
[2m[36m(func pid=150999)[0m top5: 0.8773320895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=150999)[0m f1_macro: 0.37431562561110787
[2m[36m(func pid=150999)[0m f1_weighted: 0.3576365339825569
[2m[36m(func pid=150999)[0m f1_per_class: [0.635, 0.566, 0.5, 0.49, 0.134, 0.252, 0.154, 0.309, 0.27, 0.433]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7696 | Steps: 4 | Val loss: 2.4573 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=152562)[0m top1: 0.3292910447761194
[2m[36m(func pid=152562)[0m top5: 0.867070895522388
[2m[36m(func pid=152562)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=152562)[0m f1_macro: 0.3421460165069373
[2m[36m(func pid=152562)[0m f1_weighted: 0.3435843111545872
[2m[36m(func pid=152562)[0m f1_per_class: [0.658, 0.426, 0.645, 0.225, 0.182, 0.327, 0.442, 0.205, 0.189, 0.122]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3605410447761194
[2m[36m(func pid=145792)[0m top5: 0.8819962686567164
[2m[36m(func pid=145792)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=145792)[0m f1_macro: 0.32279269228840696
[2m[36m(func pid=145792)[0m f1_weighted: 0.3797929517801882
[2m[36m(func pid=145792)[0m f1_per_class: [0.477, 0.327, 0.375, 0.543, 0.2, 0.281, 0.329, 0.345, 0.203, 0.148]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4108 | Steps: 4 | Val loss: 4.1760 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=158650)[0m top1: 0.09421641791044776
[2m[36m(func pid=158650)[0m top5: 0.458955223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=158650)[0m f1_macro: 0.06400437089672742
[2m[36m(func pid=158650)[0m f1_weighted: 0.08078039832618908
[2m[36m(func pid=158650)[0m f1_per_class: [0.08, 0.213, 0.054, 0.007, 0.0, 0.114, 0.081, 0.0, 0.09, 0.0]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:26 (running for 00:29:44.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.801 |      0.323 |                   59 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.411 |      0.282 |                   43 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  6.335 |      0.342 |                   34 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.77  |      0.064 |                   10 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.24347014925373134
[2m[36m(func pid=150999)[0m top5: 0.7705223880597015
[2m[36m(func pid=150999)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=150999)[0m f1_macro: 0.28152228756668224
[2m[36m(func pid=150999)[0m f1_weighted: 0.24885080965621545
[2m[36m(func pid=150999)[0m f1_per_class: [0.628, 0.042, 0.595, 0.459, 0.153, 0.201, 0.165, 0.305, 0.139, 0.129]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.9632 | Steps: 4 | Val loss: 34.9033 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6265 | Steps: 4 | Val loss: 1.6532 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8047 | Steps: 4 | Val loss: 2.4287 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=152562)[0m top1: 0.2849813432835821
[2m[36m(func pid=152562)[0m top5: 0.8362873134328358
[2m[36m(func pid=152562)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=152562)[0m f1_macro: 0.31471904029935927
[2m[36m(func pid=152562)[0m f1_weighted: 0.24384323834705
[2m[36m(func pid=152562)[0m f1_per_class: [0.394, 0.47, 0.714, 0.261, 0.267, 0.155, 0.107, 0.288, 0.176, 0.316]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.39925373134328357
[2m[36m(func pid=145792)[0m top5: 0.9104477611940298
[2m[36m(func pid=145792)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=145792)[0m f1_macro: 0.35625587659820707
[2m[36m(func pid=145792)[0m f1_weighted: 0.4265881180893609
[2m[36m(func pid=145792)[0m f1_per_class: [0.538, 0.381, 0.407, 0.55, 0.215, 0.298, 0.435, 0.342, 0.205, 0.191]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2529 | Steps: 4 | Val loss: 4.5029 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=158650)[0m top1: 0.09794776119402986
[2m[36m(func pid=158650)[0m top5: 0.49486940298507465
[2m[36m(func pid=158650)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=158650)[0m f1_macro: 0.06661213374127019
[2m[36m(func pid=158650)[0m f1_weighted: 0.09091002148833183
[2m[36m(func pid=158650)[0m f1_per_class: [0.083, 0.183, 0.048, 0.007, 0.0, 0.133, 0.126, 0.0, 0.087, 0.0]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:32 (running for 00:29:49.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.626 |      0.356 |                   60 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.253 |      0.246 |                   44 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.963 |      0.315 |                   35 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.805 |      0.067 |                   11 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.23647388059701493
[2m[36m(func pid=150999)[0m top5: 0.6893656716417911
[2m[36m(func pid=150999)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=150999)[0m f1_macro: 0.24647026018772503
[2m[36m(func pid=150999)[0m f1_weighted: 0.24610171852286264
[2m[36m(func pid=150999)[0m f1_per_class: [0.26, 0.005, 0.667, 0.391, 0.121, 0.166, 0.275, 0.31, 0.173, 0.096]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 5.1384 | Steps: 4 | Val loss: 35.8455 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6206 | Steps: 4 | Val loss: 1.6213 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.6614 | Steps: 4 | Val loss: 2.4203 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=152562)[0m top1: 0.2896455223880597
[2m[36m(func pid=152562)[0m top5: 0.8628731343283582
[2m[36m(func pid=152562)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=152562)[0m f1_macro: 0.26648293666182477
[2m[36m(func pid=152562)[0m f1_weighted: 0.25449565231084864
[2m[36m(func pid=152562)[0m f1_per_class: [0.265, 0.499, 0.634, 0.43, 0.235, 0.051, 0.039, 0.236, 0.146, 0.129]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.4048507462686567
[2m[36m(func pid=145792)[0m top5: 0.9165111940298507
[2m[36m(func pid=145792)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=145792)[0m f1_macro: 0.362597950898273
[2m[36m(func pid=145792)[0m f1_weighted: 0.4340835435797066
[2m[36m(func pid=145792)[0m f1_per_class: [0.586, 0.408, 0.387, 0.56, 0.154, 0.263, 0.446, 0.334, 0.189, 0.299]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6826 | Steps: 4 | Val loss: 3.7336 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=158650)[0m top1: 0.10167910447761194
[2m[36m(func pid=158650)[0m top5: 0.5107276119402985
[2m[36m(func pid=158650)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=158650)[0m f1_macro: 0.06817162675174965
[2m[36m(func pid=158650)[0m f1_weighted: 0.0947226115905694
[2m[36m(func pid=158650)[0m f1_per_class: [0.092, 0.151, 0.048, 0.007, 0.0, 0.156, 0.149, 0.0, 0.079, 0.0]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:37 (running for 00:29:55.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.621 |      0.363 |                   61 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.683 |      0.283 |                   45 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  5.138 |      0.266 |                   36 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.661 |      0.068 |                   12 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2873134328358209
[2m[36m(func pid=150999)[0m top5: 0.7915111940298507
[2m[36m(func pid=150999)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=150999)[0m f1_macro: 0.2831232400758491
[2m[36m(func pid=150999)[0m f1_weighted: 0.3237453009841185
[2m[36m(func pid=150999)[0m f1_per_class: [0.24, 0.136, 0.727, 0.468, 0.063, 0.109, 0.401, 0.335, 0.214, 0.139]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 4.8955 | Steps: 4 | Val loss: 28.6666 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6693 | Steps: 4 | Val loss: 1.6366 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7016 | Steps: 4 | Val loss: 2.4115 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=152562)[0m top1: 0.40205223880597013
[2m[36m(func pid=152562)[0m top5: 0.9081156716417911
[2m[36m(func pid=152562)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=152562)[0m f1_macro: 0.3266525600779098
[2m[36m(func pid=152562)[0m f1_weighted: 0.3401161336308934
[2m[36m(func pid=152562)[0m f1_per_class: [0.531, 0.203, 0.5, 0.544, 0.226, 0.038, 0.351, 0.355, 0.107, 0.41]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3801 | Steps: 4 | Val loss: 2.9695 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=145792)[0m top1: 0.39132462686567165
[2m[36m(func pid=145792)[0m top5: 0.9127798507462687
[2m[36m(func pid=145792)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=145792)[0m f1_macro: 0.3657872011221382
[2m[36m(func pid=145792)[0m f1_weighted: 0.4150757548906513
[2m[36m(func pid=145792)[0m f1_per_class: [0.617, 0.491, 0.373, 0.524, 0.148, 0.258, 0.365, 0.329, 0.22, 0.333]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.1142723880597015
[2m[36m(func pid=158650)[0m top5: 0.5228544776119403
[2m[36m(func pid=158650)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=158650)[0m f1_macro: 0.07460958750227606
[2m[36m(func pid=158650)[0m f1_weighted: 0.10656016779013988
[2m[36m(func pid=158650)[0m f1_per_class: [0.105, 0.136, 0.058, 0.003, 0.0, 0.171, 0.193, 0.0, 0.08, 0.0]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:43 (running for 00:30:00.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.669 |      0.366 |                   62 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.38  |      0.388 |                   46 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  4.896 |      0.327 |                   37 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.702 |      0.075 |                   13 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3908582089552239
[2m[36m(func pid=150999)[0m top5: 0.8885261194029851
[2m[36m(func pid=150999)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=150999)[0m f1_macro: 0.3878718215603073
[2m[36m(func pid=150999)[0m f1_weighted: 0.4218446350802582
[2m[36m(func pid=150999)[0m f1_per_class: [0.659, 0.484, 0.727, 0.564, 0.054, 0.062, 0.416, 0.355, 0.216, 0.341]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.7514 | Steps: 4 | Val loss: 25.2230 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6822 | Steps: 4 | Val loss: 1.6774 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6689 | Steps: 4 | Val loss: 2.3568 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=152562)[0m top1: 0.4239738805970149
[2m[36m(func pid=152562)[0m top5: 0.9132462686567164
[2m[36m(func pid=152562)[0m f1_micro: 0.4239738805970149
[2m[36m(func pid=152562)[0m f1_macro: 0.3162333633731732
[2m[36m(func pid=152562)[0m f1_weighted: 0.394878881175192
[2m[36m(func pid=152562)[0m f1_per_class: [0.576, 0.147, 0.353, 0.589, 0.141, 0.186, 0.48, 0.317, 0.146, 0.227]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.0631 | Steps: 4 | Val loss: 3.0799 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=145792)[0m top1: 0.3833955223880597
[2m[36m(func pid=145792)[0m top5: 0.9029850746268657
[2m[36m(func pid=145792)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=145792)[0m f1_macro: 0.3531994253580484
[2m[36m(func pid=145792)[0m f1_weighted: 0.39885475302795625
[2m[36m(func pid=145792)[0m f1_per_class: [0.566, 0.504, 0.353, 0.507, 0.137, 0.254, 0.32, 0.329, 0.27, 0.291]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.12826492537313433
[2m[36m(func pid=158650)[0m top5: 0.5569029850746269
[2m[36m(func pid=158650)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=158650)[0m f1_macro: 0.08393951038376046
[2m[36m(func pid=158650)[0m f1_weighted: 0.11773002931632041
[2m[36m(func pid=158650)[0m f1_per_class: [0.141, 0.131, 0.077, 0.01, 0.0, 0.188, 0.218, 0.0, 0.073, 0.0]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:48 (running for 00:30:06.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.682 |      0.353 |                   63 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.063 |      0.349 |                   47 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.751 |      0.316 |                   38 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.669 |      0.084 |                   14 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.37593283582089554
[2m[36m(func pid=150999)[0m top5: 0.8992537313432836
[2m[36m(func pid=150999)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=150999)[0m f1_macro: 0.34915711071339084
[2m[36m(func pid=150999)[0m f1_weighted: 0.37519485465764213
[2m[36m(func pid=150999)[0m f1_per_class: [0.467, 0.491, 0.727, 0.533, 0.082, 0.082, 0.295, 0.358, 0.189, 0.267]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.7510 | Steps: 4 | Val loss: 30.1186 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5806 | Steps: 4 | Val loss: 1.6257 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5817 | Steps: 4 | Val loss: 2.3024 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152562)[0m top1: 0.29244402985074625
[2m[36m(func pid=152562)[0m top5: 0.8773320895522388
[2m[36m(func pid=152562)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=152562)[0m f1_macro: 0.253467846889165
[2m[36m(func pid=152562)[0m f1_weighted: 0.32809765630694354
[2m[36m(func pid=152562)[0m f1_per_class: [0.235, 0.399, 0.347, 0.401, 0.13, 0.241, 0.308, 0.287, 0.071, 0.116]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7082 | Steps: 4 | Val loss: 2.4113 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=145792)[0m top1: 0.408115671641791
[2m[36m(func pid=145792)[0m top5: 0.9118470149253731
[2m[36m(func pid=145792)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=145792)[0m f1_macro: 0.3576807482231409
[2m[36m(func pid=145792)[0m f1_weighted: 0.4048347832269848
[2m[36m(func pid=145792)[0m f1_per_class: [0.574, 0.546, 0.381, 0.563, 0.144, 0.237, 0.269, 0.349, 0.232, 0.281]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.1310634328358209
[2m[36m(func pid=158650)[0m top5: 0.5988805970149254
[2m[36m(func pid=158650)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=158650)[0m f1_macro: 0.10100590126342515
[2m[36m(func pid=158650)[0m f1_weighted: 0.11705442313028085
[2m[36m(func pid=158650)[0m f1_per_class: [0.165, 0.111, 0.108, 0.013, 0.0, 0.195, 0.215, 0.0, 0.064, 0.138]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:54 (running for 00:30:11.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.581 |      0.358 |                   64 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.708 |      0.387 |                   48 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  4.751 |      0.253 |                   39 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.582 |      0.101 |                   15 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.4244402985074627
[2m[36m(func pid=150999)[0m top5: 0.9323694029850746
[2m[36m(func pid=150999)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=150999)[0m f1_macro: 0.3866802874304037
[2m[36m(func pid=150999)[0m f1_weighted: 0.4319944132183409
[2m[36m(func pid=150999)[0m f1_per_class: [0.559, 0.553, 0.48, 0.546, 0.17, 0.289, 0.351, 0.332, 0.237, 0.35]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.6767 | Steps: 4 | Val loss: 26.0793 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5055 | Steps: 4 | Val loss: 1.6113 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3676 | Steps: 4 | Val loss: 2.2580 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=152562)[0m top1: 0.3628731343283582
[2m[36m(func pid=152562)[0m top5: 0.8936567164179104
[2m[36m(func pid=152562)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=152562)[0m f1_macro: 0.3098377526765578
[2m[36m(func pid=152562)[0m f1_weighted: 0.3719072165204888
[2m[36m(func pid=152562)[0m f1_per_class: [0.386, 0.51, 0.329, 0.454, 0.131, 0.26, 0.305, 0.286, 0.2, 0.237]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4408 | Steps: 4 | Val loss: 2.8262 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=145792)[0m top1: 0.42257462686567165
[2m[36m(func pid=145792)[0m top5: 0.9146455223880597
[2m[36m(func pid=145792)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=145792)[0m f1_macro: 0.36713288734948746
[2m[36m(func pid=145792)[0m f1_weighted: 0.40057350813692183
[2m[36m(func pid=145792)[0m f1_per_class: [0.574, 0.536, 0.471, 0.588, 0.154, 0.19, 0.247, 0.374, 0.234, 0.303]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.13712686567164178
[2m[36m(func pid=158650)[0m top5: 0.6310634328358209
[2m[36m(func pid=158650)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=158650)[0m f1_macro: 0.11004383628920204
[2m[36m(func pid=158650)[0m f1_weighted: 0.12937613093101213
[2m[36m(func pid=158650)[0m f1_per_class: [0.187, 0.102, 0.16, 0.054, 0.0, 0.185, 0.226, 0.0, 0.072, 0.115]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:37:59 (running for 00:30:16.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.506 |      0.367 |                   65 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.441 |      0.35  |                   49 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.677 |      0.31  |                   40 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.368 |      0.11  |                   16 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.37826492537313433
[2m[36m(func pid=150999)[0m top5: 0.8773320895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=150999)[0m f1_macro: 0.3498484232335724
[2m[36m(func pid=150999)[0m f1_weighted: 0.3993261840284778
[2m[36m(func pid=150999)[0m f1_per_class: [0.615, 0.422, 0.271, 0.474, 0.215, 0.305, 0.386, 0.328, 0.197, 0.286]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.6176 | Steps: 4 | Val loss: 33.1598 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6728 | Steps: 4 | Val loss: 1.6347 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4456 | Steps: 4 | Val loss: 2.2262 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.0702 | Steps: 4 | Val loss: 3.7925 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=152562)[0m top1: 0.39365671641791045
[2m[36m(func pid=152562)[0m top5: 0.9043843283582089
[2m[36m(func pid=152562)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=152562)[0m f1_macro: 0.34747588135931257
[2m[36m(func pid=152562)[0m f1_weighted: 0.33788123928726527
[2m[36m(func pid=152562)[0m f1_per_class: [0.531, 0.486, 0.533, 0.507, 0.233, 0.239, 0.149, 0.288, 0.128, 0.381]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.4207089552238806
[2m[36m(func pid=145792)[0m top5: 0.9104477611940298
[2m[36m(func pid=145792)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=145792)[0m f1_macro: 0.35824533319463886
[2m[36m(func pid=145792)[0m f1_weighted: 0.405490364797725
[2m[36m(func pid=145792)[0m f1_per_class: [0.507, 0.519, 0.511, 0.589, 0.122, 0.16, 0.291, 0.371, 0.234, 0.28]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.14272388059701493
[2m[36m(func pid=158650)[0m top5: 0.667910447761194
[2m[36m(func pid=158650)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=158650)[0m f1_macro: 0.11869435351165995
[2m[36m(func pid=158650)[0m f1_weighted: 0.14642314419512328
[2m[36m(func pid=158650)[0m f1_per_class: [0.192, 0.086, 0.233, 0.122, 0.0, 0.17, 0.234, 0.0, 0.071, 0.08]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:38:04 (running for 00:30:22.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.673 |      0.358 |                   66 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.07  |      0.264 |                   50 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.618 |      0.347 |                   41 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.446 |      0.119 |                   17 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.27798507462686567
[2m[36m(func pid=150999)[0m top5: 0.7957089552238806
[2m[36m(func pid=150999)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=150999)[0m f1_macro: 0.2638908553453599
[2m[36m(func pid=150999)[0m f1_weighted: 0.318338449616184
[2m[36m(func pid=150999)[0m f1_per_class: [0.459, 0.181, 0.22, 0.402, 0.075, 0.189, 0.376, 0.35, 0.239, 0.146]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6916 | Steps: 4 | Val loss: 29.9841 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4762 | Steps: 4 | Val loss: 1.6712 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2679 | Steps: 4 | Val loss: 2.1745 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5711 | Steps: 4 | Val loss: 4.3147 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=152562)[0m top1: 0.37220149253731344
[2m[36m(func pid=152562)[0m top5: 0.898320895522388
[2m[36m(func pid=152562)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=152562)[0m f1_macro: 0.3165626170170213
[2m[36m(func pid=152562)[0m f1_weighted: 0.3325495269615625
[2m[36m(func pid=152562)[0m f1_per_class: [0.47, 0.406, 0.4, 0.526, 0.135, 0.205, 0.166, 0.375, 0.124, 0.359]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.39972014925373134
[2m[36m(func pid=145792)[0m top5: 0.9104477611940298
[2m[36m(func pid=145792)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=145792)[0m f1_macro: 0.3430561734237772
[2m[36m(func pid=145792)[0m f1_weighted: 0.40753839664283176
[2m[36m(func pid=145792)[0m f1_per_class: [0.436, 0.461, 0.49, 0.593, 0.08, 0.141, 0.339, 0.383, 0.239, 0.27]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:10 (running for 00:30:27.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.476 |      0.343 |                   67 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.07  |      0.264 |                   50 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.692 |      0.317 |                   42 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.268 |      0.141 |                   18 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m top1: 0.1623134328358209
[2m[36m(func pid=158650)[0m top5: 0.7098880597014925
[2m[36m(func pid=158650)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=158650)[0m f1_macro: 0.14142573127514654
[2m[36m(func pid=158650)[0m f1_weighted: 0.1791840365463643
[2m[36m(func pid=158650)[0m f1_per_class: [0.249, 0.085, 0.208, 0.239, 0.0, 0.175, 0.207, 0.11, 0.077, 0.064]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m top1: 0.24253731343283583
[2m[36m(func pid=150999)[0m top5: 0.7639925373134329
[2m[36m(func pid=150999)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=150999)[0m f1_macro: 0.23693300004298767
[2m[36m(func pid=150999)[0m f1_weighted: 0.29620056010493473
[2m[36m(func pid=150999)[0m f1_per_class: [0.368, 0.193, 0.19, 0.394, 0.043, 0.119, 0.336, 0.364, 0.24, 0.122]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.1950 | Steps: 4 | Val loss: 34.4733 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.6907 | Steps: 4 | Val loss: 1.7134 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6586 | Steps: 4 | Val loss: 3.7733 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=152562)[0m top1: 0.2751865671641791
[2m[36m(func pid=152562)[0m top5: 0.8325559701492538
[2m[36m(func pid=152562)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=152562)[0m f1_macro: 0.2552422660970127
[2m[36m(func pid=152562)[0m f1_weighted: 0.3049759955634706
[2m[36m(func pid=152562)[0m f1_per_class: [0.151, 0.404, 0.413, 0.445, 0.083, 0.184, 0.191, 0.357, 0.132, 0.194]
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2056 | Steps: 4 | Val loss: 2.1280 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3894589552238806
[2m[36m(func pid=145792)[0m top5: 0.9011194029850746
[2m[36m(func pid=145792)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=145792)[0m f1_macro: 0.35027610764615585
[2m[36m(func pid=145792)[0m f1_weighted: 0.4187779303115755
[2m[36m(func pid=145792)[0m f1_per_class: [0.481, 0.478, 0.453, 0.562, 0.067, 0.142, 0.391, 0.368, 0.268, 0.292]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:15 (running for 00:30:33.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.691 |      0.35  |                   68 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.659 |      0.27  |                   52 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.195 |      0.255 |                   43 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.268 |      0.141 |                   18 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.28824626865671643
[2m[36m(func pid=150999)[0m top5: 0.7915111940298507
[2m[36m(func pid=150999)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=150999)[0m f1_macro: 0.2701897216810759
[2m[36m(func pid=150999)[0m f1_weighted: 0.30133583783282314
[2m[36m(func pid=150999)[0m f1_per_class: [0.465, 0.421, 0.289, 0.45, 0.081, 0.143, 0.167, 0.284, 0.195, 0.206]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.20102611940298507
[2m[36m(func pid=158650)[0m top5: 0.726679104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=158650)[0m f1_macro: 0.16316752100500886
[2m[36m(func pid=158650)[0m f1_weighted: 0.2158943063919434
[2m[36m(func pid=158650)[0m f1_per_class: [0.239, 0.071, 0.234, 0.377, 0.0, 0.146, 0.199, 0.222, 0.08, 0.063]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 12.4990 | Steps: 4 | Val loss: 37.2090 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4929 | Steps: 4 | Val loss: 1.7596 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.3130 | Steps: 4 | Val loss: 3.5132 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=152562)[0m top1: 0.23600746268656717
[2m[36m(func pid=152562)[0m top5: 0.8250932835820896
[2m[36m(func pid=152562)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=152562)[0m f1_macro: 0.2294697982287222
[2m[36m(func pid=152562)[0m f1_weighted: 0.2663242665617384
[2m[36m(func pid=152562)[0m f1_per_class: [0.164, 0.202, 0.371, 0.332, 0.091, 0.223, 0.288, 0.267, 0.079, 0.278]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.1542 | Steps: 4 | Val loss: 2.1182 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=145792)[0m top1: 0.3805970149253731
[2m[36m(func pid=145792)[0m top5: 0.8796641791044776
[2m[36m(func pid=145792)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=145792)[0m f1_macro: 0.33573792959773474
[2m[36m(func pid=145792)[0m f1_weighted: 0.4158028974184275
[2m[36m(func pid=145792)[0m f1_per_class: [0.537, 0.487, 0.366, 0.488, 0.085, 0.159, 0.445, 0.362, 0.245, 0.183]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:21 (running for 00:30:38.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.493 |      0.336 |                   69 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.313 |      0.365 |                   53 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 12.499 |      0.229 |                   44 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.206 |      0.163 |                   19 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3530783582089552
[2m[36m(func pid=150999)[0m top5: 0.8768656716417911
[2m[36m(func pid=150999)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=150999)[0m f1_macro: 0.3653139138540108
[2m[36m(func pid=150999)[0m f1_weighted: 0.3087454794580149
[2m[36m(func pid=150999)[0m f1_per_class: [0.694, 0.505, 0.667, 0.437, 0.18, 0.113, 0.127, 0.298, 0.222, 0.41]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.2248134328358209
[2m[36m(func pid=158650)[0m top5: 0.7164179104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=158650)[0m f1_macro: 0.16558430809110686
[2m[36m(func pid=158650)[0m f1_weighted: 0.21016092370812703
[2m[36m(func pid=158650)[0m f1_per_class: [0.25, 0.064, 0.256, 0.446, 0.0, 0.11, 0.123, 0.279, 0.06, 0.068]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 7.0194 | Steps: 4 | Val loss: 26.9108 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4412 | Steps: 4 | Val loss: 1.8958 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3142 | Steps: 4 | Val loss: 2.4019 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=152562)[0m top1: 0.37220149253731344
[2m[36m(func pid=152562)[0m top5: 0.8404850746268657
[2m[36m(func pid=152562)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=152562)[0m f1_macro: 0.2903414767300351
[2m[36m(func pid=152562)[0m f1_weighted: 0.3677815001560487
[2m[36m(func pid=152562)[0m f1_per_class: [0.516, 0.12, 0.248, 0.414, 0.159, 0.214, 0.578, 0.255, 0.1, 0.3]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1062 | Steps: 4 | Val loss: 2.1053 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=145792)[0m top1: 0.3362873134328358
[2m[36m(func pid=145792)[0m top5: 0.8423507462686567
[2m[36m(func pid=145792)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=145792)[0m f1_macro: 0.30126048712878883
[2m[36m(func pid=145792)[0m f1_weighted: 0.3769846329307676
[2m[36m(func pid=145792)[0m f1_per_class: [0.579, 0.451, 0.16, 0.419, 0.102, 0.187, 0.397, 0.351, 0.22, 0.147]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:26 (running for 00:30:43.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.441 |      0.301 |                   70 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.314 |      0.413 |                   54 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  7.019 |      0.29  |                   45 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.154 |      0.166 |                   20 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.47201492537313433
[2m[36m(func pid=150999)[0m top5: 0.9379664179104478
[2m[36m(func pid=150999)[0m f1_micro: 0.47201492537313433
[2m[36m(func pid=150999)[0m f1_macro: 0.4130390684430664
[2m[36m(func pid=150999)[0m f1_weighted: 0.46212888001555624
[2m[36m(func pid=150999)[0m f1_per_class: [0.651, 0.56, 0.6, 0.554, 0.19, 0.131, 0.491, 0.339, 0.226, 0.389]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7171 | Steps: 4 | Val loss: 33.9560 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=158650)[0m top1: 0.2439365671641791
[2m[36m(func pid=158650)[0m top5: 0.7131529850746269
[2m[36m(func pid=158650)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=158650)[0m f1_macro: 0.19120821921815384
[2m[36m(func pid=158650)[0m f1_weighted: 0.20777744329448594
[2m[36m(func pid=158650)[0m f1_per_class: [0.293, 0.082, 0.407, 0.476, 0.059, 0.063, 0.078, 0.31, 0.081, 0.062]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6898 | Steps: 4 | Val loss: 1.9306 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2116 | Steps: 4 | Val loss: 2.4222 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=152562)[0m top1: 0.27798507462686567
[2m[36m(func pid=152562)[0m top5: 0.8456156716417911
[2m[36m(func pid=152562)[0m f1_micro: 0.27798507462686567
[2m[36m(func pid=152562)[0m f1_macro: 0.29137533038422647
[2m[36m(func pid=152562)[0m f1_weighted: 0.276366360248948
[2m[36m(func pid=152562)[0m f1_per_class: [0.675, 0.079, 0.471, 0.521, 0.224, 0.206, 0.179, 0.29, 0.112, 0.157]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.9274 | Steps: 4 | Val loss: 2.0959 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=145792)[0m top1: 0.32276119402985076
[2m[36m(func pid=145792)[0m top5: 0.8362873134328358
[2m[36m(func pid=145792)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=145792)[0m f1_macro: 0.2969328587510598
[2m[36m(func pid=145792)[0m f1_weighted: 0.36492291744834954
[2m[36m(func pid=145792)[0m f1_per_class: [0.579, 0.418, 0.148, 0.431, 0.116, 0.204, 0.356, 0.36, 0.228, 0.13]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.46455223880597013
[2m[36m(func pid=150999)[0m top5: 0.9347014925373134
[2m[36m(func pid=150999)[0m f1_micro: 0.46455223880597013
[2m[36m(func pid=150999)[0m f1_macro: 0.4141061834489343
[2m[36m(func pid=150999)[0m f1_weighted: 0.458003519718619
[2m[36m(func pid=150999)[0m f1_per_class: [0.667, 0.531, 0.636, 0.526, 0.186, 0.199, 0.493, 0.345, 0.225, 0.333]
== Status ==
Current time: 2024-01-07 11:38:32 (running for 00:30:49.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.69  |      0.297 |                   71 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.212 |      0.414 |                   55 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.717 |      0.291 |                   46 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.106 |      0.191 |                   21 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3179 | Steps: 4 | Val loss: 36.6107 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=158650)[0m top1: 0.26399253731343286
[2m[36m(func pid=158650)[0m top5: 0.6986940298507462
[2m[36m(func pid=158650)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.19174452472649933
[2m[36m(func pid=158650)[0m f1_weighted: 0.19819284807571302
[2m[36m(func pid=158650)[0m f1_per_class: [0.333, 0.105, 0.393, 0.49, 0.111, 0.021, 0.036, 0.315, 0.033, 0.079]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4313 | Steps: 4 | Val loss: 1.8022 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9124 | Steps: 4 | Val loss: 2.5436 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=152562)[0m top1: 0.2896455223880597
[2m[36m(func pid=152562)[0m top5: 0.8316231343283582
[2m[36m(func pid=152562)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=152562)[0m f1_macro: 0.3341168487639621
[2m[36m(func pid=152562)[0m f1_weighted: 0.2734092006046369
[2m[36m(func pid=152562)[0m f1_per_class: [0.563, 0.32, 0.815, 0.503, 0.29, 0.137, 0.059, 0.33, 0.148, 0.176]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0526 | Steps: 4 | Val loss: 2.1062 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=145792)[0m top1: 0.3614738805970149
[2m[36m(func pid=145792)[0m top5: 0.8600746268656716
[2m[36m(func pid=145792)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=145792)[0m f1_macro: 0.3199977680251875
[2m[36m(func pid=145792)[0m f1_weighted: 0.3829368545425413
[2m[36m(func pid=145792)[0m f1_per_class: [0.559, 0.494, 0.245, 0.4, 0.134, 0.226, 0.394, 0.327, 0.25, 0.171]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:37 (running for 00:30:54.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.431 |      0.32  |                   72 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.912 |      0.429 |                   56 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.318 |      0.334 |                   47 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.927 |      0.192 |                   22 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.4314365671641791
[2m[36m(func pid=150999)[0m top5: 0.9090485074626866
[2m[36m(func pid=150999)[0m f1_micro: 0.4314365671641791
[2m[36m(func pid=150999)[0m f1_macro: 0.42888034177246015
[2m[36m(func pid=150999)[0m f1_weighted: 0.4376593031030776
[2m[36m(func pid=150999)[0m f1_per_class: [0.597, 0.52, 0.815, 0.498, 0.219, 0.257, 0.429, 0.376, 0.222, 0.356]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.0617 | Steps: 4 | Val loss: 31.7051 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=158650)[0m top1: 0.28218283582089554
[2m[36m(func pid=158650)[0m top5: 0.6809701492537313
[2m[36m(func pid=158650)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=158650)[0m f1_macro: 0.21098539435600522
[2m[36m(func pid=158650)[0m f1_weighted: 0.19500553204748727
[2m[36m(func pid=158650)[0m f1_per_class: [0.337, 0.108, 0.511, 0.499, 0.182, 0.016, 0.012, 0.313, 0.041, 0.092]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5419 | Steps: 4 | Val loss: 1.7182 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.1119 | Steps: 4 | Val loss: 4.8397 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=152562)[0m top1: 0.3460820895522388
[2m[36m(func pid=152562)[0m top5: 0.8656716417910447
[2m[36m(func pid=152562)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=152562)[0m f1_macro: 0.2778580210193587
[2m[36m(func pid=152562)[0m f1_weighted: 0.3201175151107017
[2m[36m(func pid=152562)[0m f1_per_class: [0.386, 0.472, 0.143, 0.45, 0.255, 0.105, 0.206, 0.345, 0.212, 0.206]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.8665 | Steps: 4 | Val loss: 2.0992 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=145792)[0m top1: 0.39132462686567165
[2m[36m(func pid=145792)[0m top5: 0.8833955223880597
[2m[36m(func pid=145792)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=145792)[0m f1_macro: 0.3484466843248777
[2m[36m(func pid=145792)[0m f1_weighted: 0.39113068569069426
[2m[36m(func pid=145792)[0m f1_per_class: [0.598, 0.529, 0.313, 0.375, 0.142, 0.279, 0.394, 0.341, 0.255, 0.26]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:43 (running for 00:31:00.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.542 |      0.348 |                   73 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.112 |      0.285 |                   57 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.062 |      0.278 |                   48 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  2.053 |      0.211 |                   23 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.18703358208955223
[2m[36m(func pid=150999)[0m top5: 0.8041044776119403
[2m[36m(func pid=150999)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=150999)[0m f1_macro: 0.28456195347600743
[2m[36m(func pid=150999)[0m f1_weighted: 0.2270376601436425
[2m[36m(func pid=150999)[0m f1_per_class: [0.539, 0.166, 0.714, 0.213, 0.197, 0.196, 0.244, 0.357, 0.175, 0.043]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1634 | Steps: 4 | Val loss: 25.0536 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=158650)[0m top1: 0.2947761194029851
[2m[36m(func pid=158650)[0m top5: 0.6791044776119403
[2m[36m(func pid=158650)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=158650)[0m f1_macro: 0.20776041033986004
[2m[36m(func pid=158650)[0m f1_weighted: 0.20110783836069032
[2m[36m(func pid=158650)[0m f1_per_class: [0.342, 0.161, 0.429, 0.501, 0.2, 0.008, 0.006, 0.307, 0.022, 0.102]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4738 | Steps: 4 | Val loss: 1.7102 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8508 | Steps: 4 | Val loss: 4.4326 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=152562)[0m top1: 0.3978544776119403
[2m[36m(func pid=152562)[0m top5: 0.9197761194029851
[2m[36m(func pid=152562)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=152562)[0m f1_macro: 0.3384064884281569
[2m[36m(func pid=152562)[0m f1_weighted: 0.3991371257606904
[2m[36m(func pid=152562)[0m f1_per_class: [0.516, 0.46, 0.375, 0.506, 0.264, 0.186, 0.392, 0.301, 0.19, 0.194]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.8658 | Steps: 4 | Val loss: 2.0859 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=145792)[0m top1: 0.3917910447761194
[2m[36m(func pid=145792)[0m top5: 0.8941231343283582
[2m[36m(func pid=145792)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=145792)[0m f1_macro: 0.36174828513144414
[2m[36m(func pid=145792)[0m f1_weighted: 0.3890293055922114
[2m[36m(func pid=145792)[0m f1_per_class: [0.596, 0.55, 0.356, 0.428, 0.15, 0.264, 0.327, 0.322, 0.271, 0.352]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:38:48 (running for 00:31:06.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.474 |      0.362 |                   74 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.851 |      0.273 |                   58 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.163 |      0.338 |                   49 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.866 |      0.208 |                   24 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2224813432835821
[2m[36m(func pid=150999)[0m top5: 0.7751865671641791
[2m[36m(func pid=150999)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=150999)[0m f1_macro: 0.2725391350284015
[2m[36m(func pid=150999)[0m f1_weighted: 0.241677647010433
[2m[36m(func pid=150999)[0m f1_per_class: [0.326, 0.15, 0.733, 0.336, 0.196, 0.224, 0.196, 0.349, 0.13, 0.084]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.8030 | Steps: 4 | Val loss: 26.1582 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=158650)[0m top1: 0.30736940298507465
[2m[36m(func pid=158650)[0m top5: 0.6944962686567164
[2m[36m(func pid=158650)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=158650)[0m f1_macro: 0.20973978744827887
[2m[36m(func pid=158650)[0m f1_weighted: 0.2186234258871706
[2m[36m(func pid=158650)[0m f1_per_class: [0.377, 0.244, 0.358, 0.512, 0.147, 0.008, 0.006, 0.312, 0.023, 0.109]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7079 | Steps: 4 | Val loss: 1.7273 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.0920 | Steps: 4 | Val loss: 3.4627 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=152562)[0m top1: 0.37919776119402987
[2m[36m(func pid=152562)[0m top5: 0.9188432835820896
[2m[36m(func pid=152562)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=152562)[0m f1_macro: 0.38380633225250327
[2m[36m(func pid=152562)[0m f1_weighted: 0.4018712250376013
[2m[36m(func pid=152562)[0m f1_per_class: [0.629, 0.269, 0.815, 0.54, 0.338, 0.237, 0.453, 0.276, 0.15, 0.131]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.7873 | Steps: 4 | Val loss: 2.0606 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:38:53 (running for 00:31:11.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.353
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.474 |      0.362 |                   74 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.092 |      0.298 |                   59 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.803 |      0.384 |                   50 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.866 |      0.21  |                   25 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.37033582089552236
[2m[36m(func pid=145792)[0m top5: 0.8955223880597015
[2m[36m(func pid=145792)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=145792)[0m f1_macro: 0.3568121404205319
[2m[36m(func pid=145792)[0m f1_weighted: 0.3715153905928482
[2m[36m(func pid=145792)[0m f1_per_class: [0.561, 0.527, 0.415, 0.45, 0.192, 0.235, 0.28, 0.309, 0.21, 0.388]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.376865671641791
[2m[36m(func pid=150999)[0m top5: 0.8847947761194029
[2m[36m(func pid=150999)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=150999)[0m f1_macro: 0.2976680062469962
[2m[36m(func pid=150999)[0m f1_weighted: 0.3524710508088456
[2m[36m(func pid=150999)[0m f1_per_class: [0.362, 0.12, 0.69, 0.602, 0.19, 0.185, 0.352, 0.365, 0.111, 0.0]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 4.1321 | Steps: 4 | Val loss: 24.4309 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=158650)[0m top1: 0.2980410447761194
[2m[36m(func pid=158650)[0m top5: 0.7257462686567164
[2m[36m(func pid=158650)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=158650)[0m f1_macro: 0.21132981824075342
[2m[36m(func pid=158650)[0m f1_weighted: 0.227552586290814
[2m[36m(func pid=158650)[0m f1_per_class: [0.36, 0.303, 0.343, 0.51, 0.14, 0.008, 0.009, 0.279, 0.043, 0.118]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.2259 | Steps: 4 | Val loss: 2.9904 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3924 | Steps: 4 | Val loss: 1.8099 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=152562)[0m top1: 0.39925373134328357
[2m[36m(func pid=152562)[0m top5: 0.871268656716418
[2m[36m(func pid=152562)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=152562)[0m f1_macro: 0.32462060563804473
[2m[36m(func pid=152562)[0m f1_weighted: 0.4254443347700964
[2m[36m(func pid=152562)[0m f1_per_class: [0.447, 0.377, 0.302, 0.467, 0.282, 0.314, 0.541, 0.236, 0.143, 0.135]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.7646 | Steps: 4 | Val loss: 2.0231 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:38:59 (running for 00:31:16.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.708 |      0.357 |                   75 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.226 |      0.313 |                   60 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  4.132 |      0.325 |                   51 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.787 |      0.211 |                   26 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.43097014925373134
[2m[36m(func pid=150999)[0m top5: 0.9081156716417911
[2m[36m(func pid=150999)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=150999)[0m f1_macro: 0.31266233460633186
[2m[36m(func pid=150999)[0m f1_weighted: 0.40349997095104934
[2m[36m(func pid=150999)[0m f1_per_class: [0.436, 0.22, 0.5, 0.611, 0.173, 0.191, 0.443, 0.397, 0.155, 0.0]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.34048507462686567
[2m[36m(func pid=145792)[0m top5: 0.8782649253731343
[2m[36m(func pid=145792)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=145792)[0m f1_macro: 0.3368671913841911
[2m[36m(func pid=145792)[0m f1_weighted: 0.3501959824925895
[2m[36m(func pid=145792)[0m f1_per_class: [0.556, 0.491, 0.379, 0.45, 0.12, 0.194, 0.247, 0.314, 0.202, 0.415]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5837 | Steps: 4 | Val loss: 32.8083 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=158650)[0m top1: 0.30830223880597013
[2m[36m(func pid=158650)[0m top5: 0.75
[2m[36m(func pid=158650)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=158650)[0m f1_macro: 0.22710977119653525
[2m[36m(func pid=158650)[0m f1_weighted: 0.2429662099577656
[2m[36m(func pid=158650)[0m f1_per_class: [0.395, 0.36, 0.393, 0.526, 0.14, 0.008, 0.012, 0.276, 0.024, 0.137]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1185 | Steps: 4 | Val loss: 2.5808 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4939 | Steps: 4 | Val loss: 1.8144 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=152562)[0m top1: 0.3316231343283582
[2m[36m(func pid=152562)[0m top5: 0.8129664179104478
[2m[36m(func pid=152562)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=152562)[0m f1_macro: 0.26537546968139153
[2m[36m(func pid=152562)[0m f1_weighted: 0.3340347746270504
[2m[36m(func pid=152562)[0m f1_per_class: [0.283, 0.517, 0.128, 0.224, 0.195, 0.304, 0.385, 0.31, 0.165, 0.143]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.7740 | Steps: 4 | Val loss: 2.0006 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 11:39:04 (running for 00:31:21.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.392 |      0.337 |                   76 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.119 |      0.37  |                   61 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  1.584 |      0.265 |                   52 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.765 |      0.227 |                   27 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.43656716417910446
[2m[36m(func pid=150999)[0m top5: 0.9057835820895522
[2m[36m(func pid=150999)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=150999)[0m f1_macro: 0.37001367564877363
[2m[36m(func pid=150999)[0m f1_weighted: 0.4488430244613969
[2m[36m(func pid=150999)[0m f1_per_class: [0.619, 0.553, 0.321, 0.499, 0.126, 0.222, 0.482, 0.328, 0.202, 0.348]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3493470149253731
[2m[36m(func pid=145792)[0m top5: 0.8717350746268657
[2m[36m(func pid=145792)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=145792)[0m f1_macro: 0.33274182846531447
[2m[36m(func pid=145792)[0m f1_weighted: 0.3693664027251866
[2m[36m(func pid=145792)[0m f1_per_class: [0.59, 0.485, 0.276, 0.469, 0.093, 0.164, 0.305, 0.335, 0.203, 0.407]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 15.3709 | Steps: 4 | Val loss: 44.5261 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=158650)[0m top1: 0.2989738805970149
[2m[36m(func pid=158650)[0m top5: 0.7840485074626866
[2m[36m(func pid=158650)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=158650)[0m f1_macro: 0.2249213857270858
[2m[36m(func pid=158650)[0m f1_weighted: 0.2483254842470392
[2m[36m(func pid=158650)[0m f1_per_class: [0.393, 0.379, 0.369, 0.538, 0.106, 0.015, 0.009, 0.26, 0.024, 0.157]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0419 | Steps: 4 | Val loss: 3.6717 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.5163 | Steps: 4 | Val loss: 1.8643 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=152562)[0m top1: 0.251865671641791
[2m[36m(func pid=152562)[0m top5: 0.7476679104477612
[2m[36m(func pid=152562)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=152562)[0m f1_macro: 0.21078572582923633
[2m[36m(func pid=152562)[0m f1_weighted: 0.23625618564166392
[2m[36m(func pid=152562)[0m f1_per_class: [0.307, 0.5, 0.14, 0.246, 0.083, 0.187, 0.097, 0.284, 0.186, 0.077]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.5091 | Steps: 4 | Val loss: 1.9860 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:39:09 (running for 00:31:27.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.494 |      0.333 |                   77 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.042 |      0.325 |                   62 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 15.371 |      0.211 |                   53 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.774 |      0.225 |                   28 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.36800373134328357
[2m[36m(func pid=150999)[0m top5: 0.8372201492537313
[2m[36m(func pid=150999)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=150999)[0m f1_macro: 0.3254450073731597
[2m[36m(func pid=150999)[0m f1_weighted: 0.34697219154459336
[2m[36m(func pid=150999)[0m f1_per_class: [0.674, 0.495, 0.306, 0.201, 0.109, 0.192, 0.468, 0.327, 0.148, 0.333]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m top1: 0.3376865671641791
[2m[36m(func pid=145792)[0m top5: 0.8680037313432836
[2m[36m(func pid=145792)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=145792)[0m f1_macro: 0.32378390747970376
[2m[36m(func pid=145792)[0m f1_weighted: 0.37370350312671774
[2m[36m(func pid=145792)[0m f1_per_class: [0.574, 0.464, 0.267, 0.467, 0.066, 0.13, 0.349, 0.343, 0.204, 0.375]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 7.4005 | Steps: 4 | Val loss: 43.6128 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=158650)[0m top1: 0.2989738805970149
[2m[36m(func pid=158650)[0m top5: 0.7910447761194029
[2m[36m(func pid=158650)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=158650)[0m f1_macro: 0.26024498909163496
[2m[36m(func pid=158650)[0m f1_weighted: 0.2632367800532972
[2m[36m(func pid=158650)[0m f1_per_class: [0.458, 0.43, 0.471, 0.528, 0.077, 0.021, 0.015, 0.253, 0.157, 0.191]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8409 | Steps: 4 | Val loss: 3.6958 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.5336 | Steps: 4 | Val loss: 1.8352 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=152562)[0m top1: 0.2728544776119403
[2m[36m(func pid=152562)[0m top5: 0.7649253731343284
[2m[36m(func pid=152562)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=152562)[0m f1_macro: 0.27366255234555287
[2m[36m(func pid=152562)[0m f1_weighted: 0.2502837226697553
[2m[36m(func pid=152562)[0m f1_per_class: [0.541, 0.107, 0.553, 0.575, 0.067, 0.15, 0.046, 0.333, 0.108, 0.256]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:39:15 (running for 00:31:32.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.516 |      0.324 |                   78 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.841 |      0.318 |                   63 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  7.401 |      0.274 |                   54 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.509 |      0.26  |                   29 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3512126865671642
[2m[36m(func pid=150999)[0m top5: 0.8218283582089553
[2m[36m(func pid=150999)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=150999)[0m f1_macro: 0.31759512257058276
[2m[36m(func pid=150999)[0m f1_weighted: 0.33969052924612697
[2m[36m(func pid=150999)[0m f1_per_class: [0.659, 0.497, 0.28, 0.194, 0.116, 0.235, 0.429, 0.334, 0.226, 0.205]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.6373 | Steps: 4 | Val loss: 1.9792 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=145792)[0m top1: 0.345615671641791
[2m[36m(func pid=145792)[0m top5: 0.875
[2m[36m(func pid=145792)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=145792)[0m f1_macro: 0.31969079770245107
[2m[36m(func pid=145792)[0m f1_weighted: 0.3855206358343102
[2m[36m(func pid=145792)[0m f1_per_class: [0.577, 0.437, 0.283, 0.483, 0.07, 0.151, 0.388, 0.316, 0.211, 0.281]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.3726 | Steps: 4 | Val loss: 55.2953 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=158650)[0m top1: 0.2775186567164179
[2m[36m(func pid=158650)[0m top5: 0.8078358208955224
[2m[36m(func pid=158650)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=158650)[0m f1_macro: 0.25764156018583423
[2m[36m(func pid=158650)[0m f1_weighted: 0.25563650817549766
[2m[36m(func pid=158650)[0m f1_per_class: [0.452, 0.456, 0.414, 0.465, 0.074, 0.043, 0.024, 0.251, 0.177, 0.22]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0216 | Steps: 4 | Val loss: 4.2110 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3524 | Steps: 4 | Val loss: 1.8164 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=152562)[0m top1: 0.25046641791044777
[2m[36m(func pid=152562)[0m top5: 0.7434701492537313
[2m[36m(func pid=152562)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=152562)[0m f1_macro: 0.288923982295176
[2m[36m(func pid=152562)[0m f1_weighted: 0.22541737998207617
[2m[36m(func pid=152562)[0m f1_per_class: [0.592, 0.011, 0.733, 0.541, 0.154, 0.167, 0.036, 0.333, 0.094, 0.228]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m top1: 0.2462686567164179
[2m[36m(func pid=150999)[0m top5: 0.8390858208955224
[2m[36m(func pid=150999)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=150999)[0m f1_macro: 0.3055030204198342
[2m[36m(func pid=150999)[0m f1_weighted: 0.2929858849798693
[2m[36m(func pid=150999)[0m f1_per_class: [0.629, 0.224, 0.415, 0.347, 0.17, 0.18, 0.32, 0.304, 0.096, 0.37]
[2m[36m(func pid=150999)[0m 
== Status ==
Current time: 2024-01-07 11:39:20 (running for 00:31:37.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.534 |      0.32  |                   79 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.022 |      0.306 |                   64 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.373 |      0.289 |                   55 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.637 |      0.258 |                   30 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4836 | Steps: 4 | Val loss: 1.9875 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=145792)[0m top1: 0.3530783582089552
[2m[36m(func pid=145792)[0m top5: 0.8796641791044776
[2m[36m(func pid=145792)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=145792)[0m f1_macro: 0.3296891027547726
[2m[36m(func pid=145792)[0m f1_weighted: 0.37864810334548604
[2m[36m(func pid=145792)[0m f1_per_class: [0.566, 0.476, 0.375, 0.474, 0.094, 0.17, 0.341, 0.314, 0.235, 0.252]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 4.8374 | Steps: 4 | Val loss: 43.5552 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2279 | Steps: 4 | Val loss: 3.9700 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=158650)[0m top1: 0.26119402985074625
[2m[36m(func pid=158650)[0m top5: 0.8101679104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=158650)[0m f1_macro: 0.2617765176978216
[2m[36m(func pid=158650)[0m f1_weighted: 0.24833977246379402
[2m[36m(func pid=158650)[0m f1_per_class: [0.435, 0.459, 0.436, 0.408, 0.059, 0.07, 0.039, 0.262, 0.164, 0.286]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3715 | Steps: 4 | Val loss: 1.7796 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=152562)[0m top1: 0.2868470149253731
[2m[36m(func pid=152562)[0m top5: 0.8306902985074627
[2m[36m(func pid=152562)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=152562)[0m f1_macro: 0.297051692132981
[2m[36m(func pid=152562)[0m f1_weighted: 0.26036306419477023
[2m[36m(func pid=152562)[0m f1_per_class: [0.576, 0.057, 0.786, 0.551, 0.171, 0.16, 0.131, 0.279, 0.122, 0.139]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:39:25 (running for 00:31:43.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.352 |      0.33  |                   80 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.228 |      0.315 |                   65 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  4.837 |      0.297 |                   56 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.484 |      0.262 |                   31 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2789179104477612
[2m[36m(func pid=150999)[0m top5: 0.8656716417910447
[2m[36m(func pid=150999)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=150999)[0m f1_macro: 0.3147000675136998
[2m[36m(func pid=150999)[0m f1_weighted: 0.3163514888515636
[2m[36m(func pid=150999)[0m f1_per_class: [0.64, 0.088, 0.431, 0.522, 0.187, 0.174, 0.311, 0.311, 0.097, 0.385]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.3651 | Steps: 4 | Val loss: 1.9893 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=145792)[0m top1: 0.3558768656716418
[2m[36m(func pid=145792)[0m top5: 0.8824626865671642
[2m[36m(func pid=145792)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=145792)[0m f1_macro: 0.3310094821728991
[2m[36m(func pid=145792)[0m f1_weighted: 0.3710369994046307
[2m[36m(func pid=145792)[0m f1_per_class: [0.584, 0.493, 0.381, 0.482, 0.121, 0.177, 0.298, 0.295, 0.238, 0.243]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7799 | Steps: 4 | Val loss: 31.5913 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4225 | Steps: 4 | Val loss: 3.2522 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=158650)[0m top1: 0.24300373134328357
[2m[36m(func pid=158650)[0m top5: 0.8171641791044776
[2m[36m(func pid=158650)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=158650)[0m f1_macro: 0.26677716770531723
[2m[36m(func pid=158650)[0m f1_weighted: 0.23872949615291528
[2m[36m(func pid=158650)[0m f1_per_class: [0.453, 0.431, 0.429, 0.332, 0.047, 0.111, 0.068, 0.304, 0.167, 0.327]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.375
[2m[36m(func pid=152562)[0m top5: 0.8950559701492538
[2m[36m(func pid=152562)[0m f1_micro: 0.375
[2m[36m(func pid=152562)[0m f1_macro: 0.3522518552770197
[2m[36m(func pid=152562)[0m f1_weighted: 0.3706401161925073
[2m[36m(func pid=152562)[0m f1_per_class: [0.588, 0.337, 0.8, 0.567, 0.092, 0.156, 0.324, 0.239, 0.179, 0.241]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.6488 | Steps: 4 | Val loss: 1.7459 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:39:31 (running for 00:31:48.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.371 |      0.331 |                   81 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.422 |      0.361 |                   66 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.78  |      0.352 |                   57 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.365 |      0.267 |                   32 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.42490671641791045
[2m[36m(func pid=150999)[0m top5: 0.9253731343283582
[2m[36m(func pid=150999)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=150999)[0m f1_macro: 0.36127951081361026
[2m[36m(func pid=150999)[0m f1_weighted: 0.3938905070732829
[2m[36m(func pid=150999)[0m f1_per_class: [0.63, 0.336, 0.462, 0.575, 0.164, 0.188, 0.358, 0.348, 0.183, 0.369]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3457 | Steps: 4 | Val loss: 1.9815 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=145792)[0m top1: 0.37826492537313433
[2m[36m(func pid=145792)[0m top5: 0.8871268656716418
[2m[36m(func pid=145792)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=145792)[0m f1_macro: 0.34642866674402367
[2m[36m(func pid=145792)[0m f1_weighted: 0.39359958214658786
[2m[36m(func pid=145792)[0m f1_per_class: [0.567, 0.515, 0.387, 0.482, 0.136, 0.207, 0.345, 0.305, 0.249, 0.271]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.2875 | Steps: 4 | Val loss: 28.6643 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.1484 | Steps: 4 | Val loss: 2.8683 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=158650)[0m top1: 0.22994402985074627
[2m[36m(func pid=158650)[0m top5: 0.8334888059701493
[2m[36m(func pid=158650)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=158650)[0m f1_macro: 0.2797124824922145
[2m[36m(func pid=158650)[0m f1_weighted: 0.2398841402044089
[2m[36m(func pid=158650)[0m f1_per_class: [0.533, 0.37, 0.468, 0.297, 0.046, 0.121, 0.124, 0.325, 0.172, 0.34]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3871268656716418
[2m[36m(func pid=152562)[0m top5: 0.8987873134328358
[2m[36m(func pid=152562)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=152562)[0m f1_macro: 0.36672284149736206
[2m[36m(func pid=152562)[0m f1_weighted: 0.4118934606684768
[2m[36m(func pid=152562)[0m f1_per_class: [0.63, 0.501, 0.632, 0.487, 0.068, 0.148, 0.443, 0.203, 0.242, 0.313]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:39:36 (running for 00:31:54.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.649 |      0.346 |                   82 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.148 |      0.369 |                   67 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.288 |      0.367 |                   58 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.346 |      0.28  |                   33 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3755 | Steps: 4 | Val loss: 1.5986 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=150999)[0m top1: 0.439365671641791
[2m[36m(func pid=150999)[0m top5: 0.9109141791044776
[2m[36m(func pid=150999)[0m f1_micro: 0.439365671641791
[2m[36m(func pid=150999)[0m f1_macro: 0.36914971857482026
[2m[36m(func pid=150999)[0m f1_weighted: 0.4348367811302803
[2m[36m(func pid=150999)[0m f1_per_class: [0.377, 0.51, 0.542, 0.577, 0.143, 0.196, 0.4, 0.37, 0.202, 0.375]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.2551 | Steps: 4 | Val loss: 1.9883 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 7.4997 | Steps: 4 | Val loss: 35.8495 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=145792)[0m top1: 0.42350746268656714
[2m[36m(func pid=145792)[0m top5: 0.9109141791044776
[2m[36m(func pid=145792)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=145792)[0m f1_macro: 0.3683174950593507
[2m[36m(func pid=145792)[0m f1_weighted: 0.44374165627386214
[2m[36m(func pid=145792)[0m f1_per_class: [0.534, 0.514, 0.449, 0.465, 0.152, 0.237, 0.523, 0.297, 0.218, 0.294]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.1878 | Steps: 4 | Val loss: 3.3556 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=158650)[0m top1: 0.22294776119402984
[2m[36m(func pid=158650)[0m top5: 0.8316231343283582
[2m[36m(func pid=158650)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=158650)[0m f1_macro: 0.2943833311889904
[2m[36m(func pid=158650)[0m f1_weighted: 0.2403409988661051
[2m[36m(func pid=158650)[0m f1_per_class: [0.553, 0.357, 0.579, 0.255, 0.048, 0.154, 0.154, 0.339, 0.156, 0.348]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.35634328358208955
[2m[36m(func pid=152562)[0m top5: 0.8330223880597015
[2m[36m(func pid=152562)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=152562)[0m f1_macro: 0.3137937912239422
[2m[36m(func pid=152562)[0m f1_weighted: 0.35918321311708723
[2m[36m(func pid=152562)[0m f1_per_class: [0.659, 0.505, 0.347, 0.275, 0.069, 0.184, 0.461, 0.216, 0.169, 0.254]
[2m[36m(func pid=152562)[0m 
== Status ==
Current time: 2024-01-07 11:39:41 (running for 00:31:59.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.375 |      0.368 |                   83 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.188 |      0.329 |                   68 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  7.5   |      0.314 |                   59 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.255 |      0.294 |                   34 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.3726679104477612
[2m[36m(func pid=150999)[0m top5: 0.8740671641791045
[2m[36m(func pid=150999)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=150999)[0m f1_macro: 0.32905659536158444
[2m[36m(func pid=150999)[0m f1_weighted: 0.3864442722234929
[2m[36m(func pid=150999)[0m f1_per_class: [0.198, 0.495, 0.553, 0.472, 0.119, 0.184, 0.364, 0.345, 0.247, 0.313]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4309 | Steps: 4 | Val loss: 1.5622 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.3353 | Steps: 4 | Val loss: 1.9933 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 9.2004 | Steps: 4 | Val loss: 57.1471 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=145792)[0m top1: 0.43470149253731344
[2m[36m(func pid=145792)[0m top5: 0.9183768656716418
[2m[36m(func pid=145792)[0m f1_micro: 0.43470149253731344
[2m[36m(func pid=145792)[0m f1_macro: 0.3665945569191559
[2m[36m(func pid=145792)[0m f1_weighted: 0.4537950545081753
[2m[36m(func pid=145792)[0m f1_per_class: [0.542, 0.511, 0.373, 0.474, 0.175, 0.27, 0.542, 0.285, 0.209, 0.286]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.1702 | Steps: 4 | Val loss: 3.8709 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=158650)[0m top1: 0.2271455223880597
[2m[36m(func pid=158650)[0m top5: 0.8302238805970149
[2m[36m(func pid=158650)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=158650)[0m f1_macro: 0.3052953040173826
[2m[36m(func pid=158650)[0m f1_weighted: 0.2479844356475623
[2m[36m(func pid=158650)[0m f1_per_class: [0.568, 0.372, 0.579, 0.216, 0.052, 0.166, 0.199, 0.338, 0.162, 0.4]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:39:47 (running for 00:32:04.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.431 |      0.367 |                   84 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.188 |      0.329 |                   68 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  9.2   |      0.222 |                   60 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.335 |      0.305 |                   35 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m top1: 0.20988805970149255
[2m[36m(func pid=152562)[0m top5: 0.6735074626865671
[2m[36m(func pid=152562)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=152562)[0m f1_macro: 0.2217988521448305
[2m[36m(func pid=152562)[0m f1_weighted: 0.19466586425064883
[2m[36m(func pid=152562)[0m f1_per_class: [0.579, 0.486, 0.052, 0.115, 0.043, 0.054, 0.104, 0.365, 0.155, 0.265]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m top1: 0.33255597014925375
[2m[36m(func pid=150999)[0m top5: 0.8325559701492538
[2m[36m(func pid=150999)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=150999)[0m f1_macro: 0.30901646008323147
[2m[36m(func pid=150999)[0m f1_weighted: 0.3430880312681656
[2m[36m(func pid=150999)[0m f1_per_class: [0.398, 0.474, 0.462, 0.366, 0.065, 0.136, 0.337, 0.367, 0.235, 0.25]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.4452 | Steps: 4 | Val loss: 1.6242 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.2742 | Steps: 4 | Val loss: 2.0027 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=145792)[0m top1: 0.41044776119402987
[2m[36m(func pid=145792)[0m top5: 0.9081156716417911
[2m[36m(func pid=145792)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=145792)[0m f1_macro: 0.3580787956401055
[2m[36m(func pid=145792)[0m f1_weighted: 0.4359847457831291
[2m[36m(func pid=145792)[0m f1_per_class: [0.527, 0.489, 0.358, 0.458, 0.168, 0.281, 0.504, 0.292, 0.212, 0.291]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5442 | Steps: 4 | Val loss: 3.8971 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.9638 | Steps: 4 | Val loss: 53.8456 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=158650)[0m top1: 0.23274253731343283
[2m[36m(func pid=158650)[0m top5: 0.8264925373134329
[2m[36m(func pid=158650)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.30668293420886616
[2m[36m(func pid=158650)[0m f1_weighted: 0.24970657239362948
[2m[36m(func pid=158650)[0m f1_per_class: [0.554, 0.401, 0.564, 0.198, 0.062, 0.168, 0.204, 0.352, 0.155, 0.409]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:39:52 (running for 00:32:09.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.445 |      0.358 |                   85 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.544 |      0.308 |                   70 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  9.2   |      0.222 |                   60 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.274 |      0.307 |                   36 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.30830223880597013
[2m[36m(func pid=150999)[0m top5: 0.8166977611940298
[2m[36m(func pid=150999)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=150999)[0m f1_macro: 0.30767463463766415
[2m[36m(func pid=150999)[0m f1_weighted: 0.3283693621908006
[2m[36m(func pid=150999)[0m f1_per_class: [0.5, 0.475, 0.375, 0.341, 0.068, 0.165, 0.293, 0.368, 0.234, 0.257]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.20475746268656717
[2m[36m(func pid=152562)[0m top5: 0.6399253731343284
[2m[36m(func pid=152562)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=152562)[0m f1_macro: 0.19614071871349834
[2m[36m(func pid=152562)[0m f1_weighted: 0.21900922447430388
[2m[36m(func pid=152562)[0m f1_per_class: [0.402, 0.29, 0.074, 0.416, 0.045, 0.028, 0.037, 0.419, 0.124, 0.125]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.5870 | Steps: 4 | Val loss: 1.6220 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.4310 | Steps: 4 | Val loss: 1.9789 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.0276 | Steps: 4 | Val loss: 4.4589 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.3663 | Steps: 4 | Val loss: 53.6621 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=145792)[0m top1: 0.4141791044776119
[2m[36m(func pid=145792)[0m top5: 0.914179104477612
[2m[36m(func pid=145792)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=145792)[0m f1_macro: 0.3667072269958365
[2m[36m(func pid=145792)[0m f1_weighted: 0.44190767076806464
[2m[36m(func pid=145792)[0m f1_per_class: [0.526, 0.5, 0.344, 0.511, 0.165, 0.277, 0.463, 0.325, 0.199, 0.357]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=158650)[0m top1: 0.251865671641791
[2m[36m(func pid=158650)[0m top5: 0.8208955223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=158650)[0m f1_macro: 0.30391974844796343
[2m[36m(func pid=158650)[0m f1_weighted: 0.26736179547944594
[2m[36m(func pid=158650)[0m f1_per_class: [0.566, 0.413, 0.537, 0.183, 0.069, 0.19, 0.263, 0.362, 0.157, 0.3]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:39:57 (running for 00:32:15.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.587 |      0.367 |                   86 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.028 |      0.314 |                   71 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  2.964 |      0.196 |                   61 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.431 |      0.304 |                   37 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2630597014925373
[2m[36m(func pid=150999)[0m top5: 0.8041044776119403
[2m[36m(func pid=150999)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=150999)[0m f1_macro: 0.31365944405523866
[2m[36m(func pid=150999)[0m f1_weighted: 0.28509987476716764
[2m[36m(func pid=150999)[0m f1_per_class: [0.642, 0.384, 0.44, 0.33, 0.209, 0.24, 0.185, 0.35, 0.125, 0.233]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.24113805970149255
[2m[36m(func pid=152562)[0m top5: 0.6217350746268657
[2m[36m(func pid=152562)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=152562)[0m f1_macro: 0.20205965150915164
[2m[36m(func pid=152562)[0m f1_weighted: 0.22289353967712042
[2m[36m(func pid=152562)[0m f1_per_class: [0.371, 0.103, 0.151, 0.537, 0.069, 0.099, 0.019, 0.42, 0.108, 0.144]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2949 | Steps: 4 | Val loss: 1.7215 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1871 | Steps: 4 | Val loss: 1.9176 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.9823 | Steps: 4 | Val loss: 4.2549 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7303 | Steps: 4 | Val loss: 59.4606 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=145792)[0m top1: 0.396455223880597
[2m[36m(func pid=145792)[0m top5: 0.8889925373134329
[2m[36m(func pid=145792)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=145792)[0m f1_macro: 0.3501507045015098
[2m[36m(func pid=145792)[0m f1_weighted: 0.42412052613782936
[2m[36m(func pid=145792)[0m f1_per_class: [0.537, 0.493, 0.349, 0.522, 0.141, 0.233, 0.413, 0.329, 0.235, 0.248]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:40:03 (running for 00:32:20.57)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.295 |      0.35  |                   87 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.982 |      0.308 |                   72 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.366 |      0.202 |                   62 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.431 |      0.304 |                   37 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2728544776119403
[2m[36m(func pid=150999)[0m top5: 0.8125
[2m[36m(func pid=150999)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=150999)[0m f1_macro: 0.30810618061855244
[2m[36m(func pid=150999)[0m f1_weighted: 0.2923186851516565
[2m[36m(func pid=150999)[0m f1_per_class: [0.496, 0.356, 0.431, 0.322, 0.254, 0.261, 0.232, 0.351, 0.126, 0.252]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.26865671641791045
[2m[36m(func pid=158650)[0m top5: 0.8493470149253731
[2m[36m(func pid=158650)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=158650)[0m f1_macro: 0.31042670117596033
[2m[36m(func pid=158650)[0m f1_weighted: 0.2831706865149169
[2m[36m(func pid=158650)[0m f1_per_class: [0.587, 0.429, 0.5, 0.227, 0.096, 0.219, 0.257, 0.345, 0.164, 0.281]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.2080223880597015
[2m[36m(func pid=152562)[0m top5: 0.6665111940298507
[2m[36m(func pid=152562)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=152562)[0m f1_macro: 0.23765960119163326
[2m[36m(func pid=152562)[0m f1_weighted: 0.20365217336000324
[2m[36m(func pid=152562)[0m f1_per_class: [0.439, 0.021, 0.4, 0.502, 0.115, 0.112, 0.025, 0.368, 0.077, 0.317]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3025 | Steps: 4 | Val loss: 1.7448 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4735 | Steps: 4 | Val loss: 3.8712 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.2200 | Steps: 4 | Val loss: 1.8083 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 16.7175 | Steps: 4 | Val loss: 44.2126 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=145792)[0m top1: 0.38572761194029853
[2m[36m(func pid=145792)[0m top5: 0.8833955223880597
[2m[36m(func pid=145792)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=145792)[0m f1_macro: 0.34563080279690783
[2m[36m(func pid=145792)[0m f1_weighted: 0.4124621463582209
[2m[36m(func pid=145792)[0m f1_per_class: [0.56, 0.49, 0.4, 0.505, 0.129, 0.214, 0.398, 0.327, 0.26, 0.174]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:40:08 (running for 00:32:25.90)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.303 |      0.346 |                   88 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.473 |      0.318 |                   73 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.73  |      0.238 |                   63 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.187 |      0.31  |                   38 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.33255597014925375
[2m[36m(func pid=150999)[0m top5: 0.8334888059701493
[2m[36m(func pid=150999)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=150999)[0m f1_macro: 0.31775464970130546
[2m[36m(func pid=150999)[0m f1_weighted: 0.3721043380268347
[2m[36m(func pid=150999)[0m f1_per_class: [0.11, 0.369, 0.629, 0.309, 0.2, 0.308, 0.512, 0.299, 0.213, 0.229]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.30223880597014924
[2m[36m(func pid=158650)[0m top5: 0.8773320895522388
[2m[36m(func pid=158650)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=158650)[0m f1_macro: 0.32569811162069895
[2m[36m(func pid=158650)[0m f1_weighted: 0.3180078515076285
[2m[36m(func pid=158650)[0m f1_per_class: [0.602, 0.451, 0.449, 0.304, 0.122, 0.24, 0.277, 0.349, 0.194, 0.27]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.261660447761194
[2m[36m(func pid=152562)[0m top5: 0.8027052238805971
[2m[36m(func pid=152562)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=152562)[0m f1_macro: 0.2655159890451354
[2m[36m(func pid=152562)[0m f1_weighted: 0.2646279272732458
[2m[36m(func pid=152562)[0m f1_per_class: [0.36, 0.061, 0.471, 0.543, 0.11, 0.172, 0.147, 0.367, 0.091, 0.333]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.4239 | Steps: 4 | Val loss: 1.8235 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3119 | Steps: 4 | Val loss: 3.3577 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.0445 | Steps: 4 | Val loss: 1.7814 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 8.9609 | Steps: 4 | Val loss: 28.7896 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=145792)[0m top1: 0.3572761194029851
[2m[36m(func pid=145792)[0m top5: 0.8638059701492538
[2m[36m(func pid=145792)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=145792)[0m f1_macro: 0.3278647725995566
[2m[36m(func pid=145792)[0m f1_weighted: 0.3785477725573773
[2m[36m(func pid=145792)[0m f1_per_class: [0.551, 0.475, 0.393, 0.488, 0.12, 0.188, 0.319, 0.342, 0.239, 0.163]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:40:13 (running for 00:32:31.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.356
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.424 |      0.328 |                   89 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.312 |      0.326 |                   74 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 16.717 |      0.266 |                   64 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.22  |      0.326 |                   39 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.40158582089552236
[2m[36m(func pid=150999)[0m top5: 0.8810634328358209
[2m[36m(func pid=150999)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=150999)[0m f1_macro: 0.3259571612760653
[2m[36m(func pid=150999)[0m f1_weighted: 0.4287717068105541
[2m[36m(func pid=150999)[0m f1_per_class: [0.133, 0.443, 0.595, 0.497, 0.179, 0.129, 0.561, 0.3, 0.104, 0.32]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.32649253731343286
[2m[36m(func pid=158650)[0m top5: 0.8861940298507462
[2m[36m(func pid=158650)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.3430204197184158
[2m[36m(func pid=158650)[0m f1_weighted: 0.3470827975012355
[2m[36m(func pid=158650)[0m f1_per_class: [0.614, 0.448, 0.522, 0.367, 0.143, 0.248, 0.315, 0.33, 0.2, 0.243]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=152562)[0m top1: 0.35774253731343286
[2m[36m(func pid=152562)[0m top5: 0.8591417910447762
[2m[36m(func pid=152562)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=152562)[0m f1_macro: 0.3262749342329696
[2m[36m(func pid=152562)[0m f1_weighted: 0.38830233133255104
[2m[36m(func pid=152562)[0m f1_per_class: [0.333, 0.361, 0.579, 0.485, 0.116, 0.26, 0.428, 0.245, 0.133, 0.323]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.5438 | Steps: 4 | Val loss: 1.7971 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9270 | Steps: 4 | Val loss: 2.8586 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0792 | Steps: 4 | Val loss: 1.7600 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.0580 | Steps: 4 | Val loss: 50.4817 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=145792)[0m top1: 0.3628731343283582
[2m[36m(func pid=145792)[0m top5: 0.8717350746268657
[2m[36m(func pid=145792)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=145792)[0m f1_macro: 0.3357374978033954
[2m[36m(func pid=145792)[0m f1_weighted: 0.3869112722519244
[2m[36m(func pid=145792)[0m f1_per_class: [0.547, 0.475, 0.44, 0.495, 0.102, 0.17, 0.348, 0.331, 0.219, 0.229]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.45149253731343286
[2m[36m(func pid=150999)[0m top5: 0.8973880597014925
[2m[36m(func pid=150999)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=150999)[0m f1_macro: 0.3612417521021027
[2m[36m(func pid=150999)[0m f1_weighted: 0.4416016742314471
[2m[36m(func pid=150999)[0m f1_per_class: [0.402, 0.543, 0.51, 0.578, 0.188, 0.139, 0.431, 0.358, 0.178, 0.286]
== Status ==
Current time: 2024-01-07 11:40:19 (running for 00:32:36.65)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.544 |      0.336 |                   90 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.927 |      0.361 |                   75 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  8.961 |      0.326 |                   65 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.044 |      0.343 |                   40 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.20569029850746268
[2m[36m(func pid=152562)[0m top5: 0.8488805970149254
[2m[36m(func pid=152562)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=152562)[0m f1_macro: 0.30017587863936523
[2m[36m(func pid=152562)[0m f1_weighted: 0.20744874647309974
[2m[36m(func pid=152562)[0m f1_per_class: [0.634, 0.453, 0.8, 0.082, 0.224, 0.14, 0.19, 0.144, 0.099, 0.235]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.34328358208955223
[2m[36m(func pid=158650)[0m top5: 0.8880597014925373
[2m[36m(func pid=158650)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=158650)[0m f1_macro: 0.34361036190256605
[2m[36m(func pid=158650)[0m f1_weighted: 0.3650924205970334
[2m[36m(func pid=158650)[0m f1_per_class: [0.596, 0.442, 0.48, 0.415, 0.16, 0.262, 0.328, 0.343, 0.211, 0.199]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.4699 | Steps: 4 | Val loss: 1.8316 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2524 | Steps: 4 | Val loss: 3.9220 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 3.1741 | Steps: 4 | Val loss: 33.6771 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.1583 | Steps: 4 | Val loss: 1.7182 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=145792)[0m top1: 0.36427238805970147
[2m[36m(func pid=145792)[0m top5: 0.8642723880597015
[2m[36m(func pid=145792)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=145792)[0m f1_macro: 0.3263513457901481
[2m[36m(func pid=145792)[0m f1_weighted: 0.38828691030249957
[2m[36m(func pid=145792)[0m f1_per_class: [0.452, 0.469, 0.423, 0.482, 0.111, 0.16, 0.377, 0.343, 0.216, 0.229]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:40:24 (running for 00:32:41.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.47  |      0.326 |                   91 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.252 |      0.332 |                   76 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  1.058 |      0.3   |                   66 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.079 |      0.344 |                   41 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.36800373134328357
[2m[36m(func pid=150999)[0m top5: 0.8325559701492538
[2m[36m(func pid=150999)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=150999)[0m f1_macro: 0.33237261199196994
[2m[36m(func pid=150999)[0m f1_weighted: 0.3293849546582354
[2m[36m(func pid=150999)[0m f1_per_class: [0.63, 0.515, 0.414, 0.555, 0.088, 0.072, 0.099, 0.379, 0.173, 0.4]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3521455223880597
[2m[36m(func pid=152562)[0m top5: 0.8819962686567164
[2m[36m(func pid=152562)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=152562)[0m f1_macro: 0.34570215667600146
[2m[36m(func pid=152562)[0m f1_weighted: 0.3372427133121685
[2m[36m(func pid=152562)[0m f1_per_class: [0.634, 0.544, 0.667, 0.295, 0.185, 0.246, 0.312, 0.268, 0.118, 0.188]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.35774253731343286
[2m[36m(func pid=158650)[0m top5: 0.9090485074626866
[2m[36m(func pid=158650)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.36166069075565715
[2m[36m(func pid=158650)[0m f1_weighted: 0.3753274643971127
[2m[36m(func pid=158650)[0m f1_per_class: [0.613, 0.452, 0.522, 0.442, 0.196, 0.282, 0.312, 0.362, 0.278, 0.158]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3364 | Steps: 4 | Val loss: 1.8729 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.0805 | Steps: 4 | Val loss: 4.0213 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0486 | Steps: 4 | Val loss: 29.6883 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8588 | Steps: 4 | Val loss: 1.6560 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=145792)[0m top1: 0.36100746268656714
[2m[36m(func pid=145792)[0m top5: 0.8530783582089553
[2m[36m(func pid=145792)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=145792)[0m f1_macro: 0.30664660103441765
[2m[36m(func pid=145792)[0m f1_weighted: 0.3800106780517403
[2m[36m(func pid=145792)[0m f1_per_class: [0.394, 0.513, 0.274, 0.486, 0.093, 0.159, 0.326, 0.342, 0.223, 0.256]
[2m[36m(func pid=145792)[0m 
== Status ==
Current time: 2024-01-07 11:40:29 (running for 00:32:47.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.336 |      0.307 |                   92 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.081 |      0.294 |                   77 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.174 |      0.346 |                   67 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.158 |      0.362 |                   42 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.34841417910447764
[2m[36m(func pid=150999)[0m top5: 0.8484141791044776
[2m[36m(func pid=150999)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=150999)[0m f1_macro: 0.2943900681446157
[2m[36m(func pid=150999)[0m f1_weighted: 0.3255801504288774
[2m[36m(func pid=150999)[0m f1_per_class: [0.24, 0.492, 0.4, 0.551, 0.075, 0.089, 0.123, 0.389, 0.157, 0.429]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.4123134328358209
[2m[36m(func pid=152562)[0m top5: 0.9281716417910447
[2m[36m(func pid=152562)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=152562)[0m f1_macro: 0.32472372275746325
[2m[36m(func pid=152562)[0m f1_weighted: 0.3961208520010522
[2m[36m(func pid=152562)[0m f1_per_class: [0.563, 0.561, 0.375, 0.504, 0.202, 0.296, 0.31, 0.232, 0.075, 0.129]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3773320895522388
[2m[36m(func pid=158650)[0m top5: 0.9230410447761194
[2m[36m(func pid=158650)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=158650)[0m f1_macro: 0.3508064703798062
[2m[36m(func pid=158650)[0m f1_weighted: 0.39681094156024127
[2m[36m(func pid=158650)[0m f1_per_class: [0.567, 0.446, 0.453, 0.506, 0.19, 0.296, 0.337, 0.329, 0.249, 0.136]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.5590 | Steps: 4 | Val loss: 1.8439 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.4702 | Steps: 4 | Val loss: 4.8530 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 13.0855 | Steps: 4 | Val loss: 29.4269 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.9130 | Steps: 4 | Val loss: 1.6055 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:40:34 (running for 00:32:52.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.559 |      0.305 |                   93 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.081 |      0.294 |                   77 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.049 |      0.325 |                   68 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.859 |      0.351 |                   43 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.3666044776119403
[2m[36m(func pid=145792)[0m top5: 0.8577425373134329
[2m[36m(func pid=145792)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=145792)[0m f1_macro: 0.30502172154706
[2m[36m(func pid=145792)[0m f1_weighted: 0.38402428758496326
[2m[36m(func pid=145792)[0m f1_per_class: [0.389, 0.517, 0.257, 0.487, 0.1, 0.153, 0.339, 0.354, 0.208, 0.243]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.29011194029850745
[2m[36m(func pid=150999)[0m top5: 0.8833955223880597
[2m[36m(func pid=150999)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=150999)[0m f1_macro: 0.26133802291074326
[2m[36m(func pid=150999)[0m f1_weighted: 0.27342701323264923
[2m[36m(func pid=150999)[0m f1_per_class: [0.167, 0.337, 0.5, 0.508, 0.25, 0.258, 0.068, 0.155, 0.116, 0.255]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.4141791044776119
[2m[36m(func pid=152562)[0m top5: 0.9286380597014925
[2m[36m(func pid=152562)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=152562)[0m f1_macro: 0.3889521009413074
[2m[36m(func pid=152562)[0m f1_weighted: 0.4035815537636705
[2m[36m(func pid=152562)[0m f1_per_class: [0.595, 0.556, 0.75, 0.534, 0.208, 0.285, 0.301, 0.158, 0.156, 0.348]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.408115671641791
[2m[36m(func pid=158650)[0m top5: 0.9291044776119403
[2m[36m(func pid=158650)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=158650)[0m f1_macro: 0.3610899763356677
[2m[36m(func pid=158650)[0m f1_weighted: 0.4302351577785267
[2m[36m(func pid=158650)[0m f1_per_class: [0.54, 0.451, 0.436, 0.559, 0.229, 0.306, 0.392, 0.374, 0.192, 0.132]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3083 | Steps: 4 | Val loss: 1.6874 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3711 | Steps: 4 | Val loss: 3.9130 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 10.8698 | Steps: 4 | Val loss: 32.1020 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8941 | Steps: 4 | Val loss: 1.6052 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:40:40 (running for 00:32:57.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.308 |      0.348 |                   94 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.47  |      0.261 |                   78 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 13.085 |      0.389 |                   69 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.913 |      0.361 |                   44 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=145792)[0m top1: 0.41091417910447764
[2m[36m(func pid=145792)[0m top5: 0.8871268656716418
[2m[36m(func pid=145792)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=145792)[0m f1_macro: 0.3479564047378763
[2m[36m(func pid=145792)[0m f1_weighted: 0.4271508004167541
[2m[36m(func pid=145792)[0m f1_per_class: [0.529, 0.514, 0.306, 0.553, 0.124, 0.178, 0.403, 0.341, 0.198, 0.333]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=150999)[0m top1: 0.3204291044776119
[2m[36m(func pid=150999)[0m top5: 0.8568097014925373
[2m[36m(func pid=150999)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=150999)[0m f1_macro: 0.26999471989133983
[2m[36m(func pid=150999)[0m f1_weighted: 0.3325598937494225
[2m[36m(func pid=150999)[0m f1_per_class: [0.351, 0.298, 0.108, 0.531, 0.228, 0.288, 0.25, 0.122, 0.15, 0.375]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m top1: 0.36427238805970147
[2m[36m(func pid=152562)[0m top5: 0.8833955223880597
[2m[36m(func pid=152562)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=152562)[0m f1_macro: 0.34753840107959594
[2m[36m(func pid=152562)[0m f1_weighted: 0.3930301530130936
[2m[36m(func pid=152562)[0m f1_per_class: [0.675, 0.449, 0.611, 0.418, 0.152, 0.252, 0.456, 0.13, 0.204, 0.127]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3987873134328358
[2m[36m(func pid=158650)[0m top5: 0.929570895522388
[2m[36m(func pid=158650)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=158650)[0m f1_macro: 0.347482205262948
[2m[36m(func pid=158650)[0m f1_weighted: 0.4170265871722323
[2m[36m(func pid=158650)[0m f1_per_class: [0.528, 0.413, 0.381, 0.583, 0.232, 0.293, 0.355, 0.357, 0.217, 0.116]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.5650 | Steps: 4 | Val loss: 3.1716 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3514 | Steps: 4 | Val loss: 1.6059 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 12.1409 | Steps: 4 | Val loss: 31.1644 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 11:40:45 (running for 00:33:03.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.308 |      0.348 |                   94 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.565 |      0.312 |                   80 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 10.87  |      0.348 |                   70 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.894 |      0.347 |                   45 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.40904850746268656
[2m[36m(func pid=150999)[0m top5: 0.8805970149253731
[2m[36m(func pid=150999)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=150999)[0m f1_macro: 0.31229645920716137
[2m[36m(func pid=150999)[0m f1_weighted: 0.43357737528144624
[2m[36m(func pid=150999)[0m f1_per_class: [0.582, 0.349, 0.094, 0.529, 0.178, 0.226, 0.571, 0.115, 0.178, 0.301]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1796 | Steps: 4 | Val loss: 1.6227 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=145792)[0m top1: 0.4295708955223881
[2m[36m(func pid=145792)[0m top5: 0.909981343283582
[2m[36m(func pid=145792)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=145792)[0m f1_macro: 0.36242199095560845
[2m[36m(func pid=145792)[0m f1_weighted: 0.4418588962437475
[2m[36m(func pid=145792)[0m f1_per_class: [0.566, 0.525, 0.329, 0.584, 0.142, 0.188, 0.413, 0.323, 0.197, 0.357]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3810634328358209
[2m[36m(func pid=152562)[0m top5: 0.8777985074626866
[2m[36m(func pid=152562)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=152562)[0m f1_macro: 0.33266493454596385
[2m[36m(func pid=152562)[0m f1_weighted: 0.3947624265448936
[2m[36m(func pid=152562)[0m f1_per_class: [0.619, 0.22, 0.579, 0.475, 0.139, 0.153, 0.559, 0.277, 0.164, 0.141]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.39225746268656714
[2m[36m(func pid=158650)[0m top5: 0.9197761194029851
[2m[36m(func pid=158650)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=158650)[0m f1_macro: 0.34062558552080324
[2m[36m(func pid=158650)[0m f1_weighted: 0.396607730733602
[2m[36m(func pid=158650)[0m f1_per_class: [0.548, 0.423, 0.358, 0.572, 0.246, 0.282, 0.295, 0.362, 0.193, 0.126]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.0360 | Steps: 4 | Val loss: 3.2514 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4592 | Steps: 4 | Val loss: 1.5747 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2574 | Steps: 4 | Val loss: 37.5075 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:40:51 (running for 00:33:08.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.351 |      0.362 |                   95 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.036 |      0.365 |                   81 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 12.141 |      0.333 |                   71 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.18  |      0.341 |                   46 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.4855410447761194
[2m[36m(func pid=150999)[0m top5: 0.8964552238805971
[2m[36m(func pid=150999)[0m f1_micro: 0.4855410447761194
[2m[36m(func pid=150999)[0m f1_macro: 0.36470786730319976
[2m[36m(func pid=150999)[0m f1_weighted: 0.44480743719449334
[2m[36m(func pid=150999)[0m f1_per_class: [0.512, 0.514, 0.71, 0.477, 0.183, 0.045, 0.617, 0.161, 0.18, 0.248]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.9945 | Steps: 4 | Val loss: 1.5781 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=145792)[0m top1: 0.4319029850746269
[2m[36m(func pid=145792)[0m top5: 0.9160447761194029
[2m[36m(func pid=145792)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=145792)[0m f1_macro: 0.3744841679604455
[2m[36m(func pid=145792)[0m f1_weighted: 0.4284173946796545
[2m[36m(func pid=145792)[0m f1_per_class: [0.606, 0.525, 0.406, 0.579, 0.168, 0.182, 0.364, 0.34, 0.227, 0.348]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3306902985074627
[2m[36m(func pid=152562)[0m top5: 0.8745335820895522
[2m[36m(func pid=152562)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=152562)[0m f1_macro: 0.3082568845490615
[2m[36m(func pid=152562)[0m f1_weighted: 0.3659387865023167
[2m[36m(func pid=152562)[0m f1_per_class: [0.496, 0.191, 0.647, 0.458, 0.119, 0.06, 0.538, 0.301, 0.114, 0.158]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=158650)[0m top1: 0.40531716417910446
[2m[36m(func pid=158650)[0m top5: 0.925839552238806
[2m[36m(func pid=158650)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=158650)[0m f1_macro: 0.3329956401102317
[2m[36m(func pid=158650)[0m f1_weighted: 0.40178729813517383
[2m[36m(func pid=158650)[0m f1_per_class: [0.537, 0.45, 0.343, 0.571, 0.194, 0.231, 0.319, 0.37, 0.18, 0.135]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.1897 | Steps: 4 | Val loss: 4.8886 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5372 | Steps: 4 | Val loss: 1.6541 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:40:56 (running for 00:33:14.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.459 |      0.374 |                   96 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.19  |      0.251 |                   82 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.257 |      0.308 |                   72 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.994 |      0.333 |                   47 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.2677238805970149
[2m[36m(func pid=150999)[0m top5: 0.7835820895522388
[2m[36m(func pid=150999)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=150999)[0m f1_macro: 0.2508441893165672
[2m[36m(func pid=150999)[0m f1_weighted: 0.2445264785538831
[2m[36m(func pid=150999)[0m f1_per_class: [0.145, 0.447, 0.375, 0.354, 0.157, 0.059, 0.09, 0.319, 0.177, 0.385]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 3.0288 | Steps: 4 | Val loss: 38.8943 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.9588 | Steps: 4 | Val loss: 1.5542 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=145792)[0m top1: 0.40904850746268656
[2m[36m(func pid=145792)[0m top5: 0.902518656716418
[2m[36m(func pid=145792)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=145792)[0m f1_macro: 0.35536539355744634
[2m[36m(func pid=145792)[0m f1_weighted: 0.411161003392188
[2m[36m(func pid=145792)[0m f1_per_class: [0.59, 0.506, 0.387, 0.566, 0.157, 0.201, 0.327, 0.353, 0.205, 0.263]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.2947761194029851
[2m[36m(func pid=152562)[0m top5: 0.8484141791044776
[2m[36m(func pid=152562)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=152562)[0m f1_macro: 0.26661410564363286
[2m[36m(func pid=152562)[0m f1_weighted: 0.31769683771555424
[2m[36m(func pid=152562)[0m f1_per_class: [0.376, 0.271, 0.629, 0.51, 0.126, 0.031, 0.307, 0.306, 0.11, 0.0]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.7307 | Steps: 4 | Val loss: 6.2096 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=158650)[0m top1: 0.41324626865671643
[2m[36m(func pid=158650)[0m top5: 0.9253731343283582
[2m[36m(func pid=158650)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=158650)[0m f1_macro: 0.33170016255764356
[2m[36m(func pid=158650)[0m f1_weighted: 0.412363296462194
[2m[36m(func pid=158650)[0m f1_per_class: [0.527, 0.454, 0.325, 0.575, 0.19, 0.19, 0.368, 0.351, 0.185, 0.151]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2990 | Steps: 4 | Val loss: 1.7037 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:41:02 (running for 00:33:19.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.537 |      0.355 |                   97 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.731 |      0.255 |                   83 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  3.029 |      0.267 |                   73 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.959 |      0.332 |                   48 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.27845149253731344
[2m[36m(func pid=150999)[0m top5: 0.6529850746268657
[2m[36m(func pid=150999)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=150999)[0m f1_macro: 0.25458211423834937
[2m[36m(func pid=150999)[0m f1_weighted: 0.23155524713763528
[2m[36m(func pid=150999)[0m f1_per_class: [0.187, 0.483, 0.375, 0.366, 0.15, 0.072, 0.006, 0.308, 0.199, 0.4]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9297 | Steps: 4 | Val loss: 32.7138 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.9193 | Steps: 4 | Val loss: 1.5619 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=145792)[0m top1: 0.39132462686567165
[2m[36m(func pid=145792)[0m top5: 0.9034514925373134
[2m[36m(func pid=145792)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=145792)[0m f1_macro: 0.34887153237538754
[2m[36m(func pid=145792)[0m f1_weighted: 0.4031460017489917
[2m[36m(func pid=145792)[0m f1_per_class: [0.552, 0.485, 0.393, 0.564, 0.182, 0.264, 0.298, 0.318, 0.226, 0.207]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.42630597014925375
[2m[36m(func pid=152562)[0m top5: 0.8796641791044776
[2m[36m(func pid=152562)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=152562)[0m f1_macro: 0.3443586731587766
[2m[36m(func pid=152562)[0m f1_weighted: 0.40590920686882626
[2m[36m(func pid=152562)[0m f1_per_class: [0.342, 0.39, 0.649, 0.585, 0.134, 0.015, 0.44, 0.361, 0.177, 0.35]
[2m[36m(func pid=152562)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.1626 | Steps: 4 | Val loss: 5.2052 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=158650)[0m top1: 0.4141791044776119
[2m[36m(func pid=158650)[0m top5: 0.9165111940298507
[2m[36m(func pid=158650)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=158650)[0m f1_macro: 0.33198989772599635
[2m[36m(func pid=158650)[0m f1_weighted: 0.4108710500565703
[2m[36m(func pid=158650)[0m f1_per_class: [0.526, 0.495, 0.306, 0.562, 0.2, 0.165, 0.361, 0.344, 0.19, 0.17]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3672 | Steps: 4 | Val loss: 1.7782 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=150999)[0m top1: 0.3521455223880597
[2m[36m(func pid=150999)[0m top5: 0.7033582089552238
[2m[36m(func pid=150999)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=150999)[0m f1_macro: 0.3669801036771453
[2m[36m(func pid=150999)[0m f1_weighted: 0.30027559862692393
[2m[36m(func pid=150999)[0m f1_per_class: [0.547, 0.502, 0.769, 0.487, 0.14, 0.241, 0.0, 0.385, 0.199, 0.4]
[2m[36m(func pid=150999)[0m 
== Status ==
Current time: 2024-01-07 11:41:07 (running for 00:33:25.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.359
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.299 |      0.349 |                   98 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.163 |      0.367 |                   84 |
| train_98a10_00015 | RUNNING    | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 |  0.93  |      0.344 |                   74 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.919 |      0.332 |                   49 |
| train_98a10_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=152562)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 26.1090 | Steps: 4 | Val loss: 35.3591 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8947 | Steps: 4 | Val loss: 1.5798 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=145792)[0m top1: 0.36613805970149255
[2m[36m(func pid=145792)[0m top5: 0.8899253731343284
[2m[36m(func pid=145792)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=145792)[0m f1_macro: 0.33847178654636245
[2m[36m(func pid=145792)[0m f1_weighted: 0.38372447987834785
[2m[36m(func pid=145792)[0m f1_per_class: [0.574, 0.474, 0.4, 0.517, 0.144, 0.238, 0.291, 0.325, 0.239, 0.183]
[2m[36m(func pid=145792)[0m 
[2m[36m(func pid=152562)[0m top1: 0.3833955223880597
[2m[36m(func pid=152562)[0m top5: 0.8684701492537313
[2m[36m(func pid=152562)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=152562)[0m f1_macro: 0.3265255837207725
[2m[36m(func pid=152562)[0m f1_weighted: 0.3640512293992264
[2m[36m(func pid=152562)[0m f1_per_class: [0.294, 0.452, 0.522, 0.562, 0.107, 0.108, 0.248, 0.382, 0.205, 0.386]
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.6730 | Steps: 4 | Val loss: 5.5209 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=158650)[0m top1: 0.4216417910447761
[2m[36m(func pid=158650)[0m top5: 0.909981343283582
[2m[36m(func pid=158650)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=158650)[0m f1_macro: 0.34086096530436755
[2m[36m(func pid=158650)[0m f1_weighted: 0.41709200284453307
[2m[36m(func pid=158650)[0m f1_per_class: [0.515, 0.516, 0.317, 0.555, 0.237, 0.145, 0.38, 0.345, 0.214, 0.184]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=145792)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3978 | Steps: 4 | Val loss: 1.8057 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=150999)[0m top1: 0.34468283582089554
[2m[36m(func pid=150999)[0m top5: 0.7588619402985075
[2m[36m(func pid=150999)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=150999)[0m f1_macro: 0.3177880244695604
[2m[36m(func pid=150999)[0m f1_weighted: 0.30128421737627264
[2m[36m(func pid=150999)[0m f1_per_class: [0.291, 0.497, 0.69, 0.519, 0.256, 0.302, 0.018, 0.145, 0.239, 0.221]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.1079 | Steps: 4 | Val loss: 1.5541 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=145792)[0m top1: 0.36240671641791045
[2m[36m(func pid=145792)[0m top5: 0.8894589552238806
[2m[36m(func pid=145792)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=145792)[0m f1_macro: 0.34322887323792783
[2m[36m(func pid=145792)[0m f1_weighted: 0.37435401060200274
[2m[36m(func pid=145792)[0m f1_per_class: [0.596, 0.517, 0.393, 0.452, 0.144, 0.238, 0.29, 0.324, 0.276, 0.203]
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2231 | Steps: 4 | Val loss: 4.9144 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=158650)[0m top1: 0.43050373134328357
[2m[36m(func pid=158650)[0m top5: 0.9127798507462687
[2m[36m(func pid=158650)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=158650)[0m f1_macro: 0.34495602399660547
[2m[36m(func pid=158650)[0m f1_weighted: 0.425208134767214
[2m[36m(func pid=158650)[0m f1_per_class: [0.507, 0.53, 0.333, 0.557, 0.196, 0.149, 0.395, 0.341, 0.242, 0.199]
[2m[36m(func pid=150999)[0m top1: 0.33115671641791045
[2m[36m(func pid=150999)[0m top5: 0.8115671641791045
[2m[36m(func pid=150999)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=150999)[0m f1_macro: 0.2655668073637046
[2m[36m(func pid=150999)[0m f1_weighted: 0.31023146393635853
[2m[36m(func pid=150999)[0m f1_per_class: [0.044, 0.501, 0.706, 0.547, 0.065, 0.14, 0.113, 0.106, 0.227, 0.206]
== Status ==
Current time: 2024-01-07 11:41:13 (running for 00:33:30.55)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.367 |      0.338 |                   99 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.673 |      0.318 |                   85 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.895 |      0.341 |                   50 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 11:41:19 (running for 00:33:36.82)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00013 | RUNNING    | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.367 |      0.338 |                   99 |
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.223 |      0.266 |                   86 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.895 |      0.341 |                   50 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=170962)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=170962)[0m Configuration completed!
[2m[36m(func pid=170962)[0m New optimizer parameters:
[2m[36m(func pid=170962)[0m SGD (
[2m[36m(func pid=170962)[0m Parameter Group 0
[2m[36m(func pid=170962)[0m     dampening: 0
[2m[36m(func pid=170962)[0m     differentiable: False
[2m[36m(func pid=170962)[0m     foreach: None
[2m[36m(func pid=170962)[0m     lr: 0.001
[2m[36m(func pid=170962)[0m     maximize: False
[2m[36m(func pid=170962)[0m     momentum: 0.99
[2m[36m(func pid=170962)[0m     nesterov: False
[2m[36m(func pid=170962)[0m     weight_decay: 1e-05
[2m[36m(func pid=170962)[0m )
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.1409 | Steps: 4 | Val loss: 4.3847 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7889 | Steps: 4 | Val loss: 1.5561 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0593 | Steps: 4 | Val loss: 2.4649 | Batch size: 32 | lr: 0.001 | Duration: 4.69s
[2m[36m(func pid=150999)[0m top1: 0.353544776119403
[2m[36m(func pid=150999)[0m top5: 0.8577425373134329
[2m[36m(func pid=150999)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=150999)[0m f1_macro: 0.2595731278966796
[2m[36m(func pid=150999)[0m f1_weighted: 0.3465790579998608
[2m[36m(func pid=150999)[0m f1_per_class: [0.087, 0.504, 0.49, 0.548, 0.061, 0.109, 0.242, 0.142, 0.196, 0.217]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4300373134328358
[2m[36m(func pid=158650)[0m top5: 0.9109141791044776
[2m[36m(func pid=158650)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=158650)[0m f1_macro: 0.3528373326411812
[2m[36m(func pid=158650)[0m f1_weighted: 0.41788368769495926
[2m[36m(func pid=158650)[0m f1_per_class: [0.551, 0.542, 0.333, 0.562, 0.23, 0.139, 0.359, 0.341, 0.224, 0.248]
[2m[36m(func pid=170962)[0m top1: 0.06436567164179105
[2m[36m(func pid=170962)[0m top5: 0.4608208955223881
[2m[36m(func pid=170962)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=170962)[0m f1_macro: 0.046717141301359635
[2m[36m(func pid=170962)[0m f1_weighted: 0.05545433979804699
[2m[36m(func pid=170962)[0m f1_per_class: [0.097, 0.065, 0.0, 0.115, 0.0, 0.031, 0.0, 0.085, 0.043, 0.03]
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.5997 | Steps: 4 | Val loss: 3.1339 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:41:24 (running for 00:33:42.33)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.141 |      0.26  |                   87 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  1.108 |      0.345 |                   51 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=171528)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=171528)[0m Configuration completed!
[2m[36m(func pid=171528)[0m New optimizer parameters:
[2m[36m(func pid=171528)[0m SGD (
[2m[36m(func pid=171528)[0m Parameter Group 0
[2m[36m(func pid=171528)[0m     dampening: 0
[2m[36m(func pid=171528)[0m     differentiable: False
[2m[36m(func pid=171528)[0m     foreach: None
[2m[36m(func pid=171528)[0m     lr: 0.01
[2m[36m(func pid=171528)[0m     maximize: False
[2m[36m(func pid=171528)[0m     momentum: 0.99
[2m[36m(func pid=171528)[0m     nesterov: False
[2m[36m(func pid=171528)[0m     weight_decay: 1e-05
[2m[36m(func pid=171528)[0m )
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.404384328358209
[2m[36m(func pid=150999)[0m top5: 0.8964552238805971
[2m[36m(func pid=150999)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=150999)[0m f1_macro: 0.3712032160584968
[2m[36m(func pid=150999)[0m f1_weighted: 0.4198607159898118
[2m[36m(func pid=150999)[0m f1_per_class: [0.693, 0.494, 0.387, 0.539, 0.185, 0.257, 0.37, 0.29, 0.195, 0.302]
[2m[36m(func pid=150999)[0m 
== Status ==
Current time: 2024-01-07 11:41:30 (running for 00:33:47.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.6   |      0.371 |                   88 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.789 |      0.353 |                   52 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  3.059 |      0.047 |                    1 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7639 | Steps: 4 | Val loss: 1.5560 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0745 | Steps: 4 | Val loss: 2.3626 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0939 | Steps: 4 | Val loss: 2.4005 | Batch size: 32 | lr: 0.01 | Duration: 4.68s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3035 | Steps: 4 | Val loss: 4.1739 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=158650)[0m top1: 0.43050373134328357
[2m[36m(func pid=158650)[0m top5: 0.9123134328358209
[2m[36m(func pid=158650)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=158650)[0m f1_macro: 0.3557566822398291
[2m[36m(func pid=158650)[0m f1_weighted: 0.4145900291779937
[2m[36m(func pid=158650)[0m f1_per_class: [0.571, 0.556, 0.321, 0.569, 0.209, 0.122, 0.335, 0.347, 0.229, 0.298]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m top1: 0.08675373134328358
[2m[36m(func pid=170962)[0m top5: 0.503731343283582
[2m[36m(func pid=170962)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=170962)[0m f1_macro: 0.056143340429938314
[2m[36m(func pid=170962)[0m f1_weighted: 0.08728906706723723
[2m[36m(func pid=170962)[0m f1_per_class: [0.083, 0.13, 0.0, 0.145, 0.0, 0.097, 0.029, 0.025, 0.053, 0.0]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:41:35 (running for 00:33:52.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.6   |      0.371 |                   88 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.764 |      0.356 |                   53 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  3.074 |      0.056 |                    2 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  3.094 |      0.075 |                    1 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.21595149253731344
[2m[36m(func pid=171528)[0m top5: 0.5093283582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=171528)[0m f1_macro: 0.0748474018135052
[2m[36m(func pid=171528)[0m f1_weighted: 0.13926624622777467
[2m[36m(func pid=171528)[0m f1_per_class: [0.192, 0.0, 0.061, 0.0, 0.023, 0.013, 0.444, 0.016, 0.0, 0.0]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.3003731343283582
[2m[36m(func pid=150999)[0m top5: 0.8372201492537313
[2m[36m(func pid=150999)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=150999)[0m f1_macro: 0.3375332241418287
[2m[36m(func pid=150999)[0m f1_weighted: 0.3274461090238539
[2m[36m(func pid=150999)[0m f1_per_class: [0.391, 0.41, 0.5, 0.333, 0.343, 0.292, 0.313, 0.254, 0.131, 0.409]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7563 | Steps: 4 | Val loss: 1.5615 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8785 | Steps: 4 | Val loss: 2.3634 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7095 | Steps: 4 | Val loss: 2.1578 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.4095 | Steps: 4 | Val loss: 4.5776 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170962)[0m top1: 0.11007462686567164
[2m[36m(func pid=170962)[0m top5: 0.5424440298507462
[2m[36m(func pid=170962)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=170962)[0m f1_macro: 0.06623727192238696
[2m[36m(func pid=170962)[0m f1_weighted: 0.11255045495306709
[2m[36m(func pid=170962)[0m f1_per_class: [0.114, 0.052, 0.025, 0.016, 0.0, 0.109, 0.275, 0.0, 0.073, 0.0]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.42677238805970147
[2m[36m(func pid=158650)[0m top5: 0.9146455223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=158650)[0m f1_macro: 0.36342845327089723
[2m[36m(func pid=158650)[0m f1_weighted: 0.4080293141372425
[2m[36m(func pid=158650)[0m f1_per_class: [0.565, 0.56, 0.387, 0.553, 0.242, 0.125, 0.322, 0.349, 0.233, 0.299]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:41:40 (running for 00:33:58.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.304 |      0.338 |                   89 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.756 |      0.363 |                   54 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  2.878 |      0.066 |                    3 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.71  |      0.199 |                    2 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.1259328358208955
[2m[36m(func pid=171528)[0m top5: 0.7346082089552238
[2m[36m(func pid=171528)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=171528)[0m f1_macro: 0.1985427665549657
[2m[36m(func pid=171528)[0m f1_weighted: 0.09615826786740435
[2m[36m(func pid=171528)[0m f1_per_class: [0.418, 0.031, 0.769, 0.102, 0.094, 0.018, 0.099, 0.165, 0.17, 0.117]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.28638059701492535
[2m[36m(func pid=150999)[0m top5: 0.7943097014925373
[2m[36m(func pid=150999)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=150999)[0m f1_macro: 0.2678119118316514
[2m[36m(func pid=150999)[0m f1_weighted: 0.3116381132809975
[2m[36m(func pid=150999)[0m f1_per_class: [0.255, 0.381, 0.202, 0.217, 0.258, 0.247, 0.427, 0.238, 0.12, 0.333]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6829 | Steps: 4 | Val loss: 1.5695 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8242 | Steps: 4 | Val loss: 2.3715 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6133 | Steps: 4 | Val loss: 1.8124 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.1655 | Steps: 4 | Val loss: 3.2340 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=170962)[0m top1: 0.1921641791044776
[2m[36m(func pid=170962)[0m top5: 0.5615671641791045
[2m[36m(func pid=170962)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=170962)[0m f1_macro: 0.0763956579291774
[2m[36m(func pid=170962)[0m f1_weighted: 0.14567854684721054
[2m[36m(func pid=170962)[0m f1_per_class: [0.175, 0.011, 0.031, 0.007, 0.0, 0.041, 0.442, 0.0, 0.057, 0.0]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4281716417910448
[2m[36m(func pid=158650)[0m top5: 0.914179104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.4281716417910448
[2m[36m(func pid=158650)[0m f1_macro: 0.3701010635719301
[2m[36m(func pid=158650)[0m f1_weighted: 0.4175005730171842
[2m[36m(func pid=158650)[0m f1_per_class: [0.567, 0.568, 0.375, 0.546, 0.235, 0.187, 0.331, 0.358, 0.224, 0.31]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:41:46 (running for 00:34:03.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.41  |      0.268 |                   90 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.683 |      0.37  |                   55 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  2.824 |      0.076 |                    4 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.613 |      0.368 |                    3 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.37966417910447764
[2m[36m(func pid=171528)[0m top5: 0.8591417910447762
[2m[36m(func pid=171528)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=171528)[0m f1_macro: 0.36836213632828146
[2m[36m(func pid=171528)[0m f1_weighted: 0.34598044713290904
[2m[36m(func pid=171528)[0m f1_per_class: [0.512, 0.581, 0.7, 0.521, 0.212, 0.264, 0.082, 0.366, 0.161, 0.286]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.42024253731343286
[2m[36m(func pid=150999)[0m top5: 0.8745335820895522
[2m[36m(func pid=150999)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=150999)[0m f1_macro: 0.33996368501891305
[2m[36m(func pid=150999)[0m f1_weighted: 0.4343596812332864
[2m[36m(func pid=150999)[0m f1_per_class: [0.472, 0.463, 0.222, 0.448, 0.316, 0.269, 0.556, 0.191, 0.163, 0.3]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.8107 | Steps: 4 | Val loss: 1.5736 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.4944 | Steps: 4 | Val loss: 2.2239 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.5207 | Steps: 4 | Val loss: 2.2770 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.7837 | Steps: 4 | Val loss: 2.6505 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=158650)[0m top1: 0.4230410447761194
[2m[36m(func pid=158650)[0m top5: 0.9146455223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=158650)[0m f1_macro: 0.3742615285870876
[2m[36m(func pid=158650)[0m f1_weighted: 0.4169828316306213
[2m[36m(func pid=158650)[0m f1_per_class: [0.555, 0.564, 0.393, 0.547, 0.228, 0.225, 0.317, 0.344, 0.241, 0.329]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m top1: 0.28777985074626866
[2m[36m(func pid=170962)[0m top5: 0.5834888059701493
[2m[36m(func pid=170962)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=170962)[0m f1_macro: 0.1334010646512141
[2m[36m(func pid=170962)[0m f1_weighted: 0.19158719693634943
[2m[36m(func pid=170962)[0m f1_per_class: [0.263, 0.098, 0.086, 0.013, 0.076, 0.015, 0.531, 0.0, 0.074, 0.178]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m top1: 0.2691231343283582
[2m[36m(func pid=171528)[0m top5: 0.8199626865671642
[2m[36m(func pid=171528)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=171528)[0m f1_macro: 0.21560809355037228
[2m[36m(func pid=171528)[0m f1_weighted: 0.31257010777774763
[2m[36m(func pid=171528)[0m f1_per_class: [0.462, 0.156, 0.046, 0.489, 0.07, 0.058, 0.357, 0.364, 0.156, 0.0]
== Status ==
Current time: 2024-01-07 11:41:51 (running for 00:34:08.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.165 |      0.34  |                   91 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.374 |                   56 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  2.494 |      0.133 |                    5 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.521 |      0.216 |                    4 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.4766791044776119
[2m[36m(func pid=150999)[0m top5: 0.9277052238805971
[2m[36m(func pid=150999)[0m f1_micro: 0.4766791044776119
[2m[36m(func pid=150999)[0m f1_macro: 0.3929778581005149
[2m[36m(func pid=150999)[0m f1_weighted: 0.4839774114952929
[2m[36m(func pid=150999)[0m f1_per_class: [0.639, 0.557, 0.3, 0.548, 0.264, 0.265, 0.552, 0.216, 0.197, 0.392]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.2144 | Steps: 4 | Val loss: 2.1405 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6803 | Steps: 4 | Val loss: 1.6174 | Batch size: 32 | lr: 0.0001 | Duration: 3.31s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.1882 | Steps: 4 | Val loss: 2.5021 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.0492 | Steps: 4 | Val loss: 4.0284 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=170962)[0m top1: 0.21921641791044777
[2m[36m(func pid=170962)[0m top5: 0.71875
[2m[36m(func pid=170962)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=170962)[0m f1_macro: 0.20048057794454627
[2m[36m(func pid=170962)[0m f1_weighted: 0.23435188558182657
[2m[36m(func pid=170962)[0m f1_per_class: [0.392, 0.14, 0.632, 0.15, 0.058, 0.016, 0.509, 0.0, 0.068, 0.041]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.404384328358209
[2m[36m(func pid=158650)[0m top5: 0.9095149253731343
[2m[36m(func pid=158650)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=158650)[0m f1_macro: 0.35661078928899564
[2m[36m(func pid=158650)[0m f1_weighted: 0.40376479325028586
[2m[36m(func pid=158650)[0m f1_per_class: [0.534, 0.563, 0.338, 0.52, 0.194, 0.236, 0.3, 0.346, 0.226, 0.31]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:41:56 (running for 00:34:14.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.784 |      0.393 |                   92 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.68  |      0.357 |                   57 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  2.214 |      0.2   |                    6 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.188 |      0.272 |                    5 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.3591417910447761
[2m[36m(func pid=171528)[0m top5: 0.835820895522388
[2m[36m(func pid=171528)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=171528)[0m f1_macro: 0.2720106274131423
[2m[36m(func pid=171528)[0m f1_weighted: 0.27295704734711107
[2m[36m(func pid=171528)[0m f1_per_class: [0.285, 0.016, 0.364, 0.555, 0.364, 0.083, 0.22, 0.387, 0.094, 0.353]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.376865671641791
[2m[36m(func pid=150999)[0m top5: 0.9351679104477612
[2m[36m(func pid=150999)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=150999)[0m f1_macro: 0.3587481825441382
[2m[36m(func pid=150999)[0m f1_weighted: 0.3570790782045745
[2m[36m(func pid=150999)[0m f1_per_class: [0.579, 0.444, 0.593, 0.38, 0.145, 0.165, 0.365, 0.314, 0.25, 0.353]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.2236 | Steps: 4 | Val loss: 2.2343 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4796 | Steps: 4 | Val loss: 1.5956 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.9867 | Steps: 4 | Val loss: 2.6035 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.6161 | Steps: 4 | Val loss: 3.5680 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=170962)[0m top1: 0.11753731343283583
[2m[36m(func pid=170962)[0m top5: 0.7327425373134329
[2m[36m(func pid=170962)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=170962)[0m f1_macro: 0.21618687158564948
[2m[36m(func pid=170962)[0m f1_weighted: 0.14209831304886386
[2m[36m(func pid=170962)[0m f1_per_class: [0.519, 0.072, 0.833, 0.245, 0.065, 0.016, 0.088, 0.291, 0.0, 0.033]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.41044776119402987
[2m[36m(func pid=158650)[0m top5: 0.9197761194029851
[2m[36m(func pid=158650)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=158650)[0m f1_macro: 0.37186117249101114
[2m[36m(func pid=158650)[0m f1_weighted: 0.41656078424747967
[2m[36m(func pid=158650)[0m f1_per_class: [0.544, 0.549, 0.393, 0.532, 0.167, 0.249, 0.329, 0.344, 0.248, 0.364]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:42:02 (running for 00:34:19.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.049 |      0.359 |                   93 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.48  |      0.372 |                   58 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  2.224 |      0.216 |                    7 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.987 |      0.356 |                    6 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.310634328358209
[2m[36m(func pid=171528)[0m top5: 0.8843283582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=171528)[0m f1_macro: 0.35637358212979375
[2m[36m(func pid=171528)[0m f1_weighted: 0.3295166786724181
[2m[36m(func pid=171528)[0m f1_per_class: [0.638, 0.126, 0.815, 0.548, 0.222, 0.337, 0.224, 0.392, 0.2, 0.062]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.416044776119403
[2m[36m(func pid=150999)[0m top5: 0.9272388059701493
[2m[36m(func pid=150999)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=150999)[0m f1_macro: 0.4003882799081186
[2m[36m(func pid=150999)[0m f1_weighted: 0.40175884362701697
[2m[36m(func pid=150999)[0m f1_per_class: [0.623, 0.489, 0.609, 0.466, 0.186, 0.189, 0.387, 0.331, 0.25, 0.475]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.8721 | Steps: 4 | Val loss: 2.1979 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6441 | Steps: 4 | Val loss: 1.5932 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.4057 | Steps: 4 | Val loss: 3.2156 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2616 | Steps: 4 | Val loss: 3.5516 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=170962)[0m top1: 0.22667910447761194
[2m[36m(func pid=170962)[0m top5: 0.6930970149253731
[2m[36m(func pid=170962)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=170962)[0m f1_macro: 0.24574932580542722
[2m[36m(func pid=170962)[0m f1_weighted: 0.21399120808221045
[2m[36m(func pid=170962)[0m f1_per_class: [0.418, 0.361, 0.786, 0.433, 0.09, 0.02, 0.0, 0.211, 0.048, 0.09]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:42:07 (running for 00:34:24.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.616 |      0.4   |                   94 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.48  |      0.372 |                   58 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.872 |      0.246 |                    8 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.406 |      0.389 |                    7 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.39505597014925375
[2m[36m(func pid=171528)[0m top5: 0.8381529850746269
[2m[36m(func pid=171528)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=171528)[0m f1_macro: 0.38902166060481536
[2m[36m(func pid=171528)[0m f1_weighted: 0.3387200734264354
[2m[36m(func pid=171528)[0m f1_per_class: [0.648, 0.503, 0.667, 0.023, 0.218, 0.298, 0.527, 0.368, 0.321, 0.318]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4006529850746269
[2m[36m(func pid=158650)[0m top5: 0.9193097014925373
[2m[36m(func pid=158650)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=158650)[0m f1_macro: 0.3743384654086667
[2m[36m(func pid=158650)[0m f1_weighted: 0.4106184781536733
[2m[36m(func pid=158650)[0m f1_per_class: [0.567, 0.542, 0.453, 0.519, 0.173, 0.221, 0.338, 0.34, 0.218, 0.375]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m top1: 0.37779850746268656
[2m[36m(func pid=150999)[0m top5: 0.8871268656716418
[2m[36m(func pid=150999)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=150999)[0m f1_macro: 0.36798348601822883
[2m[36m(func pid=150999)[0m f1_weighted: 0.3997942711968488
[2m[36m(func pid=150999)[0m f1_per_class: [0.667, 0.36, 0.667, 0.586, 0.154, 0.213, 0.343, 0.342, 0.234, 0.114]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5823 | Steps: 4 | Val loss: 2.1290 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1038 | Steps: 4 | Val loss: 4.1007 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4972 | Steps: 4 | Val loss: 1.6109 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.4451 | Steps: 4 | Val loss: 4.4000 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=170962)[0m top1: 0.31902985074626866
[2m[36m(func pid=170962)[0m top5: 0.7504664179104478
[2m[36m(func pid=170962)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=170962)[0m f1_macro: 0.3490436003648384
[2m[36m(func pid=170962)[0m f1_weighted: 0.28239224710457056
[2m[36m(func pid=170962)[0m f1_per_class: [0.611, 0.58, 0.733, 0.439, 0.24, 0.189, 0.0, 0.216, 0.071, 0.41]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:42:12 (running for 00:34:30.34)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.262 |      0.368 |                   95 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.644 |      0.374 |                   59 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.582 |      0.349 |                    9 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.104 |      0.37  |                    8 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.4221082089552239
[2m[36m(func pid=171528)[0m top5: 0.8246268656716418
[2m[36m(func pid=171528)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=171528)[0m f1_macro: 0.3695734836744621
[2m[36m(func pid=171528)[0m f1_weighted: 0.33424974901991256
[2m[36m(func pid=171528)[0m f1_per_class: [0.676, 0.514, 0.88, 0.055, 0.131, 0.0, 0.611, 0.301, 0.218, 0.311]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3983208955223881
[2m[36m(func pid=158650)[0m top5: 0.9202425373134329
[2m[36m(func pid=158650)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=158650)[0m f1_macro: 0.3771782267265297
[2m[36m(func pid=158650)[0m f1_weighted: 0.4096332641346807
[2m[36m(func pid=158650)[0m f1_per_class: [0.581, 0.554, 0.462, 0.51, 0.143, 0.217, 0.33, 0.356, 0.244, 0.375]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=150999)[0m top1: 0.33255597014925375
[2m[36m(func pid=150999)[0m top5: 0.840018656716418
[2m[36m(func pid=150999)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=150999)[0m f1_macro: 0.31319418452761544
[2m[36m(func pid=150999)[0m f1_weighted: 0.3515467966099306
[2m[36m(func pid=150999)[0m f1_per_class: [0.635, 0.104, 0.571, 0.606, 0.058, 0.14, 0.351, 0.324, 0.209, 0.134]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.4807 | Steps: 4 | Val loss: 1.9166 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.6291 | Steps: 4 | Val loss: 5.4052 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4745 | Steps: 4 | Val loss: 1.6201 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3451 | Steps: 4 | Val loss: 4.6355 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=170962)[0m top1: 0.3880597014925373
[2m[36m(func pid=170962)[0m top5: 0.800839552238806
[2m[36m(func pid=170962)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=170962)[0m f1_macro: 0.37751665584583466
[2m[36m(func pid=170962)[0m f1_weighted: 0.3377229639671803
[2m[36m(func pid=170962)[0m f1_per_class: [0.632, 0.596, 0.595, 0.545, 0.255, 0.278, 0.009, 0.345, 0.199, 0.323]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:42:18 (running for 00:34:35.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.445 |      0.313 |                   96 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.497 |      0.377 |                   60 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.481 |      0.378 |                   10 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.629 |      0.291 |                    9 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.24113805970149255
[2m[36m(func pid=171528)[0m top5: 0.8596082089552238
[2m[36m(func pid=171528)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=171528)[0m f1_macro: 0.29107095489773854
[2m[36m(func pid=171528)[0m f1_weighted: 0.2720949817955781
[2m[36m(func pid=171528)[0m f1_per_class: [0.5, 0.165, 0.615, 0.501, 0.035, 0.036, 0.188, 0.309, 0.235, 0.326]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.3619402985074627
[2m[36m(func pid=150999)[0m top5: 0.8316231343283582
[2m[36m(func pid=150999)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=150999)[0m f1_macro: 0.31864368081509464
[2m[36m(func pid=150999)[0m f1_weighted: 0.3796526747249192
[2m[36m(func pid=150999)[0m f1_per_class: [0.554, 0.052, 0.609, 0.609, 0.044, 0.064, 0.517, 0.239, 0.208, 0.291]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.38899253731343286
[2m[36m(func pid=158650)[0m top5: 0.9174440298507462
[2m[36m(func pid=158650)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.3824992240691807
[2m[36m(func pid=158650)[0m f1_weighted: 0.4042209661584394
[2m[36m(func pid=158650)[0m f1_per_class: [0.579, 0.533, 0.5, 0.515, 0.134, 0.223, 0.316, 0.36, 0.22, 0.444]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.3280 | Steps: 4 | Val loss: 1.6439 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.4100 | Steps: 4 | Val loss: 4.8551 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.1679 | Steps: 4 | Val loss: 3.9523 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5610 | Steps: 4 | Val loss: 1.6877 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 11:42:23 (running for 00:34:40.78)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.345 |      0.319 |                   97 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.475 |      0.382 |                   61 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.328 |      0.386 |                   11 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.629 |      0.291 |                    9 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.408115671641791
[2m[36m(func pid=170962)[0m top5: 0.8908582089552238
[2m[36m(func pid=170962)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=170962)[0m f1_macro: 0.3864863403181239
[2m[36m(func pid=170962)[0m f1_weighted: 0.3791315152550841
[2m[36m(func pid=170962)[0m f1_per_class: [0.624, 0.537, 0.5, 0.575, 0.269, 0.31, 0.138, 0.372, 0.19, 0.35]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3414179104477612
[2m[36m(func pid=171528)[0m top5: 0.8218283582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=171528)[0m f1_macro: 0.2887385695566706
[2m[36m(func pid=171528)[0m f1_weighted: 0.3091776586363509
[2m[36m(func pid=171528)[0m f1_per_class: [0.529, 0.248, 0.123, 0.626, 0.202, 0.269, 0.069, 0.327, 0.139, 0.356]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.39225746268656714
[2m[36m(func pid=150999)[0m top5: 0.8414179104477612
[2m[36m(func pid=150999)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=150999)[0m f1_macro: 0.30771859478673935
[2m[36m(func pid=150999)[0m f1_weighted: 0.383691055829077
[2m[36m(func pid=150999)[0m f1_per_class: [0.459, 0.061, 0.407, 0.614, 0.082, 0.099, 0.512, 0.269, 0.149, 0.424]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.37080223880597013
[2m[36m(func pid=158650)[0m top5: 0.9067164179104478
[2m[36m(func pid=158650)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=158650)[0m f1_macro: 0.36977994246469376
[2m[36m(func pid=158650)[0m f1_weighted: 0.3839006388007511
[2m[36m(func pid=158650)[0m f1_per_class: [0.59, 0.513, 0.5, 0.522, 0.125, 0.226, 0.254, 0.355, 0.212, 0.4]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.1697 | Steps: 4 | Val loss: 1.5058 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8489 | Steps: 4 | Val loss: 6.1509 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.0160 | Steps: 4 | Val loss: 4.0636 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5661 | Steps: 4 | Val loss: 1.7087 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:42:28 (running for 00:34:46.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  1.168 |      0.308 |                   98 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.561 |      0.37  |                   62 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.17  |      0.38  |                   12 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.41  |      0.289 |                   10 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.44822761194029853
[2m[36m(func pid=170962)[0m top5: 0.9388992537313433
[2m[36m(func pid=170962)[0m f1_micro: 0.44822761194029853
[2m[36m(func pid=170962)[0m f1_macro: 0.3800703879266655
[2m[36m(func pid=170962)[0m f1_weighted: 0.4631511157580312
[2m[36m(func pid=170962)[0m f1_per_class: [0.6, 0.481, 0.343, 0.597, 0.235, 0.315, 0.45, 0.324, 0.162, 0.293]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m top1: 0.30597014925373134
[2m[36m(func pid=171528)[0m top5: 0.7388059701492538
[2m[36m(func pid=171528)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=171528)[0m f1_macro: 0.2553626047930056
[2m[36m(func pid=171528)[0m f1_weighted: 0.29432177450116226
[2m[36m(func pid=171528)[0m f1_per_class: [0.34, 0.241, 0.074, 0.55, 0.286, 0.348, 0.072, 0.417, 0.092, 0.134]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=150999)[0m top1: 0.3941231343283582
[2m[36m(func pid=150999)[0m top5: 0.8880597014925373
[2m[36m(func pid=150999)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=150999)[0m f1_macro: 0.31634286410802637
[2m[36m(func pid=150999)[0m f1_weighted: 0.34504620863446617
[2m[36m(func pid=150999)[0m f1_per_class: [0.443, 0.237, 0.321, 0.57, 0.197, 0.111, 0.295, 0.367, 0.163, 0.458]
[2m[36m(func pid=150999)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3605410447761194
[2m[36m(func pid=158650)[0m top5: 0.9085820895522388
[2m[36m(func pid=158650)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=158650)[0m f1_macro: 0.362275907131256
[2m[36m(func pid=158650)[0m f1_weighted: 0.378324521412322
[2m[36m(func pid=158650)[0m f1_per_class: [0.561, 0.473, 0.462, 0.529, 0.111, 0.228, 0.253, 0.355, 0.21, 0.441]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8653 | Steps: 4 | Val loss: 1.6283 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.3994 | Steps: 4 | Val loss: 5.8269 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=150999)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2963 | Steps: 4 | Val loss: 4.2401 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6808 | Steps: 4 | Val loss: 1.7051 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=171528)[0m top1: 0.34794776119402987
[2m[36m(func pid=171528)[0m top5: 0.7905783582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=171528)[0m f1_macro: 0.2645475232695342
[2m[36m(func pid=171528)[0m f1_weighted: 0.3304176536286557
[2m[36m(func pid=171528)[0m f1_per_class: [0.327, 0.354, 0.19, 0.588, 0.19, 0.344, 0.118, 0.284, 0.108, 0.142]
[2m[36m(func pid=171528)[0m 
== Status ==
Current time: 2024-01-07 11:42:34 (running for 00:34:51.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00014 | RUNNING    | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.016 |      0.316 |                   99 |
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.566 |      0.362 |                   63 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.17  |      0.38  |                   12 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.399 |      0.265 |                   12 |
| train_98a10_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=150999)[0m top1: 0.37919776119402987
[2m[36m(func pid=150999)[0m top5: 0.8708022388059702
[2m[36m(func pid=150999)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=150999)[0m f1_macro: 0.3210098190838138
[2m[36m(func pid=150999)[0m f1_weighted: 0.3210491993689637
[2m[36m(func pid=150999)[0m f1_per_class: [0.503, 0.39, 0.271, 0.544, 0.286, 0.108, 0.151, 0.354, 0.155, 0.449]
[2m[36m(func pid=170962)[0m top1: 0.41324626865671643
[2m[36m(func pid=170962)[0m top5: 0.9123134328358209
[2m[36m(func pid=170962)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=170962)[0m f1_macro: 0.3160948788011958
[2m[36m(func pid=170962)[0m f1_weighted: 0.4425179588548536
[2m[36m(func pid=170962)[0m f1_per_class: [0.571, 0.387, 0.292, 0.536, 0.186, 0.232, 0.575, 0.127, 0.153, 0.102]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3572761194029851
[2m[36m(func pid=158650)[0m top5: 0.9146455223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=158650)[0m f1_macro: 0.365482139164864
[2m[36m(func pid=158650)[0m f1_weighted: 0.37379501274553817
[2m[36m(func pid=158650)[0m f1_per_class: [0.603, 0.467, 0.49, 0.536, 0.098, 0.207, 0.237, 0.361, 0.232, 0.423]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.4487 | Steps: 4 | Val loss: 4.4314 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.1266 | Steps: 4 | Val loss: 1.8482 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5627 | Steps: 4 | Val loss: 1.7990 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=170962)[0m top1: 0.36380597014925375
[2m[36m(func pid=170962)[0m top5: 0.8819962686567164
[2m[36m(func pid=170962)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=170962)[0m f1_macro: 0.2801080311939671
[2m[36m(func pid=170962)[0m f1_weighted: 0.3983107568428801
[2m[36m(func pid=170962)[0m f1_per_class: [0.569, 0.276, 0.276, 0.472, 0.17, 0.142, 0.599, 0.06, 0.167, 0.071]
[2m[36m(func pid=171528)[0m top1: 0.40205223880597013
[2m[36m(func pid=171528)[0m top5: 0.8694029850746269
[2m[36m(func pid=171528)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=171528)[0m f1_macro: 0.33463912175190835
[2m[36m(func pid=171528)[0m f1_weighted: 0.41128964963045644
[2m[36m(func pid=171528)[0m f1_per_class: [0.483, 0.481, 0.317, 0.573, 0.207, 0.209, 0.347, 0.351, 0.15, 0.227]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.34794776119402987
[2m[36m(func pid=158650)[0m top5: 0.894589552238806
[2m[36m(func pid=158650)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=158650)[0m f1_macro: 0.3551855703644488
[2m[36m(func pid=158650)[0m f1_weighted: 0.3602925418255273
[2m[36m(func pid=158650)[0m f1_per_class: [0.576, 0.503, 0.462, 0.513, 0.093, 0.19, 0.204, 0.351, 0.223, 0.436]
== Status ==
Current time: 2024-01-07 11:42:40 (running for 00:34:57.53)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.681 |      0.365 |                   64 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.865 |      0.316 |                   13 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  3.449 |      0.335 |                   13 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2852 | Steps: 4 | Val loss: 6.2485 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=174857)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=174857)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=174857)[0m Configuration completed!
[2m[36m(func pid=174857)[0m New optimizer parameters:
[2m[36m(func pid=174857)[0m SGD (
[2m[36m(func pid=174857)[0m Parameter Group 0
[2m[36m(func pid=174857)[0m     dampening: 0
[2m[36m(func pid=174857)[0m     differentiable: False
[2m[36m(func pid=174857)[0m     foreach: None
[2m[36m(func pid=174857)[0m     lr: 0.1
[2m[36m(func pid=174857)[0m     maximize: False
[2m[36m(func pid=174857)[0m     momentum: 0.99
[2m[36m(func pid=174857)[0m     nesterov: False
[2m[36m(func pid=174857)[0m     weight_decay: 1e-05
[2m[36m(func pid=174857)[0m )
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:42:45 (running for 00:35:02.84)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.563 |      0.355 |                   65 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.127 |      0.28  |                   14 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.285 |      0.351 |                   14 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.34841417910447764
[2m[36m(func pid=171528)[0m top5: 0.851679104477612
[2m[36m(func pid=171528)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=171528)[0m f1_macro: 0.35095248850626504
[2m[36m(func pid=171528)[0m f1_weighted: 0.3832634812628786
[2m[36m(func pid=171528)[0m f1_per_class: [0.609, 0.462, 0.585, 0.564, 0.211, 0.0, 0.341, 0.357, 0.104, 0.278]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4498 | Steps: 4 | Val loss: 1.7724 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8010 | Steps: 4 | Val loss: 1.8858 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.6967 | Steps: 4 | Val loss: 18.8462 | Batch size: 32 | lr: 0.1 | Duration: 4.66s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7095 | Steps: 4 | Val loss: 6.5802 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170962)[0m top1: 0.36986940298507465
[2m[36m(func pid=170962)[0m top5: 0.8768656716417911
[2m[36m(func pid=170962)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=170962)[0m f1_macro: 0.311571780393527
[2m[36m(func pid=170962)[0m f1_weighted: 0.40875643042997883
[2m[36m(func pid=170962)[0m f1_per_class: [0.561, 0.277, 0.353, 0.479, 0.165, 0.123, 0.582, 0.326, 0.145, 0.103]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.355410447761194
[2m[36m(func pid=158650)[0m top5: 0.9011194029850746
[2m[36m(func pid=158650)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=158650)[0m f1_macro: 0.3591984589567728
[2m[36m(func pid=158650)[0m f1_weighted: 0.37241243994171425
[2m[36m(func pid=158650)[0m f1_per_class: [0.581, 0.499, 0.462, 0.528, 0.089, 0.19, 0.231, 0.356, 0.23, 0.426]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m top1: 0.22621268656716417
[2m[36m(func pid=174857)[0m top5: 0.6627798507462687
[2m[36m(func pid=174857)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=174857)[0m f1_macro: 0.06597434250017345
[2m[36m(func pid=174857)[0m f1_weighted: 0.0979936712547914
[2m[36m(func pid=174857)[0m f1_per_class: [0.0, 0.391, 0.0, 0.0, 0.0, 0.269, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:42:50 (running for 00:35:08.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.45  |      0.359 |                   66 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.801 |      0.312 |                   15 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.71  |      0.383 |                   15 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  4.697 |      0.066 |                    1 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.42630597014925375
[2m[36m(func pid=171528)[0m top5: 0.835820895522388
[2m[36m(func pid=171528)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=171528)[0m f1_macro: 0.3833413383568606
[2m[36m(func pid=171528)[0m f1_weighted: 0.4254697039287859
[2m[36m(func pid=171528)[0m f1_per_class: [0.6, 0.576, 0.733, 0.567, 0.09, 0.0, 0.403, 0.344, 0.218, 0.303]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7683 | Steps: 4 | Val loss: 1.8671 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6423 | Steps: 4 | Val loss: 1.8333 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 40.6234 | Steps: 4 | Val loss: 45.1358 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5879 | Steps: 4 | Val loss: 8.4636 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=170962)[0m top1: 0.37173507462686567
[2m[36m(func pid=170962)[0m top5: 0.8666044776119403
[2m[36m(func pid=170962)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=170962)[0m f1_macro: 0.3207722993416592
[2m[36m(func pid=170962)[0m f1_weighted: 0.40862587397668904
[2m[36m(func pid=170962)[0m f1_per_class: [0.516, 0.386, 0.316, 0.52, 0.148, 0.12, 0.476, 0.363, 0.125, 0.237]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.33722014925373134
[2m[36m(func pid=158650)[0m top5: 0.8852611940298507
[2m[36m(func pid=158650)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=158650)[0m f1_macro: 0.3443103509901726
[2m[36m(func pid=158650)[0m f1_weighted: 0.3600161106855217
[2m[36m(func pid=158650)[0m f1_per_class: [0.547, 0.472, 0.444, 0.507, 0.08, 0.169, 0.235, 0.362, 0.237, 0.39]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m top1: 0.29617537313432835
[2m[36m(func pid=174857)[0m top5: 0.5625
[2m[36m(func pid=174857)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=174857)[0m f1_macro: 0.09238024263323526
[2m[36m(func pid=174857)[0m f1_weighted: 0.14873153936311176
[2m[36m(func pid=174857)[0m f1_per_class: [0.085, 0.0, 0.0, 0.445, 0.0, 0.0, 0.0, 0.393, 0.0, 0.0]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.37080223880597013
[2m[36m(func pid=171528)[0m top5: 0.8083022388059702
[2m[36m(func pid=171528)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=171528)[0m f1_macro: 0.3696864017590059
[2m[36m(func pid=171528)[0m f1_weighted: 0.36863910812747214
[2m[36m(func pid=171528)[0m f1_per_class: [0.598, 0.554, 0.786, 0.558, 0.053, 0.0, 0.224, 0.382, 0.229, 0.312]
[2m[36m(func pid=171528)[0m 
== Status ==
Current time: 2024-01-07 11:42:55 (running for 00:35:13.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.642 |      0.344 |                   67 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.768 |      0.321 |                   16 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.588 |      0.37  |                   16 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 40.623 |      0.092 |                    2 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4231 | Steps: 4 | Val loss: 1.9045 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5668 | Steps: 4 | Val loss: 1.7606 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 56.5553 | Steps: 4 | Val loss: 55.3535 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7553 | Steps: 4 | Val loss: 10.4929 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=170962)[0m top1: 0.36427238805970147
[2m[36m(func pid=170962)[0m top5: 0.8549440298507462
[2m[36m(func pid=170962)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=170962)[0m f1_macro: 0.34849860894636286
[2m[36m(func pid=170962)[0m f1_weighted: 0.3751360590868864
[2m[36m(func pid=170962)[0m f1_per_class: [0.587, 0.508, 0.369, 0.504, 0.139, 0.144, 0.285, 0.348, 0.156, 0.444]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.3516791044776119
[2m[36m(func pid=158650)[0m top5: 0.8959888059701493
[2m[36m(func pid=158650)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=158650)[0m f1_macro: 0.3499151398742014
[2m[36m(func pid=158650)[0m f1_weighted: 0.3742449469220226
[2m[36m(func pid=158650)[0m f1_per_class: [0.551, 0.478, 0.462, 0.513, 0.09, 0.186, 0.265, 0.371, 0.26, 0.323]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m top1: 0.13619402985074627
[2m[36m(func pid=174857)[0m top5: 0.375
[2m[36m(func pid=174857)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=174857)[0m f1_macro: 0.11045186024347349
[2m[36m(func pid=174857)[0m f1_weighted: 0.0959309103730631
[2m[36m(func pid=174857)[0m f1_per_class: [0.306, 0.414, 0.026, 0.0, 0.0, 0.0, 0.0, 0.257, 0.101, 0.0]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:43:01 (running for 00:35:18.47)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.567 |      0.35  |                   68 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.423 |      0.348 |                   17 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.755 |      0.333 |                   17 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 56.555 |      0.11  |                    3 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.2905783582089552
[2m[36m(func pid=171528)[0m top5: 0.7863805970149254
[2m[36m(func pid=171528)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=171528)[0m f1_macro: 0.3325932594940911
[2m[36m(func pid=171528)[0m f1_weighted: 0.28971292636305973
[2m[36m(func pid=171528)[0m f1_per_class: [0.5, 0.425, 0.786, 0.554, 0.053, 0.0, 0.06, 0.281, 0.223, 0.444]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6750 | Steps: 4 | Val loss: 2.0566 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4919 | Steps: 4 | Val loss: 1.7230 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 34.7020 | Steps: 4 | Val loss: 39.7332 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7198 | Steps: 4 | Val loss: 10.9067 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170962)[0m top1: 0.35494402985074625
[2m[36m(func pid=170962)[0m top5: 0.840018656716418
[2m[36m(func pid=170962)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=170962)[0m f1_macro: 0.34258301239495476
[2m[36m(func pid=170962)[0m f1_weighted: 0.34496600255508997
[2m[36m(func pid=170962)[0m f1_per_class: [0.584, 0.56, 0.371, 0.481, 0.135, 0.147, 0.175, 0.317, 0.199, 0.457]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.365205223880597
[2m[36m(func pid=158650)[0m top5: 0.9067164179104478
[2m[36m(func pid=158650)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=158650)[0m f1_macro: 0.358586049959133
[2m[36m(func pid=158650)[0m f1_weighted: 0.38529962162539017
[2m[36m(func pid=158650)[0m f1_per_class: [0.583, 0.486, 0.48, 0.544, 0.099, 0.202, 0.263, 0.36, 0.257, 0.312]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m top1: 0.22667910447761194
[2m[36m(func pid=174857)[0m top5: 0.5918843283582089
[2m[36m(func pid=174857)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=174857)[0m f1_macro: 0.2377430410490758
[2m[36m(func pid=174857)[0m f1_weighted: 0.20266781096021075
[2m[36m(func pid=174857)[0m f1_per_class: [0.593, 0.0, 0.769, 0.587, 0.054, 0.041, 0.0, 0.238, 0.095, 0.0]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:43:06 (running for 00:35:23.90)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.492 |      0.359 |                   69 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.675 |      0.343 |                   18 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.72  |      0.266 |                   18 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 34.702 |      0.238 |                    4 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.23274253731343283
[2m[36m(func pid=171528)[0m top5: 0.7490671641791045
[2m[36m(func pid=171528)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=171528)[0m f1_macro: 0.2664488155784567
[2m[36m(func pid=171528)[0m f1_weighted: 0.23239794842649156
[2m[36m(func pid=171528)[0m f1_per_class: [0.478, 0.097, 0.786, 0.525, 0.066, 0.062, 0.08, 0.273, 0.183, 0.115]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5249 | Steps: 4 | Val loss: 1.7251 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3441 | Steps: 4 | Val loss: 2.2119 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 28.4739 | Steps: 4 | Val loss: 17.7421 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.6443 | Steps: 4 | Val loss: 10.9185 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=158650)[0m top1: 0.3656716417910448
[2m[36m(func pid=158650)[0m top5: 0.8992537313432836
[2m[36m(func pid=158650)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=158650)[0m f1_macro: 0.3554908267973478
[2m[36m(func pid=158650)[0m f1_weighted: 0.3872363072712002
[2m[36m(func pid=158650)[0m f1_per_class: [0.591, 0.503, 0.453, 0.52, 0.102, 0.202, 0.284, 0.353, 0.248, 0.298]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m top1: 0.36240671641791045
[2m[36m(func pid=170962)[0m top5: 0.8255597014925373
[2m[36m(func pid=170962)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=170962)[0m f1_macro: 0.3468659344419537
[2m[36m(func pid=170962)[0m f1_weighted: 0.33484750598175544
[2m[36m(func pid=170962)[0m f1_per_class: [0.596, 0.586, 0.414, 0.449, 0.13, 0.216, 0.119, 0.341, 0.275, 0.343]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m top1: 0.39972014925373134
[2m[36m(func pid=174857)[0m top5: 0.8950559701492538
[2m[36m(func pid=174857)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=174857)[0m f1_macro: 0.26720099006477926
[2m[36m(func pid=174857)[0m f1_weighted: 0.3874066762082646
[2m[36m(func pid=174857)[0m f1_per_class: [0.332, 0.062, 0.7, 0.622, 0.0, 0.335, 0.51, 0.0, 0.028, 0.083]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:43:11 (running for 00:35:29.08)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.525 |      0.355 |                   70 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.344 |      0.347 |                   19 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.644 |      0.273 |                   19 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 28.474 |      0.267 |                    5 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.22901119402985073
[2m[36m(func pid=171528)[0m top5: 0.8069029850746269
[2m[36m(func pid=171528)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=171528)[0m f1_macro: 0.27263703421310714
[2m[36m(func pid=171528)[0m f1_weighted: 0.25481175957958296
[2m[36m(func pid=171528)[0m f1_per_class: [0.464, 0.047, 0.786, 0.433, 0.124, 0.179, 0.229, 0.33, 0.062, 0.072]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3885 | Steps: 4 | Val loss: 2.3058 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4017 | Steps: 4 | Val loss: 1.7176 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 25.8319 | Steps: 4 | Val loss: 45.5210 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4711 | Steps: 4 | Val loss: 8.9011 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=170962)[0m top1: 0.37406716417910446
[2m[36m(func pid=170962)[0m top5: 0.8297574626865671
[2m[36m(func pid=170962)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=170962)[0m f1_macro: 0.35935105775553655
[2m[36m(func pid=170962)[0m f1_weighted: 0.3387915896605774
[2m[36m(func pid=170962)[0m f1_per_class: [0.603, 0.598, 0.448, 0.451, 0.145, 0.261, 0.103, 0.376, 0.219, 0.389]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m top1: 0.23880597014925373
[2m[36m(func pid=174857)[0m top5: 0.7098880597014925
[2m[36m(func pid=174857)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=174857)[0m f1_macro: 0.2727011604777495
[2m[36m(func pid=174857)[0m f1_weighted: 0.22492724972343944
[2m[36m(func pid=174857)[0m f1_per_class: [0.652, 0.573, 0.75, 0.0, 0.0, 0.0, 0.328, 0.122, 0.0, 0.302]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m top1: 0.37173507462686567
[2m[36m(func pid=158650)[0m top5: 0.9029850746268657
[2m[36m(func pid=158650)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=158650)[0m f1_macro: 0.3537340521769996
[2m[36m(func pid=158650)[0m f1_weighted: 0.39267109936491523
[2m[36m(func pid=158650)[0m f1_per_class: [0.598, 0.492, 0.436, 0.534, 0.104, 0.214, 0.293, 0.357, 0.241, 0.268]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:43:16 (running for 00:35:34.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.402 |      0.354 |                   71 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.389 |      0.359 |                   20 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.471 |      0.296 |                   20 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 25.832 |      0.273 |                    6 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.30736940298507465
[2m[36m(func pid=171528)[0m top5: 0.8563432835820896
[2m[36m(func pid=171528)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=171528)[0m f1_macro: 0.29564083120036
[2m[36m(func pid=171528)[0m f1_weighted: 0.3257354658943416
[2m[36m(func pid=171528)[0m f1_per_class: [0.421, 0.095, 0.714, 0.506, 0.175, 0.219, 0.373, 0.236, 0.073, 0.144]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.9738 | Steps: 4 | Val loss: 6.7126 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 42.5210 | Steps: 4 | Val loss: 45.7928 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5165 | Steps: 4 | Val loss: 1.6652 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7113 | Steps: 4 | Val loss: 2.3421 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 11:43:21 (running for 00:35:39.30)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.402 |      0.354 |                   71 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.389 |      0.359 |                   20 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.471 |      0.296 |                   20 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 25.832 |      0.273 |                    6 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.4197761194029851
[2m[36m(func pid=171528)[0m top5: 0.909981343283582
[2m[36m(func pid=171528)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=171528)[0m f1_macro: 0.35507061065987805
[2m[36m(func pid=171528)[0m f1_weighted: 0.4242329157067276
[2m[36m(func pid=171528)[0m f1_per_class: [0.371, 0.28, 0.759, 0.582, 0.253, 0.264, 0.528, 0.065, 0.142, 0.308]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.3069029850746269
[2m[36m(func pid=174857)[0m top5: 0.7131529850746269
[2m[36m(func pid=174857)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=174857)[0m f1_macro: 0.2856206639045316
[2m[36m(func pid=174857)[0m f1_weighted: 0.23635920651942563
[2m[36m(func pid=174857)[0m f1_per_class: [0.624, 0.434, 0.44, 0.003, 0.205, 0.016, 0.4, 0.215, 0.197, 0.323]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.37779850746268656
[2m[36m(func pid=170962)[0m top5: 0.8367537313432836
[2m[36m(func pid=170962)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=170962)[0m f1_macro: 0.36792214902832737
[2m[36m(func pid=170962)[0m f1_weighted: 0.34518008519129484
[2m[36m(func pid=170962)[0m f1_per_class: [0.624, 0.602, 0.406, 0.472, 0.155, 0.26, 0.097, 0.379, 0.23, 0.455]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.38619402985074625
[2m[36m(func pid=158650)[0m top5: 0.914179104477612
[2m[36m(func pid=158650)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=158650)[0m f1_macro: 0.36509479275195184
[2m[36m(func pid=158650)[0m f1_weighted: 0.4080917128179284
[2m[36m(func pid=158650)[0m f1_per_class: [0.582, 0.504, 0.471, 0.534, 0.118, 0.207, 0.337, 0.354, 0.264, 0.281]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.0141 | Steps: 4 | Val loss: 6.4853 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 10.1398 | Steps: 4 | Val loss: 58.1325 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5831 | Steps: 4 | Val loss: 2.1960 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4379 | Steps: 4 | Val loss: 1.6930 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 11:43:27 (running for 00:35:44.86)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.516 |      0.365 |                   72 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.711 |      0.368 |                   21 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.974 |      0.355 |                   21 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 42.521 |      0.286 |                    7 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.457089552238806
[2m[36m(func pid=171528)[0m top5: 0.9211753731343284
[2m[36m(func pid=171528)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=171528)[0m f1_macro: 0.37853833753843114
[2m[36m(func pid=171528)[0m f1_weighted: 0.47037999375747164
[2m[36m(func pid=171528)[0m f1_per_class: [0.307, 0.499, 0.733, 0.578, 0.215, 0.242, 0.565, 0.065, 0.191, 0.389]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.2019589552238806
[2m[36m(func pid=174857)[0m top5: 0.7238805970149254
[2m[36m(func pid=174857)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=174857)[0m f1_macro: 0.1565493872060485
[2m[36m(func pid=174857)[0m f1_weighted: 0.2050312892309073
[2m[36m(func pid=174857)[0m f1_per_class: [0.409, 0.016, 0.127, 0.464, 0.098, 0.201, 0.118, 0.016, 0.116, 0.0]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.3894589552238806
[2m[36m(func pid=170962)[0m top5: 0.8549440298507462
[2m[36m(func pid=170962)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=170962)[0m f1_macro: 0.3705070352627909
[2m[36m(func pid=170962)[0m f1_weighted: 0.37281944315834015
[2m[36m(func pid=170962)[0m f1_per_class: [0.584, 0.579, 0.382, 0.524, 0.166, 0.277, 0.149, 0.388, 0.249, 0.407]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.37966417910447764
[2m[36m(func pid=158650)[0m top5: 0.9081156716417911
[2m[36m(func pid=158650)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=158650)[0m f1_macro: 0.35244595731946965
[2m[36m(func pid=158650)[0m f1_weighted: 0.40422335137029597
[2m[36m(func pid=158650)[0m f1_per_class: [0.584, 0.497, 0.393, 0.522, 0.114, 0.217, 0.342, 0.347, 0.243, 0.265]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8054 | Steps: 4 | Val loss: 8.8670 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 29.3446 | Steps: 4 | Val loss: 90.0800 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4398 | Steps: 4 | Val loss: 1.9770 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3924 | Steps: 4 | Val loss: 1.6862 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 11:43:32 (running for 00:35:50.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.438 |      0.352 |                   73 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.583 |      0.371 |                   22 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.805 |      0.311 |                   23 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 10.14  |      0.157 |                    8 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.3773320895522388
[2m[36m(func pid=171528)[0m top5: 0.8684701492537313
[2m[36m(func pid=171528)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=171528)[0m f1_macro: 0.31126452962897694
[2m[36m(func pid=171528)[0m f1_weighted: 0.38481778090835567
[2m[36m(func pid=171528)[0m f1_per_class: [0.274, 0.552, 0.71, 0.547, 0.179, 0.125, 0.304, 0.28, 0.141, 0.0]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.27052238805970147
[2m[36m(func pid=174857)[0m top5: 0.5923507462686567
[2m[36m(func pid=174857)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=174857)[0m f1_macro: 0.18297914804031082
[2m[36m(func pid=174857)[0m f1_weighted: 0.21072697229455092
[2m[36m(func pid=174857)[0m f1_per_class: [0.361, 0.0, 0.253, 0.593, 0.116, 0.239, 0.009, 0.0, 0.111, 0.148]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m top1: 0.38386194029850745
[2m[36m(func pid=158650)[0m top5: 0.90625
[2m[36m(func pid=158650)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=158650)[0m f1_macro: 0.35644840507680586
[2m[36m(func pid=158650)[0m f1_weighted: 0.4094366226611617
[2m[36m(func pid=158650)[0m f1_per_class: [0.577, 0.482, 0.444, 0.517, 0.138, 0.247, 0.36, 0.355, 0.251, 0.194]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4099813432835821
[2m[36m(func pid=170962)[0m top5: 0.8908582089552238
[2m[36m(func pid=170962)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=170962)[0m f1_macro: 0.3828161020336569
[2m[36m(func pid=170962)[0m f1_weighted: 0.4188418732430367
[2m[36m(func pid=170962)[0m f1_per_class: [0.598, 0.556, 0.407, 0.536, 0.158, 0.271, 0.311, 0.396, 0.201, 0.394]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3652 | Steps: 4 | Val loss: 12.6929 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 36.6365 | Steps: 4 | Val loss: 80.6899 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4775 | Steps: 4 | Val loss: 1.9430 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3202 | Steps: 4 | Val loss: 1.6814 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:43:38 (running for 00:35:55.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.358
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.392 |      0.356 |                   74 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.44  |      0.383 |                   23 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.365 |      0.269 |                   24 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 29.345 |      0.183 |                    9 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.3358208955223881
[2m[36m(func pid=171528)[0m top5: 0.7509328358208955
[2m[36m(func pid=171528)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=171528)[0m f1_macro: 0.2688875677114081
[2m[36m(func pid=171528)[0m f1_weighted: 0.28125990591936945
[2m[36m(func pid=171528)[0m f1_per_class: [0.298, 0.511, 0.647, 0.495, 0.14, 0.057, 0.042, 0.345, 0.154, 0.0]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.31669776119402987
[2m[36m(func pid=174857)[0m top5: 0.6431902985074627
[2m[36m(func pid=174857)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=174857)[0m f1_macro: 0.21222815242957555
[2m[36m(func pid=174857)[0m f1_weighted: 0.2519539891461628
[2m[36m(func pid=174857)[0m f1_per_class: [0.268, 0.0, 0.444, 0.557, 0.133, 0.035, 0.255, 0.0, 0.12, 0.311]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.43283582089552236
[2m[36m(func pid=170962)[0m top5: 0.9039179104477612
[2m[36m(func pid=170962)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=170962)[0m f1_macro: 0.39280210924349057
[2m[36m(func pid=170962)[0m f1_weighted: 0.4522303225662246
[2m[36m(func pid=170962)[0m f1_per_class: [0.634, 0.543, 0.436, 0.556, 0.145, 0.192, 0.439, 0.408, 0.184, 0.391]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.38899253731343286
[2m[36m(func pid=158650)[0m top5: 0.9095149253731343
[2m[36m(func pid=158650)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=158650)[0m f1_macro: 0.3595688413728425
[2m[36m(func pid=158650)[0m f1_weighted: 0.4132024284890173
[2m[36m(func pid=158650)[0m f1_per_class: [0.615, 0.486, 0.444, 0.54, 0.156, 0.245, 0.351, 0.345, 0.237, 0.176]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1060 | Steps: 4 | Val loss: 15.6869 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 24.8903 | Steps: 4 | Val loss: 75.9898 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.1805 | Steps: 4 | Val loss: 1.8974 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3709 | Steps: 4 | Val loss: 1.6664 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:43:43 (running for 00:36:00.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.32  |      0.36  |                   75 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.477 |      0.393 |                   24 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.106 |      0.265 |                   25 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 36.637 |      0.212 |                   10 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.34421641791044777
[2m[36m(func pid=171528)[0m top5: 0.6861007462686567
[2m[36m(func pid=171528)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=171528)[0m f1_macro: 0.26528290006842437
[2m[36m(func pid=171528)[0m f1_weighted: 0.26619511031300613
[2m[36m(func pid=171528)[0m f1_per_class: [0.404, 0.481, 0.571, 0.488, 0.1, 0.075, 0.0, 0.344, 0.189, 0.0]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.29011194029850745
[2m[36m(func pid=174857)[0m top5: 0.7024253731343284
[2m[36m(func pid=174857)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=174857)[0m f1_macro: 0.2149760047845981
[2m[36m(func pid=174857)[0m f1_weighted: 0.3041138549611577
[2m[36m(func pid=174857)[0m f1_per_class: [0.404, 0.104, 0.248, 0.551, 0.21, 0.008, 0.383, 0.016, 0.124, 0.103]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.45149253731343286
[2m[36m(func pid=170962)[0m top5: 0.9235074626865671
[2m[36m(func pid=170962)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=170962)[0m f1_macro: 0.3871031180885132
[2m[36m(func pid=170962)[0m f1_weighted: 0.4613354149151684
[2m[36m(func pid=170962)[0m f1_per_class: [0.627, 0.471, 0.512, 0.593, 0.183, 0.191, 0.494, 0.342, 0.152, 0.305]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.39598880597014924
[2m[36m(func pid=158650)[0m top5: 0.9127798507462687
[2m[36m(func pid=158650)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=158650)[0m f1_macro: 0.35407389202562195
[2m[36m(func pid=158650)[0m f1_weighted: 0.4197668544142147
[2m[36m(func pid=158650)[0m f1_per_class: [0.584, 0.496, 0.4, 0.561, 0.152, 0.228, 0.358, 0.341, 0.239, 0.182]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8675 | Steps: 4 | Val loss: 17.0708 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 32.9648 | Steps: 4 | Val loss: 112.8841 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2180 | Steps: 4 | Val loss: 1.8796 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.4287 | Steps: 4 | Val loss: 1.6896 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=171528)[0m top1: 0.3787313432835821
[2m[36m(func pid=171528)[0m top5: 0.6618470149253731
[2m[36m(func pid=171528)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=171528)[0m f1_macro: 0.3169994393617575
[2m[36m(func pid=171528)[0m f1_weighted: 0.2977936165654413
[2m[36m(func pid=171528)[0m f1_per_class: [0.514, 0.542, 0.55, 0.548, 0.192, 0.082, 0.0, 0.304, 0.202, 0.235]
[2m[36m(func pid=171528)[0m 
== Status ==
Current time: 2024-01-07 11:43:48 (running for 00:36:06.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.371 |      0.354 |                   76 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.181 |      0.387 |                   25 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.867 |      0.317 |                   26 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 24.89  |      0.215 |                   11 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.21222014925373134
[2m[36m(func pid=174857)[0m top5: 0.6305970149253731
[2m[36m(func pid=174857)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=174857)[0m f1_macro: 0.18900422531125374
[2m[36m(func pid=174857)[0m f1_weighted: 0.1504292127712279
[2m[36m(func pid=174857)[0m f1_per_class: [0.24, 0.325, 0.202, 0.123, 0.239, 0.0, 0.118, 0.129, 0.173, 0.341]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.458955223880597
[2m[36m(func pid=170962)[0m top5: 0.9286380597014925
[2m[36m(func pid=170962)[0m f1_micro: 0.458955223880597
[2m[36m(func pid=170962)[0m f1_macro: 0.3740431318085133
[2m[36m(func pid=170962)[0m f1_weighted: 0.4618247961893801
[2m[36m(func pid=170962)[0m f1_per_class: [0.639, 0.398, 0.524, 0.605, 0.211, 0.168, 0.554, 0.247, 0.165, 0.23]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.39365671641791045
[2m[36m(func pid=158650)[0m top5: 0.9034514925373134
[2m[36m(func pid=158650)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=158650)[0m f1_macro: 0.3450030465295803
[2m[36m(func pid=158650)[0m f1_weighted: 0.41801904187597927
[2m[36m(func pid=158650)[0m f1_per_class: [0.569, 0.489, 0.348, 0.554, 0.136, 0.22, 0.367, 0.345, 0.246, 0.176]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9181 | Steps: 4 | Val loss: 16.8161 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 35.9868 | Steps: 4 | Val loss: 115.7718 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2154 | Steps: 4 | Val loss: 1.9546 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.7436 | Steps: 4 | Val loss: 1.7018 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:43:54 (running for 00:36:11.53)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.429 |      0.345 |                   77 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.218 |      0.374 |                   26 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.918 |      0.346 |                   27 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 32.965 |      0.189 |                   12 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.38759328358208955
[2m[36m(func pid=171528)[0m top5: 0.6847014925373134
[2m[36m(func pid=171528)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=171528)[0m f1_macro: 0.3461460134949304
[2m[36m(func pid=171528)[0m f1_weighted: 0.328129481770409
[2m[36m(func pid=171528)[0m f1_per_class: [0.583, 0.593, 0.52, 0.605, 0.177, 0.135, 0.0, 0.253, 0.177, 0.418]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.23507462686567165
[2m[36m(func pid=174857)[0m top5: 0.6473880597014925
[2m[36m(func pid=174857)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=174857)[0m f1_macro: 0.21814246509437152
[2m[36m(func pid=174857)[0m f1_weighted: 0.1714362420353493
[2m[36m(func pid=174857)[0m f1_per_class: [0.308, 0.338, 0.172, 0.091, 0.112, 0.008, 0.148, 0.378, 0.258, 0.369]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.45755597014925375
[2m[36m(func pid=170962)[0m top5: 0.9300373134328358
[2m[36m(func pid=170962)[0m f1_micro: 0.45755597014925375
[2m[36m(func pid=170962)[0m f1_macro: 0.37337377458026777
[2m[36m(func pid=170962)[0m f1_weighted: 0.45667686218070136
[2m[36m(func pid=170962)[0m f1_per_class: [0.651, 0.341, 0.512, 0.611, 0.233, 0.17, 0.556, 0.282, 0.161, 0.216]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.39365671641791045
[2m[36m(func pid=158650)[0m top5: 0.9015858208955224
[2m[36m(func pid=158650)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=158650)[0m f1_macro: 0.34232277606513944
[2m[36m(func pid=158650)[0m f1_weighted: 0.41745991116892417
[2m[36m(func pid=158650)[0m f1_per_class: [0.557, 0.491, 0.338, 0.542, 0.138, 0.226, 0.374, 0.348, 0.24, 0.169]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9128 | Steps: 4 | Val loss: 16.5749 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 8.1913 | Steps: 4 | Val loss: 82.4849 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6708 | Steps: 4 | Val loss: 1.9479 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:43:59 (running for 00:36:16.85)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.744 |      0.342 |                   78 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.215 |      0.373 |                   27 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.913 |      0.302 |                   28 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 35.987 |      0.218 |                   13 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.3521455223880597
[2m[36m(func pid=171528)[0m top5: 0.6963619402985075
[2m[36m(func pid=171528)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=171528)[0m f1_macro: 0.30187259619238016
[2m[36m(func pid=171528)[0m f1_weighted: 0.30730258422020745
[2m[36m(func pid=171528)[0m f1_per_class: [0.535, 0.449, 0.456, 0.612, 0.151, 0.162, 0.003, 0.283, 0.195, 0.171]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.29244402985074625
[2m[36m(func pid=174857)[0m top5: 0.7607276119402985
[2m[36m(func pid=174857)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=174857)[0m f1_macro: 0.26473264979696676
[2m[36m(func pid=174857)[0m f1_weighted: 0.2959513752853898
[2m[36m(func pid=174857)[0m f1_per_class: [0.459, 0.458, 0.151, 0.401, 0.06, 0.044, 0.198, 0.323, 0.242, 0.311]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3146 | Steps: 4 | Val loss: 1.6896 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=170962)[0m top1: 0.457089552238806
[2m[36m(func pid=170962)[0m top5: 0.9323694029850746
[2m[36m(func pid=170962)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=170962)[0m f1_macro: 0.3794873466192272
[2m[36m(func pid=170962)[0m f1_weighted: 0.46431423021249474
[2m[36m(func pid=170962)[0m f1_per_class: [0.667, 0.375, 0.512, 0.597, 0.226, 0.173, 0.568, 0.31, 0.174, 0.194]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.7246 | Steps: 4 | Val loss: 17.5527 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=158650)[0m top1: 0.4001865671641791
[2m[36m(func pid=158650)[0m top5: 0.9020522388059702
[2m[36m(func pid=158650)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=158650)[0m f1_macro: 0.3456253256434768
[2m[36m(func pid=158650)[0m f1_weighted: 0.4255015324029521
[2m[36m(func pid=158650)[0m f1_per_class: [0.565, 0.475, 0.316, 0.543, 0.147, 0.251, 0.399, 0.353, 0.235, 0.174]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.7659 | Steps: 4 | Val loss: 96.4982 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2323 | Steps: 4 | Val loss: 2.0650 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:44:04 (running for 00:36:22.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.315 |      0.346 |                   79 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.671 |      0.379 |                   28 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.725 |      0.249 |                   29 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.191 |      0.265 |                   14 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.28218283582089554
[2m[36m(func pid=171528)[0m top5: 0.715018656716418
[2m[36m(func pid=171528)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=171528)[0m f1_macro: 0.24919586153063408
[2m[36m(func pid=171528)[0m f1_weighted: 0.26104004975585426
[2m[36m(func pid=171528)[0m f1_per_class: [0.523, 0.173, 0.351, 0.603, 0.084, 0.108, 0.034, 0.348, 0.187, 0.081]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.24860074626865672
[2m[36m(func pid=174857)[0m top5: 0.7784514925373134
[2m[36m(func pid=174857)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=174857)[0m f1_macro: 0.2337864377136974
[2m[36m(func pid=174857)[0m f1_weighted: 0.25652036979853454
[2m[36m(func pid=174857)[0m f1_per_class: [0.442, 0.217, 0.211, 0.528, 0.06, 0.17, 0.074, 0.212, 0.116, 0.308]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.4143 | Steps: 4 | Val loss: 1.6768 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=170962)[0m top1: 0.4435634328358209
[2m[36m(func pid=170962)[0m top5: 0.9309701492537313
[2m[36m(func pid=170962)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=170962)[0m f1_macro: 0.39826633438437303
[2m[36m(func pid=170962)[0m f1_weighted: 0.46955918897540777
[2m[36m(func pid=170962)[0m f1_per_class: [0.658, 0.342, 0.55, 0.601, 0.238, 0.202, 0.57, 0.4, 0.162, 0.261]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 5.5883 | Steps: 4 | Val loss: 20.7428 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 42.4346 | Steps: 4 | Val loss: 112.5051 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=158650)[0m top1: 0.4006529850746269
[2m[36m(func pid=158650)[0m top5: 0.9048507462686567
[2m[36m(func pid=158650)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=158650)[0m f1_macro: 0.34681704944326963
[2m[36m(func pid=158650)[0m f1_weighted: 0.422318527369551
[2m[36m(func pid=158650)[0m f1_per_class: [0.553, 0.478, 0.329, 0.55, 0.163, 0.257, 0.38, 0.332, 0.254, 0.174]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:10 (running for 00:36:27.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.414 |      0.347 |                   80 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.232 |      0.398 |                   29 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  5.588 |      0.215 |                   30 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  3.766 |      0.234 |                   15 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.22014925373134328
[2m[36m(func pid=171528)[0m top5: 0.7220149253731343
[2m[36m(func pid=171528)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=171528)[0m f1_macro: 0.21527681606581334
[2m[36m(func pid=171528)[0m f1_weighted: 0.24630722570280122
[2m[36m(func pid=171528)[0m f1_per_class: [0.448, 0.058, 0.289, 0.565, 0.043, 0.063, 0.117, 0.332, 0.163, 0.076]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2239 | Steps: 4 | Val loss: 2.3579 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=174857)[0m top1: 0.2621268656716418
[2m[36m(func pid=174857)[0m top5: 0.7416044776119403
[2m[36m(func pid=174857)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=174857)[0m f1_macro: 0.2558241445545799
[2m[36m(func pid=174857)[0m f1_weighted: 0.2333538961001804
[2m[36m(func pid=174857)[0m f1_per_class: [0.489, 0.0, 0.462, 0.553, 0.12, 0.247, 0.062, 0.206, 0.093, 0.327]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.4562 | Steps: 4 | Val loss: 1.6732 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=170962)[0m top1: 0.38526119402985076
[2m[36m(func pid=170962)[0m top5: 0.9155783582089553
[2m[36m(func pid=170962)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=170962)[0m f1_macro: 0.38633731227317386
[2m[36m(func pid=170962)[0m f1_weighted: 0.42968340345198064
[2m[36m(func pid=170962)[0m f1_per_class: [0.649, 0.326, 0.537, 0.543, 0.261, 0.235, 0.493, 0.377, 0.134, 0.309]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 3.7808 | Steps: 4 | Val loss: 21.0574 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 14.8201 | Steps: 4 | Val loss: 89.2265 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=158650)[0m top1: 0.4076492537313433
[2m[36m(func pid=158650)[0m top5: 0.9067164179104478
[2m[36m(func pid=158650)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=158650)[0m f1_macro: 0.3482004858314237
[2m[36m(func pid=158650)[0m f1_weighted: 0.4233320042827451
[2m[36m(func pid=158650)[0m f1_per_class: [0.579, 0.516, 0.282, 0.556, 0.153, 0.244, 0.355, 0.362, 0.23, 0.205]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:15 (running for 00:36:32.69)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.456 |      0.348 |                   81 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.224 |      0.386 |                   30 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  3.781 |      0.2   |                   31 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 42.435 |      0.256 |                   16 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.19636194029850745
[2m[36m(func pid=171528)[0m top5: 0.7434701492537313
[2m[36m(func pid=171528)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=171528)[0m f1_macro: 0.20003182981129236
[2m[36m(func pid=171528)[0m f1_weighted: 0.24109789899977294
[2m[36m(func pid=171528)[0m f1_per_class: [0.436, 0.021, 0.274, 0.469, 0.035, 0.073, 0.23, 0.229, 0.133, 0.1]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.3614738805970149
[2m[36m(func pid=174857)[0m top5: 0.7859141791044776
[2m[36m(func pid=174857)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=174857)[0m f1_macro: 0.34202632424766094
[2m[36m(func pid=174857)[0m f1_weighted: 0.3203264748948002
[2m[36m(func pid=174857)[0m f1_per_class: [0.4, 0.063, 0.733, 0.611, 0.289, 0.326, 0.197, 0.328, 0.169, 0.303]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2742 | Steps: 4 | Val loss: 2.5134 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3928 | Steps: 4 | Val loss: 1.6198 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=170962)[0m top1: 0.3521455223880597
[2m[36m(func pid=170962)[0m top5: 0.9015858208955224
[2m[36m(func pid=170962)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=170962)[0m f1_macro: 0.3723423541862016
[2m[36m(func pid=170962)[0m f1_weighted: 0.39377299142002087
[2m[36m(func pid=170962)[0m f1_per_class: [0.65, 0.387, 0.468, 0.497, 0.238, 0.248, 0.377, 0.368, 0.132, 0.359]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9958 | Steps: 4 | Val loss: 17.5378 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 10.0525 | Steps: 4 | Val loss: 76.2572 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=158650)[0m top1: 0.4221082089552239
[2m[36m(func pid=158650)[0m top5: 0.9137126865671642
[2m[36m(func pid=158650)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=158650)[0m f1_macro: 0.36593648152140934
[2m[36m(func pid=158650)[0m f1_weighted: 0.43916429044856853
[2m[36m(func pid=158650)[0m f1_per_class: [0.6, 0.522, 0.369, 0.575, 0.168, 0.234, 0.387, 0.355, 0.243, 0.206]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:20 (running for 00:36:38.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.393 |      0.366 |                   82 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.274 |      0.372 |                   31 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.996 |      0.237 |                   32 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 14.82  |      0.342 |                   17 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.271455223880597
[2m[36m(func pid=171528)[0m top5: 0.7910447761194029
[2m[36m(func pid=171528)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=171528)[0m f1_macro: 0.23688419699281188
[2m[36m(func pid=171528)[0m f1_weighted: 0.3116112264723396
[2m[36m(func pid=171528)[0m f1_per_class: [0.552, 0.027, 0.299, 0.492, 0.053, 0.095, 0.452, 0.08, 0.112, 0.208]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.40531716417910446
[2m[36m(func pid=174857)[0m top5: 0.8106343283582089
[2m[36m(func pid=174857)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=174857)[0m f1_macro: 0.384205817383625
[2m[36m(func pid=174857)[0m f1_weighted: 0.3928891183058895
[2m[36m(func pid=174857)[0m f1_per_class: [0.602, 0.11, 0.75, 0.6, 0.35, 0.307, 0.409, 0.386, 0.185, 0.143]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.1709 | Steps: 4 | Val loss: 2.8366 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3195 | Steps: 4 | Val loss: 1.6271 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=170962)[0m top1: 0.3362873134328358
[2m[36m(func pid=170962)[0m top5: 0.8684701492537313
[2m[36m(func pid=170962)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=170962)[0m f1_macro: 0.35935861697612553
[2m[36m(func pid=170962)[0m f1_weighted: 0.3584845670458613
[2m[36m(func pid=170962)[0m f1_per_class: [0.674, 0.447, 0.379, 0.47, 0.233, 0.265, 0.242, 0.372, 0.137, 0.375]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8911 | Steps: 4 | Val loss: 15.9810 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 35.0806 | Steps: 4 | Val loss: 74.9382 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=158650)[0m top1: 0.4197761194029851
[2m[36m(func pid=158650)[0m top5: 0.9123134328358209
[2m[36m(func pid=158650)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=158650)[0m f1_macro: 0.35855984706143695
[2m[36m(func pid=158650)[0m f1_weighted: 0.43982756224124386
[2m[36m(func pid=158650)[0m f1_per_class: [0.574, 0.529, 0.347, 0.563, 0.156, 0.228, 0.404, 0.35, 0.236, 0.2]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:26 (running for 00:36:43.54)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.319 |      0.359 |                   83 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.171 |      0.359 |                   32 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.996 |      0.237 |                   32 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 35.081 |      0.353 |                   19 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.43283582089552236
[2m[36m(func pid=174857)[0m top5: 0.8138992537313433
[2m[36m(func pid=174857)[0m f1_micro: 0.43283582089552236
[2m[36m(func pid=174857)[0m f1_macro: 0.3526676666202324
[2m[36m(func pid=174857)[0m f1_weighted: 0.429529534057293
[2m[36m(func pid=174857)[0m f1_per_class: [0.701, 0.223, 0.5, 0.583, 0.095, 0.256, 0.509, 0.378, 0.205, 0.077]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3260261194029851
[2m[36m(func pid=171528)[0m top5: 0.8334888059701493
[2m[36m(func pid=171528)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=171528)[0m f1_macro: 0.2735292766957658
[2m[36m(func pid=171528)[0m f1_weighted: 0.34422911626073693
[2m[36m(func pid=171528)[0m f1_per_class: [0.575, 0.032, 0.325, 0.451, 0.106, 0.103, 0.589, 0.058, 0.097, 0.4]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2312 | Steps: 4 | Val loss: 2.9102 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.5493 | Steps: 4 | Val loss: 1.5925 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9636 | Steps: 4 | Val loss: 14.6147 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 34.3090 | Steps: 4 | Val loss: 74.7690 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=170962)[0m top1: 0.33908582089552236
[2m[36m(func pid=170962)[0m top5: 0.8530783582089553
[2m[36m(func pid=170962)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=170962)[0m f1_macro: 0.34556077258944
[2m[36m(func pid=170962)[0m f1_weighted: 0.3454684115606047
[2m[36m(func pid=170962)[0m f1_per_class: [0.61, 0.489, 0.364, 0.435, 0.209, 0.268, 0.211, 0.356, 0.162, 0.352]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.44029850746268656
[2m[36m(func pid=158650)[0m top5: 0.9146455223880597
[2m[36m(func pid=158650)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=158650)[0m f1_macro: 0.37530209106699114
[2m[36m(func pid=158650)[0m f1_weighted: 0.4620013886898054
[2m[36m(func pid=158650)[0m f1_per_class: [0.574, 0.526, 0.353, 0.565, 0.195, 0.286, 0.452, 0.354, 0.254, 0.195]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:31 (running for 00:36:48.84)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.549 |      0.375 |                   84 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.231 |      0.346 |                   33 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.964 |      0.282 |                   34 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 35.081 |      0.353 |                   19 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.35774253731343286
[2m[36m(func pid=171528)[0m top5: 0.8694029850746269
[2m[36m(func pid=171528)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=171528)[0m f1_macro: 0.2823423854778292
[2m[36m(func pid=171528)[0m f1_weighted: 0.3537106598166214
[2m[36m(func pid=171528)[0m f1_per_class: [0.556, 0.105, 0.31, 0.42, 0.161, 0.118, 0.61, 0.015, 0.085, 0.444]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.47388059701492535
[2m[36m(func pid=174857)[0m top5: 0.816231343283582
[2m[36m(func pid=174857)[0m f1_micro: 0.47388059701492535
[2m[36m(func pid=174857)[0m f1_macro: 0.39157505022674977
[2m[36m(func pid=174857)[0m f1_weighted: 0.4766515816006577
[2m[36m(func pid=174857)[0m f1_per_class: [0.675, 0.496, 0.5, 0.53, 0.1, 0.213, 0.574, 0.339, 0.247, 0.242]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1295 | Steps: 4 | Val loss: 2.8831 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2852 | Steps: 4 | Val loss: 1.6282 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 14.7437 | Steps: 4 | Val loss: 81.7750 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 4.2393 | Steps: 4 | Val loss: 13.2003 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=170962)[0m top1: 0.34095149253731344
[2m[36m(func pid=170962)[0m top5: 0.8470149253731343
[2m[36m(func pid=170962)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=170962)[0m f1_macro: 0.3465987808821526
[2m[36m(func pid=170962)[0m f1_weighted: 0.3413977804093903
[2m[36m(func pid=170962)[0m f1_per_class: [0.557, 0.504, 0.407, 0.381, 0.211, 0.287, 0.237, 0.329, 0.197, 0.358]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.427705223880597
[2m[36m(func pid=158650)[0m top5: 0.9137126865671642
[2m[36m(func pid=158650)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=158650)[0m f1_macro: 0.36503954906393765
[2m[36m(func pid=158650)[0m f1_weighted: 0.44950523733993264
[2m[36m(func pid=158650)[0m f1_per_class: [0.574, 0.521, 0.32, 0.545, 0.18, 0.285, 0.436, 0.346, 0.236, 0.208]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:36 (running for 00:36:54.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.285 |      0.365 |                   85 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.13  |      0.347 |                   34 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.964 |      0.282 |                   34 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 14.744 |      0.425 |                   21 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.4449626865671642
[2m[36m(func pid=174857)[0m top5: 0.8069029850746269
[2m[36m(func pid=174857)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=174857)[0m f1_macro: 0.42461591064220644
[2m[36m(func pid=174857)[0m f1_weighted: 0.4403643063740207
[2m[36m(func pid=174857)[0m f1_per_class: [0.639, 0.549, 0.714, 0.417, 0.16, 0.283, 0.484, 0.339, 0.316, 0.344]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3675373134328358
[2m[36m(func pid=171528)[0m top5: 0.8847947761194029
[2m[36m(func pid=171528)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=171528)[0m f1_macro: 0.26971334495793664
[2m[36m(func pid=171528)[0m f1_weighted: 0.38133965343232085
[2m[36m(func pid=171528)[0m f1_per_class: [0.456, 0.262, 0.234, 0.413, 0.21, 0.191, 0.598, 0.065, 0.087, 0.182]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.1135 | Steps: 4 | Val loss: 2.9653 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.4980 | Steps: 4 | Val loss: 1.6684 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 51.0129 | Steps: 4 | Val loss: 105.6005 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.5464 | Steps: 4 | Val loss: 11.8728 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=170962)[0m top1: 0.34421641791044777
[2m[36m(func pid=170962)[0m top5: 0.8395522388059702
[2m[36m(func pid=170962)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=170962)[0m f1_macro: 0.3356906499879117
[2m[36m(func pid=170962)[0m f1_weighted: 0.3402044997162084
[2m[36m(func pid=170962)[0m f1_per_class: [0.565, 0.507, 0.348, 0.359, 0.19, 0.28, 0.252, 0.355, 0.202, 0.299]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m top1: 0.41511194029850745
[2m[36m(func pid=158650)[0m top5: 0.9067164179104478
[2m[36m(func pid=158650)[0m f1_micro: 0.4151119402985075
[2m[36m(func pid=158650)[0m f1_macro: 0.354575045003894
[2m[36m(func pid=158650)[0m f1_weighted: 0.4334181352405159
[2m[36m(func pid=158650)[0m f1_per_class: [0.564, 0.533, 0.286, 0.532, 0.153, 0.261, 0.397, 0.348, 0.234, 0.238]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:44:41 (running for 00:36:59.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.498 |      0.355 |                   86 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.114 |      0.336 |                   35 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  4.239 |      0.27  |                   35 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 14.744 |      0.425 |                   21 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3978544776119403
[2m[36m(func pid=174857)[0m top5: 0.7588619402985075
[2m[36m(func pid=174857)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=174857)[0m f1_macro: 0.3791372890773848
[2m[36m(func pid=174857)[0m f1_weighted: 0.37920847353436865
[2m[36m(func pid=174857)[0m f1_per_class: [0.53, 0.508, 0.667, 0.332, 0.182, 0.3, 0.389, 0.372, 0.246, 0.267]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3666044776119403
[2m[36m(func pid=171528)[0m top5: 0.8726679104477612
[2m[36m(func pid=171528)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=171528)[0m f1_macro: 0.31342031897945055
[2m[36m(func pid=171528)[0m f1_weighted: 0.4027742164806261
[2m[36m(func pid=171528)[0m f1_per_class: [0.449, 0.481, 0.243, 0.399, 0.239, 0.27, 0.496, 0.173, 0.133, 0.25]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6344 | Steps: 4 | Val loss: 3.0655 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.4674 | Steps: 4 | Val loss: 1.6898 | Batch size: 32 | lr: 0.0001 | Duration: 3.33s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 12.0015 | Steps: 4 | Val loss: 120.5872 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.9502 | Steps: 4 | Val loss: 17.6132 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170962)[0m top1: 0.34048507462686567
[2m[36m(func pid=170962)[0m top5: 0.8292910447761194
[2m[36m(func pid=170962)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=170962)[0m f1_macro: 0.32942730321857416
[2m[36m(func pid=170962)[0m f1_weighted: 0.3334036638324817
[2m[36m(func pid=170962)[0m f1_per_class: [0.58, 0.512, 0.292, 0.32, 0.186, 0.281, 0.26, 0.368, 0.197, 0.298]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:44:47 (running for 00:37:05.09)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.498 |      0.355 |                   86 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.634 |      0.329 |                   36 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.546 |      0.313 |                   36 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 12.002 |      0.316 |                   23 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.33955223880597013
[2m[36m(func pid=174857)[0m top5: 0.7509328358208955
[2m[36m(func pid=174857)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=174857)[0m f1_macro: 0.31620733858391925
[2m[36m(func pid=174857)[0m f1_weighted: 0.33137799335782114
[2m[36m(func pid=174857)[0m f1_per_class: [0.426, 0.502, 0.55, 0.349, 0.12, 0.258, 0.252, 0.399, 0.165, 0.142]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.2635261194029851
[2m[36m(func pid=171528)[0m top5: 0.8484141791044776
[2m[36m(func pid=171528)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=171528)[0m f1_macro: 0.27929498865485186
[2m[36m(func pid=171528)[0m f1_weighted: 0.2673615975774455
[2m[36m(func pid=171528)[0m f1_per_class: [0.493, 0.586, 0.302, 0.207, 0.188, 0.191, 0.185, 0.165, 0.165, 0.312]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.41138059701492535
[2m[36m(func pid=158650)[0m top5: 0.9001865671641791
[2m[36m(func pid=158650)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=158650)[0m f1_macro: 0.35427190383472207
[2m[36m(func pid=158650)[0m f1_weighted: 0.4296754269945758
[2m[36m(func pid=158650)[0m f1_per_class: [0.585, 0.528, 0.31, 0.536, 0.141, 0.226, 0.396, 0.353, 0.214, 0.253]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.0724 | Steps: 4 | Val loss: 2.8192 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 22.3128 | Steps: 4 | Val loss: 136.7679 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 7.7648 | Steps: 4 | Val loss: 22.3842 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2939 | Steps: 4 | Val loss: 1.6669 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=170962)[0m top1: 0.37826492537313433
[2m[36m(func pid=170962)[0m top5: 0.8586753731343284
[2m[36m(func pid=170962)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=170962)[0m f1_macro: 0.346802971952003
[2m[36m(func pid=170962)[0m f1_weighted: 0.3783966975050397
[2m[36m(func pid=170962)[0m f1_per_class: [0.563, 0.544, 0.277, 0.379, 0.176, 0.289, 0.33, 0.371, 0.256, 0.283]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:44:52 (running for 00:37:10.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.467 |      0.354 |                   87 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.072 |      0.347 |                   37 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.95  |      0.279 |                   37 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 22.313 |      0.303 |                   24 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.30923507462686567
[2m[36m(func pid=174857)[0m top5: 0.7271455223880597
[2m[36m(func pid=174857)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=174857)[0m f1_macro: 0.3027738088785049
[2m[36m(func pid=174857)[0m f1_weighted: 0.3226653722272048
[2m[36m(func pid=174857)[0m f1_per_class: [0.388, 0.502, 0.55, 0.488, 0.059, 0.16, 0.13, 0.39, 0.211, 0.15]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.23134328358208955
[2m[36m(func pid=171528)[0m top5: 0.8493470149253731
[2m[36m(func pid=171528)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=171528)[0m f1_macro: 0.24300749844696426
[2m[36m(func pid=171528)[0m f1_weighted: 0.2174758293597323
[2m[36m(func pid=171528)[0m f1_per_class: [0.553, 0.59, 0.351, 0.199, 0.111, 0.137, 0.051, 0.149, 0.151, 0.138]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.41651119402985076
[2m[36m(func pid=158650)[0m top5: 0.9053171641791045
[2m[36m(func pid=158650)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=158650)[0m f1_macro: 0.3578234135036991
[2m[36m(func pid=158650)[0m f1_weighted: 0.4357364831782021
[2m[36m(func pid=158650)[0m f1_per_class: [0.569, 0.528, 0.317, 0.541, 0.144, 0.223, 0.413, 0.349, 0.228, 0.266]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.1542 | Steps: 4 | Val loss: 2.7947 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 16.0623 | Steps: 4 | Val loss: 195.9069 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6903 | Steps: 4 | Val loss: 18.6622 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=170962)[0m top1: 0.3871268656716418
[2m[36m(func pid=170962)[0m top5: 0.8614738805970149
[2m[36m(func pid=170962)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=170962)[0m f1_macro: 0.34087594970778995
[2m[36m(func pid=170962)[0m f1_weighted: 0.3951776345484817
[2m[36m(func pid=170962)[0m f1_per_class: [0.517, 0.546, 0.245, 0.383, 0.154, 0.285, 0.386, 0.391, 0.246, 0.256]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.5068 | Steps: 4 | Val loss: 1.6718 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 11:44:58 (running for 00:37:15.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.294 |      0.358 |                   88 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.154 |      0.341 |                   38 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  7.765 |      0.243 |                   38 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 16.062 |      0.241 |                   25 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.23227611940298507
[2m[36m(func pid=174857)[0m top5: 0.6539179104477612
[2m[36m(func pid=174857)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=174857)[0m f1_macro: 0.24118251799481422
[2m[36m(func pid=174857)[0m f1_weighted: 0.24966412089029932
[2m[36m(func pid=174857)[0m f1_per_class: [0.394, 0.318, 0.369, 0.492, 0.036, 0.05, 0.04, 0.383, 0.165, 0.165]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3675373134328358
[2m[36m(func pid=171528)[0m top5: 0.8064365671641791
[2m[36m(func pid=171528)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=171528)[0m f1_macro: 0.3090762674839994
[2m[36m(func pid=171528)[0m f1_weighted: 0.32879876454472123
[2m[36m(func pid=171528)[0m f1_per_class: [0.567, 0.608, 0.377, 0.556, 0.16, 0.206, 0.034, 0.227, 0.15, 0.207]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.40951492537313433
[2m[36m(func pid=158650)[0m top5: 0.9090485074626866
[2m[36m(func pid=158650)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=158650)[0m f1_macro: 0.35113086350493466
[2m[36m(func pid=158650)[0m f1_weighted: 0.43002071726210717
[2m[36m(func pid=158650)[0m f1_per_class: [0.54, 0.512, 0.289, 0.555, 0.144, 0.237, 0.388, 0.341, 0.229, 0.276]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3640 | Steps: 4 | Val loss: 2.6321 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 4.3293 | Steps: 4 | Val loss: 207.4386 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.8290 | Steps: 4 | Val loss: 19.1147 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3978 | Steps: 4 | Val loss: 1.6515 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=170962)[0m top1: 0.41884328358208955
[2m[36m(func pid=170962)[0m top5: 0.878731343283582
[2m[36m(func pid=170962)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=170962)[0m f1_macro: 0.34990492803503426
[2m[36m(func pid=170962)[0m f1_weighted: 0.4323352454019519
[2m[36m(func pid=170962)[0m f1_per_class: [0.507, 0.548, 0.241, 0.392, 0.153, 0.287, 0.506, 0.359, 0.249, 0.258]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:45:03 (running for 00:37:20.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.507 |      0.351 |                   89 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.364 |      0.35  |                   39 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.69  |      0.309 |                   39 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  4.329 |      0.229 |                   26 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.23134328358208955
[2m[36m(func pid=174857)[0m top5: 0.6497201492537313
[2m[36m(func pid=174857)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=174857)[0m f1_macro: 0.22878356215435844
[2m[36m(func pid=174857)[0m f1_weighted: 0.23992341391300356
[2m[36m(func pid=174857)[0m f1_per_class: [0.43, 0.247, 0.264, 0.512, 0.038, 0.045, 0.031, 0.383, 0.166, 0.174]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.40904850746268656
[2m[36m(func pid=171528)[0m top5: 0.7714552238805971
[2m[36m(func pid=171528)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=171528)[0m f1_macro: 0.320281006844666
[2m[36m(func pid=171528)[0m f1_weighted: 0.33243930713456277
[2m[36m(func pid=171528)[0m f1_per_class: [0.596, 0.571, 0.441, 0.569, 0.171, 0.278, 0.018, 0.324, 0.027, 0.207]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4137126865671642
[2m[36m(func pid=158650)[0m top5: 0.9151119402985075
[2m[36m(func pid=158650)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=158650)[0m f1_macro: 0.367885929828864
[2m[36m(func pid=158650)[0m f1_weighted: 0.4350251639102312
[2m[36m(func pid=158650)[0m f1_per_class: [0.569, 0.529, 0.361, 0.52, 0.152, 0.257, 0.416, 0.328, 0.25, 0.297]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5447 | Steps: 4 | Val loss: 2.4972 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 74.9435 | Steps: 4 | Val loss: 232.0163 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 5.9764 | Steps: 4 | Val loss: 20.9461 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=170962)[0m top1: 0.44216417910447764
[2m[36m(func pid=170962)[0m top5: 0.8908582089552238
[2m[36m(func pid=170962)[0m f1_micro: 0.44216417910447764
[2m[36m(func pid=170962)[0m f1_macro: 0.358474539371919
[2m[36m(func pid=170962)[0m f1_weighted: 0.4609972449789369
[2m[36m(func pid=170962)[0m f1_per_class: [0.494, 0.527, 0.295, 0.439, 0.151, 0.292, 0.575, 0.328, 0.249, 0.235]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2760 | Steps: 4 | Val loss: 1.6458 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 11:45:08 (running for 00:37:25.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.398 |      0.368 |                   90 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.545 |      0.358 |                   40 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.829 |      0.32  |                   40 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 74.944 |      0.207 |                   27 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.21688432835820895
[2m[36m(func pid=174857)[0m top5: 0.6385261194029851
[2m[36m(func pid=174857)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=174857)[0m f1_macro: 0.2067370457862289
[2m[36m(func pid=174857)[0m f1_weighted: 0.21682787991837102
[2m[36m(func pid=174857)[0m f1_per_class: [0.419, 0.167, 0.147, 0.5, 0.047, 0.034, 0.019, 0.395, 0.113, 0.226]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.4141791044776119
[2m[36m(func pid=171528)[0m top5: 0.7588619402985075
[2m[36m(func pid=171528)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=171528)[0m f1_macro: 0.3531378970068597
[2m[36m(func pid=171528)[0m f1_weighted: 0.33404904627240933
[2m[36m(func pid=171528)[0m f1_per_class: [0.591, 0.561, 0.481, 0.543, 0.233, 0.29, 0.028, 0.387, 0.027, 0.389]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4155783582089552
[2m[36m(func pid=158650)[0m top5: 0.9160447761194029
[2m[36m(func pid=158650)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=158650)[0m f1_macro: 0.36858952023321034
[2m[36m(func pid=158650)[0m f1_weighted: 0.43146642724368905
[2m[36m(func pid=158650)[0m f1_per_class: [0.565, 0.546, 0.356, 0.531, 0.161, 0.259, 0.382, 0.342, 0.228, 0.316]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.0452 | Steps: 4 | Val loss: 2.5010 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.3093 | Steps: 4 | Val loss: 217.2798 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.6029 | Steps: 4 | Val loss: 19.5205 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:45:13 (running for 00:37:31.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.276 |      0.369 |                   91 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.045 |      0.365 |                   41 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  5.976 |      0.353 |                   41 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 74.944 |      0.207 |                   27 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.44776119402985076
[2m[36m(func pid=170962)[0m top5: 0.8936567164179104
[2m[36m(func pid=170962)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=170962)[0m f1_macro: 0.3652321602043579
[2m[36m(func pid=170962)[0m f1_weighted: 0.46425831462616385
[2m[36m(func pid=170962)[0m f1_per_class: [0.559, 0.528, 0.277, 0.441, 0.145, 0.259, 0.591, 0.332, 0.253, 0.27]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.5015 | Steps: 4 | Val loss: 1.6910 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=174857)[0m top1: 0.22807835820895522
[2m[36m(func pid=174857)[0m top5: 0.636660447761194
[2m[36m(func pid=174857)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=174857)[0m f1_macro: 0.21228147573218198
[2m[36m(func pid=174857)[0m f1_weighted: 0.2229625731339874
[2m[36m(func pid=174857)[0m f1_per_class: [0.413, 0.123, 0.084, 0.493, 0.09, 0.144, 0.031, 0.39, 0.105, 0.25]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.41138059701492535
[2m[36m(func pid=171528)[0m top5: 0.7789179104477612
[2m[36m(func pid=171528)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=171528)[0m f1_macro: 0.3480247163246285
[2m[36m(func pid=171528)[0m f1_weighted: 0.3408422274187635
[2m[36m(func pid=171528)[0m f1_per_class: [0.537, 0.57, 0.491, 0.543, 0.154, 0.25, 0.06, 0.388, 0.08, 0.408]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.40578358208955223
[2m[36m(func pid=158650)[0m top5: 0.9109141791044776
[2m[36m(func pid=158650)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=158650)[0m f1_macro: 0.3673323356455108
[2m[36m(func pid=158650)[0m f1_weighted: 0.42045935349901714
[2m[36m(func pid=158650)[0m f1_per_class: [0.574, 0.55, 0.347, 0.527, 0.126, 0.231, 0.351, 0.352, 0.252, 0.364]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5753 | Steps: 4 | Val loss: 2.4404 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 59.9348 | Steps: 4 | Val loss: 206.0277 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 6.3081 | Steps: 4 | Val loss: 17.0908 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:45:19 (running for 00:37:36.59)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.501 |      0.367 |                   92 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.045 |      0.365 |                   41 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  5.603 |      0.348 |                   42 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 59.935 |      0.233 |                   29 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.45149253731343286
[2m[36m(func pid=170962)[0m top5: 0.9137126865671642
[2m[36m(func pid=170962)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=170962)[0m f1_macro: 0.3659023993932693
[2m[36m(func pid=170962)[0m f1_weighted: 0.4707687215910812
[2m[36m(func pid=170962)[0m f1_per_class: [0.598, 0.511, 0.274, 0.472, 0.146, 0.232, 0.611, 0.297, 0.205, 0.314]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m top1: 0.2453358208955224
[2m[36m(func pid=174857)[0m top5: 0.6497201492537313
[2m[36m(func pid=174857)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=174857)[0m f1_macro: 0.23345869713834863
[2m[36m(func pid=174857)[0m f1_weighted: 0.24172819286960368
[2m[36m(func pid=174857)[0m f1_per_class: [0.4, 0.102, 0.066, 0.492, 0.144, 0.265, 0.057, 0.393, 0.11, 0.306]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2616 | Steps: 4 | Val loss: 1.6942 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=171528)[0m top1: 0.40345149253731344
[2m[36m(func pid=171528)[0m top5: 0.808768656716418
[2m[36m(func pid=171528)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=171528)[0m f1_macro: 0.33543996203104653
[2m[36m(func pid=171528)[0m f1_weighted: 0.3613233291279668
[2m[36m(func pid=171528)[0m f1_per_class: [0.481, 0.578, 0.413, 0.559, 0.13, 0.169, 0.142, 0.355, 0.19, 0.337]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m top1: 0.4001865671641791
[2m[36m(func pid=158650)[0m top5: 0.9127798507462687
[2m[36m(func pid=158650)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=158650)[0m f1_macro: 0.37364879913517185
[2m[36m(func pid=158650)[0m f1_weighted: 0.4086121048616839
[2m[36m(func pid=158650)[0m f1_per_class: [0.607, 0.562, 0.387, 0.536, 0.125, 0.219, 0.298, 0.342, 0.25, 0.411]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.1788 | Steps: 4 | Val loss: 172.4205 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.0981 | Steps: 4 | Val loss: 2.4086 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7688 | Steps: 4 | Val loss: 18.5657 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:45:24 (running for 00:37:42.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.262 |      0.374 |                   93 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.575 |      0.366 |                   42 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  6.308 |      0.335 |                   43 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  1.179 |      0.27  |                   30 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.2980410447761194
[2m[36m(func pid=174857)[0m top5: 0.7140858208955224
[2m[36m(func pid=174857)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=174857)[0m f1_macro: 0.2695205564912522
[2m[36m(func pid=174857)[0m f1_weighted: 0.28937506139207136
[2m[36m(func pid=174857)[0m f1_per_class: [0.361, 0.234, 0.079, 0.51, 0.242, 0.297, 0.106, 0.39, 0.16, 0.316]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.45382462686567165
[2m[36m(func pid=170962)[0m top5: 0.9174440298507462
[2m[36m(func pid=170962)[0m f1_micro: 0.45382462686567165
[2m[36m(func pid=170962)[0m f1_macro: 0.37193192263493113
[2m[36m(func pid=170962)[0m f1_weighted: 0.4793970889562463
[2m[36m(func pid=170962)[0m f1_per_class: [0.589, 0.468, 0.302, 0.522, 0.162, 0.24, 0.609, 0.345, 0.177, 0.305]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3031716417910448
[2m[36m(func pid=171528)[0m top5: 0.78125
[2m[36m(func pid=171528)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=171528)[0m f1_macro: 0.2680993427065409
[2m[36m(func pid=171528)[0m f1_weighted: 0.3201554308109515
[2m[36m(func pid=171528)[0m f1_per_class: [0.339, 0.464, 0.364, 0.515, 0.093, 0.1, 0.182, 0.245, 0.119, 0.26]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.4187 | Steps: 4 | Val loss: 1.7126 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.6137 | Steps: 4 | Val loss: 151.4263 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=158650)[0m top1: 0.3983208955223881
[2m[36m(func pid=158650)[0m top5: 0.9085820895522388
[2m[36m(func pid=158650)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=158650)[0m f1_macro: 0.37541129169057147
[2m[36m(func pid=158650)[0m f1_weighted: 0.40710855754344616
[2m[36m(func pid=158650)[0m f1_per_class: [0.59, 0.56, 0.361, 0.538, 0.125, 0.229, 0.287, 0.352, 0.244, 0.469]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9269 | Steps: 4 | Val loss: 30.3846 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.1390 | Steps: 4 | Val loss: 2.4145 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 11:45:29 (running for 00:37:47.42)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.419 |      0.375 |                   94 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.098 |      0.372 |                   43 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.769 |      0.268 |                   44 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  1.614 |      0.31  |                   31 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.353544776119403
[2m[36m(func pid=174857)[0m top5: 0.7639925373134329
[2m[36m(func pid=174857)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=174857)[0m f1_macro: 0.3095413347185931
[2m[36m(func pid=174857)[0m f1_weighted: 0.34021155363609856
[2m[36m(func pid=174857)[0m f1_per_class: [0.423, 0.363, 0.126, 0.554, 0.367, 0.339, 0.148, 0.34, 0.153, 0.281]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.13152985074626866
[2m[36m(func pid=171528)[0m top5: 0.7527985074626866
[2m[36m(func pid=171528)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=171528)[0m f1_macro: 0.1974831957721678
[2m[36m(func pid=171528)[0m f1_weighted: 0.14879739247707152
[2m[36m(func pid=171528)[0m f1_per_class: [0.421, 0.163, 0.407, 0.218, 0.086, 0.036, 0.093, 0.186, 0.07, 0.295]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4361007462686567
[2m[36m(func pid=170962)[0m top5: 0.9216417910447762
[2m[36m(func pid=170962)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=170962)[0m f1_macro: 0.3815794927308792
[2m[36m(func pid=170962)[0m f1_weighted: 0.4671127724244217
[2m[36m(func pid=170962)[0m f1_per_class: [0.675, 0.426, 0.369, 0.523, 0.168, 0.23, 0.591, 0.335, 0.15, 0.351]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3398 | Steps: 4 | Val loss: 1.7783 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 26.7317 | Steps: 4 | Val loss: 139.8626 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.9978 | Steps: 4 | Val loss: 35.0331 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=158650)[0m top1: 0.39132462686567165
[2m[36m(func pid=158650)[0m top5: 0.8997201492537313
[2m[36m(func pid=158650)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=158650)[0m f1_macro: 0.3640150642542905
[2m[36m(func pid=158650)[0m f1_weighted: 0.39490905852262964
[2m[36m(func pid=158650)[0m f1_per_class: [0.6, 0.569, 0.316, 0.522, 0.126, 0.219, 0.26, 0.355, 0.245, 0.429]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.0445 | Steps: 4 | Val loss: 2.4121 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:45:35 (running for 00:37:52.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.34  |      0.364 |                   95 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.139 |      0.382 |                   44 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.927 |      0.197 |                   45 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 26.732 |      0.324 |                   32 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.376865671641791
[2m[36m(func pid=174857)[0m top5: 0.7835820895522388
[2m[36m(func pid=174857)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=174857)[0m f1_macro: 0.32388158742442363
[2m[36m(func pid=174857)[0m f1_weighted: 0.36894672486614877
[2m[36m(func pid=174857)[0m f1_per_class: [0.482, 0.441, 0.152, 0.564, 0.267, 0.325, 0.195, 0.322, 0.164, 0.327]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.11007462686567164
[2m[36m(func pid=171528)[0m top5: 0.7243470149253731
[2m[36m(func pid=171528)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=171528)[0m f1_macro: 0.18049280189567524
[2m[36m(func pid=171528)[0m f1_weighted: 0.11665362904524973
[2m[36m(func pid=171528)[0m f1_per_class: [0.478, 0.062, 0.407, 0.104, 0.075, 0.038, 0.15, 0.171, 0.072, 0.248]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4351679104477612
[2m[36m(func pid=170962)[0m top5: 0.9333022388059702
[2m[36m(func pid=170962)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=170962)[0m f1_macro: 0.39259996845928435
[2m[36m(func pid=170962)[0m f1_weighted: 0.4694845735453704
[2m[36m(func pid=170962)[0m f1_per_class: [0.684, 0.439, 0.4, 0.552, 0.194, 0.232, 0.561, 0.327, 0.143, 0.395]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3979 | Steps: 4 | Val loss: 1.8236 | Batch size: 32 | lr: 0.0001 | Duration: 3.30s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 15.9514 | Steps: 4 | Val loss: 125.2159 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 5.8778 | Steps: 4 | Val loss: 36.2692 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=158650)[0m top1: 0.37966417910447764
[2m[36m(func pid=158650)[0m top5: 0.8903917910447762
[2m[36m(func pid=158650)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=158650)[0m f1_macro: 0.3503034257860945
[2m[36m(func pid=158650)[0m f1_weighted: 0.3785401305724951
[2m[36m(func pid=158650)[0m f1_per_class: [0.579, 0.539, 0.329, 0.519, 0.131, 0.248, 0.219, 0.356, 0.234, 0.349]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3782 | Steps: 4 | Val loss: 2.5468 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:45:40 (running for 00:37:58.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.398 |      0.35  |                   96 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.044 |      0.393 |                   45 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.998 |      0.18  |                   46 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 15.951 |      0.348 |                   33 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.40298507462686567
[2m[36m(func pid=174857)[0m top5: 0.7980410447761194
[2m[36m(func pid=174857)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=174857)[0m f1_macro: 0.3482986270161457
[2m[36m(func pid=174857)[0m f1_weighted: 0.40734627756165237
[2m[36m(func pid=174857)[0m f1_per_class: [0.507, 0.496, 0.187, 0.562, 0.263, 0.34, 0.284, 0.314, 0.182, 0.348]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.12313432835820895
[2m[36m(func pid=171528)[0m top5: 0.6786380597014925
[2m[36m(func pid=171528)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=171528)[0m f1_macro: 0.1692563132957225
[2m[36m(func pid=171528)[0m f1_weighted: 0.13314988153473328
[2m[36m(func pid=171528)[0m f1_per_class: [0.412, 0.052, 0.379, 0.083, 0.051, 0.029, 0.243, 0.156, 0.08, 0.207]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.41744402985074625
[2m[36m(func pid=170962)[0m top5: 0.9328358208955224
[2m[36m(func pid=170962)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=170962)[0m f1_macro: 0.3931231552473116
[2m[36m(func pid=170962)[0m f1_weighted: 0.45668704000966953
[2m[36m(func pid=170962)[0m f1_per_class: [0.658, 0.399, 0.44, 0.554, 0.205, 0.241, 0.533, 0.346, 0.136, 0.419]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2689 | Steps: 4 | Val loss: 1.8260 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 26.6300 | Steps: 4 | Val loss: 107.8697 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1410 | Steps: 4 | Val loss: 32.2374 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.0977 | Steps: 4 | Val loss: 2.6388 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=158650)[0m top1: 0.37826492537313433
[2m[36m(func pid=158650)[0m top5: 0.8908582089552238
[2m[36m(func pid=158650)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=158650)[0m f1_macro: 0.3543116841855992
[2m[36m(func pid=158650)[0m f1_weighted: 0.3769986834815051
[2m[36m(func pid=158650)[0m f1_per_class: [0.576, 0.542, 0.342, 0.494, 0.16, 0.267, 0.228, 0.345, 0.255, 0.333]
[2m[36m(func pid=158650)[0m 
[2m[36m(func pid=171528)[0m top1: 0.17583955223880596
[2m[36m(func pid=171528)[0m top5: 0.6902985074626866
[2m[36m(func pid=171528)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=171528)[0m f1_macro: 0.18407000530307094
[2m[36m(func pid=171528)[0m f1_weighted: 0.19938013876917646
[2m[36m(func pid=171528)[0m f1_per_class: [0.377, 0.062, 0.379, 0.136, 0.045, 0.042, 0.41, 0.152, 0.09, 0.147]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.4468283582089552
[2m[36m(func pid=174857)[0m top5: 0.8148320895522388
[2m[36m(func pid=174857)[0m f1_micro: 0.4468283582089552
[2m[36m(func pid=174857)[0m f1_macro: 0.3584524878195698
[2m[36m(func pid=174857)[0m f1_weighted: 0.4511338423009034
[2m[36m(func pid=174857)[0m f1_per_class: [0.539, 0.516, 0.32, 0.569, 0.222, 0.281, 0.437, 0.327, 0.157, 0.216]
== Status ==
Current time: 2024-01-07 11:45:46 (running for 00:38:03.69)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.269 |      0.354 |                   97 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.378 |      0.393 |                   46 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.141 |      0.184 |                   48 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 15.951 |      0.348 |                   33 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4207089552238806
[2m[36m(func pid=170962)[0m top5: 0.9281716417910447
[2m[36m(func pid=170962)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=170962)[0m f1_macro: 0.3946377646557491
[2m[36m(func pid=170962)[0m f1_weighted: 0.4535599778013092
[2m[36m(func pid=170962)[0m f1_per_class: [0.667, 0.461, 0.423, 0.567, 0.188, 0.245, 0.468, 0.364, 0.152, 0.412]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2321 | Steps: 4 | Val loss: 1.8234 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 4.6043 | Steps: 4 | Val loss: 27.0478 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 13.9125 | Steps: 4 | Val loss: 110.8771 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1331 | Steps: 4 | Val loss: 2.6089 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=158650)[0m top1: 0.38152985074626866
[2m[36m(func pid=158650)[0m top5: 0.894589552238806
[2m[36m(func pid=158650)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=158650)[0m f1_macro: 0.36112282114503924
[2m[36m(func pid=158650)[0m f1_weighted: 0.3798132536513632
[2m[36m(func pid=158650)[0m f1_per_class: [0.588, 0.547, 0.4, 0.501, 0.16, 0.267, 0.227, 0.35, 0.248, 0.323]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:45:51 (running for 00:38:09.05)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.232 |      0.361 |                   98 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.098 |      0.395 |                   47 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.141 |      0.184 |                   48 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 13.912 |      0.355 |                   35 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=171528)[0m top1: 0.24486940298507462
[2m[36m(func pid=171528)[0m top5: 0.7430037313432836
[2m[36m(func pid=171528)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=171528)[0m f1_macro: 0.2132804550045393
[2m[36m(func pid=171528)[0m f1_weighted: 0.27621637828042167
[2m[36m(func pid=171528)[0m f1_per_class: [0.333, 0.151, 0.4, 0.253, 0.047, 0.064, 0.5, 0.14, 0.128, 0.117]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m top1: 0.46175373134328357
[2m[36m(func pid=174857)[0m top5: 0.8152985074626866
[2m[36m(func pid=174857)[0m f1_micro: 0.46175373134328357
[2m[36m(func pid=174857)[0m f1_macro: 0.3554929901559656
[2m[36m(func pid=174857)[0m f1_weighted: 0.4603686109368728
[2m[36m(func pid=174857)[0m f1_per_class: [0.467, 0.513, 0.253, 0.558, 0.273, 0.232, 0.501, 0.34, 0.141, 0.278]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.42490671641791045
[2m[36m(func pid=170962)[0m top5: 0.9281716417910447
[2m[36m(func pid=170962)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=170962)[0m f1_macro: 0.397702447253501
[2m[36m(func pid=170962)[0m f1_weighted: 0.45066476857593635
[2m[36m(func pid=170962)[0m f1_per_class: [0.658, 0.467, 0.449, 0.577, 0.195, 0.262, 0.437, 0.372, 0.154, 0.406]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.8293 | Steps: 4 | Val loss: 1.8820 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 17.4022 | Steps: 4 | Val loss: 116.5954 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5211 | Steps: 4 | Val loss: 21.9121 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.0853 | Steps: 4 | Val loss: 2.6578 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=158650)[0m top1: 0.3787313432835821
[2m[36m(func pid=158650)[0m top5: 0.878731343283582
[2m[36m(func pid=158650)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=158650)[0m f1_macro: 0.3556383567616649
[2m[36m(func pid=158650)[0m f1_weighted: 0.3674302146325185
[2m[36m(func pid=158650)[0m f1_per_class: [0.614, 0.555, 0.343, 0.493, 0.134, 0.276, 0.18, 0.367, 0.25, 0.344]
[2m[36m(func pid=158650)[0m 
== Status ==
Current time: 2024-01-07 11:45:56 (running for 00:38:14.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00016 | RUNNING    | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.829 |      0.356 |                   99 |
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.133 |      0.398 |                   48 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  4.604 |      0.213 |                   49 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 17.402 |      0.359 |                   36 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.47574626865671643
[2m[36m(func pid=174857)[0m top5: 0.8138992537313433
[2m[36m(func pid=174857)[0m f1_micro: 0.47574626865671643
[2m[36m(func pid=174857)[0m f1_macro: 0.3592181332734297
[2m[36m(func pid=174857)[0m f1_weighted: 0.4731722850415808
[2m[36m(func pid=174857)[0m f1_per_class: [0.383, 0.535, 0.216, 0.56, 0.25, 0.205, 0.54, 0.313, 0.21, 0.381]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.314365671641791
[2m[36m(func pid=171528)[0m top5: 0.8111007462686567
[2m[36m(func pid=171528)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=171528)[0m f1_macro: 0.24324920819918408
[2m[36m(func pid=171528)[0m f1_weighted: 0.34370232667048134
[2m[36m(func pid=171528)[0m f1_per_class: [0.312, 0.219, 0.426, 0.424, 0.062, 0.065, 0.523, 0.166, 0.128, 0.108]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.43423507462686567
[2m[36m(func pid=170962)[0m top5: 0.9146455223880597
[2m[36m(func pid=170962)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=170962)[0m f1_macro: 0.3963060479345898
[2m[36m(func pid=170962)[0m f1_weighted: 0.4437717937858058
[2m[36m(func pid=170962)[0m f1_per_class: [0.667, 0.498, 0.458, 0.587, 0.207, 0.295, 0.372, 0.384, 0.167, 0.327]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=158650)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.8108 | Steps: 4 | Val loss: 1.9300 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7608 | Steps: 4 | Val loss: 121.4357 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.1183 | Steps: 4 | Val loss: 18.9336 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.1359 | Steps: 4 | Val loss: 2.7833 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:46:01 (running for 00:38:19.40)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.085 |      0.396 |                   49 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.521 |      0.243 |                   50 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 17.402 |      0.359 |                   36 |
| train_98a10_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=158650)[0m top1: 0.37453358208955223
[2m[36m(func pid=158650)[0m top5: 0.8614738805970149
[2m[36m(func pid=158650)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=158650)[0m f1_macro: 0.3517546512123569
[2m[36m(func pid=158650)[0m f1_weighted: 0.36541195033035934
[2m[36m(func pid=158650)[0m f1_per_class: [0.614, 0.562, 0.333, 0.479, 0.121, 0.235, 0.198, 0.369, 0.256, 0.352]
[2m[36m(func pid=174857)[0m top1: 0.46968283582089554
[2m[36m(func pid=174857)[0m top5: 0.8078358208955224
[2m[36m(func pid=174857)[0m f1_micro: 0.46968283582089554
[2m[36m(func pid=174857)[0m f1_macro: 0.35844525540357797
[2m[36m(func pid=174857)[0m f1_weighted: 0.4666172509548654
[2m[36m(func pid=174857)[0m f1_per_class: [0.383, 0.507, 0.211, 0.566, 0.242, 0.162, 0.545, 0.304, 0.202, 0.464]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.363339552238806
[2m[36m(func pid=171528)[0m top5: 0.8596082089552238
[2m[36m(func pid=171528)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=171528)[0m f1_macro: 0.28130011793558274
[2m[36m(func pid=171528)[0m f1_weighted: 0.38741529413171155
[2m[36m(func pid=171528)[0m f1_per_class: [0.327, 0.252, 0.453, 0.507, 0.089, 0.089, 0.533, 0.302, 0.14, 0.122]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4300373134328358
[2m[36m(func pid=170962)[0m top5: 0.9048507462686567
[2m[36m(func pid=170962)[0m f1_micro: 0.4300373134328358
[2m[36m(func pid=170962)[0m f1_macro: 0.39506346671368353
[2m[36m(func pid=170962)[0m f1_weighted: 0.4267798228215482
[2m[36m(func pid=170962)[0m f1_per_class: [0.684, 0.507, 0.491, 0.588, 0.207, 0.31, 0.3, 0.383, 0.212, 0.27]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 13.1837 | Steps: 4 | Val loss: 131.9779 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6927 | Steps: 4 | Val loss: 17.0737 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5430 | Steps: 4 | Val loss: 2.8984 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=174857)[0m top1: 0.44076492537313433
[2m[36m(func pid=174857)[0m top5: 0.7933768656716418
[2m[36m(func pid=174857)[0m f1_micro: 0.44076492537313433
[2m[36m(func pid=174857)[0m f1_macro: 0.3318638291832768
[2m[36m(func pid=174857)[0m f1_weighted: 0.44489987017714355
[2m[36m(func pid=174857)[0m f1_per_class: [0.391, 0.438, 0.198, 0.554, 0.195, 0.158, 0.531, 0.312, 0.162, 0.378]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m top1: 0.38619402985074625
[2m[36m(func pid=171528)[0m top5: 0.8717350746268657
[2m[36m(func pid=171528)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=171528)[0m f1_macro: 0.303207933814155
[2m[36m(func pid=171528)[0m f1_weighted: 0.4027906526459399
[2m[36m(func pid=171528)[0m f1_per_class: [0.37, 0.314, 0.489, 0.524, 0.138, 0.164, 0.504, 0.309, 0.08, 0.141]
[2m[36m(func pid=170962)[0m top1: 0.4319029850746269
[2m[36m(func pid=170962)[0m top5: 0.902518656716418
[2m[36m(func pid=170962)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=170962)[0m f1_macro: 0.3979834740645026
[2m[36m(func pid=170962)[0m f1_weighted: 0.4127296074149682
[2m[36m(func pid=170962)[0m f1_per_class: [0.684, 0.52, 0.456, 0.594, 0.224, 0.318, 0.229, 0.386, 0.262, 0.308]
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 13.6019 | Steps: 4 | Val loss: 149.0655 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:46:07 (running for 00:38:25.02)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.136 |      0.395 |                   50 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.118 |      0.281 |                   51 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 13.184 |      0.332 |                   38 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=183693)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=183693)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=183693)[0m Configuration completed!
[2m[36m(func pid=183693)[0m New optimizer parameters:
[2m[36m(func pid=183693)[0m SGD (
[2m[36m(func pid=183693)[0m Parameter Group 0
[2m[36m(func pid=183693)[0m     dampening: 0
[2m[36m(func pid=183693)[0m     differentiable: False
[2m[36m(func pid=183693)[0m     foreach: None
[2m[36m(func pid=183693)[0m     lr: 0.0001
[2m[36m(func pid=183693)[0m     maximize: False
[2m[36m(func pid=183693)[0m     momentum: 0.9
[2m[36m(func pid=183693)[0m     nesterov: False
[2m[36m(func pid=183693)[0m     weight_decay: 1e-05
[2m[36m(func pid=183693)[0m )
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:46:13 (running for 00:38:30.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.543 |      0.398 |                   51 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.693 |      0.303 |                   52 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 13.602 |      0.324 |                   39 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.417910447761194
[2m[36m(func pid=174857)[0m top5: 0.8069029850746269
[2m[36m(func pid=174857)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=174857)[0m f1_macro: 0.32404937001327794
[2m[36m(func pid=174857)[0m f1_weighted: 0.4148207480059397
[2m[36m(func pid=174857)[0m f1_per_class: [0.432, 0.423, 0.343, 0.571, 0.132, 0.101, 0.449, 0.274, 0.17, 0.346]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.6729 | Steps: 4 | Val loss: 16.8558 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1323 | Steps: 4 | Val loss: 2.9641 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2979 | Steps: 4 | Val loss: 2.5497 | Batch size: 32 | lr: 0.0001 | Duration: 4.45s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 8.4490 | Steps: 4 | Val loss: 163.8349 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=171528)[0m top1: 0.394589552238806
[2m[36m(func pid=171528)[0m top5: 0.8899253731343284
[2m[36m(func pid=171528)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=171528)[0m f1_macro: 0.32314729278800747
[2m[36m(func pid=171528)[0m f1_weighted: 0.4164422580721937
[2m[36m(func pid=171528)[0m f1_per_class: [0.434, 0.395, 0.522, 0.579, 0.139, 0.162, 0.448, 0.298, 0.081, 0.174]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4244402985074627
[2m[36m(func pid=170962)[0m top5: 0.8917910447761194
[2m[36m(func pid=170962)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=170962)[0m f1_macro: 0.3835002436224014
[2m[36m(func pid=170962)[0m f1_weighted: 0.41255689510302723
[2m[36m(func pid=170962)[0m f1_per_class: [0.636, 0.544, 0.441, 0.592, 0.211, 0.3, 0.229, 0.402, 0.234, 0.244]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.06063432835820896
[2m[36m(func pid=183693)[0m top5: 0.488339552238806
[2m[36m(func pid=183693)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=183693)[0m f1_macro: 0.03545775645417722
[2m[36m(func pid=183693)[0m f1_weighted: 0.033330000606031904
[2m[36m(func pid=183693)[0m f1_per_class: [0.086, 0.015, 0.0, 0.069, 0.0, 0.025, 0.0, 0.097, 0.024, 0.039]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:46:18 (running for 00:38:35.72)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.132 |      0.384 |                   52 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.673 |      0.323 |                   53 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.449 |      0.331 |                   40 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  3.298 |      0.035 |                    1 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.38572761194029853
[2m[36m(func pid=174857)[0m top5: 0.7817164179104478
[2m[36m(func pid=174857)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=174857)[0m f1_macro: 0.33109836928017694
[2m[36m(func pid=174857)[0m f1_weighted: 0.39606610845260004
[2m[36m(func pid=174857)[0m f1_per_class: [0.458, 0.363, 0.471, 0.553, 0.115, 0.105, 0.428, 0.296, 0.172, 0.352]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.1782 | Steps: 4 | Val loss: 3.0478 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.9063 | Steps: 4 | Val loss: 17.5800 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9696 | Steps: 4 | Val loss: 2.5503 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 5.0753 | Steps: 4 | Val loss: 203.8734 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=170962)[0m top1: 0.4244402985074627
[2m[36m(func pid=170962)[0m top5: 0.8903917910447762
[2m[36m(func pid=170962)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=170962)[0m f1_macro: 0.37633736515112276
[2m[36m(func pid=170962)[0m f1_weighted: 0.4064248103182621
[2m[36m(func pid=170962)[0m f1_per_class: [0.63, 0.543, 0.419, 0.607, 0.211, 0.316, 0.196, 0.381, 0.222, 0.237]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m top1: 0.39552238805970147
[2m[36m(func pid=171528)[0m top5: 0.9039179104477612
[2m[36m(func pid=171528)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=171528)[0m f1_macro: 0.34862615928819374
[2m[36m(func pid=171528)[0m f1_weighted: 0.4127707428676322
[2m[36m(func pid=171528)[0m f1_per_class: [0.547, 0.445, 0.558, 0.605, 0.179, 0.187, 0.366, 0.264, 0.077, 0.257]
[2m[36m(func pid=171528)[0m 
== Status ==
Current time: 2024-01-07 11:46:23 (running for 00:38:40.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.178 |      0.376 |                   53 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.906 |      0.349 |                   54 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.449 |      0.331 |                   40 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.97  |      0.033 |                    2 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.06063432835820896
[2m[36m(func pid=183693)[0m top5: 0.47994402985074625
[2m[36m(func pid=183693)[0m f1_micro: 0.06063432835820896
[2m[36m(func pid=183693)[0m f1_macro: 0.032950744987609776
[2m[36m(func pid=183693)[0m f1_weighted: 0.039127229367926404
[2m[36m(func pid=183693)[0m f1_per_class: [0.057, 0.041, 0.0, 0.083, 0.0, 0.019, 0.0, 0.093, 0.0, 0.038]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.29850746268656714
[2m[36m(func pid=174857)[0m top5: 0.7476679104477612
[2m[36m(func pid=174857)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=174857)[0m f1_macro: 0.28731978784907664
[2m[36m(func pid=174857)[0m f1_weighted: 0.3175788298339527
[2m[36m(func pid=174857)[0m f1_per_class: [0.492, 0.339, 0.468, 0.506, 0.106, 0.067, 0.251, 0.253, 0.129, 0.263]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.8625 | Steps: 4 | Val loss: 19.0547 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.3872 | Steps: 4 | Val loss: 3.2270 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1153 | Steps: 4 | Val loss: 2.5641 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.0200 | Steps: 4 | Val loss: 227.7872 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=171528)[0m top1: 0.37966417910447764
[2m[36m(func pid=171528)[0m top5: 0.9155783582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=171528)[0m f1_macro: 0.3596250973631043
[2m[36m(func pid=171528)[0m f1_weighted: 0.39261968131585784
[2m[36m(func pid=171528)[0m f1_per_class: [0.535, 0.452, 0.632, 0.615, 0.224, 0.17, 0.288, 0.242, 0.12, 0.319]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.3987873134328358
[2m[36m(func pid=170962)[0m top5: 0.882929104477612
[2m[36m(func pid=170962)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=170962)[0m f1_macro: 0.3444281791114143
[2m[36m(func pid=170962)[0m f1_weighted: 0.38147123626626356
[2m[36m(func pid=170962)[0m f1_per_class: [0.574, 0.5, 0.388, 0.597, 0.184, 0.288, 0.171, 0.376, 0.159, 0.207]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:28 (running for 00:38:46.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.387 |      0.344 |                   54 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.863 |      0.36  |                   55 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  5.075 |      0.287 |                   41 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  3.115 |      0.034 |                    3 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.06110074626865672
[2m[36m(func pid=183693)[0m top5: 0.4566231343283582
[2m[36m(func pid=183693)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=183693)[0m f1_macro: 0.03393033426144258
[2m[36m(func pid=183693)[0m f1_weighted: 0.045533046046268705
[2m[36m(func pid=183693)[0m f1_per_class: [0.045, 0.077, 0.0, 0.09, 0.0, 0.006, 0.0, 0.09, 0.0, 0.031]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.26259328358208955
[2m[36m(func pid=174857)[0m top5: 0.7234141791044776
[2m[36m(func pid=174857)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=174857)[0m f1_macro: 0.29717069528261353
[2m[36m(func pid=174857)[0m f1_weighted: 0.29329298292369593
[2m[36m(func pid=174857)[0m f1_per_class: [0.547, 0.371, 0.647, 0.457, 0.118, 0.066, 0.187, 0.289, 0.114, 0.174]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3635 | Steps: 4 | Val loss: 18.1777 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4165 | Steps: 4 | Val loss: 3.5739 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0605 | Steps: 4 | Val loss: 2.5702 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 16.3396 | Steps: 4 | Val loss: 263.6180 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=171528)[0m top1: 0.4085820895522388
[2m[36m(func pid=171528)[0m top5: 0.9291044776119403
[2m[36m(func pid=171528)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=171528)[0m f1_macro: 0.3797093692216051
[2m[36m(func pid=171528)[0m f1_weighted: 0.40909980302843485
[2m[36m(func pid=171528)[0m f1_per_class: [0.543, 0.549, 0.571, 0.616, 0.226, 0.228, 0.254, 0.272, 0.133, 0.405]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.3773320895522388
[2m[36m(func pid=170962)[0m top5: 0.8577425373134329
[2m[36m(func pid=170962)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=170962)[0m f1_macro: 0.32203376535247596
[2m[36m(func pid=170962)[0m f1_weighted: 0.3596106345595861
[2m[36m(func pid=170962)[0m f1_per_class: [0.484, 0.456, 0.356, 0.58, 0.14, 0.25, 0.153, 0.391, 0.212, 0.198]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:34 (running for 00:38:51.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.416 |      0.322 |                   55 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.364 |      0.38  |                   56 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  0.02  |      0.297 |                   42 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  3.06  |      0.041 |                    4 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.06576492537313433
[2m[36m(func pid=183693)[0m top5: 0.4449626865671642
[2m[36m(func pid=183693)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=183693)[0m f1_macro: 0.0405534288109742
[2m[36m(func pid=183693)[0m f1_weighted: 0.058867847078696973
[2m[36m(func pid=183693)[0m f1_per_class: [0.048, 0.101, 0.0, 0.119, 0.0, 0.012, 0.0, 0.087, 0.022, 0.017]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.22761194029850745
[2m[36m(func pid=174857)[0m top5: 0.6856343283582089
[2m[36m(func pid=174857)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=174857)[0m f1_macro: 0.28474564026439825
[2m[36m(func pid=174857)[0m f1_weighted: 0.2597790757435745
[2m[36m(func pid=174857)[0m f1_per_class: [0.568, 0.387, 0.71, 0.419, 0.105, 0.047, 0.112, 0.274, 0.113, 0.112]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2893 | Steps: 4 | Val loss: 17.9478 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.1267 | Steps: 4 | Val loss: 3.7908 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0341 | Steps: 4 | Val loss: 2.5585 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 11.4065 | Steps: 4 | Val loss: 259.6634 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=171528)[0m top1: 0.42350746268656714
[2m[36m(func pid=171528)[0m top5: 0.9407649253731343
[2m[36m(func pid=171528)[0m f1_micro: 0.42350746268656714
[2m[36m(func pid=171528)[0m f1_macro: 0.3960322081845035
[2m[36m(func pid=171528)[0m f1_weighted: 0.4141654187151539
[2m[36m(func pid=171528)[0m f1_per_class: [0.492, 0.577, 0.611, 0.617, 0.25, 0.243, 0.242, 0.287, 0.151, 0.492]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.36240671641791045
[2m[36m(func pid=170962)[0m top5: 0.8344216417910447
[2m[36m(func pid=170962)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=170962)[0m f1_macro: 0.30625745721217645
[2m[36m(func pid=170962)[0m f1_weighted: 0.34627912643780706
[2m[36m(func pid=170962)[0m f1_per_class: [0.371, 0.426, 0.356, 0.572, 0.149, 0.252, 0.138, 0.395, 0.229, 0.174]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:39 (running for 00:38:56.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.127 |      0.306 |                   56 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.289 |      0.396 |                   57 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 16.34  |      0.285 |                   43 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  3.034 |      0.05  |                    5 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.07602611940298508
[2m[36m(func pid=183693)[0m top5: 0.435634328358209
[2m[36m(func pid=183693)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=183693)[0m f1_macro: 0.04956189965283084
[2m[36m(func pid=183693)[0m f1_weighted: 0.07258811558135228
[2m[36m(func pid=183693)[0m f1_per_class: [0.047, 0.127, 0.0, 0.148, 0.0, 0.016, 0.0, 0.088, 0.038, 0.032]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.22807835820895522
[2m[36m(func pid=174857)[0m top5: 0.6898320895522388
[2m[36m(func pid=174857)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=174857)[0m f1_macro: 0.28232270268789095
[2m[36m(func pid=174857)[0m f1_weighted: 0.2645374763156527
[2m[36m(func pid=174857)[0m f1_per_class: [0.489, 0.416, 0.667, 0.372, 0.091, 0.065, 0.135, 0.363, 0.149, 0.077]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2659 | Steps: 4 | Val loss: 17.0946 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3739 | Steps: 4 | Val loss: 3.8786 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9795 | Steps: 4 | Val loss: 2.5267 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.7075 | Steps: 4 | Val loss: 259.4466 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=171528)[0m top1: 0.4375
[2m[36m(func pid=171528)[0m top5: 0.9463619402985075
[2m[36m(func pid=171528)[0m f1_micro: 0.4375
[2m[36m(func pid=171528)[0m f1_macro: 0.40468988708141584
[2m[36m(func pid=171528)[0m f1_weighted: 0.42231649292383594
[2m[36m(func pid=171528)[0m f1_per_class: [0.467, 0.581, 0.632, 0.603, 0.216, 0.299, 0.251, 0.332, 0.148, 0.519]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m top1: 0.35494402985074625
[2m[36m(func pid=170962)[0m top5: 0.8264925373134329
[2m[36m(func pid=170962)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=170962)[0m f1_macro: 0.3048509899917456
[2m[36m(func pid=170962)[0m f1_weighted: 0.34543012398071204
[2m[36m(func pid=170962)[0m f1_per_class: [0.292, 0.429, 0.4, 0.562, 0.181, 0.279, 0.141, 0.379, 0.226, 0.16]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:44 (running for 00:39:02.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.374 |      0.305 |                   57 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.266 |      0.405 |                   58 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 11.407 |      0.282 |                   44 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.979 |      0.058 |                    6 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.08815298507462686
[2m[36m(func pid=183693)[0m top5: 0.43656716417910446
[2m[36m(func pid=183693)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=183693)[0m f1_macro: 0.05758088283568891
[2m[36m(func pid=183693)[0m f1_weighted: 0.08655600674768195
[2m[36m(func pid=183693)[0m f1_per_class: [0.04, 0.134, 0.0, 0.182, 0.0, 0.04, 0.0, 0.088, 0.058, 0.034]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.2490671641791045
[2m[36m(func pid=174857)[0m top5: 0.6926305970149254
[2m[36m(func pid=174857)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=174857)[0m f1_macro: 0.30073925500995674
[2m[36m(func pid=174857)[0m f1_weighted: 0.2693798203713087
[2m[36m(func pid=174857)[0m f1_per_class: [0.543, 0.501, 0.714, 0.346, 0.092, 0.064, 0.117, 0.382, 0.15, 0.097]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 3.9532 | Steps: 4 | Val loss: 16.9486 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2252 | Steps: 4 | Val loss: 3.9978 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9553 | Steps: 4 | Val loss: 2.4995 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=171528)[0m top1: 0.43796641791044777
[2m[36m(func pid=171528)[0m top5: 0.9505597014925373
[2m[36m(func pid=171528)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=171528)[0m f1_macro: 0.40777490015780093
[2m[36m(func pid=171528)[0m f1_weighted: 0.4196289187309439
[2m[36m(func pid=171528)[0m f1_per_class: [0.467, 0.566, 0.632, 0.574, 0.232, 0.336, 0.258, 0.363, 0.151, 0.5]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 27.4297 | Steps: 4 | Val loss: 266.4177 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=170962)[0m top1: 0.33348880597014924
[2m[36m(func pid=170962)[0m top5: 0.8157649253731343
[2m[36m(func pid=170962)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=170962)[0m f1_macro: 0.2912100943070574
[2m[36m(func pid=170962)[0m f1_weighted: 0.33421414300115665
[2m[36m(func pid=170962)[0m f1_per_class: [0.241, 0.385, 0.406, 0.538, 0.184, 0.284, 0.156, 0.384, 0.183, 0.15]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:50 (running for 00:39:07.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.225 |      0.291 |                   58 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  3.953 |      0.408 |                   59 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  1.708 |      0.301 |                   45 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.955 |      0.058 |                    7 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.08908582089552239
[2m[36m(func pid=183693)[0m top5: 0.4458955223880597
[2m[36m(func pid=183693)[0m f1_micro: 0.08908582089552237
[2m[36m(func pid=183693)[0m f1_macro: 0.05797241242837135
[2m[36m(func pid=183693)[0m f1_weighted: 0.08960204355448834
[2m[36m(func pid=183693)[0m f1_per_class: [0.045, 0.143, 0.0, 0.183, 0.0, 0.048, 0.003, 0.079, 0.051, 0.028]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.26259328358208955
[2m[36m(func pid=174857)[0m top5: 0.6996268656716418
[2m[36m(func pid=174857)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=174857)[0m f1_macro: 0.29847198689960913
[2m[36m(func pid=174857)[0m f1_weighted: 0.2663208761821159
[2m[36m(func pid=174857)[0m f1_per_class: [0.552, 0.528, 0.69, 0.302, 0.083, 0.075, 0.134, 0.342, 0.161, 0.119]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0742 | Steps: 4 | Val loss: 16.8798 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.1647 | Steps: 4 | Val loss: 4.0362 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8899 | Steps: 4 | Val loss: 2.4923 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=171528)[0m top1: 0.4412313432835821
[2m[36m(func pid=171528)[0m top5: 0.949160447761194
[2m[36m(func pid=171528)[0m f1_micro: 0.4412313432835821
[2m[36m(func pid=171528)[0m f1_macro: 0.42615166385550685
[2m[36m(func pid=171528)[0m f1_weighted: 0.42560620355966206
[2m[36m(func pid=171528)[0m f1_per_class: [0.525, 0.561, 0.645, 0.586, 0.233, 0.328, 0.255, 0.369, 0.259, 0.5]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0621 | Steps: 4 | Val loss: 250.9862 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=170962)[0m top1: 0.31763059701492535
[2m[36m(func pid=170962)[0m top5: 0.8120335820895522
[2m[36m(func pid=170962)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=170962)[0m f1_macro: 0.2875877543412145
[2m[36m(func pid=170962)[0m f1_weighted: 0.327529609342639
[2m[36m(func pid=170962)[0m f1_per_class: [0.199, 0.371, 0.433, 0.518, 0.194, 0.253, 0.174, 0.383, 0.189, 0.162]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:46:55 (running for 00:39:13.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.165 |      0.288 |                   59 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.074 |      0.426 |                   60 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 27.43  |      0.298 |                   46 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.89  |      0.053 |                    8 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.08722014925373134
[2m[36m(func pid=183693)[0m top5: 0.4375
[2m[36m(func pid=183693)[0m f1_micro: 0.08722014925373134
[2m[36m(func pid=183693)[0m f1_macro: 0.05288510160856086
[2m[36m(func pid=183693)[0m f1_weighted: 0.08982289384126589
[2m[36m(func pid=183693)[0m f1_per_class: [0.042, 0.133, 0.0, 0.202, 0.0, 0.039, 0.003, 0.051, 0.037, 0.022]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.2957089552238806
[2m[36m(func pid=174857)[0m top5: 0.707089552238806
[2m[36m(func pid=174857)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=174857)[0m f1_macro: 0.31703298423124643
[2m[36m(func pid=174857)[0m f1_weighted: 0.29097994472499317
[2m[36m(func pid=174857)[0m f1_per_class: [0.478, 0.541, 0.741, 0.31, 0.068, 0.078, 0.196, 0.356, 0.19, 0.213]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.4662 | Steps: 4 | Val loss: 17.3972 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0875 | Steps: 4 | Val loss: 4.0202 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8416 | Steps: 4 | Val loss: 2.4856 | Batch size: 32 | lr: 0.0001 | Duration: 3.33s
[2m[36m(func pid=171528)[0m top1: 0.4244402985074627
[2m[36m(func pid=171528)[0m top5: 0.9393656716417911
[2m[36m(func pid=171528)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=171528)[0m f1_macro: 0.4308359595044945
[2m[36m(func pid=171528)[0m f1_weighted: 0.4138186263024792
[2m[36m(func pid=171528)[0m f1_per_class: [0.571, 0.502, 0.71, 0.566, 0.24, 0.343, 0.256, 0.393, 0.236, 0.491]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 7.5052 | Steps: 4 | Val loss: 240.7659 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=170962)[0m top1: 0.3111007462686567
[2m[36m(func pid=170962)[0m top5: 0.8166977611940298
[2m[36m(func pid=170962)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=170962)[0m f1_macro: 0.2964311179212164
[2m[36m(func pid=170962)[0m f1_weighted: 0.3332363173556698
[2m[36m(func pid=170962)[0m f1_per_class: [0.181, 0.339, 0.52, 0.496, 0.207, 0.277, 0.224, 0.395, 0.149, 0.176]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:01 (running for 00:39:19.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.088 |      0.296 |                   60 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  3.466 |      0.431 |                   61 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  2.062 |      0.317 |                   47 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.842 |      0.055 |                    9 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.08442164179104478
[2m[36m(func pid=183693)[0m top5: 0.4230410447761194
[2m[36m(func pid=183693)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=183693)[0m f1_macro: 0.054986295226853034
[2m[36m(func pid=183693)[0m f1_weighted: 0.09164475776889493
[2m[36m(func pid=183693)[0m f1_per_class: [0.048, 0.123, 0.0, 0.207, 0.0, 0.05, 0.003, 0.06, 0.041, 0.018]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m top1: 0.3087686567164179
[2m[36m(func pid=174857)[0m top5: 0.7103544776119403
[2m[36m(func pid=174857)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=174857)[0m f1_macro: 0.3343062981092816
[2m[36m(func pid=174857)[0m f1_weighted: 0.3125934394429592
[2m[36m(func pid=174857)[0m f1_per_class: [0.49, 0.532, 0.741, 0.295, 0.067, 0.116, 0.269, 0.349, 0.198, 0.286]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1166 | Steps: 4 | Val loss: 18.6979 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3516 | Steps: 4 | Val loss: 4.4480 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=171528)[0m top1: 0.40718283582089554
[2m[36m(func pid=171528)[0m top5: 0.9230410447761194
[2m[36m(func pid=171528)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=171528)[0m f1_macro: 0.42013612962034663
[2m[36m(func pid=171528)[0m f1_weighted: 0.40328318419937753
[2m[36m(func pid=171528)[0m f1_per_class: [0.62, 0.457, 0.667, 0.582, 0.275, 0.335, 0.238, 0.398, 0.19, 0.441]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 7.1439 | Steps: 4 | Val loss: 221.1608 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8610 | Steps: 4 | Val loss: 2.4815 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=170962)[0m top1: 0.2719216417910448
[2m[36m(func pid=170962)[0m top5: 0.8041044776119403
[2m[36m(func pid=170962)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=170962)[0m f1_macro: 0.27170944160570476
[2m[36m(func pid=170962)[0m f1_weighted: 0.3047630282374895
[2m[36m(func pid=170962)[0m f1_per_class: [0.157, 0.275, 0.413, 0.432, 0.203, 0.272, 0.236, 0.378, 0.112, 0.239]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:07 (running for 00:39:24.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.352 |      0.272 |                   61 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.117 |      0.42  |                   62 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  7.144 |      0.339 |                   49 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.842 |      0.055 |                    9 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3302238805970149
[2m[36m(func pid=174857)[0m top5: 0.7280783582089553
[2m[36m(func pid=174857)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=174857)[0m f1_macro: 0.33913209610510003
[2m[36m(func pid=174857)[0m f1_weighted: 0.3506599133025511
[2m[36m(func pid=174857)[0m f1_per_class: [0.307, 0.549, 0.741, 0.36, 0.06, 0.127, 0.333, 0.333, 0.226, 0.356]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m top1: 0.07882462686567164
[2m[36m(func pid=183693)[0m top5: 0.41651119402985076
[2m[36m(func pid=183693)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=183693)[0m f1_macro: 0.05461649581694593
[2m[36m(func pid=183693)[0m f1_weighted: 0.08764339386728535
[2m[36m(func pid=183693)[0m f1_per_class: [0.039, 0.105, 0.008, 0.2, 0.0, 0.051, 0.006, 0.057, 0.059, 0.023]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.5440 | Steps: 4 | Val loss: 19.0974 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.1628 | Steps: 4 | Val loss: 4.4185 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=171528)[0m top1: 0.396455223880597
[2m[36m(func pid=171528)[0m top5: 0.9155783582089553
[2m[36m(func pid=171528)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=171528)[0m f1_macro: 0.4141293840658725
[2m[36m(func pid=171528)[0m f1_weighted: 0.40587893804244174
[2m[36m(func pid=171528)[0m f1_per_class: [0.602, 0.427, 0.69, 0.584, 0.281, 0.33, 0.272, 0.378, 0.162, 0.415]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 57.6824 | Steps: 4 | Val loss: 235.1930 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8243 | Steps: 4 | Val loss: 2.4806 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=170962)[0m top1: 0.26492537313432835
[2m[36m(func pid=170962)[0m top5: 0.8106343283582089
[2m[36m(func pid=170962)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=170962)[0m f1_macro: 0.276318846918285
[2m[36m(func pid=170962)[0m f1_weighted: 0.3043070069742831
[2m[36m(func pid=170962)[0m f1_per_class: [0.19, 0.274, 0.478, 0.419, 0.176, 0.238, 0.264, 0.336, 0.1, 0.286]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:12 (running for 00:39:30.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.163 |      0.276 |                   62 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.544 |      0.414 |                   63 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 57.682 |      0.328 |                   50 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.861 |      0.055 |                   10 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.30736940298507465
[2m[36m(func pid=174857)[0m top5: 0.7159514925373134
[2m[36m(func pid=174857)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=174857)[0m f1_macro: 0.328077872393582
[2m[36m(func pid=174857)[0m f1_weighted: 0.3321902390228961
[2m[36m(func pid=174857)[0m f1_per_class: [0.224, 0.547, 0.741, 0.399, 0.054, 0.077, 0.257, 0.349, 0.196, 0.436]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.0012 | Steps: 4 | Val loss: 19.2836 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=183693)[0m top1: 0.08022388059701492
[2m[36m(func pid=183693)[0m top5: 0.4123134328358209
[2m[36m(func pid=183693)[0m f1_micro: 0.08022388059701492
[2m[36m(func pid=183693)[0m f1_macro: 0.0537839905031924
[2m[36m(func pid=183693)[0m f1_weighted: 0.09027326227709673
[2m[36m(func pid=183693)[0m f1_per_class: [0.053, 0.105, 0.014, 0.21, 0.0, 0.058, 0.009, 0.017, 0.056, 0.016]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2203 | Steps: 4 | Val loss: 4.1930 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=171528)[0m top1: 0.394589552238806
[2m[36m(func pid=171528)[0m top5: 0.902518656716418
[2m[36m(func pid=171528)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=171528)[0m f1_macro: 0.39608946149425717
[2m[36m(func pid=171528)[0m f1_weighted: 0.4063384879498402
[2m[36m(func pid=171528)[0m f1_per_class: [0.523, 0.39, 0.667, 0.591, 0.261, 0.3, 0.305, 0.394, 0.163, 0.367]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 5.1358 | Steps: 4 | Val loss: 216.3664 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9681 | Steps: 4 | Val loss: 2.4636 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=170962)[0m top1: 0.2719216417910448
[2m[36m(func pid=170962)[0m top5: 0.8372201492537313
[2m[36m(func pid=170962)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=170962)[0m f1_macro: 0.2966487625314963
[2m[36m(func pid=170962)[0m f1_weighted: 0.31431590578123897
[2m[36m(func pid=170962)[0m f1_per_class: [0.261, 0.269, 0.579, 0.409, 0.175, 0.226, 0.308, 0.332, 0.1, 0.308]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:18 (running for 00:39:35.70)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.22  |      0.297 |                   63 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.001 |      0.396 |                   64 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  5.136 |      0.339 |                   51 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.824 |      0.054 |                   11 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3302238805970149
[2m[36m(func pid=174857)[0m top5: 0.7425373134328358
[2m[36m(func pid=174857)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=174857)[0m f1_macro: 0.33919810765007524
[2m[36m(func pid=174857)[0m f1_weighted: 0.355699951773482
[2m[36m(func pid=174857)[0m f1_per_class: [0.252, 0.542, 0.741, 0.466, 0.07, 0.094, 0.274, 0.299, 0.225, 0.429]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7213 | Steps: 4 | Val loss: 20.4789 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=183693)[0m top1: 0.08069029850746269
[2m[36m(func pid=183693)[0m top5: 0.42024253731343286
[2m[36m(func pid=183693)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=183693)[0m f1_macro: 0.052211956323489496
[2m[36m(func pid=183693)[0m f1_weighted: 0.09095282706821049
[2m[36m(func pid=183693)[0m f1_per_class: [0.048, 0.106, 0.013, 0.212, 0.0, 0.056, 0.012, 0.018, 0.04, 0.017]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4065 | Steps: 4 | Val loss: 4.3094 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=171528)[0m top1: 0.3726679104477612
[2m[36m(func pid=171528)[0m top5: 0.8782649253731343
[2m[36m(func pid=171528)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=171528)[0m f1_macro: 0.3639440801753763
[2m[36m(func pid=171528)[0m f1_weighted: 0.3977229863223251
[2m[36m(func pid=171528)[0m f1_per_class: [0.416, 0.363, 0.69, 0.564, 0.256, 0.273, 0.349, 0.347, 0.169, 0.213]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 9.1063 | Steps: 4 | Val loss: 212.2026 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8496 | Steps: 4 | Val loss: 2.4345 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=170962)[0m top1: 0.27705223880597013
[2m[36m(func pid=170962)[0m top5: 0.835820895522388
[2m[36m(func pid=170962)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=170962)[0m f1_macro: 0.28179875521468406
[2m[36m(func pid=170962)[0m f1_weighted: 0.32428142307718144
[2m[36m(func pid=170962)[0m f1_per_class: [0.222, 0.288, 0.431, 0.38, 0.178, 0.226, 0.371, 0.286, 0.102, 0.333]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:23 (running for 00:39:41.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.407 |      0.282 |                   64 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.721 |      0.364 |                   65 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  9.106 |      0.329 |                   52 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.968 |      0.052 |                   12 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.34095149253731344
[2m[36m(func pid=174857)[0m top5: 0.7681902985074627
[2m[36m(func pid=174857)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=174857)[0m f1_macro: 0.32867373979194187
[2m[36m(func pid=174857)[0m f1_weighted: 0.3562818684357844
[2m[36m(func pid=174857)[0m f1_per_class: [0.375, 0.527, 0.741, 0.537, 0.079, 0.062, 0.237, 0.258, 0.22, 0.25]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6449 | Steps: 4 | Val loss: 22.7831 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183693)[0m top1: 0.09188432835820895
[2m[36m(func pid=183693)[0m top5: 0.43656716417910446
[2m[36m(func pid=183693)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=183693)[0m f1_macro: 0.05937079513230607
[2m[36m(func pid=183693)[0m f1_weighted: 0.09959651849622415
[2m[36m(func pid=183693)[0m f1_per_class: [0.055, 0.121, 0.026, 0.228, 0.0, 0.055, 0.015, 0.02, 0.047, 0.027]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6987 | Steps: 4 | Val loss: 4.0492 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=171528)[0m top1: 0.3498134328358209
[2m[36m(func pid=171528)[0m top5: 0.8512126865671642
[2m[36m(func pid=171528)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=171528)[0m f1_macro: 0.3339548798049521
[2m[36m(func pid=171528)[0m f1_weighted: 0.38622101735229675
[2m[36m(func pid=171528)[0m f1_per_class: [0.312, 0.298, 0.714, 0.567, 0.224, 0.216, 0.381, 0.332, 0.169, 0.127]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 8.9870 | Steps: 4 | Val loss: 207.9016 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8324 | Steps: 4 | Val loss: 2.4248 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=170962)[0m top1: 0.3031716417910448
[2m[36m(func pid=170962)[0m top5: 0.8526119402985075
[2m[36m(func pid=170962)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=170962)[0m f1_macro: 0.2881705348680316
[2m[36m(func pid=170962)[0m f1_weighted: 0.3497178305122128
[2m[36m(func pid=170962)[0m f1_per_class: [0.24, 0.319, 0.386, 0.378, 0.195, 0.261, 0.435, 0.244, 0.103, 0.321]
[2m[36m(func pid=170962)[0m 
== Status ==
Current time: 2024-01-07 11:47:29 (running for 00:39:46.54)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.699 |      0.288 |                   65 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.645 |      0.334 |                   66 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.987 |      0.325 |                   53 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.85  |      0.059 |                   13 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.35027985074626866
[2m[36m(func pid=174857)[0m top5: 0.784981343283582
[2m[36m(func pid=174857)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=174857)[0m f1_macro: 0.32482401072794137
[2m[36m(func pid=174857)[0m f1_weighted: 0.3588817179932328
[2m[36m(func pid=174857)[0m f1_per_class: [0.49, 0.524, 0.696, 0.543, 0.104, 0.052, 0.248, 0.24, 0.204, 0.148]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.5552 | Steps: 4 | Val loss: 25.5606 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=183693)[0m top1: 0.08815298507462686
[2m[36m(func pid=183693)[0m top5: 0.44822761194029853
[2m[36m(func pid=183693)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=183693)[0m f1_macro: 0.06041009032313989
[2m[36m(func pid=183693)[0m f1_weighted: 0.0935220848260591
[2m[36m(func pid=183693)[0m f1_per_class: [0.06, 0.136, 0.041, 0.199, 0.0, 0.054, 0.012, 0.021, 0.051, 0.03]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3549 | Steps: 4 | Val loss: 3.7777 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=171528)[0m top1: 0.31296641791044777
[2m[36m(func pid=171528)[0m top5: 0.835820895522388
[2m[36m(func pid=171528)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=171528)[0m f1_macro: 0.3178190920882278
[2m[36m(func pid=171528)[0m f1_weighted: 0.36113066035174063
[2m[36m(func pid=171528)[0m f1_per_class: [0.233, 0.282, 0.769, 0.522, 0.211, 0.158, 0.367, 0.365, 0.191, 0.08]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 8.5322 | Steps: 4 | Val loss: 208.3998 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6922 | Steps: 4 | Val loss: 2.4140 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:47:34 (running for 00:39:51.54)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.355 |      0.304 |                   66 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  2.555 |      0.318 |                   67 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.987 |      0.325 |                   53 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.832 |      0.06  |                   14 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.33302238805970147
[2m[36m(func pid=170962)[0m top5: 0.8684701492537313
[2m[36m(func pid=170962)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=170962)[0m f1_macro: 0.3041102116506734
[2m[36m(func pid=170962)[0m f1_weighted: 0.3753962401892906
[2m[36m(func pid=170962)[0m f1_per_class: [0.333, 0.396, 0.407, 0.423, 0.158, 0.213, 0.45, 0.218, 0.112, 0.329]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2709 | Steps: 4 | Val loss: 30.7307 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=174857)[0m top1: 0.36613805970149255
[2m[36m(func pid=174857)[0m top5: 0.8036380597014925
[2m[36m(func pid=174857)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=174857)[0m f1_macro: 0.32226195650201506
[2m[36m(func pid=174857)[0m f1_weighted: 0.35967635433178685
[2m[36m(func pid=174857)[0m f1_per_class: [0.598, 0.454, 0.571, 0.573, 0.153, 0.076, 0.253, 0.241, 0.156, 0.148]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09095149253731344
[2m[36m(func pid=183693)[0m top5: 0.4566231343283582
[2m[36m(func pid=183693)[0m f1_micro: 0.09095149253731345
[2m[36m(func pid=183693)[0m f1_macro: 0.06399227032822186
[2m[36m(func pid=183693)[0m f1_weighted: 0.09970796566179092
[2m[36m(func pid=183693)[0m f1_per_class: [0.069, 0.146, 0.03, 0.2, 0.0, 0.062, 0.021, 0.032, 0.052, 0.027]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.26632462686567165
[2m[36m(func pid=171528)[0m top5: 0.8129664179104478
[2m[36m(func pid=171528)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=171528)[0m f1_macro: 0.29584278239358175
[2m[36m(func pid=171528)[0m f1_weighted: 0.3147369362882598
[2m[36m(func pid=171528)[0m f1_per_class: [0.212, 0.199, 0.759, 0.485, 0.225, 0.085, 0.314, 0.406, 0.22, 0.054]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.0150 | Steps: 4 | Val loss: 3.6946 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 40.8717 | Steps: 4 | Val loss: 220.4703 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7352 | Steps: 4 | Val loss: 2.3979 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:47:39 (running for 00:39:57.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  1.015 |      0.306 |                   67 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.271 |      0.296 |                   68 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  8.532 |      0.322 |                   54 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.692 |      0.064 |                   15 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.3460820895522388
[2m[36m(func pid=170962)[0m top5: 0.8745335820895522
[2m[36m(func pid=170962)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=170962)[0m f1_macro: 0.30560333861217315
[2m[36m(func pid=170962)[0m f1_weighted: 0.3839186789352128
[2m[36m(func pid=170962)[0m f1_per_class: [0.398, 0.424, 0.386, 0.461, 0.135, 0.162, 0.446, 0.199, 0.12, 0.325]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m top1: 0.37406716417910446
[2m[36m(func pid=174857)[0m top5: 0.8069029850746269
[2m[36m(func pid=174857)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=174857)[0m f1_macro: 0.2992032888551892
[2m[36m(func pid=174857)[0m f1_weighted: 0.3390346876163224
[2m[36m(func pid=174857)[0m f1_per_class: [0.588, 0.333, 0.571, 0.564, 0.194, 0.082, 0.261, 0.28, 0.118, 0.0]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7044 | Steps: 4 | Val loss: 35.5816 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=183693)[0m top1: 0.09468283582089553
[2m[36m(func pid=183693)[0m top5: 0.470615671641791
[2m[36m(func pid=183693)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=183693)[0m f1_macro: 0.06649200294137095
[2m[36m(func pid=183693)[0m f1_weighted: 0.1032009961166565
[2m[36m(func pid=183693)[0m f1_per_class: [0.081, 0.17, 0.032, 0.178, 0.0, 0.054, 0.043, 0.022, 0.057, 0.028]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.23787313432835822
[2m[36m(func pid=171528)[0m top5: 0.7877798507462687
[2m[36m(func pid=171528)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=171528)[0m f1_macro: 0.273738711854757
[2m[36m(func pid=171528)[0m f1_weighted: 0.29007960315598047
[2m[36m(func pid=171528)[0m f1_per_class: [0.218, 0.145, 0.667, 0.471, 0.19, 0.072, 0.28, 0.404, 0.244, 0.046]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.0566 | Steps: 4 | Val loss: 3.6562 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 23.2304 | Steps: 4 | Val loss: 217.9738 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8135 | Steps: 4 | Val loss: 2.3910 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:47:45 (running for 00:40:02.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.057 |      0.323 |                   68 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.704 |      0.274 |                   69 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 40.872 |      0.299 |                   55 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.735 |      0.066 |                   16 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.36847014925373134
[2m[36m(func pid=170962)[0m top5: 0.875
[2m[36m(func pid=170962)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=170962)[0m f1_macro: 0.32280756060383187
[2m[36m(func pid=170962)[0m f1_weighted: 0.39321161459582926
[2m[36m(func pid=170962)[0m f1_per_class: [0.471, 0.453, 0.394, 0.518, 0.13, 0.144, 0.398, 0.249, 0.13, 0.342]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1075 | Steps: 4 | Val loss: 35.5476 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=174857)[0m top1: 0.37453358208955223
[2m[36m(func pid=174857)[0m top5: 0.8073694029850746
[2m[36m(func pid=174857)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=174857)[0m f1_macro: 0.32817008676092635
[2m[36m(func pid=174857)[0m f1_weighted: 0.3335358306126032
[2m[36m(func pid=174857)[0m f1_per_class: [0.567, 0.324, 0.636, 0.554, 0.182, 0.146, 0.218, 0.311, 0.129, 0.214]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09421641791044776
[2m[36m(func pid=183693)[0m top5: 0.4762126865671642
[2m[36m(func pid=183693)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=183693)[0m f1_macro: 0.06652457211642508
[2m[36m(func pid=183693)[0m f1_weighted: 0.10277802506454908
[2m[36m(func pid=183693)[0m f1_per_class: [0.078, 0.169, 0.036, 0.176, 0.0, 0.066, 0.04, 0.022, 0.056, 0.022]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.23600746268656717
[2m[36m(func pid=171528)[0m top5: 0.7761194029850746
[2m[36m(func pid=171528)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=171528)[0m f1_macro: 0.26280788358016005
[2m[36m(func pid=171528)[0m f1_weighted: 0.2832956938285849
[2m[36m(func pid=171528)[0m f1_per_class: [0.164, 0.136, 0.688, 0.45, 0.159, 0.054, 0.296, 0.393, 0.235, 0.053]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 13.9160 | Steps: 4 | Val loss: 183.5057 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.9953 | Steps: 4 | Val loss: 3.5005 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6818 | Steps: 4 | Val loss: 2.3877 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:47:50 (running for 00:40:08.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.057 |      0.323 |                   68 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.108 |      0.263 |                   70 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 13.916 |      0.354 |                   57 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.813 |      0.067 |                   17 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.39598880597014924
[2m[36m(func pid=174857)[0m top5: 0.8022388059701493
[2m[36m(func pid=174857)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=174857)[0m f1_macro: 0.35443027429187146
[2m[36m(func pid=174857)[0m f1_weighted: 0.37923034314213816
[2m[36m(func pid=174857)[0m f1_per_class: [0.554, 0.302, 0.667, 0.575, 0.209, 0.227, 0.33, 0.321, 0.138, 0.222]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 7.5988 | Steps: 4 | Val loss: 32.8480 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=170962)[0m top1: 0.37779850746268656
[2m[36m(func pid=170962)[0m top5: 0.8763992537313433
[2m[36m(func pid=170962)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=170962)[0m f1_macro: 0.34835759144636225
[2m[36m(func pid=170962)[0m f1_weighted: 0.3910972326386486
[2m[36m(func pid=170962)[0m f1_per_class: [0.527, 0.469, 0.531, 0.516, 0.134, 0.134, 0.371, 0.282, 0.14, 0.38]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09514925373134328
[2m[36m(func pid=183693)[0m top5: 0.46968283582089554
[2m[36m(func pid=183693)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=183693)[0m f1_macro: 0.07036560829936471
[2m[36m(func pid=183693)[0m f1_weighted: 0.10544574043986973
[2m[36m(func pid=183693)[0m f1_per_class: [0.086, 0.18, 0.035, 0.161, 0.0, 0.074, 0.051, 0.032, 0.06, 0.025]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.26632462686567165
[2m[36m(func pid=171528)[0m top5: 0.7784514925373134
[2m[36m(func pid=171528)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=171528)[0m f1_macro: 0.26387790222374796
[2m[36m(func pid=171528)[0m f1_weighted: 0.31270943876190643
[2m[36m(func pid=171528)[0m f1_per_class: [0.151, 0.209, 0.571, 0.484, 0.17, 0.053, 0.325, 0.403, 0.203, 0.069]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 44.2893 | Steps: 4 | Val loss: 178.1218 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2181 | Steps: 4 | Val loss: 3.4229 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7933 | Steps: 4 | Val loss: 2.3805 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 11:47:56 (running for 00:40:13.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.995 |      0.348 |                   69 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  7.599 |      0.264 |                   71 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 44.289 |      0.369 |                   58 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.682 |      0.07  |                   18 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.40904850746268656
[2m[36m(func pid=174857)[0m top5: 0.7686567164179104
[2m[36m(func pid=174857)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=174857)[0m f1_macro: 0.3694796559149179
[2m[36m(func pid=174857)[0m f1_weighted: 0.402165973551601
[2m[36m(func pid=174857)[0m f1_per_class: [0.554, 0.223, 0.583, 0.586, 0.226, 0.278, 0.416, 0.324, 0.165, 0.34]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4478 | Steps: 4 | Val loss: 26.9307 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=170962)[0m top1: 0.40951492537313433
[2m[36m(func pid=170962)[0m top5: 0.8885261194029851
[2m[36m(func pid=170962)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=170962)[0m f1_macro: 0.35389959053394504
[2m[36m(func pid=170962)[0m f1_weighted: 0.4014706654370939
[2m[36m(func pid=170962)[0m f1_per_class: [0.586, 0.499, 0.419, 0.567, 0.147, 0.11, 0.338, 0.335, 0.143, 0.395]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09561567164179105
[2m[36m(func pid=183693)[0m top5: 0.47388059701492535
[2m[36m(func pid=183693)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=183693)[0m f1_macro: 0.07121005203208479
[2m[36m(func pid=183693)[0m f1_weighted: 0.10724086100822203
[2m[36m(func pid=183693)[0m f1_per_class: [0.075, 0.172, 0.04, 0.163, 0.0, 0.09, 0.053, 0.032, 0.062, 0.024]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.31763059701492535
[2m[36m(func pid=171528)[0m top5: 0.7971082089552238
[2m[36m(func pid=171528)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=171528)[0m f1_macro: 0.27091324010753753
[2m[36m(func pid=171528)[0m f1_weighted: 0.35445872936792955
[2m[36m(func pid=171528)[0m f1_per_class: [0.163, 0.35, 0.429, 0.521, 0.15, 0.063, 0.356, 0.363, 0.189, 0.126]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 7.0691 | Steps: 4 | Val loss: 183.1278 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2258 | Steps: 4 | Val loss: 3.4664 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6489 | Steps: 4 | Val loss: 2.3703 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.0844 | Steps: 4 | Val loss: 25.2547 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=174857)[0m top1: 0.38526119402985076
[2m[36m(func pid=174857)[0m top5: 0.7481343283582089
[2m[36m(func pid=174857)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=174857)[0m f1_macro: 0.3615317307822473
[2m[36m(func pid=174857)[0m f1_weighted: 0.39776502795273
[2m[36m(func pid=174857)[0m f1_per_class: [0.492, 0.258, 0.56, 0.56, 0.238, 0.277, 0.414, 0.305, 0.151, 0.361]
== Status ==
Current time: 2024-01-07 11:48:01 (running for 00:40:18.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.218 |      0.354 |                   70 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.448 |      0.271 |                   72 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  7.069 |      0.362 |                   59 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.793 |      0.071 |                   19 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.4221082089552239
[2m[36m(func pid=170962)[0m top5: 0.894589552238806
[2m[36m(func pid=170962)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=170962)[0m f1_macro: 0.3611389870407524
[2m[36m(func pid=170962)[0m f1_weighted: 0.3983944766543986
[2m[36m(func pid=170962)[0m f1_per_class: [0.574, 0.502, 0.441, 0.562, 0.189, 0.091, 0.329, 0.362, 0.161, 0.4]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.10307835820895522
[2m[36m(func pid=183693)[0m top5: 0.49113805970149255
[2m[36m(func pid=183693)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=183693)[0m f1_macro: 0.07706396742413033
[2m[36m(func pid=183693)[0m f1_weighted: 0.11681089607977062
[2m[36m(func pid=183693)[0m f1_per_class: [0.078, 0.198, 0.048, 0.16, 0.0, 0.09, 0.073, 0.031, 0.071, 0.022]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3558768656716418
[2m[36m(func pid=171528)[0m top5: 0.8227611940298507
[2m[36m(func pid=171528)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=171528)[0m f1_macro: 0.2880224442465674
[2m[36m(func pid=171528)[0m f1_weighted: 0.37852668371963605
[2m[36m(func pid=171528)[0m f1_per_class: [0.188, 0.462, 0.342, 0.519, 0.123, 0.052, 0.375, 0.359, 0.164, 0.296]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4233 | Steps: 4 | Val loss: 216.4938 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1348 | Steps: 4 | Val loss: 3.7377 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7212 | Steps: 4 | Val loss: 2.3628 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2457 | Steps: 4 | Val loss: 25.6241 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=174857)[0m top1: 0.33908582089552236
[2m[36m(func pid=174857)[0m top5: 0.7080223880597015
[2m[36m(func pid=174857)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=174857)[0m f1_macro: 0.3341217252461813
[2m[36m(func pid=174857)[0m f1_weighted: 0.35899934639035486
[2m[36m(func pid=174857)[0m f1_per_class: [0.421, 0.2, 0.519, 0.518, 0.273, 0.271, 0.365, 0.311, 0.149, 0.315]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:48:06 (running for 00:40:24.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.226 |      0.361 |                   71 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  0.084 |      0.288 |                   73 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  2.423 |      0.334 |                   60 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.649 |      0.077 |                   20 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.4197761194029851
[2m[36m(func pid=170962)[0m top5: 0.8885261194029851
[2m[36m(func pid=170962)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=170962)[0m f1_macro: 0.3501303472686371
[2m[36m(func pid=170962)[0m f1_weighted: 0.3814195239649111
[2m[36m(func pid=170962)[0m f1_per_class: [0.574, 0.491, 0.361, 0.568, 0.179, 0.052, 0.281, 0.396, 0.173, 0.427]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.10074626865671642
[2m[36m(func pid=183693)[0m top5: 0.5051305970149254
[2m[36m(func pid=183693)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=183693)[0m f1_macro: 0.07688408146876637
[2m[36m(func pid=183693)[0m f1_weighted: 0.1148036805516308
[2m[36m(func pid=183693)[0m f1_per_class: [0.064, 0.183, 0.059, 0.15, 0.0, 0.102, 0.08, 0.028, 0.068, 0.035]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.3694029850746269
[2m[36m(func pid=171528)[0m top5: 0.8274253731343284
[2m[36m(func pid=171528)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=171528)[0m f1_macro: 0.3108585338791929
[2m[36m(func pid=171528)[0m f1_weighted: 0.37506550904841196
[2m[36m(func pid=171528)[0m f1_per_class: [0.252, 0.512, 0.257, 0.464, 0.126, 0.059, 0.37, 0.354, 0.186, 0.528]
[2m[36m(func pid=171528)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 21.5369 | Steps: 4 | Val loss: 257.8029 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9318 | Steps: 4 | Val loss: 4.4229 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6299 | Steps: 4 | Val loss: 2.3565 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=171528)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0012 | Steps: 4 | Val loss: 27.3007 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:48:12 (running for 00:40:29.77)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.36
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.135 |      0.35  |                   72 |
| train_98a10_00018 | RUNNING    | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.246 |      0.311 |                   74 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 21.537 |      0.299 |                   61 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.721 |      0.077 |                   21 |
| train_98a10_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.28078358208955223
[2m[36m(func pid=174857)[0m top5: 0.6501865671641791
[2m[36m(func pid=174857)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=174857)[0m f1_macro: 0.2994776366558724
[2m[36m(func pid=174857)[0m f1_weighted: 0.30941914258384134
[2m[36m(func pid=174857)[0m f1_per_class: [0.4, 0.147, 0.476, 0.388, 0.278, 0.267, 0.357, 0.33, 0.12, 0.231]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.39365671641791045
[2m[36m(func pid=170962)[0m top5: 0.871268656716418
[2m[36m(func pid=170962)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=170962)[0m f1_macro: 0.33069487861736596
[2m[36m(func pid=170962)[0m f1_weighted: 0.336224378463443
[2m[36m(func pid=170962)[0m f1_per_class: [0.635, 0.432, 0.342, 0.556, 0.152, 0.03, 0.18, 0.393, 0.181, 0.405]
[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1021455223880597
[2m[36m(func pid=183693)[0m top5: 0.5083955223880597
[2m[36m(func pid=183693)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=183693)[0m f1_macro: 0.07791444181280054
[2m[36m(func pid=183693)[0m f1_weighted: 0.11528737846431476
[2m[36m(func pid=183693)[0m f1_per_class: [0.068, 0.167, 0.062, 0.161, 0.0, 0.109, 0.077, 0.027, 0.073, 0.036]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=171528)[0m top1: 0.36473880597014924
[2m[36m(func pid=171528)[0m top5: 0.8348880597014925
[2m[36m(func pid=171528)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=171528)[0m f1_macro: 0.29685697894685487
[2m[36m(func pid=171528)[0m f1_weighted: 0.3507235842905549
[2m[36m(func pid=171528)[0m f1_per_class: [0.368, 0.5, 0.255, 0.453, 0.119, 0.059, 0.306, 0.356, 0.152, 0.4]
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 54.5298 | Steps: 4 | Val loss: 276.3290 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5260 | Steps: 4 | Val loss: 4.5793 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6215 | Steps: 4 | Val loss: 2.3634 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=174857)[0m top1: 0.25886194029850745
[2m[36m(func pid=174857)[0m top5: 0.6268656716417911
[2m[36m(func pid=174857)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=174857)[0m f1_macro: 0.30747875313089085
[2m[36m(func pid=174857)[0m f1_weighted: 0.2902548782012844
[2m[36m(func pid=174857)[0m f1_per_class: [0.386, 0.212, 0.714, 0.286, 0.238, 0.266, 0.351, 0.321, 0.133, 0.167]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=170962)[0m top1: 0.3824626865671642
[2m[36m(func pid=170962)[0m top5: 0.8502798507462687
[2m[36m(func pid=170962)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=170962)[0m f1_macro: 0.32578797990444985
[2m[36m(func pid=170962)[0m f1_weighted: 0.3229441502481995
[2m[36m(func pid=170962)[0m f1_per_class: [0.634, 0.438, 0.371, 0.568, 0.11, 0.037, 0.119, 0.381, 0.205, 0.395]
[2m[36m(func pid=183693)[0m top1: 0.10027985074626866
[2m[36m(func pid=183693)[0m top5: 0.5060634328358209
[2m[36m(func pid=183693)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=183693)[0m f1_macro: 0.07736831444216677
[2m[36m(func pid=183693)[0m f1_weighted: 0.11011542129289753
[2m[36m(func pid=183693)[0m f1_per_class: [0.065, 0.164, 0.066, 0.155, 0.0, 0.109, 0.067, 0.027, 0.076, 0.046]
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 30.1121 | Steps: 4 | Val loss: 282.0941 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=174857)[0m top1: 0.24067164179104478
[2m[36m(func pid=174857)[0m top5: 0.6375932835820896
[2m[36m(func pid=174857)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=174857)[0m f1_macro: 0.3053712901970493
[2m[36m(func pid=174857)[0m f1_weighted: 0.2773478562407442
[2m[36m(func pid=174857)[0m f1_per_class: [0.545, 0.24, 0.625, 0.264, 0.227, 0.251, 0.305, 0.36, 0.13, 0.107]
== Status ==
Current time: 2024-01-07 11:48:17 (running for 00:40:35.12)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.35924999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.932 |      0.331 |                   73 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 54.53  |      0.307 |                   62 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.63  |      0.078 |                   22 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 11:48:23 (running for 00:40:41.37)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.35924999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.932 |      0.331 |                   73 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 30.112 |      0.305 |                   63 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.63  |      0.078 |                   22 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m 
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=1757)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=1757)[0m Configuration completed!
[2m[36m(func pid=1757)[0m New optimizer parameters:
[2m[36m(func pid=1757)[0m SGD (
[2m[36m(func pid=1757)[0m Parameter Group 0
[2m[36m(func pid=1757)[0m     dampening: 0
[2m[36m(func pid=1757)[0m     differentiable: False
[2m[36m(func pid=1757)[0m     foreach: None
[2m[36m(func pid=1757)[0m     lr: 0.001
[2m[36m(func pid=1757)[0m     maximize: False
[2m[36m(func pid=1757)[0m     momentum: 0.9
[2m[36m(func pid=1757)[0m     nesterov: False
[2m[36m(func pid=1757)[0m     weight_decay: 1e-05
[2m[36m(func pid=1757)[0m )
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 25.8205 | Steps: 4 | Val loss: 295.6966 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5850 | Steps: 4 | Val loss: 2.3559 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=170962)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.0819 | Steps: 4 | Val loss: 4.2070 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0500 | Steps: 4 | Val loss: 2.4481 | Batch size: 32 | lr: 0.001 | Duration: 4.69s
== Status ==
Current time: 2024-01-07 11:48:28 (running for 00:40:46.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.35924999999999996
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00017 | RUNNING    | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.526 |      0.326 |                   74 |
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 30.112 |      0.305 |                   63 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.622 |      0.077 |                   23 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=170962)[0m top1: 0.39365671641791045
[2m[36m(func pid=170962)[0m top5: 0.8596082089552238
[2m[36m(func pid=170962)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=170962)[0m f1_macro: 0.3370116933068269
[2m[36m(func pid=170962)[0m f1_weighted: 0.3541755987927725
[2m[36m(func pid=170962)[0m f1_per_class: [0.617, 0.517, 0.329, 0.588, 0.098, 0.063, 0.152, 0.368, 0.207, 0.431]
[2m[36m(func pid=174857)[0m top1: 0.24113805970149255
[2m[36m(func pid=174857)[0m top5: 0.65625
[2m[36m(func pid=174857)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=174857)[0m f1_macro: 0.2926181505446537
[2m[36m(func pid=174857)[0m f1_weighted: 0.2701363158728096
[2m[36m(func pid=174857)[0m f1_per_class: [0.59, 0.364, 0.455, 0.237, 0.218, 0.224, 0.244, 0.361, 0.137, 0.097]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09794776119402986
[2m[36m(func pid=183693)[0m top5: 0.5088619402985075
[2m[36m(func pid=183693)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=183693)[0m f1_macro: 0.07598039074471655
[2m[36m(func pid=183693)[0m f1_weighted: 0.11155066520897504
[2m[36m(func pid=183693)[0m f1_per_class: [0.064, 0.166, 0.069, 0.155, 0.0, 0.099, 0.075, 0.033, 0.063, 0.036]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m top1: 0.07602611940298508
[2m[36m(func pid=1757)[0m top5: 0.4832089552238806
[2m[36m(func pid=1757)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=1757)[0m f1_macro: 0.04818576000723728
[2m[36m(func pid=1757)[0m f1_weighted: 0.06485149836274957
[2m[36m(func pid=1757)[0m f1_per_class: [0.081, 0.04, 0.0, 0.165, 0.0, 0.026, 0.0, 0.095, 0.04, 0.035]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 15.8460 | Steps: 4 | Val loss: 304.3924 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5795 | Steps: 4 | Val loss: 2.3486 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9647 | Steps: 4 | Val loss: 2.3495 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=174857)[0m top1: 0.24813432835820895
[2m[36m(func pid=174857)[0m top5: 0.6767723880597015
[2m[36m(func pid=174857)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=174857)[0m f1_macro: 0.25855759978375403
[2m[36m(func pid=174857)[0m f1_weighted: 0.25360139291540235
[2m[36m(func pid=174857)[0m f1_per_class: [0.558, 0.467, 0.293, 0.207, 0.14, 0.125, 0.207, 0.328, 0.165, 0.096]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m top1: 0.10261194029850747
[2m[36m(func pid=183693)[0m top5: 0.5097947761194029
[2m[36m(func pid=183693)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=183693)[0m f1_macro: 0.08300123205164911
[2m[36m(func pid=183693)[0m f1_weighted: 0.1159981439492599
[2m[36m(func pid=183693)[0m f1_per_class: [0.081, 0.184, 0.085, 0.159, 0.0, 0.097, 0.07, 0.059, 0.067, 0.028]
[2m[36m(func pid=1757)[0m top1: 0.13712686567164178
[2m[36m(func pid=1757)[0m top5: 0.507929104477612
[2m[36m(func pid=1757)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=1757)[0m f1_macro: 0.06070454280244773
[2m[36m(func pid=1757)[0m f1_weighted: 0.11958523644376315
[2m[36m(func pid=1757)[0m f1_per_class: [0.042, 0.06, 0.0, 0.333, 0.0, 0.031, 0.021, 0.094, 0.0, 0.026]
== Status ==
Current time: 2024-01-07 11:48:34 (running for 00:40:52.36)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 15.846 |      0.259 |                   65 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.585 |      0.076 |                   24 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  3.05  |      0.048 |                    1 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=2535)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=2535)[0m Configuration completed!
[2m[36m(func pid=2535)[0m New optimizer parameters:
[2m[36m(func pid=2535)[0m SGD (
[2m[36m(func pid=2535)[0m Parameter Group 0
[2m[36m(func pid=2535)[0m     dampening: 0
[2m[36m(func pid=2535)[0m     differentiable: False
[2m[36m(func pid=2535)[0m     foreach: None
[2m[36m(func pid=2535)[0m     lr: 0.01
[2m[36m(func pid=2535)[0m     maximize: False
[2m[36m(func pid=2535)[0m     momentum: 0.9
[2m[36m(func pid=2535)[0m     nesterov: False
[2m[36m(func pid=2535)[0m     weight_decay: 1e-05
[2m[36m(func pid=2535)[0m )
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 25.5919 | Steps: 4 | Val loss: 303.0315 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 11:48:40 (running for 00:40:57.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 25.592 |      0.238 |                   66 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.579 |      0.083 |                   25 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.965 |      0.061 |                    2 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.27098880597014924
[2m[36m(func pid=174857)[0m top5: 0.6935634328358209
[2m[36m(func pid=174857)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=174857)[0m f1_macro: 0.23792527119057105
[2m[36m(func pid=174857)[0m f1_weighted: 0.2679351781923717
[2m[36m(func pid=174857)[0m f1_per_class: [0.356, 0.519, 0.241, 0.242, 0.095, 0.084, 0.222, 0.317, 0.183, 0.121]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5617 | Steps: 4 | Val loss: 2.3510 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9399 | Steps: 4 | Val loss: 2.3341 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0581 | Steps: 4 | Val loss: 2.5075 | Batch size: 32 | lr: 0.01 | Duration: 4.49s
[2m[36m(func pid=183693)[0m top1: 0.10634328358208955
[2m[36m(func pid=183693)[0m top5: 0.5121268656716418
[2m[36m(func pid=183693)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=183693)[0m f1_macro: 0.08770001625131929
[2m[36m(func pid=183693)[0m f1_weighted: 0.12149691086423113
[2m[36m(func pid=183693)[0m f1_per_class: [0.107, 0.206, 0.08, 0.152, 0.0, 0.097, 0.08, 0.064, 0.061, 0.031]
[2m[36m(func pid=1757)[0m top1: 0.11147388059701492
[2m[36m(func pid=1757)[0m top5: 0.5541044776119403
[2m[36m(func pid=1757)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=1757)[0m f1_macro: 0.0691255665536801
[2m[36m(func pid=1757)[0m f1_weighted: 0.11210021731280155
[2m[36m(func pid=1757)[0m f1_per_class: [0.052, 0.14, 0.053, 0.217, 0.0, 0.132, 0.026, 0.034, 0.036, 0.0]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 17.9381 | Steps: 4 | Val loss: 300.9139 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=2535)[0m top1: 0.07602611940298508
[2m[36m(func pid=2535)[0m top5: 0.42117537313432835
[2m[36m(func pid=2535)[0m f1_micro: 0.07602611940298508
[2m[36m(func pid=2535)[0m f1_macro: 0.05217575130031772
[2m[36m(func pid=2535)[0m f1_weighted: 0.07572765908859869
[2m[36m(func pid=2535)[0m f1_per_class: [0.086, 0.0, 0.033, 0.0, 0.026, 0.036, 0.212, 0.102, 0.0, 0.027]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7404 | Steps: 4 | Val loss: 2.3129 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:48:45 (running for 00:41:03.03)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 17.938 |      0.243 |                   67 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.562 |      0.088 |                   26 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.94  |      0.069 |                    3 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  3.058 |      0.052 |                    1 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.29757462686567165
[2m[36m(func pid=174857)[0m top5: 0.7005597014925373
[2m[36m(func pid=174857)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=174857)[0m f1_macro: 0.24282912212757646
[2m[36m(func pid=174857)[0m f1_weighted: 0.2824311335285731
[2m[36m(func pid=174857)[0m f1_per_class: [0.244, 0.538, 0.299, 0.239, 0.087, 0.065, 0.278, 0.299, 0.174, 0.206]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5795 | Steps: 4 | Val loss: 2.3704 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7746 | Steps: 4 | Val loss: 2.2076 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=1757)[0m top1: 0.09514925373134328
[2m[36m(func pid=1757)[0m top5: 0.5946828358208955
[2m[36m(func pid=1757)[0m f1_micro: 0.09514925373134328
[2m[36m(func pid=1757)[0m f1_macro: 0.07260236930764916
[2m[36m(func pid=1757)[0m f1_weighted: 0.10350339115225074
[2m[36m(func pid=1757)[0m f1_per_class: [0.112, 0.184, 0.043, 0.15, 0.019, 0.112, 0.04, 0.014, 0.053, 0.0]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m top1: 0.09841417910447761
[2m[36m(func pid=183693)[0m top5: 0.48880597014925375
[2m[36m(func pid=183693)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=183693)[0m f1_macro: 0.08716655219017003
[2m[36m(func pid=183693)[0m f1_weighted: 0.1122559227948366
[2m[36m(func pid=183693)[0m f1_per_class: [0.119, 0.174, 0.091, 0.149, 0.0, 0.092, 0.068, 0.079, 0.056, 0.044]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4288 | Steps: 4 | Val loss: 323.0901 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=2535)[0m top1: 0.14972014925373134
[2m[36m(func pid=2535)[0m top5: 0.7355410447761194
[2m[36m(func pid=2535)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=2535)[0m f1_macro: 0.20531870272566138
[2m[36m(func pid=2535)[0m f1_weighted: 0.1130364964427073
[2m[36m(func pid=2535)[0m f1_per_class: [0.561, 0.433, 0.468, 0.029, 0.154, 0.015, 0.0, 0.141, 0.096, 0.156]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=174857)[0m top1: 0.3045708955223881
[2m[36m(func pid=174857)[0m top5: 0.6949626865671642
[2m[36m(func pid=174857)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=174857)[0m f1_macro: 0.25945331775026836
[2m[36m(func pid=174857)[0m f1_weighted: 0.2921259454486782
[2m[36m(func pid=174857)[0m f1_per_class: [0.245, 0.529, 0.292, 0.258, 0.063, 0.037, 0.297, 0.322, 0.186, 0.366]
[2m[36m(func pid=174857)[0m 
== Status ==
Current time: 2024-01-07 11:48:51 (running for 00:41:08.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  |  0.429 |      0.259 |                   68 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.58  |      0.087 |                   27 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.74  |      0.073 |                    4 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  2.775 |      0.205 |                    2 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6760 | Steps: 4 | Val loss: 2.2407 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5766 | Steps: 4 | Val loss: 2.3659 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.0774 | Steps: 4 | Val loss: 2.0269 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=1757)[0m top1: 0.1296641791044776
[2m[36m(func pid=1757)[0m top5: 0.6543843283582089
[2m[36m(func pid=1757)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=1757)[0m f1_macro: 0.11205515326180482
[2m[36m(func pid=1757)[0m f1_weighted: 0.1182993358699559
[2m[36m(func pid=1757)[0m f1_per_class: [0.273, 0.3, 0.2, 0.144, 0.014, 0.056, 0.03, 0.038, 0.066, 0.0]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m top1: 0.10074626865671642
[2m[36m(func pid=183693)[0m top5: 0.4939365671641791
[2m[36m(func pid=183693)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=183693)[0m f1_macro: 0.08979636814048188
[2m[36m(func pid=183693)[0m f1_weighted: 0.11209206055405398
[2m[36m(func pid=183693)[0m f1_per_class: [0.111, 0.175, 0.077, 0.143, 0.005, 0.091, 0.067, 0.092, 0.078, 0.056]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 34.7699 | Steps: 4 | Val loss: 339.8254 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=2535)[0m top1: 0.26026119402985076
[2m[36m(func pid=2535)[0m top5: 0.75
[2m[36m(func pid=2535)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=2535)[0m f1_macro: 0.17535725046426215
[2m[36m(func pid=2535)[0m f1_weighted: 0.2136631109761019
[2m[36m(func pid=2535)[0m f1_per_class: [0.087, 0.463, 0.263, 0.332, 0.054, 0.117, 0.003, 0.362, 0.073, 0.0]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:48:56 (running for 00:41:13.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 34.77  |      0.253 |                   69 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.577 |      0.09  |                   28 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.676 |      0.112 |                    5 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  2.077 |      0.175 |                    3 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3031716417910448
[2m[36m(func pid=174857)[0m top5: 0.7014925373134329
[2m[36m(func pid=174857)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=174857)[0m f1_macro: 0.25338996730035035
[2m[36m(func pid=174857)[0m f1_weighted: 0.2906333829201628
[2m[36m(func pid=174857)[0m f1_per_class: [0.262, 0.519, 0.245, 0.274, 0.058, 0.023, 0.286, 0.336, 0.19, 0.34]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.4815 | Steps: 4 | Val loss: 2.2190 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6604 | Steps: 4 | Val loss: 2.3624 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.6709 | Steps: 4 | Val loss: 2.0600 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=1757)[0m top1: 0.16791044776119404
[2m[36m(func pid=1757)[0m top5: 0.6361940298507462
[2m[36m(func pid=1757)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=1757)[0m f1_macro: 0.1648668808230374
[2m[36m(func pid=1757)[0m f1_weighted: 0.16012067336145128
[2m[36m(func pid=1757)[0m f1_per_class: [0.28, 0.317, 0.333, 0.175, 0.013, 0.08, 0.081, 0.196, 0.105, 0.068]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 23.7675 | Steps: 4 | Val loss: 342.3967 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183693)[0m top1: 0.09981343283582089
[2m[36m(func pid=183693)[0m top5: 0.4986007462686567
[2m[36m(func pid=183693)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=183693)[0m f1_macro: 0.08926410874673704
[2m[36m(func pid=183693)[0m f1_weighted: 0.11270254397572507
[2m[36m(func pid=183693)[0m f1_per_class: [0.118, 0.167, 0.073, 0.144, 0.005, 0.095, 0.071, 0.106, 0.071, 0.043]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m top1: 0.29384328358208955
[2m[36m(func pid=2535)[0m top5: 0.824160447761194
[2m[36m(func pid=2535)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=2535)[0m f1_macro: 0.23493395632822628
[2m[36m(func pid=2535)[0m f1_weighted: 0.2672451525397051
[2m[36m(func pid=2535)[0m f1_per_class: [0.167, 0.011, 0.522, 0.543, 0.198, 0.195, 0.199, 0.4, 0.0, 0.115]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:49:01 (running for 00:41:18.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 23.768 |      0.252 |                   70 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.66  |      0.089 |                   29 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.481 |      0.165 |                    6 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.671 |      0.235 |                    4 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3111007462686567
[2m[36m(func pid=174857)[0m top5: 0.7131529850746269
[2m[36m(func pid=174857)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=174857)[0m f1_macro: 0.25152318220579656
[2m[36m(func pid=174857)[0m f1_weighted: 0.3091493872676051
[2m[36m(func pid=174857)[0m f1_per_class: [0.314, 0.519, 0.21, 0.334, 0.056, 0.016, 0.295, 0.372, 0.13, 0.27]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3890 | Steps: 4 | Val loss: 2.2081 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4916 | Steps: 4 | Val loss: 2.3405 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6436 | Steps: 4 | Val loss: 1.6279 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=1757)[0m top1: 0.17164179104477612
[2m[36m(func pid=1757)[0m top5: 0.659981343283582
[2m[36m(func pid=1757)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=1757)[0m f1_macro: 0.15435419175110027
[2m[36m(func pid=1757)[0m f1_weighted: 0.17514446626260463
[2m[36m(func pid=1757)[0m f1_per_class: [0.255, 0.271, 0.232, 0.11, 0.033, 0.083, 0.222, 0.204, 0.09, 0.044]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 23.5065 | Steps: 4 | Val loss: 316.1431 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=183693)[0m top1: 0.10727611940298508
[2m[36m(func pid=183693)[0m top5: 0.5228544776119403
[2m[36m(func pid=183693)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=183693)[0m f1_macro: 0.09645395748801289
[2m[36m(func pid=183693)[0m f1_weighted: 0.12190545428634202
[2m[36m(func pid=183693)[0m f1_per_class: [0.113, 0.178, 0.091, 0.145, 0.005, 0.091, 0.092, 0.118, 0.078, 0.052]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m top1: 0.39738805970149255
[2m[36m(func pid=2535)[0m top5: 0.8889925373134329
[2m[36m(func pid=2535)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=2535)[0m f1_macro: 0.3630847343562097
[2m[36m(func pid=2535)[0m f1_weighted: 0.38502396096977587
[2m[36m(func pid=2535)[0m f1_per_class: [0.552, 0.067, 0.5, 0.598, 0.37, 0.306, 0.419, 0.376, 0.132, 0.311]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:49:06 (running for 00:41:24.33)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 23.506 |      0.277 |                   71 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.492 |      0.096 |                   30 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.389 |      0.154 |                    7 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  2.644 |      0.363 |                    5 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.34468283582089554
[2m[36m(func pid=174857)[0m top5: 0.7355410447761194
[2m[36m(func pid=174857)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=174857)[0m f1_macro: 0.2772054348730903
[2m[36m(func pid=174857)[0m f1_weighted: 0.35852203160851676
[2m[36m(func pid=174857)[0m f1_per_class: [0.347, 0.542, 0.186, 0.446, 0.052, 0.016, 0.333, 0.387, 0.15, 0.312]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.2779 | Steps: 4 | Val loss: 2.2038 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5502 | Steps: 4 | Val loss: 2.3073 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3657 | Steps: 4 | Val loss: 2.3132 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=1757)[0m top1: 0.17257462686567165
[2m[36m(func pid=1757)[0m top5: 0.6753731343283582
[2m[36m(func pid=1757)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=1757)[0m f1_macro: 0.12245359922529694
[2m[36m(func pid=1757)[0m f1_weighted: 0.18588874908184894
[2m[36m(func pid=1757)[0m f1_per_class: [0.11, 0.124, 0.092, 0.131, 0.0, 0.114, 0.325, 0.221, 0.056, 0.052]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 21.4133 | Steps: 4 | Val loss: 301.3835 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=183693)[0m top1: 0.12406716417910447
[2m[36m(func pid=183693)[0m top5: 0.5499067164179104
[2m[36m(func pid=183693)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=183693)[0m f1_macro: 0.10577464757751678
[2m[36m(func pid=183693)[0m f1_weighted: 0.14168160059043058
[2m[36m(func pid=183693)[0m f1_per_class: [0.139, 0.198, 0.091, 0.161, 0.007, 0.093, 0.133, 0.105, 0.069, 0.062]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m top1: 0.23833955223880596
[2m[36m(func pid=2535)[0m top5: 0.8316231343283582
[2m[36m(func pid=2535)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=2535)[0m f1_macro: 0.1852756662020313
[2m[36m(func pid=2535)[0m f1_weighted: 0.23043097181342714
[2m[36m(func pid=2535)[0m f1_per_class: [0.0, 0.481, 0.054, 0.148, 0.167, 0.252, 0.161, 0.343, 0.247, 0.0]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:49:12 (running for 00:41:29.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 21.413 |      0.277 |                   72 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.55  |      0.106 |                   31 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.278 |      0.122 |                    8 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.366 |      0.185 |                    6 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.3516791044776119
[2m[36m(func pid=174857)[0m top5: 0.7551305970149254
[2m[36m(func pid=174857)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=174857)[0m f1_macro: 0.2774282530280109
[2m[36m(func pid=174857)[0m f1_weighted: 0.36605961149748467
[2m[36m(func pid=174857)[0m f1_per_class: [0.398, 0.51, 0.154, 0.507, 0.058, 0.016, 0.318, 0.38, 0.167, 0.267]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2554 | Steps: 4 | Val loss: 2.0906 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5111 | Steps: 4 | Val loss: 2.2849 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1862 | Steps: 4 | Val loss: 2.0721 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 17.5420 | Steps: 4 | Val loss: 275.5776 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=1757)[0m top1: 0.2294776119402985
[2m[36m(func pid=1757)[0m top5: 0.7593283582089553
[2m[36m(func pid=1757)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=1757)[0m f1_macro: 0.1554633633856753
[2m[36m(func pid=1757)[0m f1_weighted: 0.24295334427805076
[2m[36m(func pid=1757)[0m f1_per_class: [0.113, 0.055, 0.155, 0.404, 0.043, 0.171, 0.275, 0.253, 0.0, 0.085]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1357276119402985
[2m[36m(func pid=183693)[0m top5: 0.5746268656716418
[2m[36m(func pid=183693)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=183693)[0m f1_macro: 0.10984914466317328
[2m[36m(func pid=183693)[0m f1_weighted: 0.15309964386739225
[2m[36m(func pid=183693)[0m f1_per_class: [0.146, 0.224, 0.085, 0.145, 0.0, 0.084, 0.171, 0.122, 0.067, 0.054]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3218283582089552
[2m[36m(func pid=2535)[0m top5: 0.8535447761194029
[2m[36m(func pid=2535)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=2535)[0m f1_macro: 0.2882930987809451
[2m[36m(func pid=2535)[0m f1_weighted: 0.3109617289803465
[2m[36m(func pid=2535)[0m f1_per_class: [0.087, 0.51, 0.649, 0.517, 0.078, 0.169, 0.08, 0.317, 0.255, 0.221]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:49:17 (running for 00:41:35.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 17.542 |      0.294 |                   73 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.511 |      0.11  |                   32 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.255 |      0.155 |                    9 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.186 |      0.288 |                    7 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.37826492537313433
[2m[36m(func pid=174857)[0m top5: 0.7705223880597015
[2m[36m(func pid=174857)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=174857)[0m f1_macro: 0.2938961956271398
[2m[36m(func pid=174857)[0m f1_weighted: 0.3860459733777611
[2m[36m(func pid=174857)[0m f1_per_class: [0.482, 0.495, 0.126, 0.532, 0.072, 0.031, 0.355, 0.386, 0.194, 0.267]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1003 | Steps: 4 | Val loss: 1.9677 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.5031 | Steps: 4 | Val loss: 2.2728 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.9176 | Steps: 4 | Val loss: 1.6634 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 10.2587 | Steps: 4 | Val loss: 269.5096 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=1757)[0m top1: 0.30830223880597013
[2m[36m(func pid=1757)[0m top5: 0.8027052238805971
[2m[36m(func pid=1757)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=1757)[0m f1_macro: 0.22724222613085127
[2m[36m(func pid=1757)[0m f1_weighted: 0.2430457371127593
[2m[36m(func pid=1757)[0m f1_per_class: [0.307, 0.046, 0.465, 0.534, 0.137, 0.185, 0.122, 0.256, 0.054, 0.167]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1515858208955224
[2m[36m(func pid=183693)[0m top5: 0.5876865671641791
[2m[36m(func pid=183693)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=183693)[0m f1_macro: 0.1158886131538069
[2m[36m(func pid=183693)[0m f1_weighted: 0.16585116728819904
[2m[36m(func pid=183693)[0m f1_per_class: [0.174, 0.283, 0.083, 0.136, 0.0, 0.089, 0.189, 0.11, 0.061, 0.035]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m top1: 0.40578358208955223
[2m[36m(func pid=2535)[0m top5: 0.9333022388059702
[2m[36m(func pid=2535)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=2535)[0m f1_macro: 0.40339080551001166
[2m[36m(func pid=2535)[0m f1_weighted: 0.4133760335807415
[2m[36m(func pid=2535)[0m f1_per_class: [0.667, 0.515, 0.526, 0.528, 0.271, 0.287, 0.316, 0.341, 0.242, 0.341]
[2m[36m(func pid=2535)[0m 
== Status ==
Current time: 2024-01-07 11:49:22 (running for 00:41:40.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3585
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00019 | RUNNING    | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 10.259 |      0.292 |                   74 |
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.503 |      0.116 |                   33 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  2.1   |      0.227 |                   10 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.918 |      0.403 |                    8 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.38526119402985076
[2m[36m(func pid=174857)[0m top5: 0.7924440298507462
[2m[36m(func pid=174857)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=174857)[0m f1_macro: 0.29166405763227343
[2m[36m(func pid=174857)[0m f1_weighted: 0.37239453864622657
[2m[36m(func pid=174857)[0m f1_per_class: [0.504, 0.466, 0.153, 0.535, 0.089, 0.022, 0.333, 0.326, 0.212, 0.276]
[2m[36m(func pid=174857)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.8874 | Steps: 4 | Val loss: 1.9989 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4918 | Steps: 4 | Val loss: 1.7410 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4894 | Steps: 4 | Val loss: 2.2650 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=174857)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 30.9512 | Steps: 4 | Val loss: 277.1145 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=1757)[0m top1: 0.27845149253731344
[2m[36m(func pid=1757)[0m top5: 0.7933768656716418
[2m[36m(func pid=1757)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=1757)[0m f1_macro: 0.27810166530445657
[2m[36m(func pid=1757)[0m f1_weighted: 0.2585081597963842
[2m[36m(func pid=1757)[0m f1_per_class: [0.471, 0.209, 0.645, 0.543, 0.07, 0.158, 0.056, 0.261, 0.13, 0.238]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.4556902985074627
[2m[36m(func pid=2535)[0m top5: 0.9137126865671642
[2m[36m(func pid=2535)[0m f1_micro: 0.4556902985074627
[2m[36m(func pid=2535)[0m f1_macro: 0.40514235069673854
[2m[36m(func pid=2535)[0m f1_weighted: 0.4334486807081041
[2m[36m(func pid=2535)[0m f1_per_class: [0.314, 0.542, 0.846, 0.334, 0.179, 0.143, 0.619, 0.335, 0.253, 0.488]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.15485074626865672
[2m[36m(func pid=183693)[0m top5: 0.5923507462686567
[2m[36m(func pid=183693)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=183693)[0m f1_macro: 0.1199221820376291
[2m[36m(func pid=183693)[0m f1_weighted: 0.16690526519918222
[2m[36m(func pid=183693)[0m f1_per_class: [0.174, 0.271, 0.085, 0.146, 0.0, 0.082, 0.184, 0.146, 0.068, 0.044]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:49:28 (running for 00:41:45.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 3 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.489 |      0.12  |                   34 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.887 |      0.278 |                   11 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.492 |      0.405 |                    9 |
| train_98a10_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=174857)[0m top1: 0.373134328358209
[2m[36m(func pid=174857)[0m top5: 0.8013059701492538
[2m[36m(func pid=174857)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=174857)[0m f1_macro: 0.275279295603397
[2m[36m(func pid=174857)[0m f1_weighted: 0.34471707122601963
[2m[36m(func pid=174857)[0m f1_per_class: [0.518, 0.359, 0.183, 0.523, 0.108, 0.03, 0.315, 0.326, 0.177, 0.214]
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9573 | Steps: 4 | Val loss: 2.0180 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1894 | Steps: 4 | Val loss: 2.4322 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.4330 | Steps: 4 | Val loss: 2.2689 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=1757)[0m top1: 0.24440298507462688
[2m[36m(func pid=1757)[0m top5: 0.7896455223880597
[2m[36m(func pid=1757)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=1757)[0m f1_macro: 0.29045409408800066
[2m[36m(func pid=1757)[0m f1_weighted: 0.2512631717171758
[2m[36m(func pid=1757)[0m f1_per_class: [0.527, 0.335, 0.71, 0.448, 0.048, 0.153, 0.033, 0.325, 0.15, 0.175]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.28031716417910446
[2m[36m(func pid=2535)[0m top5: 0.8652052238805971
[2m[36m(func pid=2535)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=2535)[0m f1_macro: 0.2553753605301897
[2m[36m(func pid=2535)[0m f1_weighted: 0.33865354032184874
[2m[36m(func pid=2535)[0m f1_per_class: [0.504, 0.325, 0.05, 0.37, 0.073, 0.186, 0.414, 0.352, 0.0, 0.28]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.15904850746268656
[2m[36m(func pid=183693)[0m top5: 0.5923507462686567
[2m[36m(func pid=183693)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=183693)[0m f1_macro: 0.12522929312637787
[2m[36m(func pid=183693)[0m f1_weighted: 0.16871301618576684
[2m[36m(func pid=183693)[0m f1_per_class: [0.177, 0.278, 0.113, 0.132, 0.0, 0.083, 0.198, 0.147, 0.062, 0.063]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.6636 | Steps: 4 | Val loss: 1.9785 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.4874 | Steps: 4 | Val loss: 3.1117 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5604 | Steps: 4 | Val loss: 2.2777 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=1757)[0m top1: 0.25326492537313433
[2m[36m(func pid=1757)[0m top5: 0.8325559701492538
[2m[36m(func pid=1757)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=1757)[0m f1_macro: 0.27028787205422933
[2m[36m(func pid=1757)[0m f1_weighted: 0.24704065119182894
[2m[36m(func pid=1757)[0m f1_per_class: [0.408, 0.426, 0.6, 0.345, 0.071, 0.209, 0.065, 0.278, 0.096, 0.205]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:49:36 (running for 00:41:54.08)
Memory usage on this node: 24.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.433 |      0.125 |                   35 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.664 |      0.27  |                   13 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.189 |      0.255 |                   10 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=5446)[0m Configuration completed!
[2m[36m(func pid=5446)[0m New optimizer parameters:
[2m[36m(func pid=5446)[0m SGD (
[2m[36m(func pid=5446)[0m Parameter Group 0
[2m[36m(func pid=5446)[0m     dampening: 0
[2m[36m(func pid=5446)[0m     differentiable: False
[2m[36m(func pid=5446)[0m     foreach: None
[2m[36m(func pid=5446)[0m     lr: 0.1
[2m[36m(func pid=5446)[0m     maximize: False
[2m[36m(func pid=5446)[0m     momentum: 0.9
[2m[36m(func pid=5446)[0m     nesterov: False
[2m[36m(func pid=5446)[0m     weight_decay: 1e-05
[2m[36m(func pid=5446)[0m )
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m top1: 0.25699626865671643
[2m[36m(func pid=2535)[0m top5: 0.789179104477612
[2m[36m(func pid=2535)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=2535)[0m f1_macro: 0.2523411603554046
[2m[36m(func pid=2535)[0m f1_weighted: 0.2484101667831279
[2m[36m(func pid=2535)[0m f1_per_class: [0.658, 0.178, 0.329, 0.527, 0.138, 0.238, 0.018, 0.354, 0.0, 0.082]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1478544776119403
[2m[36m(func pid=183693)[0m top5: 0.5848880597014925
[2m[36m(func pid=183693)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=183693)[0m f1_macro: 0.11823457135107077
[2m[36m(func pid=183693)[0m f1_weighted: 0.15907315010261264
[2m[36m(func pid=183693)[0m f1_per_class: [0.163, 0.258, 0.098, 0.127, 0.0, 0.093, 0.179, 0.147, 0.063, 0.055]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7167 | Steps: 4 | Val loss: 1.9186 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7894 | Steps: 4 | Val loss: 2.4086 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4754 | Steps: 4 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 9.1864 | Steps: 4 | Val loss: 5.9933 | Batch size: 32 | lr: 0.1 | Duration: 4.59s
== Status ==
Current time: 2024-01-07 11:49:42 (running for 00:41:59.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.56  |      0.118 |                   36 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.717 |      0.27  |                   14 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.487 |      0.252 |                   11 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.27705223880597013
[2m[36m(func pid=1757)[0m top5: 0.8488805970149254
[2m[36m(func pid=1757)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=1757)[0m f1_macro: 0.2696873593859927
[2m[36m(func pid=1757)[0m f1_weighted: 0.2721067139205068
[2m[36m(func pid=1757)[0m f1_per_class: [0.408, 0.461, 0.522, 0.208, 0.113, 0.229, 0.251, 0.302, 0.058, 0.144]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.32975746268656714
[2m[36m(func pid=2535)[0m top5: 0.8446828358208955
[2m[36m(func pid=2535)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=2535)[0m f1_macro: 0.37605717422260915
[2m[36m(func pid=2535)[0m f1_weighted: 0.3231220901337979
[2m[36m(func pid=2535)[0m f1_per_class: [0.684, 0.439, 0.688, 0.572, 0.121, 0.105, 0.085, 0.338, 0.157, 0.571]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.14085820895522388
[2m[36m(func pid=183693)[0m top5: 0.5928171641791045
[2m[36m(func pid=183693)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=183693)[0m f1_macro: 0.11800898228733639
[2m[36m(func pid=183693)[0m f1_weighted: 0.15298147880996293
[2m[36m(func pid=183693)[0m f1_per_class: [0.178, 0.206, 0.108, 0.144, 0.0, 0.095, 0.17, 0.142, 0.079, 0.058]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.08768656716417911
[2m[36m(func pid=5446)[0m top5: 0.7103544776119403
[2m[36m(func pid=5446)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=5446)[0m f1_macro: 0.041552325689153846
[2m[36m(func pid=5446)[0m f1_weighted: 0.04591260501611016
[2m[36m(func pid=5446)[0m f1_per_class: [0.0, 0.211, 0.0, 0.0, 0.045, 0.0, 0.0, 0.159, 0.0, 0.0]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6889 | Steps: 4 | Val loss: 1.9491 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3670 | Steps: 4 | Val loss: 2.7568 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4247 | Steps: 4 | Val loss: 2.2717 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 15.0334 | Steps: 4 | Val loss: 17.6491 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:49:47 (running for 00:42:05.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.475 |      0.118 |                   37 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.689 |      0.254 |                   15 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.789 |      0.376 |                   12 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  9.186 |      0.042 |                    1 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.2826492537313433
[2m[36m(func pid=1757)[0m top5: 0.8190298507462687
[2m[36m(func pid=1757)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=1757)[0m f1_macro: 0.25421616257247487
[2m[36m(func pid=1757)[0m f1_weighted: 0.30323689841773727
[2m[36m(func pid=1757)[0m f1_per_class: [0.392, 0.391, 0.329, 0.243, 0.103, 0.174, 0.384, 0.308, 0.123, 0.095]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.314365671641791
[2m[36m(func pid=2535)[0m top5: 0.8498134328358209
[2m[36m(func pid=2535)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=2535)[0m f1_macro: 0.35116804573598986
[2m[36m(func pid=2535)[0m f1_weighted: 0.3425011820320278
[2m[36m(func pid=2535)[0m f1_per_class: [0.579, 0.46, 0.846, 0.319, 0.047, 0.008, 0.422, 0.355, 0.2, 0.276]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.14272388059701493
[2m[36m(func pid=183693)[0m top5: 0.6012126865671642
[2m[36m(func pid=183693)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=183693)[0m f1_macro: 0.12175623720272136
[2m[36m(func pid=183693)[0m f1_weighted: 0.15736580357262256
[2m[36m(func pid=183693)[0m f1_per_class: [0.169, 0.201, 0.122, 0.173, 0.0, 0.097, 0.158, 0.152, 0.085, 0.062]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.03917910447761194
[2m[36m(func pid=5446)[0m top5: 0.7014925373134329
[2m[36m(func pid=5446)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=5446)[0m f1_macro: 0.080714171500937
[2m[36m(func pid=5446)[0m f1_weighted: 0.011162301893878249
[2m[36m(func pid=5446)[0m f1_per_class: [0.0, 0.0, 0.7, 0.0, 0.0, 0.042, 0.0, 0.0, 0.066, 0.0]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5062 | Steps: 4 | Val loss: 2.0065 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2623 | Steps: 4 | Val loss: 1.8497 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4491 | Steps: 4 | Val loss: 2.2565 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 18.8079 | Steps: 4 | Val loss: 6.4019 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:49:53 (running for 00:42:10.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.425 |      0.122 |                   38 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.506 |      0.239 |                   16 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.367 |      0.351 |                   13 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 15.033 |      0.081 |                    2 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.2579291044776119
[2m[36m(func pid=1757)[0m top5: 0.7910447761194029
[2m[36m(func pid=1757)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=1757)[0m f1_macro: 0.23900270903376417
[2m[36m(func pid=1757)[0m f1_weighted: 0.28851579101938557
[2m[36m(func pid=1757)[0m f1_per_class: [0.441, 0.284, 0.308, 0.314, 0.064, 0.049, 0.374, 0.299, 0.154, 0.103]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.4505597014925373
[2m[36m(func pid=2535)[0m top5: 0.9291044776119403
[2m[36m(func pid=2535)[0m f1_micro: 0.4505597014925373
[2m[36m(func pid=2535)[0m f1_macro: 0.38775197341072226
[2m[36m(func pid=2535)[0m f1_weighted: 0.4586350829863242
[2m[36m(func pid=2535)[0m f1_per_class: [0.491, 0.568, 0.585, 0.585, 0.085, 0.072, 0.476, 0.355, 0.242, 0.419]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.15438432835820895
[2m[36m(func pid=183693)[0m top5: 0.6077425373134329
[2m[36m(func pid=183693)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=183693)[0m f1_macro: 0.1280879729039847
[2m[36m(func pid=183693)[0m f1_weighted: 0.17404902389379182
[2m[36m(func pid=183693)[0m f1_per_class: [0.172, 0.209, 0.132, 0.207, 0.0, 0.088, 0.179, 0.16, 0.078, 0.056]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.37966417910447764
[2m[36m(func pid=5446)[0m top5: 0.8931902985074627
[2m[36m(func pid=5446)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=5446)[0m f1_macro: 0.24358022881169245
[2m[36m(func pid=5446)[0m f1_weighted: 0.37170312573374376
[2m[36m(func pid=5446)[0m f1_per_class: [0.0, 0.047, 0.595, 0.627, 0.043, 0.03, 0.531, 0.356, 0.0, 0.207]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5990 | Steps: 4 | Val loss: 1.9073 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.7251 | Steps: 4 | Val loss: 2.5286 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3625 | Steps: 4 | Val loss: 2.2415 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 6.1623 | Steps: 4 | Val loss: 15.7788 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:49:58 (running for 00:42:15.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.449 |      0.128 |                   39 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.599 |      0.275 |                   17 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.262 |      0.388 |                   14 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 18.808 |      0.244 |                    3 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.332089552238806
[2m[36m(func pid=1757)[0m top5: 0.824160447761194
[2m[36m(func pid=1757)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=1757)[0m f1_macro: 0.2747938529532576
[2m[36m(func pid=1757)[0m f1_weighted: 0.3512638943325439
[2m[36m(func pid=1757)[0m f1_per_class: [0.451, 0.206, 0.393, 0.508, 0.085, 0.036, 0.445, 0.315, 0.154, 0.155]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.4039179104477612
[2m[36m(func pid=2535)[0m top5: 0.909981343283582
[2m[36m(func pid=2535)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=2535)[0m f1_macro: 0.36977766273991114
[2m[36m(func pid=2535)[0m f1_weighted: 0.3221654922942831
[2m[36m(func pid=2535)[0m f1_per_class: [0.667, 0.351, 0.545, 0.573, 0.174, 0.358, 0.033, 0.368, 0.194, 0.435]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1623134328358209
[2m[36m(func pid=183693)[0m top5: 0.625
[2m[36m(func pid=183693)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=183693)[0m f1_macro: 0.13486294681335836
[2m[36m(func pid=183693)[0m f1_weighted: 0.179888327205989
[2m[36m(func pid=183693)[0m f1_per_class: [0.194, 0.216, 0.134, 0.227, 0.009, 0.092, 0.171, 0.173, 0.08, 0.053]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.19682835820895522
[2m[36m(func pid=5446)[0m top5: 0.7346082089552238
[2m[36m(func pid=5446)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=5446)[0m f1_macro: 0.18735685362458804
[2m[36m(func pid=5446)[0m f1_weighted: 0.2202709965600534
[2m[36m(func pid=5446)[0m f1_per_class: [0.233, 0.0, 0.333, 0.36, 0.074, 0.0, 0.289, 0.386, 0.086, 0.113]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.5230 | Steps: 4 | Val loss: 1.8559 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8632 | Steps: 4 | Val loss: 2.8667 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3957 | Steps: 4 | Val loss: 2.2233 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 11.5492 | Steps: 4 | Val loss: 9.0896 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:50:03 (running for 00:42:21.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.362 |      0.135 |                   40 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.523 |      0.306 |                   18 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.725 |      0.37  |                   15 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  6.162 |      0.187 |                    4 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.32276119402985076
[2m[36m(func pid=1757)[0m top5: 0.847481343283582
[2m[36m(func pid=1757)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=1757)[0m f1_macro: 0.3057516187383497
[2m[36m(func pid=1757)[0m f1_weighted: 0.3289281836387617
[2m[36m(func pid=1757)[0m f1_per_class: [0.534, 0.342, 0.558, 0.54, 0.152, 0.022, 0.267, 0.242, 0.142, 0.257]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.25
[2m[36m(func pid=2535)[0m top5: 0.847481343283582
[2m[36m(func pid=2535)[0m f1_micro: 0.25
[2m[36m(func pid=2535)[0m f1_macro: 0.287130429588577
[2m[36m(func pid=2535)[0m f1_weighted: 0.24784191927533805
[2m[36m(func pid=2535)[0m f1_per_class: [0.613, 0.412, 0.193, 0.315, 0.312, 0.276, 0.066, 0.242, 0.129, 0.313]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.166044776119403
[2m[36m(func pid=183693)[0m top5: 0.6455223880597015
[2m[36m(func pid=183693)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=183693)[0m f1_macro: 0.13504930972121104
[2m[36m(func pid=183693)[0m f1_weighted: 0.1821688835595346
[2m[36m(func pid=183693)[0m f1_per_class: [0.203, 0.196, 0.148, 0.259, 0.0, 0.068, 0.168, 0.18, 0.068, 0.06]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.34281716417910446
[2m[36m(func pid=5446)[0m top5: 0.8185634328358209
[2m[36m(func pid=5446)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=5446)[0m f1_macro: 0.26384873645509305
[2m[36m(func pid=5446)[0m f1_weighted: 0.39121576154008897
[2m[36m(func pid=5446)[0m f1_per_class: [0.51, 0.227, 0.071, 0.501, 0.047, 0.023, 0.568, 0.425, 0.077, 0.189]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.4399 | Steps: 4 | Val loss: 1.8917 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.0787 | Steps: 4 | Val loss: 3.6809 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3789 | Steps: 4 | Val loss: 2.2186 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 12.3389 | Steps: 4 | Val loss: 6.6959 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=1757)[0m top1: 0.2798507462686567
[2m[36m(func pid=1757)[0m top5: 0.8484141791044776
[2m[36m(func pid=1757)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=1757)[0m f1_macro: 0.25551052676981895
[2m[36m(func pid=1757)[0m f1_weighted: 0.2642907370224624
[2m[36m(func pid=1757)[0m f1_per_class: [0.492, 0.452, 0.233, 0.497, 0.22, 0.022, 0.045, 0.205, 0.103, 0.286]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:50:09 (running for 00:42:26.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.396 |      0.135 |                   41 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.44  |      0.256 |                   19 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.863 |      0.287 |                   16 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 11.549 |      0.264 |                    5 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.21921641791044777
[2m[36m(func pid=2535)[0m top5: 0.7126865671641791
[2m[36m(func pid=2535)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=2535)[0m f1_macro: 0.22508251711797986
[2m[36m(func pid=2535)[0m f1_weighted: 0.2225232693991805
[2m[36m(func pid=2535)[0m f1_per_class: [0.563, 0.423, 0.054, 0.02, 0.244, 0.024, 0.348, 0.287, 0.196, 0.092]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1730410447761194
[2m[36m(func pid=183693)[0m top5: 0.6473880597014925
[2m[36m(func pid=183693)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=183693)[0m f1_macro: 0.1384887147094135
[2m[36m(func pid=183693)[0m f1_weighted: 0.18292954591577276
[2m[36m(func pid=183693)[0m f1_per_class: [0.209, 0.222, 0.141, 0.272, 0.018, 0.052, 0.147, 0.187, 0.069, 0.069]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.447294776119403
[2m[36m(func pid=5446)[0m top5: 0.9160447761194029
[2m[36m(func pid=5446)[0m f1_micro: 0.447294776119403
[2m[36m(func pid=5446)[0m f1_macro: 0.35952850727357577
[2m[36m(func pid=5446)[0m f1_weighted: 0.4611460206362804
[2m[36m(func pid=5446)[0m f1_per_class: [0.659, 0.474, 0.174, 0.55, 0.222, 0.323, 0.499, 0.349, 0.0, 0.345]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.2586 | Steps: 4 | Val loss: 1.8730 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.1285 | Steps: 4 | Val loss: 3.1082 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3676 | Steps: 4 | Val loss: 2.2155 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 6.2880 | Steps: 4 | Val loss: 15.4403 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:50:14 (running for 00:42:31.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.379 |      0.138 |                   42 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.259 |      0.271 |                   20 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.079 |      0.225 |                   17 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 12.339 |      0.36  |                    6 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.30223880597014924
[2m[36m(func pid=1757)[0m top5: 0.8512126865671642
[2m[36m(func pid=1757)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=1757)[0m f1_macro: 0.2708822379341042
[2m[36m(func pid=1757)[0m f1_weighted: 0.29051098444501444
[2m[36m(func pid=1757)[0m f1_per_class: [0.478, 0.462, 0.234, 0.501, 0.187, 0.141, 0.071, 0.247, 0.115, 0.273]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.28451492537313433
[2m[36m(func pid=2535)[0m top5: 0.8176305970149254
[2m[36m(func pid=2535)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=2535)[0m f1_macro: 0.27965168375440286
[2m[36m(func pid=2535)[0m f1_weighted: 0.3429763074481826
[2m[36m(func pid=2535)[0m f1_per_class: [0.545, 0.368, 0.095, 0.358, 0.049, 0.016, 0.457, 0.345, 0.144, 0.419]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.17490671641791045
[2m[36m(func pid=183693)[0m top5: 0.6515858208955224
[2m[36m(func pid=183693)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=183693)[0m f1_macro: 0.1412745030510451
[2m[36m(func pid=183693)[0m f1_weighted: 0.18167069687766685
[2m[36m(func pid=183693)[0m f1_per_class: [0.241, 0.253, 0.132, 0.271, 0.009, 0.059, 0.121, 0.185, 0.074, 0.069]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.416044776119403
[2m[36m(func pid=5446)[0m top5: 0.7280783582089553
[2m[36m(func pid=5446)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=5446)[0m f1_macro: 0.3566727316637549
[2m[36m(func pid=5446)[0m f1_weighted: 0.34854482754439803
[2m[36m(func pid=5446)[0m f1_per_class: [0.552, 0.0, 0.8, 0.567, 0.26, 0.016, 0.457, 0.422, 0.18, 0.312]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2356 | Steps: 4 | Val loss: 1.8385 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8343 | Steps: 4 | Val loss: 2.2726 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3133 | Steps: 4 | Val loss: 2.1999 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 7.0408 | Steps: 4 | Val loss: 11.4667 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:50:19 (running for 00:42:37.21)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.368 |      0.141 |                   43 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.236 |      0.281 |                   21 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.128 |      0.28  |                   18 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  6.288 |      0.357 |                    7 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.31343283582089554
[2m[36m(func pid=1757)[0m top5: 0.8740671641791045
[2m[36m(func pid=1757)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=1757)[0m f1_macro: 0.2811417865753988
[2m[36m(func pid=1757)[0m f1_weighted: 0.33258048702032933
[2m[36m(func pid=1757)[0m f1_per_class: [0.504, 0.434, 0.231, 0.481, 0.108, 0.189, 0.217, 0.309, 0.135, 0.203]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.35027985074626866
[2m[36m(func pid=2535)[0m top5: 0.9090485074626866
[2m[36m(func pid=2535)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=2535)[0m f1_macro: 0.31470785574553223
[2m[36m(func pid=2535)[0m f1_weighted: 0.3680463611043368
[2m[36m(func pid=2535)[0m f1_per_class: [0.479, 0.204, 0.26, 0.591, 0.114, 0.257, 0.326, 0.347, 0.129, 0.441]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.18097014925373134
[2m[36m(func pid=183693)[0m top5: 0.6730410447761194
[2m[36m(func pid=183693)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=183693)[0m f1_macro: 0.14653212007072902
[2m[36m(func pid=183693)[0m f1_weighted: 0.19022012226978635
[2m[36m(func pid=183693)[0m f1_per_class: [0.226, 0.262, 0.163, 0.269, 0.018, 0.062, 0.147, 0.179, 0.071, 0.068]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.37546641791044777
[2m[36m(func pid=5446)[0m top5: 0.9029850746268657
[2m[36m(func pid=5446)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=5446)[0m f1_macro: 0.34467742035533117
[2m[36m(func pid=5446)[0m f1_weighted: 0.41789203233359484
[2m[36m(func pid=5446)[0m f1_per_class: [0.438, 0.459, 0.647, 0.59, 0.042, 0.047, 0.418, 0.415, 0.123, 0.267]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3573 | Steps: 4 | Val loss: 1.7742 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5213 | Steps: 4 | Val loss: 2.5514 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3668 | Steps: 4 | Val loss: 2.1909 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 10.8467 | Steps: 4 | Val loss: 14.5412 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:50:25 (running for 00:42:42.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.313 |      0.147 |                   44 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.357 |      0.31  |                   22 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.834 |      0.315 |                   19 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  7.041 |      0.345 |                    8 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.34328358208955223
[2m[36m(func pid=1757)[0m top5: 0.8931902985074627
[2m[36m(func pid=1757)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=1757)[0m f1_macro: 0.30991994265492423
[2m[36m(func pid=1757)[0m f1_weighted: 0.3782949313214695
[2m[36m(func pid=1757)[0m f1_per_class: [0.552, 0.348, 0.414, 0.534, 0.105, 0.214, 0.363, 0.257, 0.181, 0.132]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.33861940298507465
[2m[36m(func pid=2535)[0m top5: 0.9057835820895522
[2m[36m(func pid=2535)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=2535)[0m f1_macro: 0.3086045781654985
[2m[36m(func pid=2535)[0m f1_weighted: 0.3322267451726339
[2m[36m(func pid=2535)[0m f1_per_class: [0.33, 0.155, 0.558, 0.58, 0.303, 0.362, 0.216, 0.326, 0.168, 0.086]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.31156716417910446
[2m[36m(func pid=5446)[0m top5: 0.8446828358208955
[2m[36m(func pid=5446)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=5446)[0m f1_macro: 0.30417198265973133
[2m[36m(func pid=5446)[0m f1_weighted: 0.3163660311439697
[2m[36m(func pid=5446)[0m f1_per_class: [0.684, 0.438, 0.344, 0.497, 0.149, 0.29, 0.078, 0.412, 0.149, 0.0]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.18330223880597016
[2m[36m(func pid=183693)[0m top5: 0.6753731343283582
[2m[36m(func pid=183693)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=183693)[0m f1_macro: 0.15276211907168122
[2m[36m(func pid=183693)[0m f1_weighted: 0.19410457387205915
[2m[36m(func pid=183693)[0m f1_per_class: [0.238, 0.276, 0.187, 0.259, 0.018, 0.073, 0.154, 0.186, 0.071, 0.066]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1502 | Steps: 4 | Val loss: 1.7693 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4728 | Steps: 4 | Val loss: 2.0234 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 9.1663 | Steps: 4 | Val loss: 14.5433 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3214 | Steps: 4 | Val loss: 2.1904 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:50:30 (running for 00:42:48.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.367 |      0.153 |                   45 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.15  |      0.327 |                   23 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.521 |      0.309 |                   20 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 10.847 |      0.304 |                    9 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.32742537313432835
[2m[36m(func pid=1757)[0m top5: 0.8992537313432836
[2m[36m(func pid=1757)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=1757)[0m f1_macro: 0.326757568175293
[2m[36m(func pid=1757)[0m f1_weighted: 0.34826461008449316
[2m[36m(func pid=1757)[0m f1_per_class: [0.587, 0.284, 0.564, 0.54, 0.159, 0.242, 0.273, 0.257, 0.18, 0.18]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.44869402985074625
[2m[36m(func pid=2535)[0m top5: 0.9057835820895522
[2m[36m(func pid=2535)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=2535)[0m f1_macro: 0.36946179384814676
[2m[36m(func pid=2535)[0m f1_weighted: 0.44876217049583267
[2m[36m(func pid=2535)[0m f1_per_class: [0.407, 0.447, 0.786, 0.606, 0.231, 0.039, 0.532, 0.28, 0.22, 0.148]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.34468283582089554
[2m[36m(func pid=5446)[0m top5: 0.8390858208955224
[2m[36m(func pid=5446)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=5446)[0m f1_macro: 0.26270459001484175
[2m[36m(func pid=5446)[0m f1_weighted: 0.29466063618580657
[2m[36m(func pid=5446)[0m f1_per_class: [0.639, 0.116, 0.178, 0.606, 0.0, 0.372, 0.071, 0.362, 0.156, 0.127]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.17630597014925373
[2m[36m(func pid=183693)[0m top5: 0.6777052238805971
[2m[36m(func pid=183693)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=183693)[0m f1_macro: 0.15708048124876892
[2m[36m(func pid=183693)[0m f1_weighted: 0.19152228723822737
[2m[36m(func pid=183693)[0m f1_per_class: [0.241, 0.265, 0.242, 0.25, 0.014, 0.081, 0.156, 0.183, 0.071, 0.067]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.2032 | Steps: 4 | Val loss: 1.6965 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3989 | Steps: 4 | Val loss: 2.3676 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 5.1253 | Steps: 4 | Val loss: 19.5262 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.2989 | Steps: 4 | Val loss: 2.1980 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:50:35 (running for 00:42:53.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.321 |      0.157 |                   46 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.203 |      0.364 |                   24 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.473 |      0.369 |                   21 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  9.166 |      0.263 |                   10 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3591417910447761
[2m[36m(func pid=1757)[0m top5: 0.9169776119402985
[2m[36m(func pid=1757)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=1757)[0m f1_macro: 0.3637654808445206
[2m[36m(func pid=1757)[0m f1_weighted: 0.3735897192256456
[2m[36m(func pid=1757)[0m f1_per_class: [0.607, 0.405, 0.667, 0.55, 0.194, 0.277, 0.254, 0.284, 0.189, 0.211]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.40111940298507465
[2m[36m(func pid=2535)[0m top5: 0.8815298507462687
[2m[36m(func pid=2535)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=2535)[0m f1_macro: 0.36427271373354964
[2m[36m(func pid=2535)[0m f1_weighted: 0.3734451703505526
[2m[36m(func pid=2535)[0m f1_per_class: [0.49, 0.537, 0.647, 0.491, 0.25, 0.032, 0.324, 0.292, 0.203, 0.377]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.2392723880597015
[2m[36m(func pid=5446)[0m top5: 0.7028917910447762
[2m[36m(func pid=5446)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=5446)[0m f1_macro: 0.1946931639511236
[2m[36m(func pid=5446)[0m f1_weighted: 0.19911977696861918
[2m[36m(func pid=5446)[0m f1_per_class: [0.152, 0.438, 0.215, 0.162, 0.136, 0.238, 0.081, 0.319, 0.028, 0.176]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.17164179104477612
[2m[36m(func pid=183693)[0m top5: 0.6716417910447762
[2m[36m(func pid=183693)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=183693)[0m f1_macro: 0.15771350761619465
[2m[36m(func pid=183693)[0m f1_weighted: 0.18777187187675906
[2m[36m(func pid=183693)[0m f1_per_class: [0.235, 0.246, 0.274, 0.254, 0.024, 0.077, 0.152, 0.182, 0.071, 0.06]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1969 | Steps: 4 | Val loss: 1.6576 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.1808 | Steps: 4 | Val loss: 2.4175 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 12.2319 | Steps: 4 | Val loss: 11.6673 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2909 | Steps: 4 | Val loss: 2.1818 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:50:41 (running for 00:42:58.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.299 |      0.158 |                   47 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.197 |      0.365 |                   25 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.399 |      0.364 |                   22 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.125 |      0.195 |                   11 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.38759328358208955
[2m[36m(func pid=1757)[0m top5: 0.917910447761194
[2m[36m(func pid=1757)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=1757)[0m f1_macro: 0.3645993205210097
[2m[36m(func pid=1757)[0m f1_weighted: 0.3835336343922741
[2m[36m(func pid=1757)[0m f1_per_class: [0.571, 0.432, 0.579, 0.581, 0.187, 0.256, 0.236, 0.364, 0.212, 0.226]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3530783582089552
[2m[36m(func pid=2535)[0m top5: 0.8857276119402985
[2m[36m(func pid=2535)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=2535)[0m f1_macro: 0.3481536024631756
[2m[36m(func pid=2535)[0m f1_weighted: 0.37258010472031394
[2m[36m(func pid=2535)[0m f1_per_class: [0.434, 0.525, 0.512, 0.459, 0.08, 0.182, 0.303, 0.316, 0.206, 0.464]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.37966417910447764
[2m[36m(func pid=5446)[0m top5: 0.8698694029850746
[2m[36m(func pid=5446)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=5446)[0m f1_macro: 0.3056620350432012
[2m[36m(func pid=5446)[0m f1_weighted: 0.42271352056944794
[2m[36m(func pid=5446)[0m f1_per_class: [0.51, 0.411, 0.1, 0.546, 0.087, 0.149, 0.474, 0.349, 0.214, 0.217]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.18330223880597016
[2m[36m(func pid=183693)[0m top5: 0.6893656716417911
[2m[36m(func pid=183693)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=183693)[0m f1_macro: 0.16463453264437328
[2m[36m(func pid=183693)[0m f1_weighted: 0.19771914101497842
[2m[36m(func pid=183693)[0m f1_per_class: [0.255, 0.274, 0.263, 0.263, 0.022, 0.077, 0.159, 0.18, 0.081, 0.073]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0984 | Steps: 4 | Val loss: 1.6639 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4776 | Steps: 4 | Val loss: 2.8639 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8976 | Steps: 4 | Val loss: 20.7532 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2614 | Steps: 4 | Val loss: 2.1765 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:50:46 (running for 00:43:03.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.291 |      0.165 |                   48 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.098 |      0.35  |                   26 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.181 |      0.348 |                   23 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 12.232 |      0.306 |                   12 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.39925373134328357
[2m[36m(func pid=1757)[0m top5: 0.8927238805970149
[2m[36m(func pid=1757)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=1757)[0m f1_macro: 0.34965416395096416
[2m[36m(func pid=1757)[0m f1_weighted: 0.36498653500453293
[2m[36m(func pid=1757)[0m f1_per_class: [0.553, 0.463, 0.48, 0.568, 0.189, 0.164, 0.206, 0.372, 0.178, 0.324]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.28171641791044777
[2m[36m(func pid=2535)[0m top5: 0.8498134328358209
[2m[36m(func pid=2535)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=2535)[0m f1_macro: 0.3275337599894015
[2m[36m(func pid=2535)[0m f1_weighted: 0.3170997769450498
[2m[36m(func pid=2535)[0m f1_per_class: [0.564, 0.427, 0.55, 0.284, 0.052, 0.154, 0.343, 0.317, 0.199, 0.386]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.25093283582089554
[2m[36m(func pid=5446)[0m top5: 0.8773320895522388
[2m[36m(func pid=5446)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=5446)[0m f1_macro: 0.2510969702351996
[2m[36m(func pid=5446)[0m f1_weighted: 0.2704944172085384
[2m[36m(func pid=5446)[0m f1_per_class: [0.275, 0.027, 0.741, 0.551, 0.104, 0.167, 0.198, 0.34, 0.109, 0.0]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.19029850746268656
[2m[36m(func pid=183693)[0m top5: 0.6879664179104478
[2m[36m(func pid=183693)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=183693)[0m f1_macro: 0.17380823735644627
[2m[36m(func pid=183693)[0m f1_weighted: 0.20456347296835276
[2m[36m(func pid=183693)[0m f1_per_class: [0.264, 0.295, 0.29, 0.253, 0.02, 0.087, 0.173, 0.183, 0.082, 0.09]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.1005 | Steps: 4 | Val loss: 1.6518 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2073 | Steps: 4 | Val loss: 2.7674 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.7127 | Steps: 4 | Val loss: 15.5656 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3355 | Steps: 4 | Val loss: 2.1819 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:50:51 (running for 00:43:09.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.261 |      0.174 |                   49 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.101 |      0.335 |                   27 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.478 |      0.328 |                   24 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.898 |      0.251 |                   13 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.40111940298507465
[2m[36m(func pid=1757)[0m top5: 0.9076492537313433
[2m[36m(func pid=1757)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=1757)[0m f1_macro: 0.335325098985066
[2m[36m(func pid=1757)[0m f1_weighted: 0.3833827603548829
[2m[36m(func pid=1757)[0m f1_per_class: [0.49, 0.508, 0.351, 0.581, 0.169, 0.237, 0.217, 0.342, 0.16, 0.297]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.35867537313432835
[2m[36m(func pid=2535)[0m top5: 0.8600746268656716
[2m[36m(func pid=2535)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=2535)[0m f1_macro: 0.3096477752665936
[2m[36m(func pid=2535)[0m f1_weighted: 0.3642015486813635
[2m[36m(func pid=2535)[0m f1_per_class: [0.649, 0.17, 0.55, 0.368, 0.11, 0.155, 0.606, 0.156, 0.168, 0.164]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3521455223880597
[2m[36m(func pid=5446)[0m top5: 0.8782649253731343
[2m[36m(func pid=5446)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=5446)[0m f1_macro: 0.28599495187543056
[2m[36m(func pid=5446)[0m f1_weighted: 0.32220278929588614
[2m[36m(func pid=5446)[0m f1_per_class: [0.128, 0.13, 0.72, 0.614, 0.133, 0.236, 0.219, 0.376, 0.143, 0.161]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.19169776119402984
[2m[36m(func pid=183693)[0m top5: 0.6781716417910447
[2m[36m(func pid=183693)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=183693)[0m f1_macro: 0.1696047607644848
[2m[36m(func pid=183693)[0m f1_weighted: 0.20466193869355145
[2m[36m(func pid=183693)[0m f1_per_class: [0.241, 0.288, 0.282, 0.271, 0.022, 0.084, 0.163, 0.199, 0.07, 0.077]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.9493 | Steps: 4 | Val loss: 1.7165 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.0543 | Steps: 4 | Val loss: 2.3386 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 5.5432 | Steps: 4 | Val loss: 12.6804 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3027 | Steps: 4 | Val loss: 2.1903 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:50:57 (running for 00:43:14.48)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.336 |      0.17  |                   50 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.949 |      0.304 |                   28 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.207 |      0.31  |                   25 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.713 |      0.286 |                   14 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3614738805970149
[2m[36m(func pid=1757)[0m top5: 0.8964552238805971
[2m[36m(func pid=1757)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=1757)[0m f1_macro: 0.303789268752214
[2m[36m(func pid=1757)[0m f1_weighted: 0.36043122667489874
[2m[36m(func pid=1757)[0m f1_per_class: [0.425, 0.468, 0.245, 0.546, 0.18, 0.272, 0.197, 0.337, 0.115, 0.252]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.37080223880597013
[2m[36m(func pid=2535)[0m top5: 0.8927238805970149
[2m[36m(func pid=2535)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=2535)[0m f1_macro: 0.33634399653966063
[2m[36m(func pid=2535)[0m f1_weighted: 0.41132439866758813
[2m[36m(func pid=2535)[0m f1_per_class: [0.593, 0.437, 0.415, 0.538, 0.096, 0.164, 0.414, 0.328, 0.233, 0.144]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.4155783582089552
[2m[36m(func pid=5446)[0m top5: 0.9235074626865671
[2m[36m(func pid=5446)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=5446)[0m f1_macro: 0.3616748800857147
[2m[36m(func pid=5446)[0m f1_weighted: 0.393290005617735
[2m[36m(func pid=5446)[0m f1_per_class: [0.393, 0.527, 0.714, 0.612, 0.267, 0.331, 0.187, 0.347, 0.053, 0.186]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1884328358208955
[2m[36m(func pid=183693)[0m top5: 0.6693097014925373
[2m[36m(func pid=183693)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=183693)[0m f1_macro: 0.16760338829821447
[2m[36m(func pid=183693)[0m f1_weighted: 0.2041474267574092
[2m[36m(func pid=183693)[0m f1_per_class: [0.231, 0.293, 0.247, 0.243, 0.026, 0.076, 0.183, 0.214, 0.082, 0.079]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8882 | Steps: 4 | Val loss: 1.7686 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6382 | Steps: 4 | Val loss: 3.4371 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.6826 | Steps: 4 | Val loss: 12.3705 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3177 | Steps: 4 | Val loss: 2.1591 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:51:02 (running for 00:43:19.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.303 |      0.168 |                   51 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.888 |      0.298 |                   29 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.054 |      0.336 |                   26 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.543 |      0.362 |                   15 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.33115671641791045
[2m[36m(func pid=1757)[0m top5: 0.8843283582089553
[2m[36m(func pid=1757)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=1757)[0m f1_macro: 0.29751044711782093
[2m[36m(func pid=1757)[0m f1_weighted: 0.35347760793348043
[2m[36m(func pid=1757)[0m f1_per_class: [0.398, 0.426, 0.273, 0.439, 0.129, 0.243, 0.304, 0.332, 0.202, 0.229]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3619402985074627
[2m[36m(func pid=2535)[0m top5: 0.7588619402985075
[2m[36m(func pid=2535)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=2535)[0m f1_macro: 0.33255351360634205
[2m[36m(func pid=2535)[0m f1_weighted: 0.2925468167247916
[2m[36m(func pid=2535)[0m f1_per_class: [0.446, 0.466, 0.585, 0.504, 0.152, 0.183, 0.009, 0.382, 0.236, 0.361]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3871268656716418
[2m[36m(func pid=5446)[0m top5: 0.9090485074626866
[2m[36m(func pid=5446)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=5446)[0m f1_macro: 0.3614826385513944
[2m[36m(func pid=5446)[0m f1_weighted: 0.3734248270251247
[2m[36m(func pid=5446)[0m f1_per_class: [0.653, 0.538, 0.421, 0.431, 0.197, 0.308, 0.27, 0.318, 0.164, 0.314]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.2042910447761194
[2m[36m(func pid=183693)[0m top5: 0.6926305970149254
[2m[36m(func pid=183693)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=183693)[0m f1_macro: 0.17699403903929573
[2m[36m(func pid=183693)[0m f1_weighted: 0.213783218129395
[2m[36m(func pid=183693)[0m f1_per_class: [0.276, 0.311, 0.232, 0.27, 0.023, 0.079, 0.173, 0.233, 0.077, 0.098]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0527 | Steps: 4 | Val loss: 1.7871 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6321 | Steps: 4 | Val loss: 4.1055 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.0925 | Steps: 4 | Val loss: 23.2553 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2214 | Steps: 4 | Val loss: 2.1636 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 11:51:07 (running for 00:43:25.15)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.318 |      0.177 |                   52 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.053 |      0.318 |                   30 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.638 |      0.333 |                   27 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.683 |      0.361 |                   16 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.36240671641791045
[2m[36m(func pid=1757)[0m top5: 0.8572761194029851
[2m[36m(func pid=1757)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=1757)[0m f1_macro: 0.3181741991369087
[2m[36m(func pid=1757)[0m f1_weighted: 0.3860000812858698
[2m[36m(func pid=1757)[0m f1_per_class: [0.531, 0.473, 0.296, 0.351, 0.117, 0.168, 0.482, 0.356, 0.23, 0.177]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.2681902985074627
[2m[36m(func pid=2535)[0m top5: 0.7290111940298507
[2m[36m(func pid=2535)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=2535)[0m f1_macro: 0.27289329310156984
[2m[36m(func pid=2535)[0m f1_weighted: 0.2565124477770887
[2m[36m(func pid=2535)[0m f1_per_class: [0.105, 0.186, 0.643, 0.545, 0.175, 0.24, 0.022, 0.394, 0.169, 0.25]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.28218283582089554
[2m[36m(func pid=5446)[0m top5: 0.8106343283582089
[2m[36m(func pid=5446)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=5446)[0m f1_macro: 0.24576020132065754
[2m[36m(func pid=5446)[0m f1_weighted: 0.26411494184466344
[2m[36m(func pid=5446)[0m f1_per_class: [0.371, 0.195, 0.364, 0.56, 0.08, 0.0, 0.14, 0.248, 0.101, 0.4]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.1982276119402985
[2m[36m(func pid=183693)[0m top5: 0.691231343283582
[2m[36m(func pid=183693)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=183693)[0m f1_macro: 0.17181670342489147
[2m[36m(func pid=183693)[0m f1_weighted: 0.19904564472011266
[2m[36m(func pid=183693)[0m f1_per_class: [0.267, 0.323, 0.227, 0.23, 0.023, 0.082, 0.153, 0.227, 0.091, 0.097]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8169 | Steps: 4 | Val loss: 1.7647 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7234 | Steps: 4 | Val loss: 3.2115 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4149 | Steps: 4 | Val loss: 22.7129 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1879 | Steps: 4 | Val loss: 2.1605 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:51:13 (running for 00:43:30.56)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.221 |      0.172 |                   53 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.817 |      0.323 |                   31 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.632 |      0.273 |                   28 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.093 |      0.246 |                   17 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.38899253731343286
[2m[36m(func pid=1757)[0m top5: 0.8572761194029851
[2m[36m(func pid=1757)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=1757)[0m f1_macro: 0.32287714507332943
[2m[36m(func pid=1757)[0m f1_weighted: 0.3905093492509749
[2m[36m(func pid=1757)[0m f1_per_class: [0.566, 0.486, 0.348, 0.278, 0.104, 0.143, 0.571, 0.303, 0.261, 0.168]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.29850746268656714
[2m[36m(func pid=2535)[0m top5: 0.8166977611940298
[2m[36m(func pid=2535)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=2535)[0m f1_macro: 0.26892844727399945
[2m[36m(func pid=2535)[0m f1_weighted: 0.326199543527545
[2m[36m(func pid=2535)[0m f1_per_class: [0.293, 0.061, 0.512, 0.44, 0.137, 0.244, 0.446, 0.261, 0.156, 0.14]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.2873134328358209
[2m[36m(func pid=5446)[0m top5: 0.7840485074626866
[2m[36m(func pid=5446)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=5446)[0m f1_macro: 0.233596799501945
[2m[36m(func pid=5446)[0m f1_weighted: 0.31705037984040446
[2m[36m(func pid=5446)[0m f1_per_class: [0.288, 0.126, 0.304, 0.554, 0.051, 0.0, 0.366, 0.294, 0.111, 0.242]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.21222014925373134
[2m[36m(func pid=183693)[0m top5: 0.6870335820895522
[2m[36m(func pid=183693)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=183693)[0m f1_macro: 0.17672076230057593
[2m[36m(func pid=183693)[0m f1_weighted: 0.20821087669508379
[2m[36m(func pid=183693)[0m f1_per_class: [0.256, 0.336, 0.216, 0.274, 0.024, 0.088, 0.13, 0.246, 0.09, 0.108]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.1154 | Steps: 4 | Val loss: 1.6845 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0483 | Steps: 4 | Val loss: 3.1803 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.6129 | Steps: 4 | Val loss: 20.4816 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1482 | Steps: 4 | Val loss: 2.1506 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 11:51:18 (running for 00:43:35.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.188 |      0.177 |                   54 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  1.115 |      0.335 |                   32 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.723 |      0.269 |                   29 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.415 |      0.234 |                   18 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.39972014925373134
[2m[36m(func pid=1757)[0m top5: 0.8964552238805971
[2m[36m(func pid=1757)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=1757)[0m f1_macro: 0.3352353196118803
[2m[36m(func pid=1757)[0m f1_weighted: 0.42164849588848735
[2m[36m(func pid=1757)[0m f1_per_class: [0.569, 0.486, 0.429, 0.426, 0.11, 0.13, 0.548, 0.29, 0.235, 0.131]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3208955223880597
[2m[36m(func pid=2535)[0m top5: 0.8428171641791045
[2m[36m(func pid=2535)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=2535)[0m f1_macro: 0.27128902740301153
[2m[36m(func pid=2535)[0m f1_weighted: 0.3549056399797692
[2m[36m(func pid=2535)[0m f1_per_class: [0.476, 0.125, 0.4, 0.476, 0.056, 0.04, 0.539, 0.278, 0.138, 0.184]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.24207089552238806
[2m[36m(func pid=5446)[0m top5: 0.7868470149253731
[2m[36m(func pid=5446)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=5446)[0m f1_macro: 0.24577356670706613
[2m[36m(func pid=5446)[0m f1_weighted: 0.25330144349756895
[2m[36m(func pid=5446)[0m f1_per_class: [0.5, 0.361, 0.138, 0.124, 0.126, 0.082, 0.387, 0.163, 0.178, 0.4]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.2126865671641791
[2m[36m(func pid=183693)[0m top5: 0.6921641791044776
[2m[36m(func pid=183693)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=183693)[0m f1_macro: 0.1731917376710925
[2m[36m(func pid=183693)[0m f1_weighted: 0.20742576015565134
[2m[36m(func pid=183693)[0m f1_per_class: [0.245, 0.34, 0.204, 0.259, 0.018, 0.089, 0.141, 0.24, 0.099, 0.099]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.9480 | Steps: 4 | Val loss: 1.6431 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.1870 | Steps: 4 | Val loss: 4.6579 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9998 | Steps: 4 | Val loss: 18.2523 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1248 | Steps: 4 | Val loss: 2.1581 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=1757)[0m top1: 0.40718283582089554
[2m[36m(func pid=1757)[0m top5: 0.9053171641791045
[2m[36m(func pid=1757)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=1757)[0m f1_macro: 0.3434292335344703
[2m[36m(func pid=1757)[0m f1_weighted: 0.4294800408273184
[2m[36m(func pid=1757)[0m f1_per_class: [0.552, 0.493, 0.414, 0.519, 0.126, 0.13, 0.476, 0.319, 0.235, 0.17]
== Status ==
Current time: 2024-01-07 11:51:23 (running for 00:43:41.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.148 |      0.173 |                   55 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.948 |      0.343 |                   33 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.048 |      0.271 |                   30 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.613 |      0.246 |                   19 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.1837686567164179
[2m[36m(func pid=2535)[0m top5: 0.8148320895522388
[2m[36m(func pid=2535)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=2535)[0m f1_macro: 0.22416182762219355
[2m[36m(func pid=2535)[0m f1_weighted: 0.17192492627579345
[2m[36m(func pid=2535)[0m f1_per_class: [0.262, 0.322, 0.565, 0.285, 0.081, 0.016, 0.025, 0.176, 0.11, 0.4]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3530783582089552
[2m[36m(func pid=5446)[0m top5: 0.8861940298507462
[2m[36m(func pid=5446)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=5446)[0m f1_macro: 0.3267317593708841
[2m[36m(func pid=5446)[0m f1_weighted: 0.31454668062275887
[2m[36m(func pid=5446)[0m f1_per_class: [0.644, 0.336, 0.28, 0.622, 0.25, 0.307, 0.024, 0.22, 0.206, 0.376]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.2080223880597015
[2m[36m(func pid=183693)[0m top5: 0.6888992537313433
[2m[36m(func pid=183693)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=183693)[0m f1_macro: 0.17170635506670018
[2m[36m(func pid=183693)[0m f1_weighted: 0.19915268712974127
[2m[36m(func pid=183693)[0m f1_per_class: [0.242, 0.353, 0.247, 0.234, 0.018, 0.088, 0.133, 0.233, 0.066, 0.103]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9523 | Steps: 4 | Val loss: 1.6647 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5396 | Steps: 4 | Val loss: 3.4544 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 5.6985 | Steps: 4 | Val loss: 23.1203 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 11:51:29 (running for 00:43:46.62)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.125 |      0.172 |                   56 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.952 |      0.33  |                   34 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.187 |      0.224 |                   31 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2     |      0.327 |                   20 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3792 | Steps: 4 | Val loss: 2.1688 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=1757)[0m top1: 0.38899253731343286
[2m[36m(func pid=1757)[0m top5: 0.8978544776119403
[2m[36m(func pid=1757)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=1757)[0m f1_macro: 0.33001906921682556
[2m[36m(func pid=1757)[0m f1_weighted: 0.3725845073556898
[2m[36m(func pid=1757)[0m f1_per_class: [0.579, 0.459, 0.414, 0.597, 0.176, 0.147, 0.227, 0.322, 0.183, 0.195]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3041044776119403
[2m[36m(func pid=2535)[0m top5: 0.8367537313432836
[2m[36m(func pid=2535)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=2535)[0m f1_macro: 0.28573910144849884
[2m[36m(func pid=2535)[0m f1_weighted: 0.28776270142982213
[2m[36m(func pid=2535)[0m f1_per_class: [0.336, 0.561, 0.441, 0.434, 0.092, 0.035, 0.102, 0.251, 0.188, 0.419]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3675373134328358
[2m[36m(func pid=5446)[0m top5: 0.8278917910447762
[2m[36m(func pid=5446)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=5446)[0m f1_macro: 0.3152302664377195
[2m[36m(func pid=5446)[0m f1_weighted: 0.2832457639558702
[2m[36m(func pid=5446)[0m f1_per_class: [0.629, 0.005, 0.625, 0.546, 0.194, 0.192, 0.213, 0.315, 0.146, 0.288]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.21082089552238806
[2m[36m(func pid=183693)[0m top5: 0.6725746268656716
[2m[36m(func pid=183693)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=183693)[0m f1_macro: 0.17349861759045987
[2m[36m(func pid=183693)[0m f1_weighted: 0.20629842067785126
[2m[36m(func pid=183693)[0m f1_per_class: [0.225, 0.355, 0.239, 0.259, 0.018, 0.09, 0.132, 0.231, 0.083, 0.103]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8563 | Steps: 4 | Val loss: 1.7161 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.1465 | Steps: 4 | Val loss: 2.3948 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 9.3674 | Steps: 4 | Val loss: 31.0309 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 11:51:34 (running for 00:43:51.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.379 |      0.173 |                   57 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.856 |      0.313 |                   35 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.54  |      0.286 |                   32 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.698 |      0.315 |                   21 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3516791044776119
[2m[36m(func pid=1757)[0m top5: 0.8997201492537313
[2m[36m(func pid=1757)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=1757)[0m f1_macro: 0.31335606845034947
[2m[36m(func pid=1757)[0m f1_weighted: 0.32169724625415874
[2m[36m(func pid=1757)[0m f1_per_class: [0.562, 0.335, 0.5, 0.608, 0.218, 0.146, 0.131, 0.256, 0.157, 0.22]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.1287 | Steps: 4 | Val loss: 2.1605 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=2535)[0m top1: 0.40298507462686567
[2m[36m(func pid=2535)[0m top5: 0.9123134328358209
[2m[36m(func pid=2535)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=2535)[0m f1_macro: 0.35561511196008627
[2m[36m(func pid=2535)[0m f1_weighted: 0.42123795471618714
[2m[36m(func pid=2535)[0m f1_per_class: [0.476, 0.512, 0.413, 0.518, 0.104, 0.16, 0.423, 0.342, 0.199, 0.409]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.17397388059701493
[2m[36m(func pid=5446)[0m top5: 0.7374067164179104
[2m[36m(func pid=5446)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=5446)[0m f1_macro: 0.24111027360730905
[2m[36m(func pid=5446)[0m f1_weighted: 0.16291657968990353
[2m[36m(func pid=5446)[0m f1_per_class: [0.569, 0.082, 0.696, 0.265, 0.12, 0.0, 0.135, 0.18, 0.144, 0.221]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m top1: 0.20988805970149255
[2m[36m(func pid=183693)[0m top5: 0.6870335820895522
[2m[36m(func pid=183693)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=183693)[0m f1_macro: 0.17808437405152494
[2m[36m(func pid=183693)[0m f1_weighted: 0.20933092034948136
[2m[36m(func pid=183693)[0m f1_per_class: [0.233, 0.347, 0.227, 0.281, 0.024, 0.104, 0.118, 0.233, 0.094, 0.12]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9493 | Steps: 4 | Val loss: 1.7131 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.1280 | Steps: 4 | Val loss: 2.3635 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.2701 | Steps: 4 | Val loss: 55.3202 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:51:39 (running for 00:43:57.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.129 |      0.178 |                   58 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.949 |      0.323 |                   36 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.146 |      0.356 |                   33 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  9.367 |      0.241 |                   22 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3656716417910448
[2m[36m(func pid=1757)[0m top5: 0.902518656716418
[2m[36m(func pid=1757)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=1757)[0m f1_macro: 0.3231597210706496
[2m[36m(func pid=1757)[0m f1_weighted: 0.3267077989700784
[2m[36m(func pid=1757)[0m f1_per_class: [0.54, 0.361, 0.5, 0.615, 0.232, 0.135, 0.126, 0.268, 0.163, 0.292]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.4361007462686567
[2m[36m(func pid=2535)[0m top5: 0.9230410447761194
[2m[36m(func pid=2535)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=2535)[0m f1_macro: 0.33843911733611176
[2m[36m(func pid=2535)[0m f1_weighted: 0.4484050119756876
[2m[36m(func pid=2535)[0m f1_per_class: [0.513, 0.483, 0.4, 0.533, 0.122, 0.145, 0.583, 0.047, 0.15, 0.409]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2129 | Steps: 4 | Val loss: 2.1493 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=5446)[0m top1: 0.13152985074626866
[2m[36m(func pid=5446)[0m top5: 0.42677238805970147
[2m[36m(func pid=5446)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=5446)[0m f1_macro: 0.15155264475063776
[2m[36m(func pid=5446)[0m f1_weighted: 0.10842556846934168
[2m[36m(func pid=5446)[0m f1_per_class: [0.098, 0.367, 0.143, 0.058, 0.095, 0.0, 0.0, 0.315, 0.099, 0.341]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.9371 | Steps: 4 | Val loss: 1.6308 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=183693)[0m top1: 0.2140858208955224
[2m[36m(func pid=183693)[0m top5: 0.7024253731343284
[2m[36m(func pid=183693)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=183693)[0m f1_macro: 0.17791939596570422
[2m[36m(func pid=183693)[0m f1_weighted: 0.21827839022883846
[2m[36m(func pid=183693)[0m f1_per_class: [0.237, 0.331, 0.204, 0.318, 0.027, 0.104, 0.123, 0.237, 0.092, 0.108]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6852 | Steps: 4 | Val loss: 2.2938 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 19.7576 | Steps: 4 | Val loss: 41.1868 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:51:45 (running for 00:44:02.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.213 |      0.178 |                   59 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.937 |      0.33  |                   37 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.128 |      0.338 |                   34 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  6.27  |      0.152 |                   23 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.386660447761194
[2m[36m(func pid=1757)[0m top5: 0.9151119402985075
[2m[36m(func pid=1757)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=1757)[0m f1_macro: 0.3301388238856255
[2m[36m(func pid=1757)[0m f1_weighted: 0.381839690836551
[2m[36m(func pid=1757)[0m f1_per_class: [0.436, 0.401, 0.4, 0.607, 0.198, 0.23, 0.26, 0.305, 0.171, 0.293]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.416044776119403
[2m[36m(func pid=2535)[0m top5: 0.9230410447761194
[2m[36m(func pid=2535)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=2535)[0m f1_macro: 0.3590952330757702
[2m[36m(func pid=2535)[0m f1_weighted: 0.4414572695877237
[2m[36m(func pid=2535)[0m f1_per_class: [0.611, 0.474, 0.414, 0.543, 0.142, 0.223, 0.503, 0.129, 0.154, 0.4]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.0761 | Steps: 4 | Val loss: 2.1402 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=5446)[0m top1: 0.2733208955223881
[2m[36m(func pid=5446)[0m top5: 0.6002798507462687
[2m[36m(func pid=5446)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=5446)[0m f1_macro: 0.2671155462319944
[2m[36m(func pid=5446)[0m f1_weighted: 0.23208462500884167
[2m[36m(func pid=5446)[0m f1_per_class: [0.698, 0.456, 0.183, 0.374, 0.172, 0.095, 0.0, 0.244, 0.112, 0.336]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.9865 | Steps: 4 | Val loss: 1.6891 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=183693)[0m top1: 0.21595149253731344
[2m[36m(func pid=183693)[0m top5: 0.7047574626865671
[2m[36m(func pid=183693)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=183693)[0m f1_macro: 0.17893530831525156
[2m[36m(func pid=183693)[0m f1_weighted: 0.2207940086264322
[2m[36m(func pid=183693)[0m f1_per_class: [0.252, 0.322, 0.208, 0.342, 0.024, 0.114, 0.11, 0.236, 0.082, 0.099]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3585 | Steps: 4 | Val loss: 2.9869 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 7.5877 | Steps: 4 | Val loss: 23.9439 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
== Status ==
Current time: 2024-01-07 11:51:50 (running for 00:44:08.06)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.076 |      0.179 |                   60 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.987 |      0.326 |                   38 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.685 |      0.359 |                   35 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 19.758 |      0.267 |                   24 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3824626865671642
[2m[36m(func pid=1757)[0m top5: 0.9053171641791045
[2m[36m(func pid=1757)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=1757)[0m f1_macro: 0.32610891700962197
[2m[36m(func pid=1757)[0m f1_weighted: 0.4121114348862707
[2m[36m(func pid=1757)[0m f1_per_class: [0.396, 0.436, 0.387, 0.547, 0.107, 0.197, 0.405, 0.359, 0.196, 0.231]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.34841417910447764
[2m[36m(func pid=2535)[0m top5: 0.8274253731343284
[2m[36m(func pid=2535)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=2535)[0m f1_macro: 0.30977212290353856
[2m[36m(func pid=2535)[0m f1_weighted: 0.3489386682179234
[2m[36m(func pid=2535)[0m f1_per_class: [0.569, 0.509, 0.13, 0.526, 0.131, 0.249, 0.142, 0.353, 0.197, 0.292]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1071 | Steps: 4 | Val loss: 2.1485 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=5446)[0m top1: 0.3474813432835821
[2m[36m(func pid=5446)[0m top5: 0.7896455223880597
[2m[36m(func pid=5446)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=5446)[0m f1_macro: 0.24500119645136292
[2m[36m(func pid=5446)[0m f1_weighted: 0.29335362920089797
[2m[36m(func pid=5446)[0m f1_per_class: [0.167, 0.129, 0.182, 0.618, 0.281, 0.332, 0.1, 0.29, 0.16, 0.191]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8293 | Steps: 4 | Val loss: 1.7521 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4416 | Steps: 4 | Val loss: 3.4277 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=183693)[0m top1: 0.22014925373134328
[2m[36m(func pid=183693)[0m top5: 0.6968283582089553
[2m[36m(func pid=183693)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=183693)[0m f1_macro: 0.17776566450786782
[2m[36m(func pid=183693)[0m f1_weighted: 0.22490714672750073
[2m[36m(func pid=183693)[0m f1_per_class: [0.239, 0.317, 0.188, 0.364, 0.025, 0.102, 0.11, 0.245, 0.083, 0.105]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.9197 | Steps: 4 | Val loss: 16.9645 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:51:56 (running for 00:44:13.52)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.107 |      0.178 |                   61 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.987 |      0.326 |                   38 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.442 |      0.336 |                   37 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  7.588 |      0.245 |                   25 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3605410447761194
[2m[36m(func pid=1757)[0m top5: 0.8843283582089553
[2m[36m(func pid=1757)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=1757)[0m f1_macro: 0.3165883230298937
[2m[36m(func pid=1757)[0m f1_weighted: 0.39963156737418093
[2m[36m(func pid=1757)[0m f1_per_class: [0.483, 0.404, 0.353, 0.458, 0.093, 0.172, 0.472, 0.337, 0.253, 0.141]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3726679104477612
[2m[36m(func pid=2535)[0m top5: 0.8148320895522388
[2m[36m(func pid=2535)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=2535)[0m f1_macro: 0.3360372053581354
[2m[36m(func pid=2535)[0m f1_weighted: 0.3427281941842797
[2m[36m(func pid=2535)[0m f1_per_class: [0.624, 0.478, 0.325, 0.613, 0.183, 0.282, 0.046, 0.312, 0.163, 0.333]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2239 | Steps: 4 | Val loss: 2.1551 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=5446)[0m top1: 0.4449626865671642
[2m[36m(func pid=5446)[0m top5: 0.9146455223880597
[2m[36m(func pid=5446)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=5446)[0m f1_macro: 0.30182882692337804
[2m[36m(func pid=5446)[0m f1_weighted: 0.4239761047068189
[2m[36m(func pid=5446)[0m f1_per_class: [0.467, 0.223, 0.267, 0.631, 0.235, 0.356, 0.487, 0.144, 0.0, 0.208]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2714 | Steps: 4 | Val loss: 3.4914 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9423 | Steps: 4 | Val loss: 1.8416 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=183693)[0m top1: 0.22014925373134328
[2m[36m(func pid=183693)[0m top5: 0.6935634328358209
[2m[36m(func pid=183693)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=183693)[0m f1_macro: 0.17604870207938264
[2m[36m(func pid=183693)[0m f1_weighted: 0.22381389934862117
[2m[36m(func pid=183693)[0m f1_per_class: [0.221, 0.294, 0.176, 0.371, 0.05, 0.101, 0.114, 0.24, 0.087, 0.106]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 5.3609 | Steps: 4 | Val loss: 23.6632 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 11:52:01 (running for 00:44:18.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.224 |      0.176 |                   62 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.829 |      0.317 |                   39 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.271 |      0.325 |                   38 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.92  |      0.302 |                   26 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.3087686567164179
[2m[36m(func pid=2535)[0m top5: 0.8250932835820896
[2m[36m(func pid=2535)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=2535)[0m f1_macro: 0.32478759067277463
[2m[36m(func pid=2535)[0m f1_weighted: 0.30260533940231826
[2m[36m(func pid=2535)[0m f1_per_class: [0.651, 0.253, 0.4, 0.561, 0.206, 0.273, 0.086, 0.352, 0.123, 0.343]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m top1: 0.3414179104477612
[2m[36m(func pid=1757)[0m top5: 0.8600746268656716
[2m[36m(func pid=1757)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=1757)[0m f1_macro: 0.31652602036193367
[2m[36m(func pid=1757)[0m f1_weighted: 0.3865079458157556
[2m[36m(func pid=1757)[0m f1_per_class: [0.538, 0.374, 0.421, 0.434, 0.067, 0.1, 0.487, 0.351, 0.25, 0.143]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m top1: 0.30550373134328357
[2m[36m(func pid=5446)[0m top5: 0.8297574626865671
[2m[36m(func pid=5446)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=5446)[0m f1_macro: 0.27750223081646036
[2m[36m(func pid=5446)[0m f1_weighted: 0.3178586319267162
[2m[36m(func pid=5446)[0m f1_per_class: [0.47, 0.519, 0.267, 0.144, 0.264, 0.274, 0.442, 0.142, 0.048, 0.206]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1254 | Steps: 4 | Val loss: 2.1434 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8646 | Steps: 4 | Val loss: 2.6770 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7248 | Steps: 4 | Val loss: 1.8124 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=183693)[0m top1: 0.22388059701492538
[2m[36m(func pid=183693)[0m top5: 0.7042910447761194
[2m[36m(func pid=183693)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=183693)[0m f1_macro: 0.18316556805284204
[2m[36m(func pid=183693)[0m f1_weighted: 0.23152482336502175
[2m[36m(func pid=183693)[0m f1_per_class: [0.227, 0.281, 0.216, 0.383, 0.062, 0.103, 0.133, 0.253, 0.087, 0.087]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 4.6147 | Steps: 4 | Val loss: 43.7382 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 11:52:06 (running for 00:44:24.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.125 |      0.183 |                   63 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.942 |      0.317 |                   40 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.865 |      0.337 |                   39 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.361 |      0.278 |                   27 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.3498134328358209
[2m[36m(func pid=2535)[0m top5: 0.8885261194029851
[2m[36m(func pid=2535)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=2535)[0m f1_macro: 0.33747639711653366
[2m[36m(func pid=2535)[0m f1_weighted: 0.367300477967514
[2m[36m(func pid=2535)[0m f1_per_class: [0.602, 0.511, 0.4, 0.47, 0.154, 0.231, 0.271, 0.284, 0.17, 0.283]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m top1: 0.3414179104477612
[2m[36m(func pid=1757)[0m top5: 0.875
[2m[36m(func pid=1757)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=1757)[0m f1_macro: 0.3335195370333115
[2m[36m(func pid=1757)[0m f1_weighted: 0.38485557321757363
[2m[36m(func pid=1757)[0m f1_per_class: [0.569, 0.367, 0.522, 0.454, 0.082, 0.131, 0.453, 0.358, 0.195, 0.205]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m top1: 0.2560634328358209
[2m[36m(func pid=5446)[0m top5: 0.6581156716417911
[2m[36m(func pid=5446)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=5446)[0m f1_macro: 0.27163319456179896
[2m[36m(func pid=5446)[0m f1_weighted: 0.15957557193062466
[2m[36m(func pid=5446)[0m f1_per_class: [0.421, 0.405, 0.55, 0.036, 0.128, 0.172, 0.059, 0.295, 0.229, 0.421]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.0833 | Steps: 4 | Val loss: 2.1544 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7230 | Steps: 4 | Val loss: 1.7998 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3724 | Steps: 4 | Val loss: 3.5812 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=183693)[0m top1: 0.21222014925373134
[2m[36m(func pid=183693)[0m top5: 0.7019589552238806
[2m[36m(func pid=183693)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=183693)[0m f1_macro: 0.1739011805196875
[2m[36m(func pid=183693)[0m f1_weighted: 0.22236338879175768
[2m[36m(func pid=183693)[0m f1_per_class: [0.221, 0.267, 0.196, 0.371, 0.063, 0.103, 0.128, 0.236, 0.071, 0.082]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 3.4744 | Steps: 4 | Val loss: 29.1404 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:52:12 (running for 00:44:29.71)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.083 |      0.174 |                   64 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.723 |      0.325 |                   42 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.865 |      0.337 |                   39 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.615 |      0.272 |                   28 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.32882462686567165
[2m[36m(func pid=1757)[0m top5: 0.882929104477612
[2m[36m(func pid=1757)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=1757)[0m f1_macro: 0.325301899725851
[2m[36m(func pid=1757)[0m f1_weighted: 0.3628775181484886
[2m[36m(func pid=1757)[0m f1_per_class: [0.588, 0.358, 0.458, 0.491, 0.101, 0.159, 0.343, 0.333, 0.182, 0.238]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3344216417910448
[2m[36m(func pid=2535)[0m top5: 0.8857276119402985
[2m[36m(func pid=2535)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=2535)[0m f1_macro: 0.31079739680514124
[2m[36m(func pid=2535)[0m f1_weighted: 0.29618333859107066
[2m[36m(func pid=2535)[0m f1_per_class: [0.64, 0.436, 0.667, 0.15, 0.133, 0.13, 0.417, 0.333, 0.028, 0.175]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.28218283582089554
[2m[36m(func pid=5446)[0m top5: 0.7210820895522388
[2m[36m(func pid=5446)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=5446)[0m f1_macro: 0.2395138434152198
[2m[36m(func pid=5446)[0m f1_weighted: 0.3089642881849529
[2m[36m(func pid=5446)[0m f1_per_class: [0.271, 0.421, 0.193, 0.397, 0.093, 0.129, 0.284, 0.2, 0.133, 0.275]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0804 | Steps: 4 | Val loss: 2.1563 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6716 | Steps: 4 | Val loss: 1.8188 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.8651 | Steps: 4 | Val loss: 2.6554 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=183693)[0m top1: 0.21222014925373134
[2m[36m(func pid=183693)[0m top5: 0.6977611940298507
[2m[36m(func pid=183693)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=183693)[0m f1_macro: 0.17505182651335338
[2m[36m(func pid=183693)[0m f1_weighted: 0.22651866981855437
[2m[36m(func pid=183693)[0m f1_per_class: [0.241, 0.235, 0.2, 0.38, 0.052, 0.099, 0.15, 0.254, 0.064, 0.077]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 17.2972 | Steps: 4 | Val loss: 35.0570 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=1757)[0m top1: 0.31343283582089554
[2m[36m(func pid=1757)[0m top5: 0.8782649253731343
[2m[36m(func pid=1757)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=1757)[0m f1_macro: 0.3219033052828498
[2m[36m(func pid=1757)[0m f1_weighted: 0.32963836137589847
[2m[36m(func pid=1757)[0m f1_per_class: [0.614, 0.398, 0.471, 0.483, 0.124, 0.145, 0.227, 0.288, 0.172, 0.298]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:52:17 (running for 00:44:35.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.08  |      0.175 |                   65 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.672 |      0.322 |                   43 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.372 |      0.311 |                   40 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.474 |      0.24  |                   29 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.37779850746268656
[2m[36m(func pid=2535)[0m top5: 0.9085820895522388
[2m[36m(func pid=2535)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=2535)[0m f1_macro: 0.33948922583156604
[2m[36m(func pid=2535)[0m f1_weighted: 0.40914794095442203
[2m[36m(func pid=2535)[0m f1_per_class: [0.645, 0.504, 0.489, 0.469, 0.205, 0.169, 0.454, 0.26, 0.079, 0.12]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.28404850746268656
[2m[36m(func pid=5446)[0m top5: 0.6963619402985075
[2m[36m(func pid=5446)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=5446)[0m f1_macro: 0.22221294304399733
[2m[36m(func pid=5446)[0m f1_weighted: 0.3004017189281982
[2m[36m(func pid=5446)[0m f1_per_class: [0.49, 0.0, 0.089, 0.537, 0.134, 0.157, 0.366, 0.097, 0.102, 0.25]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0712 | Steps: 4 | Val loss: 2.1298 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.1615 | Steps: 4 | Val loss: 3.3978 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7286 | Steps: 4 | Val loss: 1.7822 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 21.5707 | Steps: 4 | Val loss: 31.2362 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=183693)[0m top1: 0.21921641791044777
[2m[36m(func pid=183693)[0m top5: 0.7126865671641791
[2m[36m(func pid=183693)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=183693)[0m f1_macro: 0.1904933021726735
[2m[36m(func pid=183693)[0m f1_weighted: 0.23563847406026484
[2m[36m(func pid=183693)[0m f1_per_class: [0.255, 0.256, 0.293, 0.382, 0.056, 0.094, 0.165, 0.25, 0.07, 0.084]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:22 (running for 00:44:40.43)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.071 |      0.19  |                   66 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.672 |      0.322 |                   43 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.161 |      0.299 |                   42 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 17.297 |      0.222 |                   30 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.27098880597014924
[2m[36m(func pid=2535)[0m top5: 0.8889925373134329
[2m[36m(func pid=2535)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=2535)[0m f1_macro: 0.29917813143427197
[2m[36m(func pid=2535)[0m f1_weighted: 0.2917129951467596
[2m[36m(func pid=2535)[0m f1_per_class: [0.636, 0.067, 0.667, 0.497, 0.163, 0.136, 0.293, 0.25, 0.12, 0.162]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m top1: 0.3362873134328358
[2m[36m(func pid=1757)[0m top5: 0.8917910447761194
[2m[36m(func pid=1757)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=1757)[0m f1_macro: 0.3353590201110605
[2m[36m(func pid=1757)[0m f1_weighted: 0.3404839276974508
[2m[36m(func pid=1757)[0m f1_per_class: [0.583, 0.434, 0.471, 0.533, 0.15, 0.181, 0.178, 0.297, 0.19, 0.337]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m top1: 0.33722014925373134
[2m[36m(func pid=5446)[0m top5: 0.7611940298507462
[2m[36m(func pid=5446)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=5446)[0m f1_macro: 0.28744842270866905
[2m[36m(func pid=5446)[0m f1_weighted: 0.3064237901535546
[2m[36m(func pid=5446)[0m f1_per_class: [0.59, 0.0, 0.169, 0.616, 0.308, 0.305, 0.215, 0.223, 0.096, 0.353]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.0975 | Steps: 4 | Val loss: 2.1269 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3260 | Steps: 4 | Val loss: 4.3201 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6584 | Steps: 4 | Val loss: 1.6479 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 4.3153 | Steps: 4 | Val loss: 27.4757 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=183693)[0m top1: 0.22574626865671643
[2m[36m(func pid=183693)[0m top5: 0.7122201492537313
[2m[36m(func pid=183693)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=183693)[0m f1_macro: 0.19359067823571924
[2m[36m(func pid=183693)[0m f1_weighted: 0.24140652454183667
[2m[36m(func pid=183693)[0m f1_per_class: [0.244, 0.277, 0.31, 0.384, 0.049, 0.086, 0.173, 0.252, 0.075, 0.085]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:28 (running for 00:44:45.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.098 |      0.194 |                   67 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.729 |      0.335 |                   44 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.326 |      0.254 |                   43 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 21.571 |      0.287 |                   31 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.38619402985074625
[2m[36m(func pid=1757)[0m top5: 0.9081156716417911
[2m[36m(func pid=1757)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=1757)[0m f1_macro: 0.3593450310013106
[2m[36m(func pid=1757)[0m f1_weighted: 0.38842138712995067
[2m[36m(func pid=1757)[0m f1_per_class: [0.594, 0.54, 0.421, 0.543, 0.165, 0.213, 0.251, 0.302, 0.218, 0.346]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.20475746268656717
[2m[36m(func pid=2535)[0m top5: 0.8763992537313433
[2m[36m(func pid=2535)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=2535)[0m f1_macro: 0.25378082366337684
[2m[36m(func pid=2535)[0m f1_weighted: 0.22334020472350352
[2m[36m(func pid=2535)[0m f1_per_class: [0.625, 0.042, 0.647, 0.364, 0.149, 0.121, 0.213, 0.275, 0.102, 0.0]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.36100746268656714
[2m[36m(func pid=5446)[0m top5: 0.8638059701492538
[2m[36m(func pid=5446)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=5446)[0m f1_macro: 0.30583401390535203
[2m[36m(func pid=5446)[0m f1_weighted: 0.37010535964076263
[2m[36m(func pid=5446)[0m f1_per_class: [0.433, 0.096, 0.512, 0.6, 0.174, 0.061, 0.49, 0.197, 0.12, 0.375]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.0450 | Steps: 4 | Val loss: 2.1204 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8346 | Steps: 4 | Val loss: 1.5859 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.7823 | Steps: 4 | Val loss: 3.0646 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.1002 | Steps: 4 | Val loss: 23.0687 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=183693)[0m top1: 0.22527985074626866
[2m[36m(func pid=183693)[0m top5: 0.7136194029850746
[2m[36m(func pid=183693)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=183693)[0m f1_macro: 0.1929345039091311
[2m[36m(func pid=183693)[0m f1_weighted: 0.23873636843316898
[2m[36m(func pid=183693)[0m f1_per_class: [0.253, 0.277, 0.265, 0.374, 0.054, 0.092, 0.168, 0.246, 0.104, 0.096]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:33 (running for 00:44:51.15)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.045 |      0.193 |                   68 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.835 |      0.373 |                   46 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.326 |      0.254 |                   43 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.315 |      0.306 |                   32 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.4193097014925373
[2m[36m(func pid=1757)[0m top5: 0.9188432835820896
[2m[36m(func pid=1757)[0m f1_micro: 0.4193097014925374
[2m[36m(func pid=1757)[0m f1_macro: 0.3733920824160099
[2m[36m(func pid=1757)[0m f1_weighted: 0.4308537730905022
[2m[36m(func pid=1757)[0m f1_per_class: [0.571, 0.561, 0.414, 0.563, 0.148, 0.205, 0.364, 0.315, 0.217, 0.375]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3101679104477612
[2m[36m(func pid=2535)[0m top5: 0.8605410447761194
[2m[36m(func pid=2535)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=2535)[0m f1_macro: 0.31121029634748393
[2m[36m(func pid=2535)[0m f1_weighted: 0.3380691140024448
[2m[36m(func pid=2535)[0m f1_per_class: [0.259, 0.458, 0.473, 0.444, 0.103, 0.188, 0.26, 0.274, 0.185, 0.468]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.43423507462686567
[2m[36m(func pid=5446)[0m top5: 0.8418843283582089
[2m[36m(func pid=5446)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=5446)[0m f1_macro: 0.3833680743608743
[2m[36m(func pid=5446)[0m f1_weighted: 0.4248479788425777
[2m[36m(func pid=5446)[0m f1_per_class: [0.592, 0.569, 0.524, 0.562, 0.161, 0.039, 0.406, 0.245, 0.247, 0.489]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2133 | Steps: 4 | Val loss: 2.1349 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6621 | Steps: 4 | Val loss: 1.6189 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1713 | Steps: 4 | Val loss: 4.0167 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 9.2275 | Steps: 4 | Val loss: 32.0037 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=183693)[0m top1: 0.21688432835820895
[2m[36m(func pid=183693)[0m top5: 0.7094216417910447
[2m[36m(func pid=183693)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=183693)[0m f1_macro: 0.18996658140291245
[2m[36m(func pid=183693)[0m f1_weighted: 0.231929013406968
[2m[36m(func pid=183693)[0m f1_per_class: [0.217, 0.287, 0.278, 0.336, 0.058, 0.103, 0.171, 0.267, 0.09, 0.092]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m top1: 0.41884328358208955
[2m[36m(func pid=1757)[0m top5: 0.9015858208955224
[2m[36m(func pid=1757)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=1757)[0m f1_macro: 0.35342477893166635
[2m[36m(func pid=1757)[0m f1_weighted: 0.43103915173054824
[2m[36m(func pid=1757)[0m f1_per_class: [0.604, 0.545, 0.265, 0.501, 0.139, 0.2, 0.433, 0.356, 0.193, 0.297]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.2513992537313433
[2m[36m(func pid=2535)[0m top5: 0.8092350746268657
[2m[36m(func pid=2535)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=2535)[0m f1_macro: 0.260296297199793
[2m[36m(func pid=2535)[0m f1_weighted: 0.2807987961972466
[2m[36m(func pid=2535)[0m f1_per_class: [0.1, 0.48, 0.489, 0.351, 0.093, 0.162, 0.161, 0.366, 0.105, 0.295]
== Status ==
Current time: 2024-01-07 11:52:39 (running for 00:44:56.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.213 |      0.19  |                   69 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.662 |      0.353 |                   47 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.782 |      0.311 |                   44 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.1   |      0.383 |                   33 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.34841417910447764
[2m[36m(func pid=5446)[0m top5: 0.8283582089552238
[2m[36m(func pid=5446)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=5446)[0m f1_macro: 0.3519203392171427
[2m[36m(func pid=5446)[0m f1_weighted: 0.3285699853072206
[2m[36m(func pid=5446)[0m f1_per_class: [0.659, 0.432, 0.72, 0.413, 0.067, 0.064, 0.298, 0.198, 0.247, 0.421]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.9586 | Steps: 4 | Val loss: 2.1212 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7537 | Steps: 4 | Val loss: 1.5839 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1551 | Steps: 4 | Val loss: 4.1354 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 7.4230 | Steps: 4 | Val loss: 31.9696 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=183693)[0m top1: 0.21875
[2m[36m(func pid=183693)[0m top5: 0.7248134328358209
[2m[36m(func pid=183693)[0m f1_micro: 0.21875
[2m[36m(func pid=183693)[0m f1_macro: 0.19498999174854192
[2m[36m(func pid=183693)[0m f1_weighted: 0.23514822106863253
[2m[36m(func pid=183693)[0m f1_per_class: [0.228, 0.307, 0.319, 0.321, 0.053, 0.083, 0.192, 0.255, 0.104, 0.088]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:44 (running for 00:45:01.70)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  1.959 |      0.195 |                   70 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.754 |      0.354 |                   48 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.171 |      0.26  |                   45 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  9.228 |      0.352 |                   34 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.44263059701492535
[2m[36m(func pid=1757)[0m top5: 0.9081156716417911
[2m[36m(func pid=1757)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=1757)[0m f1_macro: 0.35417912815543995
[2m[36m(func pid=1757)[0m f1_weighted: 0.45686909459608765
[2m[36m(func pid=1757)[0m f1_per_class: [0.577, 0.543, 0.243, 0.52, 0.136, 0.188, 0.514, 0.338, 0.203, 0.281]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.2583955223880597
[2m[36m(func pid=2535)[0m top5: 0.7789179104477612
[2m[36m(func pid=2535)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=2535)[0m f1_macro: 0.24723526077170693
[2m[36m(func pid=2535)[0m f1_weighted: 0.28051018743112144
[2m[36m(func pid=2535)[0m f1_per_class: [0.529, 0.55, 0.071, 0.322, 0.059, 0.173, 0.133, 0.364, 0.028, 0.242]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.279384328358209
[2m[36m(func pid=5446)[0m top5: 0.8250932835820896
[2m[36m(func pid=5446)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=5446)[0m f1_macro: 0.2944072094983293
[2m[36m(func pid=5446)[0m f1_weighted: 0.30805910078685445
[2m[36m(func pid=5446)[0m f1_per_class: [0.588, 0.319, 0.609, 0.557, 0.048, 0.165, 0.164, 0.076, 0.226, 0.192]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0916 | Steps: 4 | Val loss: 2.1189 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6523 | Steps: 4 | Val loss: 1.5747 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7723 | Steps: 4 | Val loss: 3.8026 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 14.9266 | Steps: 4 | Val loss: 46.3400 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=183693)[0m top1: 0.2196828358208955
[2m[36m(func pid=183693)[0m top5: 0.7192164179104478
[2m[36m(func pid=183693)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=183693)[0m f1_macro: 0.192913171321146
[2m[36m(func pid=183693)[0m f1_weighted: 0.23485713766032792
[2m[36m(func pid=183693)[0m f1_per_class: [0.229, 0.289, 0.293, 0.325, 0.084, 0.086, 0.199, 0.244, 0.089, 0.09]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:49 (running for 00:45:06.98)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.092 |      0.193 |                   71 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.652 |      0.365 |                   49 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.155 |      0.247 |                   46 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  7.423 |      0.294 |                   35 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.44263059701492535
[2m[36m(func pid=1757)[0m top5: 0.917910447761194
[2m[36m(func pid=1757)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=1757)[0m f1_macro: 0.36541738581220357
[2m[36m(func pid=1757)[0m f1_weighted: 0.4568133590708609
[2m[36m(func pid=1757)[0m f1_per_class: [0.552, 0.537, 0.313, 0.532, 0.168, 0.24, 0.486, 0.329, 0.209, 0.288]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.2691231343283582
[2m[36m(func pid=2535)[0m top5: 0.7761194029850746
[2m[36m(func pid=2535)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=2535)[0m f1_macro: 0.24445812796283653
[2m[36m(func pid=2535)[0m f1_weighted: 0.2995153590851679
[2m[36m(func pid=2535)[0m f1_per_class: [0.296, 0.504, 0.116, 0.415, 0.082, 0.184, 0.133, 0.375, 0.162, 0.178]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.2126865671641791
[2m[36m(func pid=5446)[0m top5: 0.7019589552238806
[2m[36m(func pid=5446)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=5446)[0m f1_macro: 0.22448162089223703
[2m[36m(func pid=5446)[0m f1_weighted: 0.21228263496842598
[2m[36m(func pid=5446)[0m f1_per_class: [0.579, 0.021, 0.267, 0.501, 0.056, 0.184, 0.039, 0.25, 0.173, 0.175]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.9875 | Steps: 4 | Val loss: 2.1007 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.8938 | Steps: 4 | Val loss: 1.6339 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4796 | Steps: 4 | Val loss: 4.1134 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 6.8514 | Steps: 4 | Val loss: 26.9988 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=183693)[0m top1: 0.23180970149253732
[2m[36m(func pid=183693)[0m top5: 0.7467350746268657
[2m[36m(func pid=183693)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=183693)[0m f1_macro: 0.19966658289346775
[2m[36m(func pid=183693)[0m f1_weighted: 0.24803947957188177
[2m[36m(func pid=183693)[0m f1_per_class: [0.227, 0.305, 0.272, 0.323, 0.089, 0.108, 0.227, 0.242, 0.107, 0.098]
[2m[36m(func pid=183693)[0m 
== Status ==
Current time: 2024-01-07 11:52:54 (running for 00:45:12.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  1.987 |      0.2   |                   72 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.894 |      0.361 |                   50 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  2.772 |      0.244 |                   47 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 14.927 |      0.224 |                   36 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.40625
[2m[36m(func pid=1757)[0m top5: 0.9071828358208955
[2m[36m(func pid=1757)[0m f1_micro: 0.40625
[2m[36m(func pid=1757)[0m f1_macro: 0.36145313342273705
[2m[36m(func pid=1757)[0m f1_weighted: 0.4164615078516668
[2m[36m(func pid=1757)[0m f1_per_class: [0.542, 0.499, 0.375, 0.547, 0.196, 0.276, 0.34, 0.364, 0.182, 0.294]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.2332089552238806
[2m[36m(func pid=2535)[0m top5: 0.7961753731343284
[2m[36m(func pid=2535)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=2535)[0m f1_macro: 0.2944284224734581
[2m[36m(func pid=2535)[0m f1_weighted: 0.25795783683477547
[2m[36m(func pid=2535)[0m f1_per_class: [0.468, 0.276, 0.71, 0.414, 0.158, 0.182, 0.11, 0.36, 0.11, 0.157]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3460820895522388
[2m[36m(func pid=5446)[0m top5: 0.8563432835820896
[2m[36m(func pid=5446)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=5446)[0m f1_macro: 0.3601318234421057
[2m[36m(func pid=5446)[0m f1_weighted: 0.35027043898444005
[2m[36m(func pid=5446)[0m f1_per_class: [0.596, 0.535, 0.759, 0.544, 0.085, 0.123, 0.168, 0.247, 0.196, 0.349]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.9514 | Steps: 4 | Val loss: 2.0779 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5789 | Steps: 4 | Val loss: 1.5614 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3311 | Steps: 4 | Val loss: 4.6860 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 8.0845 | Steps: 4 | Val loss: 30.0861 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:53:00 (running for 00:45:17.48)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  1.951 |      0.208 |                   73 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.894 |      0.361 |                   50 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.48  |      0.294 |                   48 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  6.851 |      0.36  |                   37 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=183693)[0m top1: 0.24300373134328357
[2m[36m(func pid=183693)[0m top5: 0.7667910447761194
[2m[36m(func pid=183693)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=183693)[0m f1_macro: 0.20836182827261326
[2m[36m(func pid=183693)[0m f1_weighted: 0.25926969627002877
[2m[36m(func pid=183693)[0m f1_per_class: [0.241, 0.309, 0.293, 0.361, 0.083, 0.099, 0.226, 0.247, 0.114, 0.11]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=1757)[0m top1: 0.4244402985074627
[2m[36m(func pid=1757)[0m top5: 0.9305037313432836
[2m[36m(func pid=1757)[0m f1_micro: 0.4244402985074627
[2m[36m(func pid=1757)[0m f1_macro: 0.38285264001691005
[2m[36m(func pid=1757)[0m f1_weighted: 0.44265585904583266
[2m[36m(func pid=1757)[0m f1_per_class: [0.561, 0.485, 0.436, 0.573, 0.211, 0.3, 0.4, 0.34, 0.203, 0.318]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.20708955223880596
[2m[36m(func pid=2535)[0m top5: 0.769589552238806
[2m[36m(func pid=2535)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=2535)[0m f1_macro: 0.1980082318747412
[2m[36m(func pid=2535)[0m f1_weighted: 0.19931324702035622
[2m[36m(func pid=2535)[0m f1_per_class: [0.271, 0.216, 0.133, 0.38, 0.286, 0.045, 0.074, 0.237, 0.125, 0.213]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3591417910447761
[2m[36m(func pid=5446)[0m top5: 0.8740671641791045
[2m[36m(func pid=5446)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=5446)[0m f1_macro: 0.29559895333998537
[2m[36m(func pid=5446)[0m f1_weighted: 0.3278897283814801
[2m[36m(func pid=5446)[0m f1_per_class: [0.523, 0.531, 0.625, 0.33, 0.091, 0.039, 0.354, 0.235, 0.152, 0.077]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.0061 | Steps: 4 | Val loss: 2.0609 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6124 | Steps: 4 | Val loss: 1.5692 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4597 | Steps: 4 | Val loss: 3.4090 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 4.2513 | Steps: 4 | Val loss: 20.1326 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:53:05 (running for 00:45:22.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  1.951 |      0.208 |                   73 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.612 |      0.39  |                   52 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.331 |      0.198 |                   49 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  8.084 |      0.296 |                   38 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.417910447761194
[2m[36m(func pid=1757)[0m top5: 0.925839552238806
[2m[36m(func pid=1757)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=1757)[0m f1_macro: 0.3902738110375764
[2m[36m(func pid=1757)[0m f1_weighted: 0.437226978625345
[2m[36m(func pid=1757)[0m f1_per_class: [0.574, 0.466, 0.49, 0.572, 0.224, 0.309, 0.388, 0.332, 0.205, 0.341]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3670708955223881
[2m[36m(func pid=2535)[0m top5: 0.8563432835820896
[2m[36m(func pid=2535)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=2535)[0m f1_macro: 0.2823040223943553
[2m[36m(func pid=2535)[0m f1_weighted: 0.33556574035000736
[2m[36m(func pid=2535)[0m f1_per_class: [0.472, 0.316, 0.133, 0.617, 0.316, 0.031, 0.231, 0.261, 0.174, 0.273]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=183693)[0m top1: 0.25466417910447764
[2m[36m(func pid=183693)[0m top5: 0.7747201492537313
[2m[36m(func pid=183693)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=183693)[0m f1_macro: 0.21203925943070107
[2m[36m(func pid=183693)[0m f1_weighted: 0.2760766862129518
[2m[36m(func pid=183693)[0m f1_per_class: [0.264, 0.296, 0.297, 0.389, 0.075, 0.104, 0.264, 0.243, 0.097, 0.092]
[2m[36m(func pid=183693)[0m 
[2m[36m(func pid=5446)[0m top1: 0.42117537313432835
[2m[36m(func pid=5446)[0m top5: 0.8861940298507462
[2m[36m(func pid=5446)[0m f1_micro: 0.42117537313432835
[2m[36m(func pid=5446)[0m f1_macro: 0.3344842603278451
[2m[36m(func pid=5446)[0m f1_weighted: 0.4423321720778546
[2m[36m(func pid=5446)[0m f1_per_class: [0.342, 0.524, 0.338, 0.558, 0.25, 0.199, 0.472, 0.225, 0.186, 0.25]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.6640 | Steps: 4 | Val loss: 1.6057 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=183693)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9767 | Steps: 4 | Val loss: 2.0595 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7013 | Steps: 4 | Val loss: 2.2120 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5576 | Steps: 4 | Val loss: 21.5301 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 11:53:10 (running for 00:45:28.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.35775
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00020 | RUNNING    | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  2.006 |      0.212 |                   74 |
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.664 |      0.381 |                   53 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.46  |      0.282 |                   50 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.251 |      0.334 |                   39 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.404384328358209
[2m[36m(func pid=1757)[0m top5: 0.9202425373134329
[2m[36m(func pid=1757)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=1757)[0m f1_macro: 0.38066839857773377
[2m[36m(func pid=1757)[0m f1_weighted: 0.4272332009820436
[2m[36m(func pid=1757)[0m f1_per_class: [0.602, 0.445, 0.522, 0.559, 0.168, 0.29, 0.385, 0.334, 0.223, 0.278]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=183693)[0m top1: 0.25466417910447764
[2m[36m(func pid=183693)[0m top5: 0.7742537313432836
[2m[36m(func pid=183693)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=183693)[0m f1_macro: 0.20381672009680857
[2m[36m(func pid=183693)[0m f1_weighted: 0.2739632526273894
[2m[36m(func pid=183693)[0m f1_per_class: [0.258, 0.288, 0.216, 0.391, 0.074, 0.094, 0.262, 0.26, 0.091, 0.103]
[2m[36m(func pid=2535)[0m top1: 0.4920708955223881
[2m[36m(func pid=2535)[0m top5: 0.9174440298507462
[2m[36m(func pid=2535)[0m f1_micro: 0.4920708955223881
[2m[36m(func pid=2535)[0m f1_macro: 0.4074221760436455
[2m[36m(func pid=2535)[0m f1_weighted: 0.4837924266545421
[2m[36m(func pid=2535)[0m f1_per_class: [0.561, 0.561, 0.64, 0.603, 0.222, 0.096, 0.543, 0.313, 0.237, 0.3]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3829291044776119
[2m[36m(func pid=5446)[0m top5: 0.8815298507462687
[2m[36m(func pid=5446)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=5446)[0m f1_macro: 0.30662921011800015
[2m[36m(func pid=5446)[0m f1_weighted: 0.4189449819566847
[2m[36m(func pid=5446)[0m f1_per_class: [0.585, 0.284, 0.19, 0.535, 0.207, 0.211, 0.538, 0.262, 0.158, 0.096]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5198 | Steps: 4 | Val loss: 1.6815 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.1042 | Steps: 4 | Val loss: 3.3388 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.1127 | Steps: 4 | Val loss: 23.0489 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:53:16 (running for 00:45:33.76)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.52  |      0.359 |                   54 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.701 |      0.407 |                   51 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.558 |      0.307 |                   40 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3656716417910448
[2m[36m(func pid=1757)[0m top5: 0.9090485074626866
[2m[36m(func pid=1757)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=1757)[0m f1_macro: 0.3588472730014363
[2m[36m(func pid=1757)[0m f1_weighted: 0.38607164370426256
[2m[36m(func pid=1757)[0m f1_per_class: [0.6, 0.439, 0.558, 0.546, 0.146, 0.188, 0.311, 0.293, 0.208, 0.299]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.41091417910447764
[2m[36m(func pid=2535)[0m top5: 0.8418843283582089
[2m[36m(func pid=2535)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=2535)[0m f1_macro: 0.32862356504769813
[2m[36m(func pid=2535)[0m f1_weighted: 0.35911980659677584
[2m[36m(func pid=2535)[0m f1_per_class: [0.658, 0.571, 0.415, 0.142, 0.154, 0.183, 0.562, 0.117, 0.158, 0.327]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3591417910447761
[2m[36m(func pid=5446)[0m top5: 0.8731343283582089
[2m[36m(func pid=5446)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=5446)[0m f1_macro: 0.2985110644793473
[2m[36m(func pid=5446)[0m f1_weighted: 0.4111703877069819
[2m[36m(func pid=5446)[0m f1_per_class: [0.492, 0.486, 0.205, 0.441, 0.169, 0.214, 0.494, 0.235, 0.164, 0.084]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.9655 | Steps: 4 | Val loss: 1.6949 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.8632 | Steps: 4 | Val loss: 4.0639 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7843 | Steps: 4 | Val loss: 23.8031 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:53:21 (running for 00:45:39.14)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.52  |      0.359 |                   54 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.863 |      0.302 |                   53 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.113 |      0.299 |                   41 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.3208955223880597
[2m[36m(func pid=2535)[0m top5: 0.8022388059701493
[2m[36m(func pid=2535)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=2535)[0m f1_macro: 0.3016293722653316
[2m[36m(func pid=2535)[0m f1_weighted: 0.30586258412114037
[2m[36m(func pid=2535)[0m f1_per_class: [0.588, 0.558, 0.348, 0.071, 0.11, 0.199, 0.435, 0.262, 0.112, 0.333]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m top1: 0.36473880597014924
[2m[36m(func pid=1757)[0m top5: 0.9076492537313433
[2m[36m(func pid=1757)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=1757)[0m f1_macro: 0.35455645563321864
[2m[36m(func pid=1757)[0m f1_weighted: 0.3782357240223139
[2m[36m(func pid=1757)[0m f1_per_class: [0.587, 0.484, 0.545, 0.547, 0.151, 0.144, 0.276, 0.275, 0.231, 0.305]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3675373134328358
[2m[36m(func pid=5446)[0m top5: 0.851679104477612
[2m[36m(func pid=5446)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=5446)[0m f1_macro: 0.2846495310185071
[2m[36m(func pid=5446)[0m f1_weighted: 0.3876225028932519
[2m[36m(func pid=5446)[0m f1_per_class: [0.364, 0.538, 0.168, 0.41, 0.14, 0.191, 0.428, 0.25, 0.15, 0.209]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.1759 | Steps: 4 | Val loss: 3.5162 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5493 | Steps: 4 | Val loss: 1.7073 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0283 | Steps: 4 | Val loss: 28.8696 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:53:27 (running for 00:45:44.75)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.549 |      0.348 |                   56 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.863 |      0.302 |                   53 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.784 |      0.285 |                   42 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.36380597014925375
[2m[36m(func pid=1757)[0m top5: 0.9011194029850746
[2m[36m(func pid=1757)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=1757)[0m f1_macro: 0.3475241249686241
[2m[36m(func pid=1757)[0m f1_weighted: 0.37936615666266776
[2m[36m(func pid=1757)[0m f1_per_class: [0.574, 0.479, 0.545, 0.547, 0.149, 0.22, 0.256, 0.306, 0.21, 0.189]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.32136194029850745
[2m[36m(func pid=2535)[0m top5: 0.8544776119402985
[2m[36m(func pid=2535)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=2535)[0m f1_macro: 0.3015088141892424
[2m[36m(func pid=2535)[0m f1_weighted: 0.31928940100015274
[2m[36m(func pid=2535)[0m f1_per_class: [0.5, 0.579, 0.239, 0.393, 0.107, 0.203, 0.154, 0.335, 0.148, 0.357]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.32322761194029853
[2m[36m(func pid=5446)[0m top5: 0.8180970149253731
[2m[36m(func pid=5446)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=5446)[0m f1_macro: 0.30429765574558076
[2m[36m(func pid=5446)[0m f1_weighted: 0.3140323980345682
[2m[36m(func pid=5446)[0m f1_per_class: [0.54, 0.516, 0.22, 0.348, 0.112, 0.2, 0.218, 0.289, 0.163, 0.438]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6765 | Steps: 4 | Val loss: 1.6965 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1732 | Steps: 4 | Val loss: 3.5916 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0415 | Steps: 4 | Val loss: 36.6575 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 11:53:32 (running for 00:45:50.23)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.677 |      0.337 |                   57 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.176 |      0.302 |                   54 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.028 |      0.304 |                   43 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3736007462686567
[2m[36m(func pid=1757)[0m top5: 0.8997201492537313
[2m[36m(func pid=1757)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=1757)[0m f1_macro: 0.3367162041281686
[2m[36m(func pid=1757)[0m f1_weighted: 0.3934362798855355
[2m[36m(func pid=1757)[0m f1_per_class: [0.526, 0.485, 0.436, 0.535, 0.155, 0.251, 0.304, 0.332, 0.168, 0.176]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.37080223880597013
[2m[36m(func pid=2535)[0m top5: 0.8050373134328358
[2m[36m(func pid=2535)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=2535)[0m f1_macro: 0.3288896676760846
[2m[36m(func pid=2535)[0m f1_weighted: 0.3413429932808863
[2m[36m(func pid=2535)[0m f1_per_class: [0.523, 0.586, 0.283, 0.558, 0.123, 0.192, 0.06, 0.347, 0.212, 0.405]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.29384328358208955
[2m[36m(func pid=5446)[0m top5: 0.7112873134328358
[2m[36m(func pid=5446)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=5446)[0m f1_macro: 0.3011774644602162
[2m[36m(func pid=5446)[0m f1_weighted: 0.2734917875261357
[2m[36m(func pid=5446)[0m f1_per_class: [0.545, 0.516, 0.222, 0.408, 0.102, 0.174, 0.021, 0.343, 0.159, 0.52]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6965 | Steps: 4 | Val loss: 1.6535 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2998 | Steps: 4 | Val loss: 3.7729 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6945 | Steps: 4 | Val loss: 37.4916 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 11:53:38 (running for 00:45:55.73)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.697 |      0.349 |                   58 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.173 |      0.329 |                   55 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.042 |      0.301 |                   44 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.39738805970149255
[2m[36m(func pid=1757)[0m top5: 0.9095149253731343
[2m[36m(func pid=1757)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=1757)[0m f1_macro: 0.3486039780824376
[2m[36m(func pid=1757)[0m f1_weighted: 0.4271511685262005
[2m[36m(func pid=1757)[0m f1_per_class: [0.518, 0.479, 0.393, 0.52, 0.158, 0.252, 0.429, 0.344, 0.208, 0.186]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.37220149253731344
[2m[36m(func pid=2535)[0m top5: 0.7873134328358209
[2m[36m(func pid=2535)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=2535)[0m f1_macro: 0.32130202467510416
[2m[36m(func pid=2535)[0m f1_weighted: 0.3234741562700385
[2m[36m(func pid=2535)[0m f1_per_class: [0.515, 0.554, 0.306, 0.516, 0.144, 0.205, 0.054, 0.35, 0.2, 0.369]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.26725746268656714
[2m[36m(func pid=5446)[0m top5: 0.7779850746268657
[2m[36m(func pid=5446)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=5446)[0m f1_macro: 0.2638964445775065
[2m[36m(func pid=5446)[0m f1_weighted: 0.28178075707949723
[2m[36m(func pid=5446)[0m f1_per_class: [0.603, 0.28, 0.222, 0.563, 0.041, 0.132, 0.062, 0.373, 0.147, 0.216]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5310 | Steps: 4 | Val loss: 1.6183 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.3600 | Steps: 4 | Val loss: 3.5183 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6322 | Steps: 4 | Val loss: 31.0641 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=1757)[0m top1: 0.41884328358208955
[2m[36m(func pid=1757)[0m top5: 0.909981343283582
[2m[36m(func pid=1757)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=1757)[0m f1_macro: 0.3597583829840084
[2m[36m(func pid=1757)[0m f1_weighted: 0.4421050349256515
[2m[36m(func pid=1757)[0m f1_per_class: [0.547, 0.506, 0.387, 0.454, 0.158, 0.251, 0.52, 0.34, 0.242, 0.192]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:53:43 (running for 00:46:00.99)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.531 |      0.36  |                   59 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  1.3   |      0.321 |                   56 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.694 |      0.264 |                   45 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.32509328358208955
[2m[36m(func pid=2535)[0m top5: 0.8610074626865671
[2m[36m(func pid=2535)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=2535)[0m f1_macro: 0.3200643433255747
[2m[36m(func pid=2535)[0m f1_weighted: 0.32611581764062453
[2m[36m(func pid=2535)[0m f1_per_class: [0.59, 0.525, 0.268, 0.335, 0.256, 0.273, 0.227, 0.324, 0.205, 0.198]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3987873134328358
[2m[36m(func pid=5446)[0m top5: 0.8791977611940298
[2m[36m(func pid=5446)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=5446)[0m f1_macro: 0.3037235246611071
[2m[36m(func pid=5446)[0m f1_weighted: 0.35639561807855996
[2m[36m(func pid=5446)[0m f1_per_class: [0.617, 0.117, 0.369, 0.594, 0.084, 0.141, 0.389, 0.312, 0.0, 0.413]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5286 | Steps: 4 | Val loss: 1.5550 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.1397 | Steps: 4 | Val loss: 3.8438 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 11.2651 | Steps: 4 | Val loss: 19.5725 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:53:48 (running for 00:46:06.36)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.529 |      0.381 |                   60 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.36  |      0.32  |                   57 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.632 |      0.304 |                   46 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.45149253731343286
[2m[36m(func pid=1757)[0m top5: 0.9169776119402985
[2m[36m(func pid=1757)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=1757)[0m f1_macro: 0.3812082824126649
[2m[36m(func pid=1757)[0m f1_weighted: 0.4608688691163458
[2m[36m(func pid=1757)[0m f1_per_class: [0.569, 0.536, 0.429, 0.454, 0.161, 0.258, 0.56, 0.318, 0.258, 0.27]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.28404850746268656
[2m[36m(func pid=2535)[0m top5: 0.8138992537313433
[2m[36m(func pid=2535)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=2535)[0m f1_macro: 0.2708894049915299
[2m[36m(func pid=2535)[0m f1_weighted: 0.29400609273948536
[2m[36m(func pid=2535)[0m f1_per_class: [0.435, 0.22, 0.264, 0.261, 0.329, 0.262, 0.396, 0.281, 0.139, 0.121]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.49300373134328357
[2m[36m(func pid=5446)[0m top5: 0.9309701492537313
[2m[36m(func pid=5446)[0m f1_micro: 0.49300373134328357
[2m[36m(func pid=5446)[0m f1_macro: 0.37883500864273845
[2m[36m(func pid=5446)[0m f1_weighted: 0.47978299098423605
[2m[36m(func pid=5446)[0m f1_per_class: [0.693, 0.477, 0.333, 0.634, 0.26, 0.292, 0.505, 0.227, 0.054, 0.312]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.5649 | Steps: 4 | Val loss: 1.5337 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.0356 | Steps: 4 | Val loss: 3.7605 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.1280 | Steps: 4 | Val loss: 23.7504 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=1757)[0m top1: 0.45149253731343286
[2m[36m(func pid=1757)[0m top5: 0.9202425373134329
[2m[36m(func pid=1757)[0m f1_micro: 0.45149253731343286
[2m[36m(func pid=1757)[0m f1_macro: 0.38684621189237806
[2m[36m(func pid=1757)[0m f1_weighted: 0.45858004290704946
[2m[36m(func pid=1757)[0m f1_per_class: [0.611, 0.559, 0.413, 0.527, 0.162, 0.191, 0.489, 0.355, 0.206, 0.356]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3199626865671642== Status ==
Current time: 2024-01-07 11:53:54 (running for 00:46:11.78)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.565 |      0.387 |                   61 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.14  |      0.271 |                   58 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 11.265 |      0.379 |                   47 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=2535)[0m top5: 0.7957089552238806
[2m[36m(func pid=2535)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=2535)[0m f1_macro: 0.261248826924846
[2m[36m(func pid=2535)[0m f1_weighted: 0.322286441461163
[2m[36m(func pid=2535)[0m f1_per_class: [0.276, 0.178, 0.344, 0.29, 0.267, 0.271, 0.507, 0.227, 0.141, 0.112]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3843283582089552
[2m[36m(func pid=5446)[0m top5: 0.8652052238805971
[2m[36m(func pid=5446)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=5446)[0m f1_macro: 0.3440842909572014
[2m[36m(func pid=5446)[0m f1_weighted: 0.39279840545356637
[2m[36m(func pid=5446)[0m f1_per_class: [0.578, 0.584, 0.304, 0.443, 0.195, 0.329, 0.305, 0.272, 0.203, 0.228]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5089 | Steps: 4 | Val loss: 3.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5791 | Steps: 4 | Val loss: 1.5524 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0518 | Steps: 4 | Val loss: 35.6534 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 11:53:59 (running for 00:46:17.24)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.565 |      0.387 |                   61 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.509 |      0.278 |                   60 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.128 |      0.344 |                   48 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.34701492537313433
[2m[36m(func pid=2535)[0m top5: 0.835820895522388
[2m[36m(func pid=2535)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=2535)[0m f1_macro: 0.27750455446724376
[2m[36m(func pid=2535)[0m f1_weighted: 0.36675517389710516
[2m[36m(func pid=2535)[0m f1_per_class: [0.364, 0.208, 0.367, 0.458, 0.123, 0.156, 0.509, 0.275, 0.161, 0.154]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=1757)[0m top1: 0.44029850746268656
[2m[36m(func pid=1757)[0m top5: 0.9183768656716418
[2m[36m(func pid=1757)[0m f1_micro: 0.44029850746268656
[2m[36m(func pid=1757)[0m f1_macro: 0.3893070600402424
[2m[36m(func pid=1757)[0m f1_weighted: 0.4379355226140963
[2m[36m(func pid=1757)[0m f1_per_class: [0.6, 0.548, 0.49, 0.568, 0.185, 0.162, 0.396, 0.342, 0.238, 0.364]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m top1: 0.25886194029850745
[2m[36m(func pid=5446)[0m top5: 0.7700559701492538
[2m[36m(func pid=5446)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=5446)[0m f1_macro: 0.2778478111069922
[2m[36m(func pid=5446)[0m f1_weighted: 0.2448589059566322
[2m[36m(func pid=5446)[0m f1_per_class: [0.493, 0.482, 0.537, 0.138, 0.133, 0.215, 0.213, 0.231, 0.167, 0.169]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4793 | Steps: 4 | Val loss: 1.6340 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2646 | Steps: 4 | Val loss: 2.9746 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 3.1321 | Steps: 4 | Val loss: 38.0417 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:54:05 (running for 00:46:22.52)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.479 |      0.371 |                   63 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.509 |      0.278 |                   60 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.052 |      0.278 |                   49 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.41744402985074625
[2m[36m(func pid=1757)[0m top5: 0.9011194029850746
[2m[36m(func pid=1757)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=1757)[0m f1_macro: 0.37079596670377946
[2m[36m(func pid=1757)[0m f1_weighted: 0.40248759329339057
[2m[36m(func pid=1757)[0m f1_per_class: [0.571, 0.545, 0.49, 0.571, 0.161, 0.149, 0.283, 0.358, 0.221, 0.359]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3787313432835821
[2m[36m(func pid=2535)[0m top5: 0.90625
[2m[36m(func pid=2535)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=2535)[0m f1_macro: 0.34783220762061046
[2m[36m(func pid=2535)[0m f1_weighted: 0.4001843414667685
[2m[36m(func pid=2535)[0m f1_per_class: [0.515, 0.337, 0.5, 0.596, 0.07, 0.106, 0.398, 0.343, 0.187, 0.426]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.2178171641791045
[2m[36m(func pid=5446)[0m top5: 0.7910447761194029
[2m[36m(func pid=5446)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=5446)[0m f1_macro: 0.27278044492007375
[2m[36m(func pid=5446)[0m f1_weighted: 0.2340119092300414
[2m[36m(func pid=5446)[0m f1_per_class: [0.532, 0.252, 0.611, 0.188, 0.15, 0.181, 0.27, 0.274, 0.114, 0.156]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6972 | Steps: 4 | Val loss: 1.7138 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.0095 | Steps: 4 | Val loss: 3.0193 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6553 | Steps: 4 | Val loss: 26.5252 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:54:10 (running for 00:46:27.84)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.697 |      0.349 |                   64 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.265 |      0.348 |                   61 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.132 |      0.273 |                   50 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.39738805970149255
[2m[36m(func pid=1757)[0m top5: 0.8857276119402985
[2m[36m(func pid=1757)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=1757)[0m f1_macro: 0.34925831941086505
[2m[36m(func pid=1757)[0m f1_weighted: 0.3877555449441316
[2m[36m(func pid=1757)[0m f1_per_class: [0.554, 0.507, 0.419, 0.599, 0.125, 0.146, 0.239, 0.346, 0.196, 0.361]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.396455223880597
[2m[36m(func pid=2535)[0m top5: 0.9132462686567164
[2m[36m(func pid=2535)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=2535)[0m f1_macro: 0.37467401100499464
[2m[36m(func pid=2535)[0m f1_weighted: 0.3949076528989471
[2m[36m(func pid=2535)[0m f1_per_class: [0.58, 0.575, 0.571, 0.599, 0.086, 0.18, 0.206, 0.34, 0.198, 0.412]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3474813432835821
[2m[36m(func pid=5446)[0m top5: 0.8479477611940298
[2m[36m(func pid=5446)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=5446)[0m f1_macro: 0.32073303624092897
[2m[36m(func pid=5446)[0m f1_weighted: 0.364119517894508
[2m[36m(func pid=5446)[0m f1_per_class: [0.629, 0.106, 0.537, 0.573, 0.174, 0.185, 0.414, 0.292, 0.173, 0.126]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5599 | Steps: 4 | Val loss: 1.8308 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2096 | Steps: 4 | Val loss: 3.2836 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4994 | Steps: 4 | Val loss: 27.2433 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 11:54:15 (running for 00:46:33.44)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.56  |      0.315 |                   65 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.009 |      0.375 |                   62 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.655 |      0.321 |                   51 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.35867537313432835
[2m[36m(func pid=1757)[0m top5: 0.851679104477612
[2m[36m(func pid=1757)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=1757)[0m f1_macro: 0.31496533019849016
[2m[36m(func pid=1757)[0m f1_weighted: 0.3614448884409411
[2m[36m(func pid=1757)[0m f1_per_class: [0.474, 0.421, 0.413, 0.582, 0.083, 0.141, 0.224, 0.368, 0.214, 0.23]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.38899253731343286
[2m[36m(func pid=2535)[0m top5: 0.8903917910447762
[2m[36m(func pid=2535)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=2535)[0m f1_macro: 0.35020812363641496
[2m[36m(func pid=2535)[0m f1_weighted: 0.3655358950331522
[2m[36m(func pid=2535)[0m f1_per_class: [0.565, 0.584, 0.667, 0.573, 0.115, 0.218, 0.122, 0.351, 0.165, 0.143]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.394589552238806
[2m[36m(func pid=5446)[0m top5: 0.8582089552238806
[2m[36m(func pid=5446)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=5446)[0m f1_macro: 0.3295708306426634
[2m[36m(func pid=5446)[0m f1_weighted: 0.3783114606441127
[2m[36m(func pid=5446)[0m f1_per_class: [0.658, 0.117, 0.55, 0.643, 0.123, 0.144, 0.4, 0.315, 0.155, 0.191]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5920 | Steps: 4 | Val loss: 1.9220 | Batch size: 32 | lr: 0.001 | Duration: 3.16s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6435 | Steps: 4 | Val loss: 3.0499 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2203 | Steps: 4 | Val loss: 26.3966 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:54:21 (running for 00:46:38.98)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.592 |      0.294 |                   66 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.21  |      0.35  |                   63 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.499 |      0.33  |                   52 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3208955223880597
[2m[36m(func pid=1757)[0m top5: 0.8381529850746269
[2m[36m(func pid=1757)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=1757)[0m f1_macro: 0.29388075141284037
[2m[36m(func pid=1757)[0m f1_weighted: 0.33458109261905045
[2m[36m(func pid=1757)[0m f1_per_class: [0.426, 0.351, 0.421, 0.557, 0.086, 0.196, 0.185, 0.356, 0.209, 0.152]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3773320895522388
[2m[36m(func pid=2535)[0m top5: 0.8959888059701493
[2m[36m(func pid=2535)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=2535)[0m f1_macro: 0.35430044776967334
[2m[36m(func pid=2535)[0m f1_weighted: 0.38238582822166706
[2m[36m(func pid=2535)[0m f1_per_class: [0.569, 0.498, 0.611, 0.596, 0.092, 0.198, 0.214, 0.328, 0.179, 0.259]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.4099813432835821
[2m[36m(func pid=5446)[0m top5: 0.886660447761194
[2m[36m(func pid=5446)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=5446)[0m f1_macro: 0.37279220536091723
[2m[36m(func pid=5446)[0m f1_weighted: 0.39980702017974756
[2m[36m(func pid=5446)[0m f1_per_class: [0.623, 0.444, 0.588, 0.616, 0.086, 0.151, 0.285, 0.363, 0.228, 0.344]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4269 | Steps: 4 | Val loss: 1.8558 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6579 | Steps: 4 | Val loss: 2.9971 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.6009 | Steps: 4 | Val loss: 28.9085 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=1757)[0m top1: 0.3353544776119403
[2m[36m(func pid=1757)[0m top5: 0.8591417910447762
[2m[36m(func pid=1757)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=1757)[0m f1_macro: 0.29874072840047505
[2m[36m(func pid=1757)[0m f1_weighted: 0.35942390251630396
[2m[36m(func pid=1757)[0m f1_per_class: [0.431, 0.403, 0.317, 0.521, 0.121, 0.219, 0.265, 0.36, 0.202, 0.149]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:54:26 (running for 00:46:44.41)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.427 |      0.299 |                   67 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.643 |      0.354 |                   64 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.22  |      0.373 |                   53 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.38899253731343286
[2m[36m(func pid=2535)[0m top5: 0.8801305970149254
[2m[36m(func pid=2535)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=2535)[0m f1_macro: 0.32931669484515325
[2m[36m(func pid=2535)[0m f1_weighted: 0.38941712949594026
[2m[36m(func pid=2535)[0m f1_per_class: [0.606, 0.136, 0.647, 0.569, 0.117, 0.139, 0.508, 0.292, 0.127, 0.152]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3871268656716418
[2m[36m(func pid=5446)[0m top5: 0.8568097014925373
[2m[36m(func pid=5446)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=5446)[0m f1_macro: 0.3647504624068235
[2m[36m(func pid=5446)[0m f1_weighted: 0.3839211163449484
[2m[36m(func pid=5446)[0m f1_per_class: [0.648, 0.55, 0.55, 0.579, 0.065, 0.114, 0.221, 0.384, 0.154, 0.383]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5376 | Steps: 4 | Val loss: 1.8098 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1265 | Steps: 4 | Val loss: 2.8722 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.7637 | Steps: 4 | Val loss: 36.3397 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 11:54:32 (running for 00:46:49.79)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.427 |      0.299 |                   67 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.126 |      0.347 |                   66 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.601 |      0.365 |                   54 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.35261194029850745
[2m[36m(func pid=1757)[0m top5: 0.8666044776119403
[2m[36m(func pid=1757)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=1757)[0m f1_macro: 0.31303651775154046
[2m[36m(func pid=1757)[0m f1_weighted: 0.38161763410255284
[2m[36m(func pid=1757)[0m f1_per_class: [0.456, 0.409, 0.295, 0.5, 0.134, 0.244, 0.343, 0.356, 0.206, 0.188]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3670708955223881
[2m[36m(func pid=2535)[0m top5: 0.8894589552238806
[2m[36m(func pid=2535)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=2535)[0m f1_macro: 0.34745127415789173
[2m[36m(func pid=2535)[0m f1_weighted: 0.388174457354478
[2m[36m(func pid=2535)[0m f1_per_class: [0.61, 0.322, 0.667, 0.532, 0.12, 0.169, 0.416, 0.284, 0.145, 0.211]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3050373134328358
[2m[36m(func pid=5446)[0m top5: 0.8027052238805971
[2m[36m(func pid=5446)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=5446)[0m f1_macro: 0.29899502679118795
[2m[36m(func pid=5446)[0m f1_weighted: 0.28897314048987555
[2m[36m(func pid=5446)[0m f1_per_class: [0.462, 0.527, 0.431, 0.331, 0.061, 0.11, 0.167, 0.362, 0.179, 0.36]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2231 | Steps: 4 | Val loss: 3.1574 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4533 | Steps: 4 | Val loss: 1.7316 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.0707 | Steps: 4 | Val loss: 46.4759 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 11:54:37 (running for 00:46:55.09)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.538 |      0.313 |                   68 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.223 |      0.359 |                   67 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.764 |      0.299 |                   55 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.376865671641791
[2m[36m(func pid=1757)[0m top5: 0.8819962686567164
[2m[36m(func pid=1757)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=1757)[0m f1_macro: 0.3302222219691658
[2m[36m(func pid=1757)[0m f1_weighted: 0.4092346185471633
[2m[36m(func pid=1757)[0m f1_per_class: [0.481, 0.449, 0.276, 0.483, 0.171, 0.262, 0.425, 0.324, 0.178, 0.252]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3474813432835821
[2m[36m(func pid=2535)[0m top5: 0.909981343283582
[2m[36m(func pid=2535)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=2535)[0m f1_macro: 0.3589459242891603
[2m[36m(func pid=2535)[0m f1_weighted: 0.3656408601014786
[2m[36m(func pid=2535)[0m f1_per_class: [0.596, 0.42, 0.579, 0.57, 0.161, 0.186, 0.239, 0.242, 0.183, 0.413]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.18097014925373134
[2m[36m(func pid=5446)[0m top5: 0.7602611940298507
[2m[36m(func pid=5446)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=5446)[0m f1_macro: 0.2317270459126653
[2m[36m(func pid=5446)[0m f1_weighted: 0.18129815204703337
[2m[36m(func pid=5446)[0m f1_per_class: [0.339, 0.259, 0.512, 0.123, 0.084, 0.139, 0.193, 0.168, 0.134, 0.367]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4276 | Steps: 4 | Val loss: 1.6916 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2947 | Steps: 4 | Val loss: 3.0892 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3668 | Steps: 4 | Val loss: 41.9774 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:54:43 (running for 00:47:00.45)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.428 |      0.358 |                   70 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.223 |      0.359 |                   67 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.071 |      0.232 |                   56 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3969216417910448
[2m[36m(func pid=1757)[0m top5: 0.8927238805970149
[2m[36m(func pid=1757)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=1757)[0m f1_macro: 0.3578316441431268
[2m[36m(func pid=1757)[0m f1_weighted: 0.4305418083597677
[2m[36m(func pid=1757)[0m f1_per_class: [0.574, 0.501, 0.293, 0.515, 0.149, 0.223, 0.437, 0.357, 0.165, 0.364]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.3969216417910448
[2m[36m(func pid=2535)[0m top5: 0.9146455223880597
[2m[36m(func pid=2535)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=2535)[0m f1_macro: 0.38508872090840424
[2m[36m(func pid=2535)[0m f1_weighted: 0.37305111748113656
[2m[36m(func pid=2535)[0m f1_per_class: [0.574, 0.41, 0.645, 0.599, 0.197, 0.284, 0.182, 0.359, 0.172, 0.429]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.22527985074626866
[2m[36m(func pid=5446)[0m top5: 0.7313432835820896
[2m[36m(func pid=5446)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=5446)[0m f1_macro: 0.25620047522424916
[2m[36m(func pid=5446)[0m f1_weighted: 0.2117702907039125
[2m[36m(func pid=5446)[0m f1_per_class: [0.539, 0.09, 0.468, 0.13, 0.232, 0.254, 0.334, 0.162, 0.107, 0.246]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.8066 | Steps: 4 | Val loss: 2.7592 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5029 | Steps: 4 | Val loss: 1.6062 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3085 | Steps: 4 | Val loss: 34.2230 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=2535)[0m top1: 0.43050373134328357
[2m[36m(func pid=2535)[0m top5: 0.9398320895522388
[2m[36m(func pid=2535)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=2535)[0m f1_macro: 0.40164987639676325
[2m[36m(func pid=2535)[0m f1_weighted: 0.42586610717530055
[2m[36m(func pid=2535)[0m f1_per_class: [0.577, 0.547, 0.611, 0.592, 0.191, 0.296, 0.287, 0.338, 0.188, 0.389]
== Status ==
Current time: 2024-01-07 11:54:48 (running for 00:47:05.97)
Memory usage on this node: 21.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.428 |      0.358 |                   70 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.807 |      0.402 |                   69 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.367 |      0.256 |                   57 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.43050373134328357
[2m[36m(func pid=1757)[0m top5: 0.9071828358208955
[2m[36m(func pid=1757)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=1757)[0m f1_macro: 0.3811914252425775
[2m[36m(func pid=1757)[0m f1_weighted: 0.45606699507050774
[2m[36m(func pid=1757)[0m f1_per_class: [0.558, 0.52, 0.407, 0.515, 0.149, 0.2, 0.514, 0.355, 0.201, 0.393]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.29757462686567165
[2m[36m(func pid=5446)[0m top5: 0.8134328358208955
[2m[36m(func pid=5446)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=5446)[0m f1_macro: 0.3064170184578248
[2m[36m(func pid=5446)[0m f1_weighted: 0.3139124217893949
[2m[36m(func pid=5446)[0m f1_per_class: [0.588, 0.16, 0.571, 0.237, 0.254, 0.278, 0.508, 0.227, 0.168, 0.073]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.8299 | Steps: 4 | Val loss: 1.5696 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.1107 | Steps: 4 | Val loss: 4.3415 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.6590 | Steps: 4 | Val loss: 32.5636 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=1757)[0m top1: 0.44869402985074625
[2m[36m(func pid=1757)[0m top5: 0.9113805970149254
[2m[36m(func pid=1757)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=1757)[0m f1_macro: 0.3889275998341892
[2m[36m(func pid=1757)[0m f1_weighted: 0.4603946427105688
[2m[36m(func pid=1757)[0m f1_per_class: [0.563, 0.563, 0.436, 0.515, 0.133, 0.182, 0.505, 0.349, 0.256, 0.386]
[2m[36m(func pid=1757)[0m 
== Status ==
Current time: 2024-01-07 11:54:54 (running for 00:47:11.52)
Memory usage on this node: 22.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.83  |      0.389 |                   72 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.807 |      0.402 |                   69 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.309 |      0.306 |                   58 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=2535)[0m top1: 0.28544776119402987

[2m[36m(func pid=2535)[0m top5: 0.8708022388059702
[2m[36m(func pid=2535)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=2535)[0m f1_macro: 0.3235650550725445
[2m[36m(func pid=2535)[0m f1_weighted: 0.2569401793835939
[2m[36m(func pid=2535)[0m f1_per_class: [0.612, 0.429, 0.71, 0.284, 0.159, 0.223, 0.112, 0.326, 0.174, 0.207]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.31902985074626866
[2m[36m(func pid=5446)[0m top5: 0.8731343283582089
[2m[36m(func pid=5446)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=5446)[0m f1_macro: 0.307934895915031
[2m[36m(func pid=5446)[0m f1_weighted: 0.3544338754262196
[2m[36m(func pid=5446)[0m f1_per_class: [0.386, 0.214, 0.714, 0.346, 0.212, 0.181, 0.56, 0.237, 0.144, 0.084]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5899 | Steps: 4 | Val loss: 1.6678 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5707 | Steps: 4 | Val loss: 4.9521 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=1757)[0m top1: 0.41651119402985076
[2m[36m(func pid=1757)[0m top5: 0.898320895522388
[2m[36m(func pid=1757)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=1757)[0m f1_macro: 0.3594117805252626
[2m[36m(func pid=1757)[0m f1_weighted: 0.42809928720982676
[2m[36m(func pid=1757)[0m f1_per_class: [0.452, 0.564, 0.429, 0.516, 0.101, 0.158, 0.415, 0.364, 0.22, 0.375]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 10.0910 | Steps: 4 | Val loss: 24.9340 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 11:54:59 (running for 00:47:17.01)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.59  |      0.359 |                   73 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.111 |      0.324 |                   70 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.659 |      0.308 |                   59 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.25466417910447764
[2m[36m(func pid=2535)[0m top5: 0.8269589552238806
[2m[36m(func pid=2535)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=2535)[0m f1_macro: 0.3202447689050308
[2m[36m(func pid=2535)[0m f1_weighted: 0.22624810465535727
[2m[36m(func pid=2535)[0m f1_per_class: [0.565, 0.433, 0.733, 0.223, 0.146, 0.181, 0.077, 0.343, 0.146, 0.356]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.40951492537313433
[2m[36m(func pid=5446)[0m top5: 0.9113805970149254
[2m[36m(func pid=5446)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=5446)[0m f1_macro: 0.37971553615431375
[2m[36m(func pid=5446)[0m f1_weighted: 0.44046089394923055
[2m[36m(func pid=5446)[0m f1_per_class: [0.554, 0.432, 0.733, 0.545, 0.185, 0.129, 0.536, 0.244, 0.146, 0.293]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5000 | Steps: 4 | Val loss: 1.7266 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.0688 | Steps: 4 | Val loss: 3.9366 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.4077 | Steps: 4 | Val loss: 22.9028 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:55:05 (running for 00:47:22.58)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.357
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00021 | RUNNING    | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.5   |      0.34  |                   74 |
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.571 |      0.32  |                   71 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 10.091 |      0.38  |                   60 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3885261194029851
[2m[36m(func pid=1757)[0m top5: 0.8880597014925373
[2m[36m(func pid=1757)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=1757)[0m f1_macro: 0.3396805227380967
[2m[36m(func pid=1757)[0m f1_weighted: 0.38709166264934025
[2m[36m(func pid=1757)[0m f1_per_class: [0.394, 0.554, 0.464, 0.546, 0.112, 0.122, 0.28, 0.328, 0.202, 0.394]
[2m[36m(func pid=1757)[0m 
[2m[36m(func pid=2535)[0m top1: 0.28451492537313433
[2m[36m(func pid=2535)[0m top5: 0.8339552238805971
[2m[36m(func pid=2535)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=2535)[0m f1_macro: 0.31437166879912365
[2m[36m(func pid=2535)[0m f1_weighted: 0.30840543954458866
[2m[36m(func pid=2535)[0m f1_per_class: [0.552, 0.355, 0.579, 0.477, 0.181, 0.201, 0.159, 0.374, 0.137, 0.128]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.48880597014925375
[2m[36m(func pid=5446)[0m top5: 0.933768656716418
[2m[36m(func pid=5446)[0m f1_micro: 0.48880597014925375
[2m[36m(func pid=5446)[0m f1_macro: 0.3907817779083851
[2m[36m(func pid=5446)[0m f1_weighted: 0.46821365475218596
[2m[36m(func pid=5446)[0m f1_per_class: [0.588, 0.576, 0.593, 0.616, 0.209, 0.118, 0.486, 0.276, 0.027, 0.419]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=1757)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6317 | Steps: 4 | Val loss: 1.8245 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3794 | Steps: 4 | Val loss: 3.8223 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 7.9619 | Steps: 4 | Val loss: 30.3337 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:55:10 (running for 00:47:27.98)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.356
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.069 |      0.314 |                   72 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.408 |      0.391 |                   61 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=1757)[0m top1: 0.3628731343283582
[2m[36m(func pid=1757)[0m top5: 0.8638059701492538
[2m[36m(func pid=1757)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=1757)[0m f1_macro: 0.33009269996661744
[2m[36m(func pid=1757)[0m f1_weighted: 0.355423591510287
[2m[36m(func pid=1757)[0m f1_per_class: [0.345, 0.537, 0.533, 0.543, 0.111, 0.125, 0.187, 0.319, 0.232, 0.368]
[2m[36m(func pid=2535)[0m top1: 0.30597014925373134
[2m[36m(func pid=2535)[0m top5: 0.8512126865671642
[2m[36m(func pid=2535)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=2535)[0m f1_macro: 0.29721757336736665
[2m[36m(func pid=2535)[0m f1_weighted: 0.3300668840486177
[2m[36m(func pid=2535)[0m f1_per_class: [0.473, 0.2, 0.478, 0.531, 0.177, 0.169, 0.283, 0.397, 0.187, 0.078]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3871268656716418
[2m[36m(func pid=5446)[0m top5: 0.8978544776119403
[2m[36m(func pid=5446)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=5446)[0m f1_macro: 0.35911510557590937
[2m[36m(func pid=5446)[0m f1_weighted: 0.3701716319081582
[2m[36m(func pid=5446)[0m f1_per_class: [0.635, 0.616, 0.611, 0.578, 0.157, 0.186, 0.148, 0.26, 0.0, 0.4]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.1091 | Steps: 4 | Val loss: 3.2303 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 7.3670 | Steps: 4 | Val loss: 39.6551 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:55:16 (running for 00:47:33.53)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.356
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00022 | RUNNING    | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.109 |      0.303 |                   74 |
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  7.962 |      0.359 |                   62 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.40111940298507465
[2m[36m(func pid=2535)[0m top5: 0.8577425373134329
[2m[36m(func pid=2535)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=2535)[0m f1_macro: 0.3032741937940694
[2m[36m(func pid=2535)[0m f1_weighted: 0.4017020832645879
[2m[36m(func pid=2535)[0m f1_per_class: [0.419, 0.2, 0.489, 0.558, 0.102, 0.059, 0.555, 0.335, 0.189, 0.127]
[2m[36m(func pid=2535)[0m 
[2m[36m(func pid=5446)[0m top1: 0.3180970149253731
[2m[36m(func pid=5446)[0m top5: 0.7611940298507462
[2m[36m(func pid=5446)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=5446)[0m f1_macro: 0.30951465056086
[2m[36m(func pid=5446)[0m f1_weighted: 0.3195322338652978
[2m[36m(func pid=5446)[0m f1_per_class: [0.596, 0.555, 0.478, 0.52, 0.096, 0.199, 0.057, 0.285, 0.151, 0.158]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=2535)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2815 | Steps: 4 | Val loss: 3.3262 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:55:21 (running for 00:47:39.01)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  7.367 |      0.31  |                   63 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=2535)[0m top1: 0.4137126865671642
[2m[36m(func pid=2535)[0m top5: 0.8596082089552238
[2m[36m(func pid=2535)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=2535)[0m f1_macro: 0.33361001973674076
[2m[36m(func pid=2535)[0m f1_weighted: 0.4223056139203063
[2m[36m(func pid=2535)[0m f1_per_class: [0.441, 0.278, 0.579, 0.554, 0.065, 0.039, 0.587, 0.319, 0.17, 0.304]
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 4.5057 | Steps: 4 | Val loss: 48.7331 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=5446)[0m top1: 0.24440298507462688
[2m[36m(func pid=5446)[0m top5: 0.691231343283582
[2m[36m(func pid=5446)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=5446)[0m f1_macro: 0.25295517443829507
[2m[36m(func pid=5446)[0m f1_weighted: 0.2551900539709428
[2m[36m(func pid=5446)[0m f1_per_class: [0.328, 0.252, 0.647, 0.527, 0.078, 0.171, 0.06, 0.147, 0.195, 0.126]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.4710 | Steps: 4 | Val loss: 50.4124 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 11:55:29 (running for 00:47:46.62)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  4.506 |      0.253 |                   64 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.26725746268656714
[2m[36m(func pid=5446)[0m top5: 0.7154850746268657
[2m[36m(func pid=5446)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=5446)[0m f1_macro: 0.2638223594274297
[2m[36m(func pid=5446)[0m f1_weighted: 0.25417325837681504
[2m[36m(func pid=5446)[0m f1_per_class: [0.193, 0.032, 0.667, 0.578, 0.076, 0.067, 0.153, 0.276, 0.125, 0.471]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 14.4022 | Steps: 4 | Val loss: 42.4484 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:55:34 (running for 00:47:52.24)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.471 |      0.264 |                   65 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.3138992537313433
[2m[36m(func pid=5446)[0m top5: 0.7910447761194029
[2m[36m(func pid=5446)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=5446)[0m f1_macro: 0.2702028129102052
[2m[36m(func pid=5446)[0m f1_weighted: 0.30839799902890386
[2m[36m(func pid=5446)[0m f1_per_class: [0.222, 0.057, 0.621, 0.589, 0.075, 0.023, 0.337, 0.232, 0.126, 0.421]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.8956 | Steps: 4 | Val loss: 26.1033 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 11:55:40 (running for 00:47:58.01)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 14.402 |      0.27  |                   66 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.4001865671641791
[2m[36m(func pid=5446)[0m top5: 0.8936567164179104
[2m[36m(func pid=5446)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=5446)[0m f1_macro: 0.33609057770044737
[2m[36m(func pid=5446)[0m f1_weighted: 0.4312942878679904
[2m[36m(func pid=5446)[0m f1_per_class: [0.468, 0.499, 0.324, 0.498, 0.078, 0.106, 0.532, 0.206, 0.202, 0.449]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 5.8800 | Steps: 4 | Val loss: 31.8292 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 11:55:46 (running for 00:48:03.71)
Memory usage on this node: 16.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.896 |      0.336 |                   67 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.37406716417910446
[2m[36m(func pid=5446)[0m top5: 0.855410447761194
[2m[36m(func pid=5446)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=5446)[0m f1_macro: 0.3149374888350375
[2m[36m(func pid=5446)[0m f1_weighted: 0.3563320755807156
[2m[36m(func pid=5446)[0m f1_per_class: [0.575, 0.521, 0.304, 0.245, 0.101, 0.169, 0.471, 0.233, 0.218, 0.312]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0107 | Steps: 4 | Val loss: 32.4418 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 11:55:52 (running for 00:48:09.63)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.88  |      0.315 |                   68 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.33488805970149255
[2m[36m(func pid=5446)[0m top5: 0.8255597014925373
[2m[36m(func pid=5446)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=5446)[0m f1_macro: 0.26989276473789847
[2m[36m(func pid=5446)[0m f1_weighted: 0.3301818770051162
[2m[36m(func pid=5446)[0m f1_per_class: [0.128, 0.533, 0.31, 0.215, 0.145, 0.182, 0.428, 0.241, 0.211, 0.306]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 5.2679 | Steps: 4 | Val loss: 35.3759 | Batch size: 32 | lr: 0.1 | Duration: 3.44s
== Status ==
Current time: 2024-01-07 11:55:58 (running for 00:48:15.46)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.011 |      0.27  |                   69 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.2971082089552239
[2m[36m(func pid=5446)[0m top5: 0.8059701492537313
[2m[36m(func pid=5446)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=5446)[0m f1_macro: 0.25775015359069725
[2m[36m(func pid=5446)[0m f1_weighted: 0.3231786287226148
[2m[36m(func pid=5446)[0m f1_per_class: [0.128, 0.48, 0.338, 0.258, 0.153, 0.168, 0.407, 0.251, 0.159, 0.236]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 16.1495 | Steps: 4 | Val loss: 38.4979 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 11:56:03 (running for 00:48:21.22)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  5.268 |      0.258 |                   70 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.2621268656716418
[2m[36m(func pid=5446)[0m top5: 0.7728544776119403
[2m[36m(func pid=5446)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=5446)[0m f1_macro: 0.24761731569148693
[2m[36m(func pid=5446)[0m f1_weighted: 0.2938005577789358
[2m[36m(func pid=5446)[0m f1_per_class: [0.386, 0.451, 0.265, 0.261, 0.065, 0.082, 0.342, 0.246, 0.172, 0.207]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1089 | Steps: 4 | Val loss: 44.0281 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 11:56:09 (running for 00:48:26.77)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  | 16.149 |      0.248 |                   71 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.24253731343283583
[2m[36m(func pid=5446)[0m top5: 0.726679104477612
[2m[36m(func pid=5446)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=5446)[0m f1_macro: 0.23109022979726265
[2m[36m(func pid=5446)[0m f1_weighted: 0.25523026514510555
[2m[36m(func pid=5446)[0m f1_per_class: [0.363, 0.439, 0.23, 0.352, 0.057, 0.075, 0.128, 0.269, 0.241, 0.157]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3190 | Steps: 4 | Val loss: 52.0679 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 11:56:14 (running for 00:48:32.31)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  2.109 |      0.231 |                   72 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.208955223880597
[2m[36m(func pid=5446)[0m top5: 0.6548507462686567
[2m[36m(func pid=5446)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=5446)[0m f1_macro: 0.20532918229529873
[2m[36m(func pid=5446)[0m f1_weighted: 0.21803125086199038
[2m[36m(func pid=5446)[0m f1_per_class: [0.122, 0.33, 0.306, 0.38, 0.076, 0.077, 0.051, 0.285, 0.225, 0.201]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 3.2700 | Steps: 4 | Val loss: 44.5809 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 11:56:20 (running for 00:48:38.20)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  1.319 |      0.205 |                   73 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.24673507462686567
[2m[36m(func pid=5446)[0m top5: 0.707089552238806
[2m[36m(func pid=5446)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=5446)[0m f1_macro: 0.22322997788650983
[2m[36m(func pid=5446)[0m f1_weighted: 0.26034975802975335
[2m[36m(func pid=5446)[0m f1_per_class: [0.13, 0.375, 0.274, 0.459, 0.103, 0.093, 0.086, 0.288, 0.233, 0.193]
[2m[36m(func pid=5446)[0m 
[2m[36m(func pid=5446)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9537 | Steps: 4 | Val loss: 33.4527 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 11:56:26 (running for 00:48:43.83)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.355
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00023 | RUNNING    | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  3.27  |      0.223 |                   74 |
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=5446)[0m top1: 0.31576492537313433
[2m[36m(func pid=5446)[0m top5: 0.7840485074626866
[2m[36m(func pid=5446)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=5446)[0m f1_macro: 0.2641261488763695
[2m[36m(func pid=5446)[0m f1_weighted: 0.3310985475236236
[2m[36m(func pid=5446)[0m f1_per_class: [0.259, 0.473, 0.232, 0.527, 0.101, 0.134, 0.179, 0.289, 0.216, 0.231]
== Status ==
Current time: 2024-01-07 11:56:27 (running for 00:48:44.67)
Memory usage on this node: 16.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.354
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_98a10_00000 | TERMINATED | 192.168.7.53:83673  | 0.0001 |       0.99 |         0      |  0.424 |      0.348 |                  100 |
| train_98a10_00001 | TERMINATED | 192.168.7.53:84053  | 0.001  |       0.99 |         0      |  0.709 |      0.249 |                   75 |
| train_98a10_00002 | TERMINATED | 192.168.7.53:84475  | 0.01   |       0.99 |         0      |  3.408 |      0.271 |                  100 |
| train_98a10_00003 | TERMINATED | 192.168.7.53:84902  | 0.1    |       0.99 |         0      | 13.729 |      0.14  |                   75 |
| train_98a10_00004 | TERMINATED | 192.168.7.53:102433 | 0.0001 |       0.9  |         0      |  2.055 |      0.218 |                   75 |
| train_98a10_00005 | TERMINATED | 192.168.7.53:102477 | 0.001  |       0.9  |         0      |  0.385 |      0.351 |                  100 |
| train_98a10_00006 | TERMINATED | 192.168.7.53:108399 | 0.01   |       0.9  |         0      |  1.866 |      0.291 |                  100 |
| train_98a10_00007 | TERMINATED | 192.168.7.53:109465 | 0.1    |       0.9  |         0      |  8.096 |      0.348 |                   75 |
| train_98a10_00008 | TERMINATED | 192.168.7.53:121255 | 0.0001 |       0.99 |         0.0001 |  0.572 |      0.346 |                   75 |
| train_98a10_00009 | TERMINATED | 192.168.7.53:127193 | 0.001  |       0.99 |         0.0001 |  0.05  |      0.3   |                   75 |
| train_98a10_00010 | TERMINATED | 192.168.7.53:127722 | 0.01   |       0.99 |         0.0001 |  1.977 |      0.277 |                  100 |
| train_98a10_00011 | TERMINATED | 192.168.7.53:132824 | 0.1    |       0.99 |         0.0001 | 32.05  |      0.324 |                   75 |
| train_98a10_00012 | TERMINATED | 192.168.7.53:140546 | 0.0001 |       0.9  |         0.0001 |  1.936 |      0.185 |                   75 |
| train_98a10_00013 | TERMINATED | 192.168.7.53:145792 | 0.001  |       0.9  |         0.0001 |  0.398 |      0.343 |                  100 |
| train_98a10_00014 | TERMINATED | 192.168.7.53:150999 | 0.01   |       0.9  |         0.0001 |  0.296 |      0.321 |                  100 |
| train_98a10_00015 | TERMINATED | 192.168.7.53:152562 | 0.1    |       0.9  |         0.0001 | 26.109 |      0.327 |                   75 |
| train_98a10_00016 | TERMINATED | 192.168.7.53:158650 | 0.0001 |       0.99 |         1e-05  |  0.811 |      0.352 |                  100 |
| train_98a10_00017 | TERMINATED | 192.168.7.53:170962 | 0.001  |       0.99 |         1e-05  |  0.082 |      0.337 |                   75 |
| train_98a10_00018 | TERMINATED | 192.168.7.53:171528 | 0.01   |       0.99 |         1e-05  |  1.001 |      0.297 |                   75 |
| train_98a10_00019 | TERMINATED | 192.168.7.53:174857 | 0.1    |       0.99 |         1e-05  | 30.951 |      0.275 |                   75 |
| train_98a10_00020 | TERMINATED | 192.168.7.53:183693 | 0.0001 |       0.9  |         1e-05  |  1.977 |      0.204 |                   75 |
| train_98a10_00021 | TERMINATED | 192.168.7.53:1757   | 0.001  |       0.9  |         1e-05  |  0.632 |      0.33  |                   75 |
| train_98a10_00022 | TERMINATED | 192.168.7.53:2535   | 0.01   |       0.9  |         1e-05  |  0.281 |      0.334 |                   75 |
| train_98a10_00023 | TERMINATED | 192.168.7.53:5446   | 0.1    |       0.9  |         1e-05  |  0.954 |      0.264 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 11:56:27,241	INFO tune.py:798 -- Total run time: 2925.68 seconds (2924.65 seconds for the tuning loop).
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341346.1 ON aap04 CANCELLED AT 2024-01-07T11:56:35 ***
srun: error: aap04: task 0: Exited with exit code 1
